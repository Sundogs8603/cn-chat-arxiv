<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>Lode Encoder&#26159;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#24182;&#32467;&#21512;&#29992;&#25143;&#35774;&#35745;&#65292;&#29983;&#25104;&#26356;&#21152;&#31526;&#21512;&#35774;&#35745;&#39118;&#26684;&#30340;&#20851;&#21345;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01312</link><description>&lt;p&gt;
Lode Encoder: AI&#32422;&#26463;&#19979;&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lode Encoder: AI-constrained co-creativity. (arXiv:2308.01312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01312
&lt;/p&gt;
&lt;p&gt;
Lode Encoder&#26159;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20849;&#21019;&#24615;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#24182;&#32467;&#21512;&#29992;&#25143;&#35774;&#35745;&#65292;&#29983;&#25104;&#26356;&#21152;&#31526;&#21512;&#35774;&#35745;&#39118;&#26684;&#30340;&#20851;&#21345;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Lode Encoder&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#32463;&#20856;&#24179;&#21488;&#30410;&#26234;&#28216;&#25103;Lode Runner&#20026;&#22522;&#30784;&#30340;&#21019;&#24847;&#28216;&#25103;&#20851;&#21345;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;Lode Runner&#20851;&#21345;&#36827;&#34892;&#35757;&#32451;&#26469;&#29983;&#25104;&#19982;&#29992;&#25143;&#35774;&#35745;&#39118;&#26684;&#26356;&#30456;&#20284;&#30340;&#29256;&#26412;&#12290;Lode Encoder&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#24314;&#35758;&#26469;&#26500;&#24314;&#21644;&#32534;&#36753;&#20851;&#21345;&#12290;&#20026;&#20102;&#40723;&#21169;&#35774;&#35745;&#24072;&#25506;&#32034;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#35813;&#31995;&#32479;&#19981;&#21253;&#21547;&#20256;&#32479;&#30340;&#32534;&#36753;&#24037;&#20855;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#31995;&#32479;&#35774;&#35745;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#21450;&#31995;&#32479;&#28436;&#36827;&#21644;&#29992;&#25143;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Lode Encoder, a gamified mixed-initiative level creation system for the classic platform-puzzle game Lode Runner. The system is built around several autoencoders which are trained on sets of Lode Runner levels. When fed with the user's design, each autoencoder produces a version of that design which is closer in style to the levels that it was trained on. The Lode Encoder interface allows the user to build and edit levels through 'painting' from the suggestions provided by the autoencoders. Crucially, in order to encourage designers to explore new possibilities, the system does not include more traditional editing tools. We report on the system design and training procedure, as well as on the evolution of the system itself and user tests.
&lt;/p&gt;</description></item><item><title>Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01285</link><description>&lt;p&gt;
Flows: &#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Flows: Building Blocks of Reasoning and Collaborating AI. (arXiv:2308.01285v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01285
&lt;/p&gt;
&lt;p&gt;
Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#31995;&#32479;&#12290;&#36825;&#20026;&#32467;&#26500;&#21270;&#25512;&#29702;&#20197;&#21450;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#20316;&#21019;&#36896;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#36935;&#12290;&#20026;&#20102;&#20805;&#20998;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#65292;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#30740;&#31350;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27969;&#31243;&#30340;&#27010;&#24565;&#26694;&#26550;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#24314;&#27169;&#22797;&#26434;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#27969;&#31243;&#26159;&#35745;&#31639;&#30340;&#33258;&#21253;&#21547;&#26500;&#24314;&#27169;&#22359;&#65292;&#20855;&#26377;&#29420;&#31435;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#22522;&#20110;&#28040;&#24687;&#30340;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;&#27969;&#31243;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20219;&#24847;&#23884;&#22871;&#30340;&#20132;&#20114;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#20219;&#20309;&#20132;&#20114;&#37117;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#21253;&#25324;&#20043;&#21069;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;-&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;-&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#12289;&#25552;&#31034;&#24037;&#31243;&#26041;&#26696;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#31243;&#22312;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task 
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.01284</link><description>&lt;p&gt;
&#29992;&#28779;&#25915;&#28779;&#65306;ChatGPT&#33021;&#22815;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01284
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#20363;&#65292;&#21253;&#25324;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#38024;&#23545;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#21463;&#21040;&#23558;ChatGPT&#29992;&#20316;&#25968;&#25454;&#26631;&#27880;&#22120;&#25110;&#27880;&#37322;&#22120;&#30340;&#30740;&#31350;&#21551;&#21457;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#25110;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#26159;&#21542;&#20855;&#26377;&#23545;&#31216;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#20851;&#27880;&#38382;&#39064;&#30340;&#29305;&#23450;&#26041;&#38754;&#24182;&#20174;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#25512;&#23548;&#20986;&#20854;&#20313;&#37096;&#20998;&#65292;&#22914;&#20309;&#21033;&#29992;ChatGPT&#21644;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312; \url{https://github.com/AmritaBh/ChatGPT-as-Detector} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.
&lt;/p&gt;</description></item><item><title>BRNES&#26159;&#19968;&#20010;&#26032;&#30340;MARL&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#37051;&#23621;&#21306;&#22495;&#21644;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#38450;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01274</link><description>&lt;p&gt;
BRNES: &#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#38544;&#31169;&#24863;&#30693;&#30340;&#32463;&#39564;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems. (arXiv:2308.01274v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01274
&lt;/p&gt;
&lt;p&gt;
BRNES&#26159;&#19968;&#20010;&#26032;&#30340;MARL&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#37051;&#23621;&#21306;&#22495;&#21644;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#38450;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#38544;&#31169;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32463;&#39564;&#20849;&#20139;&#65288;ES&#65289;&#22312;&#39038;&#38382;-&#21463;&#21149;&#21578;&#26694;&#26550;&#20013;&#21152;&#36895;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20294;&#23581;&#35797;&#23558;ES&#24212;&#29992;&#20110;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36804;&#20170;&#20026;&#27490;&#20173;&#20381;&#36182;&#20110;&#20540;&#24471;&#20449;&#36182;&#30340;&#29615;&#22659;&#65292;&#24182;&#24573;&#30053;&#20102;&#23545;&#25239;&#24615;&#25805;&#32437;&#21644;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19968;&#20123;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#20882;&#20805;&#39038;&#38382;&#21521;&#21463;&#21149;&#21578;&#25552;&#20379;&#34394;&#20551;&#24314;&#35758;&#65292;&#24182;&#20005;&#37325;&#38477;&#20302;&#25972;&#20307;&#23398;&#20064;&#24615;&#33021;&#12290;&#32780;&#19988;&#65292;&#19968;&#20010;&#25512;&#29702;&#25915;&#20987;&#32773;&#20882;&#20805;&#21463;&#21149;&#21578;&#32773;&#21487;&#33021;&#36827;&#34892;&#22810;&#27425;&#26597;&#35810;&#20197;&#25512;&#26029;&#39038;&#38382;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#20351;&#25972;&#20010;ES&#36807;&#31243;&#22312;&#38544;&#31169;&#27844;&#28431;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#21644;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MARL&#26694;&#26550;&#65288;BRNES&#65289;&#65292;&#23427;&#22312;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#20013;&#21551;&#21457;&#24335;&#22320;&#20026;&#27599;&#20010;&#21463;&#21149;&#21578;&#32773;&#36873;&#25321;&#19968;&#20010;&#21160;&#24577;&#37051;&#23621;&#21306;&#22495;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#32463;&#39564;&#32858;&#21512;&#25216;&#26415;&#26469;&#20943;&#23569;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#20195;&#29702;&#30340;&#31169;&#20154;&#20449;&#24687;&#30340;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Although experience sharing (ES) accelerates multiagent reinforcement learning (MARL) in an advisor-advisee framework, attempts to apply ES to decentralized multiagent systems have so far relied on trusted environments and overlooked the possibility of adversarial manipulation and inference. Nevertheless, in a real-world setting, some Byzantine attackers, disguised as advisors, may provide false advice to the advisee and catastrophically degrade the overall learning performance. Also, an inference attacker, disguised as an advisee, may conduct several queries to infer the advisors' private information and make the entire ES process questionable in terms of privacy leakage. To address and tackle these issues, we propose a novel MARL framework (BRNES) that heuristically selects a dynamic neighbor zone for each advisee at each learning step and adopts a weighted experience aggregation technique to reduce Byzantine attack impact. Furthermore, to keep the agent's private information safe fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01271</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#27010;&#29575;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23454;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC. (arXiv:2308.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;MCMC&#30340;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#38543;&#26426;&#26799;&#24230;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#27931;&#65288;cSGHMC&#65289;&#30340;&#23454;&#29992;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#35774;&#23450;&#20808;&#39564;&#65292;&#24182;&#20351;&#29992;cSGHMC&#36817;&#20284;&#34920;&#31034;&#22810;&#32500;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#25506;&#32034;&#20016;&#23500;&#30340;&#23884;&#20837;&#21518;&#39564;&#20998;&#24067;&#65292;&#36125;&#21494;&#26031;&#33258;&#30417;&#30563;&#23398;&#20064;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#36793;&#32536;&#21270;&#36825;&#20123;&#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#26657;&#20934;&#24615;&#21644;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;SVHN&#21644;CIFAR-10&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01264</link><description>&lt;p&gt;
&#25506;&#32034;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20316;&#39640;&#24230;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#23545;&#27861;&#24459;&#21644;&#36947;&#24503;&#38382;&#39064;&#20316;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33258;&#36523;&#20869;&#37096;&#24037;&#20316;&#30340;&#25351;&#23548;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#21019;&#24314;&#24037;&#31243;&#22242;&#38431;&#20063;&#26080;&#27861;&#35299;&#37322;&#23427;&#20204;&#22914;&#20309;&#33719;&#24471;&#24403;&#21069;&#25152;&#26377;&#33021;&#21147;&#30340;&#20855;&#20307;&#36807;&#31243;&#12290;&#26426;&#22120;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#25317;&#26377;&#30340;&#36807;&#31243;&#21644;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT-4&#19982;&#20154;&#31867;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22238;&#31572;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI response
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01240</link><description>&lt;p&gt;
&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#35780;&#20272;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation. (arXiv:2308.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#23567;&#22411;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#30340;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#26356;&#22909;&#12290;&#20854;&#27425;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#22823;&#22823;&#24110;&#21161;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#31034;&#33539;&#26679;&#20363;&#26377;&#26102;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#29978;&#33267;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;BM25&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#22312;&#29983;&#25104;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#30784;&#30340;&#38543;&#26426;&#36873;&#25321;&#25110;&#22266;&#23450;&#36873;&#25321;&#12290;&#31532;&#19977;&#65292;&#23545;&#20110;&#24494;&#35843;&#35774;&#32622;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstrea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01223</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#24605;&#32771;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#27979;&#35797;&#26159;&#25552;&#39640;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#23545;&#32763;&#35793;&#21518;&#30340;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#36890;&#24120;&#26159;&#22312;&#22823;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#30475;&#19981;&#21040;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#32763;&#35793;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#26469;&#20811;&#26381;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#23545;5&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#25105;&#32763;&#35793;&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#25512;&#29702;&#65292;&#35777;&#26126;&#20102;&#24403;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;&#20854;&#22810;&#35821;&#35328;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/juletx/self-translate &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#26469;&#30740;&#31350;&#21307;&#30103;&#39046;&#22495;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#26631;&#27880;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#28857;&#21644;&#30495;&#27491;&#30340;&#32570;&#38519;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.01220</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;ScrutinAI&#36827;&#34892;DNN&#24615;&#33021;&#30340;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case. (arXiv:2308.01220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01220
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#26469;&#30740;&#31350;&#21307;&#30103;&#39046;&#22495;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#26631;&#27880;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#28857;&#21644;&#30495;&#27491;&#30340;&#32570;&#38519;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;ScrutinAI&#25903;&#25345;&#20154;&#24037;&#20998;&#26512;&#24072;&#20132;&#20114;&#24335;&#22320;&#30740;&#31350;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#27880;&#36136;&#37327;&#12290;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#29615;&#22659;&#20013;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#38656;&#35201;&#28145;&#20837;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38750;&#24120;&#26114;&#36149;&#12290;&#36890;&#24120;&#65292;&#25968;&#25454;&#38598;&#36890;&#36807;&#25910;&#38598;&#19987;&#23478;&#32676;&#20307;&#30340;&#24847;&#35265;&#36827;&#34892;&#26631;&#27880;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#26469;&#20998;&#26512;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#26631;&#27880;&#21464;&#24322;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;ScrutinAI&#26377;&#21161;&#20110;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#23558;&#30001;&#26631;&#27880;&#36136;&#37327;&#19981;&#21516;&#25110;&#32570;&#22833;&#24341;&#36215;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#32570;&#28857;&#19982;&#30495;&#27491;&#30340;&#32570;&#38519;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#35814;&#32454;&#26816;&#26597;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#39045;&#20869;&#20986;&#34880;&#30340;&#25972;&#20307;&#26816;&#27979;&#21644;&#20122;&#22411;&#20043;&#38388;&#26356;&#32454;&#24494;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.
&lt;/p&gt;</description></item><item><title>BiERL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01207</link><description>&lt;p&gt;
BiERL: &#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01207
&lt;/p&gt;
&lt;p&gt;
BiERL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#65292;&#20294;&#26159;&#22312;&#19981;&#20180;&#32454;&#35843;&#25972;&#36229;&#21442;&#25968;&#65288;&#21363;&#20803;&#21442;&#25968;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#20250;&#38754;&#20020;&#19981;&#36275;&#30340;&#25506;&#32034;&#25110;&#27169;&#22411;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#65288;BiERL&#65289;&#26469;&#21516;&#26102;&#26356;&#26032;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#20813;&#21435;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#38656;&#35201;&#20808;&#26377;&#39046;&#22495;&#30693;&#35782;&#25110;&#26114;&#36149;&#20248;&#21270;&#36807;&#31243;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#20803;&#32423;&#26550;&#26500;&#65292;&#23558;&#20869;&#37096;&#32423;&#21035;&#30340;&#36827;&#21270;&#32463;&#39564;&#23884;&#20837;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#31181;&#32676;&#34920;&#31034;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#21487;&#34892;&#30340;&#20803;&#32423;&#36866;&#24212;&#24230;&#20989;&#25968;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;MuJoCo&#21644;Box2D&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#20316;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;BiERL&#20248;&#20110;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#23458;&#25143;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#39044;&#27979;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#21644;&#21830;&#21697;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#23384;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#28040;&#36153;&#34892;&#20026;&#21644;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#31867;&#21035;&#31890;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.01195</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#8220;&#20877;&#27425;&#36141;&#20080;&#8221;&#25512;&#33616;&#20013;&#30340;&#31867;&#21035;&#39057;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Personalized Category Frequency prediction for Buy It Again recommendations. (arXiv:2308.01195v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#23458;&#25143;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#39044;&#27979;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#21644;&#21830;&#21697;&#12290;&#37319;&#29992;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#23384;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#28040;&#36153;&#34892;&#20026;&#21644;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#35757;&#32451;&#31867;&#21035;&#31890;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20877;&#27425;&#36141;&#20080;&#8221;&#65288;BIA&#65289;&#25512;&#33616;&#23545;&#20110;&#38646;&#21806;&#21830;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#26681;&#25454;&#23458;&#25143;&#33258;&#24049;&#30340;&#37325;&#22797;&#36141;&#20080;&#27169;&#24335;&#25552;&#20379;&#21487;&#33021;&#20877;&#27425;&#36141;&#20080;&#30340;&#21830;&#21697;&#25512;&#33616;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#32593;&#31449;&#21442;&#19982;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;BIA&#30740;&#31350;&#20998;&#26512;&#20102;&#23458;&#25143;&#22312;&#21830;&#21697;&#31890;&#24230;&#19978;&#30340;&#20010;&#24615;&#21270;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31867;&#21035;&#30340;&#27169;&#22411;&#21487;&#33021;&#26356;&#21512;&#36866;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#8221;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#21253;&#25324;&#20102;&#20010;&#24615;&#21270;&#31867;&#21035;&#27169;&#22411;&#65288;PC&#27169;&#22411;&#65289;&#21644;&#31867;&#21035;&#20869;&#20010;&#24615;&#21270;&#21830;&#21697;&#27169;&#22411;&#65288;IC&#27169;&#22411;&#65289;&#12290;PC&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#31867;&#21035;&#21015;&#34920;&#65292;&#26174;&#31034;&#20102;&#23458;&#25143;&#21487;&#33021;&#20877;&#27425;&#36141;&#20080;&#30340;&#31867;&#21035;&#12290;IC&#27169;&#22411;&#22312;&#31867;&#21035;&#20869;&#23545;&#21830;&#21697;&#36827;&#34892;&#25490;&#21517;&#65292;&#26174;&#31034;&#20102;&#23458;&#25143;&#22312;&#31867;&#21035;&#20869;&#21487;&#33021;&#28040;&#36153;&#30340;&#21830;&#21697;&#12290;&#23618;&#27425;&#21270;PCIC&#27169;&#22411;&#20351;&#29992;&#29983;&#23384;&#27169;&#22411;&#25429;&#25417;&#20135;&#21697;&#30340;&#19968;&#33324;&#28040;&#36153;&#29575;&#12290;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#25429;&#25417;&#20102;&#28040;&#36153;&#36235;&#21183;&#12290;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#31867;&#21035;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Mercury&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#29616;&#25104;&#30340;Nvidia DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#21160;&#21270;&#36828;&#31243;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#23427;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#21482;&#38024;&#23545;&#31616;&#21270;&#30340;&#21152;&#36895;&#22120;&#23454;&#29616;&#21644;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#20998;&#26512;&#21644;&#39046;&#22495;&#30693;&#35782;&#31561;&#12290;</title><link>http://arxiv.org/abs/2308.01193</link><description>&lt;p&gt;
Mercury:&#19968;&#31181;&#29992;&#20110;Nvidia&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#33258;&#21160;&#36828;&#31243;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mercury: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator. (arXiv:2308.01193v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Mercury&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#29616;&#25104;&#30340;Nvidia DNN&#21152;&#36895;&#22120;&#30340;&#33258;&#21160;&#21270;&#36828;&#31243;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#23427;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#21482;&#38024;&#23545;&#31616;&#21270;&#30340;&#21152;&#36895;&#22120;&#23454;&#29616;&#21644;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#20998;&#26512;&#21644;&#39046;&#22495;&#30693;&#35782;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21152;&#36895;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#35768;&#22810;&#22330;&#26223;&#65292;&#20197;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#24182;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21152;&#36895;&#22120;&#30340;&#19968;&#20010;&#37325;&#22823;&#20851;&#27880;&#28857;&#26159;&#24050;&#37096;&#32626;&#27169;&#22411;&#30340;&#20445;&#23494;&#24615;&#65306;&#22312;&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#25191;&#34892;&#21487;&#33021;&#20250;&#27844;&#38706;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#20351;&#24471;&#23545;&#25163;&#33021;&#22815;&#31934;&#30830;&#22320;&#24674;&#22797;&#27169;&#22411;&#35814;&#24773;&#12290;&#36825;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#19981;&#20165;&#20250;&#21361;&#21450;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#36824;&#20250;&#20419;&#20351;&#26576;&#20123;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks.  Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents Mercury, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#31264;&#23494;&#26631;&#31614;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#22343;Dice&#20998;&#25968;&#32771;&#34385;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#21098;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#35299;&#20915;&#25968;&#25454;&#38598;&#35268;&#27169;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01189</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#39278;&#39135;&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26377;&#25928;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Diet: Effective Multi-center Dataset Pruning for Medical Image Segmentation. (arXiv:2308.01189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#31264;&#23494;&#26631;&#31614;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#24179;&#22343;Dice&#20998;&#25968;&#32771;&#34385;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#21098;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#35299;&#20915;&#25968;&#25454;&#38598;&#35268;&#27169;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#31264;&#23494;&#26631;&#31614;&#38382;&#39064;&#65292;&#22312;&#19981;&#29306;&#29298;&#22826;&#22810;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20462;&#21098;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#26631;&#20934;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22522;&#20934;&#19978;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#20010;&#21035;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#24230;&#37327;&#26080;&#27861;&#35782;&#21035;&#20986;&#37325;&#35201;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#24179;&#22343;Dice&#20998;&#25968;&#26469;&#32771;&#34385;&#30446;&#26631;&#21306;&#22495;&#19978;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#20013;&#39318;&#27425;&#32771;&#34385;&#23494;&#38598;&#26631;&#31614;&#20219;&#21153;&#20013;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#30740;&#31350;&#32773;&#65292;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;(1)&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#30740;&#31350;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;(2)&#30830;&#23450;&#20102;&#22312;&#23494;&#38598;&#26631;&#31614;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20316;&#20026;&#36873;&#25321;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#37325;&#35201;&#26679;&#26412;&#30340;&#24378;&#22823;&#32780;&#31616;&#21333;&#30340;&#22522;&#20934;&#65292;&#32467;&#21512;&#25968;&#25454;&#26469;&#28304;&#36827;&#34892;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to address the dense labeling problems where a significant fraction of the dataset can be pruned without sacrificing much accuracy. We observe that, on standard medical image segmentation benchmarks, the loss gradient norm-based metrics of individual training examples applied in image classification fail to identify the important samples. To address this issue, we propose a data pruning method by taking into consideration the training dynamics on target regions using Dynamic Average Dice (DAD) score. To the best of our knowledge, we are among the first to address the data importance in dense labeling tasks in the field of medical image analysis, making the following contributions: (1) investigating the underlying causes with rigorous empirical analysis, and (2) determining effective data pruning approach in dense labeling problems. Our solution can be used as a strong yet simple baseline to select important examples for medical image segmentation with combined data sou
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;</title><link>http://arxiv.org/abs/2308.01157</link><description>&lt;p&gt;
LLMs&#29702;&#35299;&#29627;&#29827;&#30418;&#27169;&#22411;&#65292;&#21457;&#29616;&#24778;&#21916;&#24182;&#25552;&#20986;&#20462;&#22797;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01157
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23558;&#22797;&#26434;&#32467;&#26524;&#20998;&#35299;&#20026;&#21333;&#19968;&#21464;&#37327;&#30340;&#22270;&#34920;&#31034;&#32452;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25972;&#20010;&#27169;&#22411;&#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#24212;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#33258;&#21160;&#23436;&#25104;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#26816;&#27979;&#19982;&#20808;&#21069;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24322;&#24120;&#65292;&#25551;&#36848;&#24322;&#24120;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#21435;&#38500;&#24322;&#24120;&#30340;&#20462;&#22797;&#24314;&#35758;&#12290;&#25105;&#20204;&#20351;&#29992;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#22810;&#20010;&#31034;&#20363;&#26469;&#35777;&#26126;LLMs&#30340;&#36825;&#20123;&#26032;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;$\texttt{TalkToEBM}$&#21253;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#36827;&#34892;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01138</link><description>&lt;p&gt;
&#33021;&#21542;&#36716;&#31227;&#22122;&#22768;&#27169;&#24335;&#65311;&#20351;&#29992;&#29983;&#25104;&#26696;&#20363;&#30340;&#22810;&#29615;&#22659;&#39057;&#35889;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#27700;&#36136;&#26816;&#27979;&#20013;&#65292;&#39057;&#35889;&#20998;&#26512;&#31995;&#32479;&#26088;&#22312;&#26816;&#27979;&#27745;&#26579;&#29289;&#30340;&#31867;&#22411;&#21644;&#27987;&#24230;&#65292;&#24182;&#20351;&#30417;&#31649;&#26426;&#26500;&#33021;&#22815;&#21450;&#26102;&#22238;&#24212;&#27745;&#26579;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#39057;&#35889;&#25968;&#25454;&#30340;&#27979;&#35797;&#35774;&#22791;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#20250;&#21463;&#21040;&#22797;&#26434;&#30340;&#22122;&#22768;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20998;&#26512;&#27169;&#22411;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19981;&#21516;&#29615;&#22659;&#20013;&#26631;&#20934;&#27700;&#26679;&#21697;&#30340;&#39057;&#35889;&#20316;&#20026;&#26696;&#20363;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#22122;&#22768;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20351;&#22122;&#22768;&#27169;&#24335;&#33021;&#22815;&#24212;&#29992;&#20110;&#26410;&#30693;&#26679;&#21697;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24517;&#28982;&#23384;&#22312;&#30340;&#26679;&#26412;&#32423;&#22522;&#32447;&#22122;&#22768;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#33719;&#21462;&#21482;&#22312;&#25968;&#25454;&#38598;&#32423;&#29615;&#22659;&#22122;&#22768;&#19978;&#26377;&#24046;&#24322;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26679;&#26412;&#23545;&#26679;&#26412;&#30340;&#26696;&#20363;&#24211;&#65292;&#25490;&#38500;&#20102;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21046;&#36896;&#19994;&#28938;&#25509;&#36136;&#37327;&#30417;&#27979;&#20013;&#24212;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#21487;&#33021;&#24615;&#21644;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#28938;&#25509;&#26001;&#28857;&#30452;&#24452;&#21644;&#25152;&#23646;&#36710;&#36523;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01105</link><description>&lt;p&gt;
&#38754;&#21521;&#28938;&#25509;&#36136;&#37327;&#30417;&#27979;&#30340;&#23383;&#38754;&#24863;&#30693;&#30693;&#35782;&#22270;&#23884;&#20837;&#65306;Bosch&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Literal-Aware Knowledge Graph Embedding for Welding Quality Monitoring: A Bosch Case. (arXiv:2308.01105v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21046;&#36896;&#19994;&#28938;&#25509;&#36136;&#37327;&#30417;&#27979;&#20013;&#24212;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#21487;&#33021;&#24615;&#21644;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#28938;&#25509;&#26001;&#28857;&#30452;&#24452;&#21644;&#25152;&#23646;&#36710;&#36523;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#30693;&#35782;&#22270;&#23884;&#20837;(KGE)&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24050;&#32463;&#20986;&#29616;&#65292;&#20854;&#35797;&#22270;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;(ML)&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20316;&#20026;&#25968;&#23383;&#21521;&#37327;&#21644;&#25968;&#23398;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#24212;&#29992;KGE&#35299;&#20915;&#24037;&#19994;&#38382;&#39064;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;KGE&#22312;&#21046;&#36896;&#19994;&#28938;&#25509;&#36136;&#37327;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#21487;&#33021;&#24615;&#21644;&#31243;&#24230;&#12290;&#28938;&#25509;&#36136;&#37327;&#30417;&#27979;&#26159;&#21046;&#36896;&#19994;&#20013;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#65292;&#27599;&#24180;&#37117;&#20250;&#29983;&#20135;&#25968;&#30334;&#19975;&#36742;&#27773;&#36710;&#12290;&#26412;&#30740;&#31350;&#19982;&#23453;&#39532;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#30740;&#31350;&#30456;&#31526;&#65292;&#26088;&#22312;&#26367;&#20195;&#20256;&#32479;&#30340;&#25286;&#35299;&#27773;&#36710;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#24335;&#36153;&#29992;&#39640;&#26114;&#19988;&#20135;&#29983;&#24223;&#29289;&#12290;&#26412;&#25991;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#28938;&#25509;&#26001;&#28857;&#30452;&#24452;&#26377;&#22810;&#22823;&#20197;&#21450;&#28938;&#25509;&#26001;&#28857;&#23646;&#20110;&#21738;&#20010;&#27773;&#36710;&#36710;&#36523;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#22823;&#37327;&#30340;&#27773;&#36710;&#36710;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a series of studies in knowledge graph embedding (KGE), which attempts to learn the embeddings of the entities and relations as numerical vectors and mathematical mappings via machine learning (ML). However, there has been limited research that applies KGE for industrial problems in manufacturing. This paper investigates whether and to what extent KGE can be used for an important problem: quality monitoring for welding in manufacturing industry, which is an impactful process accounting for production of millions of cars annually. The work is in line with Bosch research of data-driven solutions that intends to replace the traditional way of destroying cars, which is extremely costly and produces waste. The paper tackles two very challenging questions simultaneously: how large the welding spot diameter is; and to which car body the welded spot belongs to. The problem setting is difficult for traditional ML because there exist a high number of car bodies that shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.01098</link><description>&lt;p&gt;
&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21033;&#29992;&#22810;&#19987;&#23478;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#26356;&#22909;&#30340;&#26597;&#35810;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search. (arXiv:2308.01098v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#20998;&#31867;&#20316;&#20026;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#24120;&#20351;&#29992;&#27973;&#23618;&#27169;&#22411;&#65288;&#22914;FastText&#65289;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;FastText&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#19981;&#36275;&#65292;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#20302;&#39057;&#26597;&#35810;&#21644;&#23614;&#37096;&#31867;&#21035;&#19978;&#12290;&#20351;&#29992;&#26356;&#28145;&#20837;&#19988;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#22312;&#32447;&#25512;&#26029;&#24310;&#36831;&#21644;&#26356;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#25512;&#26029;&#25928;&#29575;&#21644;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#26174;&#28982;&#20855;&#26377;&#37325;&#22823;&#23454;&#38469;&#24847;&#20041;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#33976;&#39311;&#65288;KC&#65289;&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#19968;&#20010;&#31163;&#32447;&#27169;&#22411;&#65292;&#36890;&#36807;&#33976;&#39311;&#30693;&#35782;&#26469;&#25913;&#21892;&#22312;&#32447;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SemCloud&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#22686;&#24378;&#20113;&#31995;&#32479;&#65292;&#20197;&#24212;&#23545;&#24037;&#19994;4.0&#21644;&#29289;&#32852;&#32593;&#26102;&#20195;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#26412;&#20307;&#21644;&#26144;&#23556;&#26469;&#36827;&#34892;&#25968;&#25454;&#38598;&#25104;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#33410;&#28857;&#19978;&#24182;&#34892;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;SemCloud&#36824;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#31639;&#27861;&#26469;&#38477;&#20302;&#29992;&#25143;&#35757;&#32451;&#26102;&#38388;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.01094</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#21644;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#25968;&#25454;&#31185;&#23398;&#35299;&#20915;&#26041;&#26696;&#65306;&#21338;&#19990;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Science Solutions with Semantics and Machine Learning: Bosch Case. (arXiv:2308.01094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SemCloud&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#22686;&#24378;&#20113;&#31995;&#32479;&#65292;&#20197;&#24212;&#23545;&#24037;&#19994;4.0&#21644;&#29289;&#32852;&#32593;&#26102;&#20195;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#26412;&#20307;&#21644;&#26144;&#23556;&#26469;&#36827;&#34892;&#25968;&#25454;&#38598;&#25104;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#33410;&#28857;&#19978;&#24182;&#34892;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;SemCloud&#36824;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#31639;&#27861;&#26469;&#38477;&#20302;&#29992;&#25143;&#35757;&#32451;&#26102;&#38388;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#37322;&#25918;&#20102;&#22823;&#37327;&#30340;&#24037;&#21378;&#29983;&#20135;&#25968;&#25454;&#65292;&#23548;&#33268;&#20102;&#22823;&#25968;&#25454;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20113;&#31995;&#32479;&#31561;&#20998;&#24067;&#24335;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#34987;&#29992;&#26469;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24182;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#38543;&#30528;&#20113;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#26356;&#22810;&#21407;&#26412;&#19981;&#26159;&#20113;&#19987;&#23478;&#30340;&#29992;&#25143;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#12289;&#39046;&#22495;&#19987;&#23478;&#65289;&#38656;&#35201;&#22312;&#20113;&#31995;&#32479;&#19978;&#37096;&#32626;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#22914;&#20309;&#28385;&#36275;&#20113;&#31995;&#32479;&#29992;&#25143;&#30340;&#39640;&#38656;&#27714;&#20197;&#21450;&#35757;&#32451;&#20182;&#20204;&#25152;&#38656;&#30340;&#36807;&#38271;&#26102;&#38388;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SemCloud&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#22686;&#24378;&#20113;&#31995;&#32479;&#12290;SemCloud&#20381;&#36182;&#20110;&#39046;&#22495;&#26412;&#20307;&#21644;&#26144;&#23556;&#36827;&#34892;&#25968;&#25454;&#38598;&#25104;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#33410;&#28857;&#19978;&#24182;&#34892;&#36827;&#34892;&#35821;&#20041;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;SemCloud&#36824;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#31639;&#27861;&#26469;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry 4.0 and Internet of Things (IoT) technologies unlock unprecedented amount of data from factory production, posing big data challenges in volume and variety. In that context, distributed computing solutions such as cloud systems are leveraged to parallelise the data processing and reduce computation time. As the cloud systems become increasingly popular, there is increased demand that more users that were originally not cloud experts (such as data scientists, domain experts) deploy their solutions on the cloud systems. However, it is non-trivial to address both the high demand for cloud system users and the excessive time required to train them. To this end, we propose SemCloud, a semantics-enhanced cloud system, that couples cloud system with semantic technologies and machine learning. SemCloud relies on domain ontologies and mappings for data integration, and parallelises the semantic data integration and data analysis on distributed computing nodes. Furthermore, SemCloud ado
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39564;&#35777;&#20102;Google MediaPipe Hand&#65288;GMH&#65289;&#21644;&#22686;&#24378;&#29256;&#26412;GMH-D&#26694;&#26550;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25163;&#37096;&#36861;&#36394;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;RGB-Depth&#30456;&#26426;&#30340;&#28145;&#24230;&#20272;&#35745;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#36816;&#21160;&#36861;&#36394;&#12290;</title><link>http://arxiv.org/abs/2308.01088</link><description>&lt;p&gt;
&#20020;&#24202;&#24212;&#29992;&#30340;&#25163;&#37096;&#36861;&#36394;&#65306;&#39564;&#35777;Google MediaPipe Hand&#65288;GMH&#65289;&#21644;&#22686;&#24378;&#22411;GMH-D&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hand tracking for clinical applications: validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks. (arXiv:2308.01088v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39564;&#35777;&#20102;Google MediaPipe Hand&#65288;GMH&#65289;&#21644;&#22686;&#24378;&#29256;&#26412;GMH-D&#26694;&#26550;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25163;&#37096;&#36861;&#36394;&#25928;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;RGB-Depth&#30456;&#26426;&#30340;&#28145;&#24230;&#20272;&#35745;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#36816;&#21160;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#20934;&#30830;&#36861;&#36394;&#25163;&#37096;&#21644;&#25163;&#25351;&#36816;&#21160;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#20154;&#26426;&#20132;&#20114;&#12289;&#34394;&#25311;&#29616;&#23454;&#12289;&#24037;&#19994;&#21644;&#21307;&#23398;&#12290;&#34429;&#28982;&#25163;&#21183;&#35782;&#21035;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#37327;&#21270;&#32454;&#24494;&#30340;&#36816;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#65292;&#25163;&#21151;&#33021;&#38556;&#30861;&#21644;&#24247;&#22797;&#35757;&#32451;&#32467;&#26524;&#30340;&#35780;&#20272;&#38656;&#35201;&#31934;&#30830;&#27979;&#37327;&#12290;&#20986;&#29616;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#31934;&#30830;&#21487;&#38752;&#22320;&#27979;&#37327;&#25163;&#25351;&#36816;&#21160;&#26041;&#38754;&#30340;&#24615;&#33021;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#19982;&#24050;&#24314;&#31435;&#30340;&#40644;&#37329;&#26631;&#20934;&#31995;&#32479;&#30340;&#39564;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#39564;&#35777;&#30001;Google MediaPipe Hand&#65288;GMH&#65289;&#23454;&#29616;&#30340;&#25163;&#37096;&#36861;&#36394;&#26694;&#26550;&#20197;&#21450;&#19968;&#31181;&#21019;&#26032;&#30340;&#22686;&#24378;&#29256;&#26412;GMH-D&#65292;&#22312;RGB-Depth&#30456;&#26426;&#30340;&#28145;&#24230;&#20272;&#35745;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#36816;&#21160;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D tracking of hand and fingers movements poses significant challenges in computer vision. The potential applications span across multiple domains, including human-computer interaction, virtual reality, industry, and medicine. While gesture recognition has achieved remarkable accuracy, quantifying fine movements remains a hurdle, particularly in clinical applications where the assessment of hand dysfunctions and rehabilitation training outcomes necessitate precise measurements. Several novel and lightweight frameworks based on Deep Learning have emerged to address this issue; however, their performance in accurately and reliably measuring fingers movements requires validation against well-established gold standard systems. In this paper, the aim is to validate the handtracking framework implemented by Google MediaPipe Hand (GMH) and an innovative enhanced version, GMH-D, that exploits the depth estimation of an RGB-Depth camera to achieve more accurate tracking of 3D movements
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35268;&#21017;&#20915;&#31574;&#19982;&#36816;&#21160;&#35268;&#21010;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#22797;&#26434;&#20132;&#36890;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#21457;&#23637;&#26426;&#22120;&#20154;&#31354;&#38388;&#24847;&#35782;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01085</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#31354;&#38388;&#26234;&#33021;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Spatial Intelligence of a Self-driving Car and Rule-Based Decision Making. (arXiv:2308.01085v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35268;&#21017;&#20915;&#31574;&#19982;&#36816;&#21160;&#35268;&#21010;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#22797;&#26434;&#20132;&#36890;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#21457;&#23637;&#26426;&#22120;&#20154;&#31354;&#38388;&#24847;&#35782;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#20915;&#31574;&#21046;&#23450;&#19982;&#20256;&#32479;&#30340;&#36816;&#21160;&#35745;&#21010;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#22797;&#26434;&#20132;&#36890;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#36817;&#20284;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#24182;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20915;&#31574;&#35268;&#21017;&#31034;&#20363;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#21457;&#23637;&#26426;&#22120;&#20154;&#31354;&#38388;&#24847;&#35782;&#25216;&#26415;&#26159;&#19968;&#39033;&#20196;&#20154;&#20852;&#22859;&#30340;&#27963;&#21160;&#65292;&#20540;&#24471;&#26356;&#22810;&#30340;&#31354;&#38388;&#25512;&#29702;&#31038;&#21306;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how rule-based decision making can be combined with traditional motion planning techniques to achieve human-like behavior of a self-driving vehicle in complex traffic situations. We give and discuss examples of decision rules in autonomous driving. We draw on these examples to illustrate that developing techniques for spatial awareness of robots is an exciting activity which deserves more attention from spatial reasoning community that it had received so far.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#20013;&#26816;&#27979;&#32452;&#32423;&#21035;&#30340;&#24322;&#24120;&#65292;&#24182;&#21033;&#29992;&#25299;&#25169;&#27169;&#24335;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01063</link><description>&lt;p&gt;
&#22312;&#32452;&#32423;&#21035;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#19968;&#31181;&#22686;&#24378;&#25299;&#25169;&#27169;&#24335;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach. (arXiv:2308.01063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#20013;&#26816;&#27979;&#32452;&#32423;&#21035;&#30340;&#24322;&#24120;&#65292;&#24182;&#21033;&#29992;&#25299;&#25169;&#27169;&#24335;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#37329;&#34701;&#23433;&#20840;&#21644;&#29983;&#29289;&#21270;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#20027;&#35201;&#20851;&#27880;&#21306;&#20998;&#22270;&#20013;&#30340;&#20010;&#20307;&#23454;&#20307;&#65288;&#33410;&#28857;&#25110;&#22270;&#65289;&#65292;&#24573;&#35270;&#20102;&#22270;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#32452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#21517;&#20026;&#32452;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;Gr-GAD&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#31181;&#26469;&#23450;&#20301;&#23646;&#20110;&#21487;&#33021;&#24322;&#24120;&#32452;&#30340;&#38170;&#23450;&#33410;&#28857;&#65292;&#36890;&#36807;&#25429;&#25417;&#38271;&#31243;&#19981;&#19968;&#33268;&#24615;&#12290;&#38543;&#21518;&#65292;&#32452;&#37319;&#26679;&#34987;&#29992;&#26469;&#37319;&#26679;&#20505;&#36873;&#32452;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#25299;&#25169;&#27169;&#24335;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;TPGCL&#65289;&#26041;&#27861;&#20013;&#12290;TPGCL&#21033;&#29992;&#32452;&#30340;&#25299;&#25169;&#27169;&#24335;&#20316;&#20026;&#32447;&#32034;&#20026;&#27599;&#20010;&#20505;&#36873;&#32452;&#29983;&#25104;&#23884;&#20837;&#65292;&#20174;&#32780;&#21306;&#20998;&#19981;&#21516;&#30340;&#24322;&#24120;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) has achieved success and has been widely applied in various domains, such as fraud detection, cybersecurity, finance security, and biochemistry. However, existing graph anomaly detection algorithms focus on distinguishing individual entities (nodes or graphs) and overlook the possibility of anomalous groups within the graph. To address this limitation, this paper introduces a novel unsupervised framework for a new task called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that belong to potential anomaly groups by capturing long-range inconsistencies. Subsequently, group sampling is employed to sample candidate groups, which are then fed into the proposed Topology Pattern-based Graph Contrastive Learning (TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to generate embeddings for each candidate group and thus distinct anomaly groups. The ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#27492;&#20030;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.01044</link><description>&lt;p&gt;
&#36741;&#21161;&#36328;&#35821;&#35328;&#20132;&#27969;&#30340;&#32842;&#22825;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Chat Translation Error Detection for Assisting Cross-lingual Communications. (arXiv:2308.01044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#27492;&#20030;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#65292;&#22240;&#20026;&#24403;&#21069;&#26426;&#22120;&#32842;&#22825;&#32763;&#35793;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;BPesona-chat&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#29992;&#20247;&#21253;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#30340;&#22810;&#36718;&#21475;&#35821;&#32842;&#22825;&#12290;&#38169;&#35823;&#26816;&#27979;&#22120;&#21487;&#20197;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25171;&#19979;&#33391;&#22909;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe the development of a communication support system that detects erroneous translations to facilitate crosslingual communications due to the limitations of current machine chat translation methods. We trained an error detector as the baseline of the system and constructed a new Japanese-English bilingual chat corpus, BPersona-chat, which comprises multiturn colloquial chats augmented with crowdsourced quality ratings. The error detector can serve as an encouraging foundation for more advanced erroneous translation detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#32032;&#26469;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#20197;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#26368;&#21518;&#65292;&#24341;&#20837;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20197;&#21516;&#26102;&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01030</link><description>&lt;p&gt;
&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#30340;&#19977;&#20010;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Three Factors to Improve Out-of-Distribution Detection. (arXiv:2308.01030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#22240;&#32032;&#26469;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#20197;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#26368;&#21518;&#65292;&#24341;&#20837;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20197;&#21516;&#26102;&#25552;&#39640;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32676;&#26816;&#27979;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#20316;&#20026;&#24322;&#24120;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#65288;ACC&#65289;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65288;AUROC&#12289;FPR&#12289;AUPR&#65289;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#65288;i&#65289;&#24341;&#20837;&#33258;&#25105;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#21487;&#20197;&#22686;&#24378;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65307;&#65288;ii&#65289;&#37319;&#26679;&#21322;&#22256;&#38590;&#31163;&#32676;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#22312;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#24341;&#20837;&#25105;&#20204;&#30340;&#26032;&#22411;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#21644;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#20998;&#31867;&#21644;&#31163;&#32676;&#26816;&#27979;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#31163;&#32676;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for training can improve OOD detection performance with minimal impact on accuracy; (iii) The introduction of our novel supervised contrastive learning can simultaneously improve OOD detection performance and the accuracy of the network. By incorporating all three factors, our approach enhances both accuracy and OOD detection performance by addressing the trade-off between classification and OOD detection. Our method achieves improvements over previous approaches in both performance metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01006</link><description>&lt;p&gt;
FusionAD: &#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01006
&lt;/p&gt;
&lt;p&gt;
FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24615;&#33021;&#24050;&#25104;&#20026;&#38081;&#26495;&#19968;&#22359;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#20173;&#28982;&#20960;&#20046;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FusionAD&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#20004;&#20010;&#26368;&#20851;&#38190;&#20256;&#24863;&#22120;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#36229;&#36234;&#24863;&#30693;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;&#22522;&#20110;&#34701;&#21512;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#19982;&#22522;&#20110;&#30456;&#26426;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;UniAD&#30456;&#27604;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#34701;&#21512;&#36741;&#21161;&#30340;&#27169;&#24577;&#24863;&#30693;&#39044;&#27979;&#21644;&#29366;&#24577;&#24863;&#30693;&#35268;&#21010;&#27169;&#22359;&#65292;&#31216;&#20026;FMSPnP&#65292;&#20805;&#20998;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;FusionAD&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#32447;&#24179;&#22343;15%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26080;&#20851;&#27491;&#21017;&#21270;&#22120;WDER&#65292;&#36890;&#36807;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WDER&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00989</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#22810;&#26679;&#24615;&#22686;&#24378;&#27491;&#21017;&#21270;&#22120;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26080;&#20851;&#27491;&#21017;&#21270;&#22120;WDER&#65292;&#36890;&#36807;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WDER&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23558;&#19981;&#21516;&#23618;&#27425;&#30340;&#23376;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#33258;&#21160;&#21457;&#29616;&#23376;&#31574;&#30053;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#29983;&#25104;&#23376;&#31574;&#30053;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#20047;&#23545;&#22810;&#26679;&#24615;&#30340;&#32771;&#34385;&#25110;&#20351;&#29992;&#24369;&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#31216;&#20026;Wasserstein&#22810;&#26679;&#24615;&#22686;&#24378;&#27491;&#21017;&#21270;&#22120;&#65288;WDER&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21160;&#20316;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#26469;&#22686;&#21152;&#23376;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;WDER&#21487;&#20197;&#36731;&#26494;&#22320;&#34701;&#20837;&#21040;&#29616;&#26377;&#26041;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#19981;&#20462;&#25913;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;WDER&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#20102;WDER&#30340;&#36866;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38548;&#31163;&#21644;&#35825;&#23548;&#65288;InI&#65289;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#38548;&#31163;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#20013;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#19981;&#21033;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00958</link><description>&lt;p&gt;
&#38548;&#31163;&#21644;&#35825;&#23548;&#65306;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks. (arXiv:2308.00958v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00958
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38548;&#31163;&#21644;&#35825;&#23548;&#65288;InI&#65289;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#25239;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#38548;&#31163;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#20013;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#19981;&#21033;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#40657;&#30418;&#26597;&#35810;&#36807;&#31243;&#22797;&#21046;&#27169;&#22411;&#21151;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#30446;&#26631;&#21463;&#23475;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#31363;&#21462;&#38450;&#24481;&#26041;&#27861;&#36890;&#36807;&#21521;&#21463;&#23475;&#32773;&#30340;&#21518;&#39564;&#27010;&#29575;&#28155;&#21152;&#27450;&#39575;&#24615;&#25200;&#21160;&#26469;&#35823;&#23548;&#25915;&#20987;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#29616;&#22312;&#38754;&#20020;&#30528;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#39640;&#21644;&#33391;&#22909;&#20934;&#30830;&#24615;&#19982;&#38450;&#31363;&#40065;&#26834;&#24615;&#20043;&#38388;&#19981;&#21033;&#26435;&#34913;&#30340;&#38382;&#39064;&#65292;&#36825;&#25361;&#25112;&#20102;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#31363;&#21462;&#38450;&#24481;&#35757;&#32451;&#26694;&#26550;Isolation and Induction&#65288;InI&#65289;&#12290;InI&#19981;&#20687;&#37096;&#32626;&#36741;&#21161;&#38450;&#24481;&#27169;&#22359;&#37027;&#26679;&#24341;&#20837;&#20887;&#20313;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#26159;&#36890;&#36807;&#23558;&#23545;&#25163;&#30340;&#35757;&#32451;&#26799;&#24230;&#19982;&#39044;&#26399;&#26799;&#24230;&#38548;&#31163;&#26469;&#30452;&#25509;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00951</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#21040;&#36719;&#24615;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#19987;&#23478;&#27169;&#22411;(MoEs)&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;MoEs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#20002;&#22833;&#26631;&#35760;&#12289;&#26080;&#27861;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#25110;&#26080;&#25928;&#30340;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Soft MoE&#65292;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;Transformer&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;MoEs&#30340;&#20248;&#28857;&#12290;Soft MoE&#36890;&#36807;&#21521;&#27599;&#20010;&#19987;&#23478;&#20256;&#36882;&#25152;&#26377;&#36755;&#20837;&#26631;&#35760;&#30340;&#19981;&#21516;&#21152;&#26435;&#32452;&#21512;&#26469;&#25191;&#34892;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#12290;&#19982;&#20854;&#20182;MoE&#20316;&#21697;&#19968;&#26679;&#65292;Soft MoE&#20013;&#30340;&#19987;&#23478;&#21482;&#22788;&#29702;&#19968;&#37096;&#20998;&#65288;&#32452;&#21512;&#30340;&#65289;&#26631;&#35760;&#65292;&#20197;&#22312;&#36739;&#20302;&#30340;&#25512;&#29702;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#65292;Soft MoE&#22312;&#26631;&#20934;Transformer&#65288;ViTs&#65289;&#21644;&#27969;&#34892;&#30340;MoE&#21464;&#20307;&#65288;Tokens Choice&#21644;Experts Choice&#65289;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;Soft MoE-Base/16&#30340;&#25512;&#29702;&#25104;&#26412;&#27604;ViT-Huge/14&#20302;10.5&#20493;&#65288;&#22681;&#38047;&#26102;&#38388;&#38477;&#20302;&#20102;5.7&#20493;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00946</link><description>&lt;p&gt;
&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00946
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20986;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#26368;&#22810;93&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#22521;&#20859;&#22810;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#24615;&#30340;&#27573;&#33853;&#29255;&#27573;&#12290;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#20040;&#36890;&#36807;&#38024;&#23545;&#38750;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#23454;&#29616;&#38646;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#26377;&#26102;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#36827;&#34892;&#12290;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21363;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#23545;&#20110;&#19981;&#23384;&#22312;&#36275;&#22815;&#20449;&#24687;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26816;&#32034;&#26102;&#65292;&#33021;&#21542;&#23454;&#29616;&#38646;&#26679;&#26412;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20026;&#22810;&#26679;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;StrategyQA&#65292;CommonsenseQA&#65292;IIRC&#65292;DROP&#65292;Musique&#21644;ARC-DA&#65289;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;GAN&#20013;&#30340;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#38382;&#39064;&#12290;FA-GAN&#20351;&#29992;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#24207;&#21015;&#26469;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;</title><link>http://arxiv.org/abs/2308.00939</link><description>&lt;p&gt;
&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feature-aware conditional GAN for category text generation. (arXiv:2308.00939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;GAN&#20013;&#30340;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#38382;&#39064;&#12290;FA-GAN&#20351;&#29992;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#24207;&#21015;&#26469;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;GAN&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#21363;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;FA-GAN&#20013;&#65292;&#29983;&#25104;&#22120;&#20855;&#26377;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#29992;&#20110;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;Gumbel SoftMax&#28608;&#27963;&#20989;&#25968;&#12290;&#37492;&#21035;&#22120;&#36824;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;&#20026;&#20102;&#29983;&#25104;&#25351;&#23450;&#31867;&#21035;&#30340;&#21477;&#23376;&#65292;
&lt;/p&gt;
&lt;p&gt;
Category text generation receives considerable attentions since it is beneficial for various natural language processing tasks. Recently, the generative adversarial network (GAN) has attained promising performance in text generation, attributed to its adversarial training process. However, there are several issues in text GANs, including discreteness, training instability, mode collapse, lack of diversity and controllability etc. To address these issues, this paper proposes a novel GAN framework, the feature-aware conditional GAN (FA-GAN), for controllable category text generation. In FA-GAN, the generator has a sequence-to-sequence structure for improving sentence diversity, which consists of three encoders including a special feature-aware encoder and a category-aware encoder, and one relational-memory-core-based decoder with the Gumbel SoftMax activation function. The discriminator has an additional category classification head. To generate sentences with specified categories, the m
&lt;/p&gt;</description></item><item><title>LEMMA&#26159;&#19968;&#20010;&#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#23427;&#25552;&#20379;&#20102;&#28041;&#21450;&#24037;&#20855;&#20351;&#29992;&#21644;&#20256;&#36882;&#30340;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2308.00937</link><description>&lt;p&gt;
LEMMA: &#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LEMMA: Learning Language-Conditioned Multi-Robot Manipulation. (arXiv:2308.00937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00937
&lt;/p&gt;
&lt;p&gt;
LEMMA&#26159;&#19968;&#20010;&#23398;&#20064;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#23427;&#25552;&#20379;&#20102;&#28041;&#21450;&#24037;&#20855;&#20351;&#29992;&#21644;&#20256;&#36882;&#30340;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#25805;&#32437;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20855;&#26377;&#20114;&#34917;&#21151;&#33021;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#30340;&#26700;&#38754;&#35774;&#32622;&#20013;&#20219;&#21153;&#20998;&#37197;&#21644;&#38271;&#26102;&#38388;&#36328;&#24230;&#29289;&#20307;&#25805;&#20316;&#30340;LanguagE-Conditioned Multi-robot MAnipulation (LEMMA)&#22522;&#20934;&#12290;LEMMA&#20855;&#26377;8&#31181;&#31867;&#22411;&#30340;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#19968;&#20123;&#20219;&#21153;&#35201;&#27714;&#26426;&#22120;&#20154;&#20351;&#29992;&#24037;&#20855;&#24182;&#30456;&#20114;&#20256;&#36882;&#24037;&#20855;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20379;800&#20010;&#19987;&#23478;&#31034;&#33539;&#21644;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#22521;&#35757;&#21644;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;LEMMA&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#31995;&#32479;&#35782;&#21035;&#27599;&#20010;&#25805;&#32437;&#22120;&#30340;&#38480;&#21046;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#37197;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#22788;&#29702;&#27599;&#20010;&#20219;&#21153;&#20013;&#30340;&#24378;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;LEMMA&#22312;&#24320;&#21457;&#26410;&#26469;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#33258;&#36866;&#24212;&#36895;&#24230;&#38480;&#21046;&#31574;&#30053;&#30340;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO-SAVL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#36827;&#21270;&#29366;&#24577;&#20272;&#35745;&#65288;ESE&#65289;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36895;&#24230;&#38480;&#21046;&#65288;VL&#65289;&#65292;&#20197;&#25552;&#39640;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00936</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#33258;&#36866;&#24212;&#36895;&#24230;&#38480;&#21046;&#31574;&#30053;&#30340;&#31890;&#23376;&#32676;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Particle swarm optimization with state-based adaptive velocity limit strategy. (arXiv:2308.00936v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#33258;&#36866;&#24212;&#36895;&#24230;&#38480;&#21046;&#31574;&#30053;&#30340;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO-SAVL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#36827;&#21270;&#29366;&#24577;&#20272;&#35745;&#65288;ESE&#65289;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36895;&#24230;&#38480;&#21046;&#65288;VL&#65289;&#65292;&#20197;&#25552;&#39640;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36895;&#24230;&#38480;&#21046;&#65288;VL&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#30340;&#21464;&#31181;&#20013;&#65292;&#20197;&#38450;&#27490;&#31890;&#23376;&#22312;&#35299;&#31354;&#38388;&#20043;&#22806;&#36827;&#34892;&#25628;&#32034;&#12290;&#24050;&#32463;&#24341;&#20837;&#20102;&#20960;&#31181;&#33258;&#36866;&#24212;VL&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;PSO&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;VL&#31574;&#30053;&#20165;&#26681;&#25454;&#36845;&#20195;&#27425;&#25968;&#35843;&#25972;VL&#65292;&#23548;&#33268;&#20248;&#21270;&#32467;&#26524;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;VL&#19982;&#31890;&#23376;&#30340;&#24403;&#21069;&#25628;&#32034;&#29366;&#24577;&#20043;&#38388;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29366;&#24577;&#30340;&#33258;&#36866;&#24212;&#36895;&#24230;&#38480;&#21046;&#31574;&#30053;&#30340;PSO&#21464;&#31181;&#65288;PSO-SAVL&#65289;&#12290;&#22312;&#25552;&#20986;&#30340;PSO-SAVL&#20013;&#65292;VL&#22522;&#20110;&#36827;&#21270;&#29366;&#24577;&#20272;&#35745;&#65288;ESE&#65289;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#20854;&#20013;&#20026;&#20840;&#23616;&#25628;&#32034;&#29366;&#24577;&#35774;&#32622;&#36739;&#39640;&#30340;VL&#20540;&#65292;&#20026;&#23616;&#37096;&#25628;&#32034;&#29366;&#24577;&#35774;&#32622;&#36739;&#20302;&#30340;VL&#20540;&#12290;&#27492;&#22806;&#65292;&#25913;&#36827;&#24182;&#37319;&#29992;&#20102;&#38480;&#21046;&#22788;&#29702;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;PSO-SAVL&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Velocity limit (VL) has been widely adopted in many variants of particle swarm optimization (PSO) to prevent particles from searching outside the solution space. Several adaptive VL strategies have been introduced with which the performance of PSO can be improved. However, the existing adaptive VL strategies simply adjust their VL based on iterations, leading to unsatisfactory optimization results because of the incompatibility between VL and the current searching state of particles. To deal with this problem, a novel PSO variant with state-based adaptive velocity limit strategy (PSO-SAVL) is proposed. In the proposed PSO-SAVL, VL is adaptively adjusted based on the evolutionary state estimation (ESE) in which a high value of VL is set for global searching state and a low value of VL is set for local searching state. Besides that, limit handling strategies have been modified and adopted to improve the capability of avoiding local optima. The good performance of PSO-SAVL has been experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22312;&#34880;&#28082;&#21160;&#21147;&#23398;&#20013;&#35299;&#20915;&#20102;&#32570;&#20047;&#23436;&#25972;&#20449;&#24687;&#21644;&#21482;&#26377;&#25955;&#23556;&#27979;&#37327;&#25968;&#25454;&#30340;&#36870;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26102;&#33021;&#22815;&#31283;&#23450;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#27969;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19982;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#32806;&#21512;&#29289;&#29702;&#31995;&#32479;&#30456;&#20851;&#30340;&#20020;&#24202;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.00927</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34880;&#27969;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks for blood flow inverse problems. (arXiv:2308.00927v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22312;&#34880;&#28082;&#21160;&#21147;&#23398;&#20013;&#35299;&#20915;&#20102;&#32570;&#20047;&#23436;&#25972;&#20449;&#24687;&#21644;&#21482;&#26377;&#25955;&#23556;&#27979;&#37327;&#25968;&#25454;&#30340;&#36870;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26102;&#33021;&#22815;&#31283;&#23450;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#27969;&#21160;&#27169;&#24335;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19982;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#32806;&#21512;&#29289;&#29702;&#31995;&#32479;&#30456;&#20851;&#30340;&#20020;&#24202;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#22312;&#27809;&#26377;&#23436;&#25972;&#31995;&#32479;&#20449;&#24687;&#24182;&#19988;&#21482;&#26377;&#25955;&#23556;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#22312;&#34880;&#28082;&#21160;&#21147;&#23398;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36793;&#30028;&#20449;&#24687;&#36890;&#24120;&#24456;&#38590;&#24314;&#27169;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#34880;&#27969;&#27979;&#37327;&#25968;&#25454;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;PINNs&#26041;&#27861;&#26469;&#20272;&#35745;&#25955;&#23556;2D&#22122;&#22768;&#27979;&#37327;&#25968;&#25454;&#22312;&#21319;&#20027;&#21160;&#33033;&#20013;&#30340;&#32553;&#20943;&#27169;&#22411;&#21442;&#25968;&#21644;&#23436;&#25972;&#30340;&#36895;&#24230;&#22330;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26102;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#20934;&#30830;&#22320;&#20272;&#35745;&#21442;&#25968;&#65292;&#32780;&#36895;&#24230;&#37325;&#24314;&#21017;&#21462;&#20915;&#20110;&#27979;&#37327;&#36136;&#37327;&#21644;&#27969;&#21160;&#27169;&#24335;&#22797;&#26434;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#19982;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#22797;&#26434;&#32806;&#21512;&#29289;&#29702;&#31995;&#32479;&#30456;&#20851;&#30340;&#20020;&#24202;&#30456;&#20851;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have emerged as a powerful tool for solving inverse problems, especially in cases where no complete information about the system is known and scatter measurements are available. This is especially useful in hemodynamics since the boundary information is often difficult to model, and high-quality blood flow measurements are generally hard to obtain. In this work, we use the PINNs methodology for estimating reduced-order model parameters and the full velocity field from scatter 2D noisy measurements in the ascending aorta. The results show stable and accurate parameter estimations when using the method with simulated data, while the velocity reconstruction shows dependence on the measurement quality and the flow pattern complexity. The method allows for solving clinical-relevant inverse problems in hemodynamics and complex coupled physical systems.
&lt;/p&gt;</description></item><item><title>VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00904</link><description>&lt;p&gt;
VLUCI: &#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00904
&lt;/p&gt;
&lt;p&gt;
VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32463;&#27982;&#23398;&#31561;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#21435;&#28151;&#28102;&#21644;&#21453;&#20107;&#23454;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;&#20294;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#25197;&#26354;&#20102;&#22240;&#26524;&#25512;&#26029;&#24182;&#24433;&#21709;&#20102;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#65288;VLUCI&#65289;&#65292;&#23427;&#29983;&#25104;&#20102;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;VLUCI&#25918;&#26494;&#20102;&#22823;&#22810;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#30340;&#26080;&#28151;&#28102;&#20551;&#35774;&#12290;&#36890;&#36807;&#35299;&#32806;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#21644;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;VLUCI&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21464;&#37327;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
&lt;/p&gt;</description></item><item><title>&#20511;&#37492;&#33021;&#21147;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#30028;&#23450;&#20102;&#36947;&#24503;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#26465;&#20214;&#65292;&#24182;&#23545;&#20960;&#31181;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00868</link><description>&lt;p&gt;
&#20161;&#29233;&#26234;&#33021;&#65306;&#36890;&#36807;AI&#31995;&#32479;&#23545;&#21033;&#30410;&#12289;&#25588;&#21161;&#21450;&#30456;&#20851;&#36947;&#24503;&#22833;&#35823;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beneficent Intelligence: A Capability Approach to Modeling Benefit, Assistance, and Associated Moral Failures through AI Systems. (arXiv:2308.00868v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00868
&lt;/p&gt;
&lt;p&gt;
&#20511;&#37492;&#33021;&#21147;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#30028;&#23450;&#20102;&#36947;&#24503;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#26465;&#20214;&#65292;&#24182;&#23545;&#20960;&#31181;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20262;&#29702;&#23398;&#20013;&#26222;&#36941;&#30340;&#35752;&#35770;&#32570;&#20047;&#25429;&#25417;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#22810;&#26679;&#21270;&#20262;&#29702;&#20851;&#20999;&#25152;&#38656;&#30340;&#35821;&#35328;&#21644;&#24418;&#24335;&#12290;&#20511;&#37492;Sen&#21644;Nussbaum&#30340;&#33021;&#21147;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#20102;AI&#31995;&#32479;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#21033;&#30410;&#25110;&#25588;&#21161;&#25152;&#24517;&#38656;&#30340;&#20262;&#29702;&#27010;&#24565;&#21644;&#26435;&#21033;&#12290;&#36825;&#20123;&#31995;&#32479;&#22686;&#24378;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#25512;&#36827;&#20854;&#20154;&#29983;&#35745;&#21010;&#21644;&#24184;&#31119;&#24863;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#32500;&#25252;&#20854;&#22522;&#26412;&#26435;&#21033;&#12290;&#25105;&#20204;&#30028;&#23450;&#20102;AI&#31995;&#32479;&#19982;&#21463;&#20854;&#21151;&#33021;&#24433;&#21709;&#30340;&#20154;&#20043;&#38388;&#36947;&#24503;&#19978;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#30340;&#20004;&#20010;&#24517;&#35201;&#26465;&#20214;&#65292;&#20197;&#21450;&#23454;&#29616;&#26377;&#24847;&#20041;&#21033;&#30410;&#29702;&#24819;&#30340;&#20004;&#20010;&#20805;&#20998;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#24819;&#19982;&#20960;&#31181;&#31361;&#20986;&#30340;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#23545;&#27604;&#65292;&#21363;&#26500;&#25104;&#19981;&#21512;&#29702;&#30340;&#23478;&#38271;&#24335;&#20027;&#20041;&#12289;&#24378;&#36843;&#12289;&#27450;&#39575;&#12289;&#21093;&#21066;&#21644;&#25903;&#37197;&#30340;&#31038;&#20132;&#20114;&#21160;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevailing discourse around AI ethics lacks the language and formalism necessary to capture the diverse ethical concerns that emerge when AI systems interact with individuals. Drawing on Sen and Nussbaum's capability approach, we present a framework formalizing a network of ethical concepts and entitlements necessary for AI systems to confer meaningful benefit or assistance to stakeholders. Such systems enhance stakeholders' ability to advance their life plans and well-being while upholding their fundamental rights. We characterize two necessary conditions for morally permissible interactions between AI systems and those impacted by their functioning, and two sufficient conditions for realizing the ideal of meaningful benefit. We then contrast this ideal with several salient failure modes, namely, forms of social interactions that constitute unjustified paternalism, coercion, deception, exploitation and domination. The proliferation of incidents involving AI in high-stakes domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2308.00864</link><description>&lt;p&gt;
PeRP&#65306;&#36890;&#36807;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#23454;&#29616;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#20197;&#32531;&#35299;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#34892;&#21160;&#26469;&#32531;&#35299;&#25317;&#22581;&#65292;&#20174;&#32780;&#25913;&#21892;&#36890;&#21220;&#26102;&#38388;&#21644;&#29123;&#27833;&#25104;&#26412;&#31561;&#20247;&#22810;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20551;&#35774;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#38431;&#20855;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20998;&#27573;&#24120;&#25968;&#65288;PC&#65289;&#31574;&#30053;&#36890;&#36807;&#32467;&#26500;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#65292;&#20197;&#25552;&#20379;&#32473;&#20154;&#31867;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;PC&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#39550;&#39542;&#21592;&#34892;&#20026;&#30456;&#20284;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PC&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#29305;&#24449;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#65292;&#21363;PeRP&#12290;PeRP&#24314;&#35758;&#39550;&#39542;&#21592;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#30340;&#26041;&#24335;&#34892;&#39542;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26080;&#30417;&#30563;&#22320;&#25512;&#26029;&#39550;&#39542;&#21592;&#22914;&#20309;&#36981;&#24490;&#25351;&#20196;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#31574;&#30053;&#19982;&#39550;&#39542;&#21592;&#29305;&#24449;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20018;&#22270;&#34920;&#36848;&#30340;&#20027;&#21160;&#25512;&#29702;&#30340;&#33539;&#30068;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#29983;&#25104;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#26356;&#26032;&#12289;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#30001;&#33021;&#30340;&#22270;&#24418;&#21270;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#20018;&#22270;&#25512;&#23548;&#32473;&#20986;&#20102;&#20027;&#21160;&#25512;&#29702;&#30340;&#20844;&#24335;&#65292;&#36824;&#30830;&#23450;&#20102;&#33258;&#30001;&#33021;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2308.00861</link><description>&lt;p&gt;
&#29992;&#20018;&#22270;&#36827;&#34892;&#20027;&#21160;&#25512;&#29702;&#65306;&#39044;&#27979;&#22788;&#29702;&#21644;&#33258;&#30001;&#33021;&#30340;&#33539;&#30068;&#21270;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Active Inference in String Diagrams: A Categorical Account of Predictive Processing and Free Energy. (arXiv:2308.00861v1 [math.CT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20018;&#22270;&#34920;&#36848;&#30340;&#20027;&#21160;&#25512;&#29702;&#30340;&#33539;&#30068;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#29983;&#25104;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#26356;&#26032;&#12289;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#30001;&#33021;&#30340;&#22270;&#24418;&#21270;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#20018;&#22270;&#25512;&#23548;&#32473;&#20986;&#20102;&#20027;&#21160;&#25512;&#29702;&#30340;&#20844;&#24335;&#65292;&#36824;&#30830;&#23450;&#20102;&#33258;&#30001;&#33021;&#30340;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33539;&#30068;&#21270;&#30340;&#34920;&#36848;&#39044;&#27979;&#22788;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35748;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#22312;&#20855;&#26377;&#22797;&#21046;&#21644;&#20002;&#24323;&#25805;&#20316;&#31526;&#30340;&#24186;&#21322;&#33539;&#30068;&#20013;&#35299;&#37322;&#30340;&#20018;&#22270;&#12290;&#36825;&#21253;&#25324;&#20102;&#29983;&#25104;&#27169;&#22411;&#12289;&#36125;&#21494;&#26031;&#26356;&#26032;&#12289;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#30001;&#33021;&#30340;&#22270;&#24418;&#21270;&#25551;&#36848;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#30340;&#20018;&#22270;&#25512;&#23548;&#25552;&#20986;&#20102;&#20027;&#21160;&#25512;&#29702;&#30340;&#20844;&#24335;&#65292;&#24182;&#30830;&#31435;&#20102;&#33258;&#30001;&#33021;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#20351;&#24471;&#33258;&#30001;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#23618;&#27425;&#19978;&#12290;&#38500;&#20102;&#20026;&#29087;&#24713;&#20027;&#21160;&#25512;&#29702;&#30340;&#20154;&#25552;&#20379;&#19968;&#20010;&#26377;&#24110;&#21161;&#30340;&#22270;&#24418;&#21270;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#20063;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25552;&#20379;&#35813;&#26694;&#26550;&#30340;&#31616;&#26126;&#34920;&#36848;&#21644;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a categorical formulation of the cognitive frameworks of Predictive Processing and Active Inference, expressed in terms of string diagrams interpreted in a monoidal category with copying and discarding. This includes diagrammatic accounts of generative models, Bayesian updating, perception, planning, active inference, and free energy. In particular we present a diagrammatic derivation of the formula for active inference via free energy minimisation, and establish a compositionality property for free energy, allowing free energy to be applied at all levels of an agent's generative model. Aside from aiming to provide a helpful graphical language for those familiar with active inference, we conversely hope that this article may provide a concise formulation and introduction to the framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.00858</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#38543;&#26426;&#36807;&#31243;&#26469;&#29702;&#35299;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes. (arXiv:2308.00858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#26469;&#30740;&#31350;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#39564;&#65292;&#30740;&#31350;&#20154;&#21592;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#65288;&#28145;&#23618;&#65289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#21644;&#23398;&#20064;&#21160;&#24577;&#65292;&#37319;&#29992;&#25968;&#23398;&#25277;&#35937;&#21644;&#27169;&#22411;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#36825;&#20123;&#24037;&#20855;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#24615;&#33021;&#30340;&#31616;&#21270;&#35270;&#35282;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#20419;&#36827;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36804;&#20170;&#20026;&#27490;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38543;&#26426;&#36807;&#31243;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#65288;&#28145;&#23618;&#65289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38408;&#20540;&#33410;&#28857;&#30340;&#28608;&#27963;&#27169;&#24335;&#24314;&#27169;&#20026;&#38543;&#26426;&#36807;&#31243;&#12290;&#25105;&#20204;&#20165;&#20851;&#27880;&#28608;&#27963;&#39057;&#29575;&#65292;&#21033;&#29992;&#29992;&#20110;&#30495;&#23454;&#31070;&#32463;&#20803;&#23574;&#23792;&#20449;&#21495;&#30340;&#31070;&#32463;&#31185;&#23398;&#25216;&#26415;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#21462;&#23574;&#23792;&#27963;&#21160;&#24182;&#20351;&#29992;&#31526;&#21512;&#27850;&#26494;&#20998;&#24067;&#30340;&#21040;&#36798;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#26816;&#26597;&#26469;&#33258;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#25311;&#21512;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25551;&#36848;&#27599;&#20010;&#32593;&#32476;&#20013;&#28608;&#27963;&#27169;&#24335;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
To gain a deeper understanding of the behavior and learning dynamics of (deep) artificial neural networks, it is valuable to employ mathematical abstractions and models. These tools provide a simplified perspective on network performance and facilitate systematic investigations through simulations. In this paper, we propose utilizing the framework of stochastic processes, which has been underutilized thus far.  Our approach models activation patterns of thresholded nodes in (deep) artificial neural networks as stochastic processes. We focus solely on activation frequency, leveraging neuroscience techniques used for real neuron spike trains. During a classification task, we extract spiking activity and use an arrival process following the Poisson distribution.  We examine observed data from various artificial neural networks in image recognition tasks, fitting the proposed model's assumptions. Through this, we derive parameters describing activation patterns in each network. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;RBlur&#36716;&#25442;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;DNN&#65292;&#20351;&#29992;RBlur&#26041;&#27861;&#35757;&#32451;&#30340;DNN&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#20854;&#20182;&#25439;&#22351;&#20855;&#26377;&#26356;&#24378;&#30340;&#20581;&#22766;&#24615;&#65292;&#25552;&#39640;&#20102;&#39640;&#36798;25%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00854</link><description>&lt;p&gt;
&#35757;&#32451;&#28966;&#28857;&#22270;&#20687;&#25552;&#39640;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Training on Foveated Images Improves Robustness to Adversarial Attacks. (arXiv:2308.00854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;RBlur&#36716;&#25442;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;DNN&#65292;&#20351;&#29992;RBlur&#26041;&#27861;&#35757;&#32451;&#30340;DNN&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#20854;&#20182;&#25439;&#22351;&#20855;&#26377;&#26356;&#24378;&#30340;&#20581;&#22766;&#24615;&#65292;&#25552;&#39640;&#20102;&#39640;&#36798;25%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#21363;&#36755;&#20837;&#30340;&#32454;&#24494;&#12289;&#34987;&#24863;&#30693;&#19981;&#21040;&#30340;&#25200;&#21160;&#20250;&#25913;&#21464;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#22312;&#35270;&#35273;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#30340;&#20581;&#22766;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#25345;&#32493;&#26292;&#38706;&#20110;&#22806;&#22260;&#35270;&#35273;&#20013;&#30340;&#20302;&#20445;&#30495;&#24230;&#35270;&#35273;&#21050;&#28608;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RBlur&#65292;&#19968;&#31181;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#27169;&#31946;&#21644;&#38477;&#20302;&#39068;&#33394;&#39281;&#21644;&#24230;&#26469;&#27169;&#25311;&#22806;&#22260;&#35270;&#35273;&#30340;&#20445;&#30495;&#24230;&#25439;&#22833;&#65292;&#26041;&#27861;&#22522;&#20110;&#32473;&#23450;&#30340;&#27880;&#35270;&#28857;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#22312;&#21407;&#22987;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;DNN&#30456;&#27604;&#65292;&#36890;&#36807;RBlur&#36716;&#25442;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;DNN&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#20854;&#20182;&#38750;&#23545;&#25239;&#24615;&#25439;&#22351;&#20855;&#26377;&#26356;&#24378;&#30340;&#20581;&#22766;&#24615;&#65292;&#22312;&#25200;&#21160;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\% higher accuracy on perturbed data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20805;&#24403;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#24515;&#26234;&#27169;&#22411;&#21644;&#35789;&#27719;&#30340;&#29992;&#25143;&#31038;&#21306;&#20043;&#38388;&#27807;&#36890;&#26725;&#26753;&#30340;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#20351;&#29992;&#21442;&#19982;&#24335;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#24449;&#27714;&#20102;&#24320;&#21457;AskJill&#38382;&#31572;&#20195;&#29702;&#30340;&#38656;&#27714;&#65292;&#24182;&#21457;&#29616;&#29992;&#25143;&#35748;&#20026;&#35789;&#27719;&#34920;&#36741;&#21161;&#26159;&#20851;&#38190;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00813</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#31038;&#21306;&#20043;&#38388;&#30340;&#27807;&#36890;&#26725;&#26753;&#65306;&#21442;&#19982;&#24335;&#35774;&#35745;&#29992;&#20110;&#38382;&#31572;AI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Designing a Communication Bridge between Communities: Participatory Design for a Question-Answering AI Agent. (arXiv:2308.00813v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#20805;&#24403;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#24515;&#26234;&#27169;&#22411;&#21644;&#35789;&#27719;&#30340;&#29992;&#25143;&#31038;&#21306;&#20043;&#38388;&#27807;&#36890;&#26725;&#26753;&#30340;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#20351;&#29992;&#21442;&#19982;&#24335;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25104;&#21151;&#22320;&#24449;&#27714;&#20102;&#24320;&#21457;AskJill&#38382;&#31572;&#20195;&#29702;&#30340;&#38656;&#27714;&#65292;&#24182;&#21457;&#29616;&#29992;&#25143;&#35748;&#20026;&#35789;&#27719;&#34920;&#36741;&#21161;&#26159;&#20851;&#38190;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#26088;&#22312;&#20805;&#24403;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#24515;&#26234;&#27169;&#22411;&#21644;&#35789;&#27719;&#30340;&#29992;&#25143;&#31038;&#21306;&#20043;&#38388;&#27807;&#36890;&#26725;&#26753;&#30340;AI&#31995;&#32479;&#65311; Skillsync&#26159;&#19968;&#20010;&#20132;&#20114;&#29615;&#22659;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#23545;&#35805;&#23558;&#38599;&#20027;&#65288;&#20844;&#21496;&#65289;&#21644;&#22521;&#35757;&#25552;&#20379;&#32773;&#65288;&#22823;&#23398;&#65289;&#24341;&#20837;&#20854;&#20013;&#65292;&#24110;&#21161;&#20182;&#20204;&#23454;&#29616;&#24314;&#31435;&#33021;&#22815;&#25104;&#21151;&#28385;&#36275;&#38599;&#20027;&#21644;&#21592;&#24037;&#38656;&#27714;&#30340;&#22521;&#35757;&#25552;&#26696;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#21442;&#19982;&#24335;&#35774;&#35745;&#30340;&#21464;&#20307;&#65292;&#20197;&#24449;&#27714;&#24320;&#21457;AskJill&#30340;&#38656;&#27714;&#65292;AskJill&#26159;&#19968;&#20010;&#35299;&#37322;Skillsync&#24037;&#20316;&#21407;&#29702;&#30340;&#38382;&#31572;&#20195;&#29702;&#65292;&#22240;&#27492;&#20805;&#24403;&#20102;&#20225;&#19994;&#21644;&#23398;&#38498;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#24335;&#35774;&#35745;&#22312;&#25351;&#23548;&#38656;&#27714;&#25910;&#38598;&#21644;&#24341;&#21457;&#29992;&#25143;&#38382;&#39064;&#20197;&#24320;&#21457;AskJill&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20004;&#20010;Skillsync&#29992;&#25143;&#31038;&#21306;&#35748;&#20026;&#35789;&#27719;&#34920;&#36741;&#21161;&#26159;AskJill&#38656;&#35201;&#25552;&#20379;&#30340;&#20851;&#38190;&#21151;&#33021;&#65292;&#20182;&#20204;&#20250;&#21463;&#30410;&#20110;&#36825;&#26679;&#19968;&#20010;&#20849;&#20139;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we design an AI system that is intended to act as a communication bridge between two user communities with different mental models and vocabularies? Skillsync is an interactive environment that engages employers (companies) and training providers (colleges) in a sustained dialogue to help them achieve the goal of building a training proposal that successfully meets the needs of the employers and employees. We used a variation of participatory design to elicit requirements for developing AskJill, a question-answering agent that explains how Skillsync works and thus acts as a communication bridge between company and college users. Our study finds that participatory design was useful in guiding the requirements gathering and eliciting user questions for the development of AskJill. Our results also suggest that the two Skillsync user communities perceived glossary assistance as a key feature that AskJill needs to offer, and they would benefit from such a shared vocabulary.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#30524;&#30555;&#27169;&#22411;&#36890;&#36807;&#32593;&#32476;&#25668;&#20687;&#22836;&#12289;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#21644;&#36719;&#20214;&#27169;&#22411;&#23454;&#29616;&#30450;&#20154;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.00801</link><description>&lt;p&gt;
&#30450;&#20154;&#30340;&#20154;&#24037;&#30524;&#30555;
&lt;/p&gt;
&lt;p&gt;
Artificial Eye for the Blind. (arXiv:2308.00801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00801
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#30524;&#30555;&#27169;&#22411;&#36890;&#36807;&#32593;&#32476;&#25668;&#20687;&#22836;&#12289;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#21644;&#36719;&#20214;&#27169;&#22411;&#23454;&#29616;&#30450;&#20154;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#20154;&#24037;&#30524;&#30555;&#27169;&#22411;&#30340;&#20027;&#35201;&#25903;&#26609;&#26159;&#36830;&#25509;&#21040;&#32593;&#32476;&#25668;&#20687;&#22836;&#12289;&#36229;&#22768;&#27874;&#36817;&#36317;&#31163;&#20256;&#24863;&#22120;&#12289;&#25196;&#22768;&#22120;&#30340;&#26641;&#33683;&#27966;3&#65292;&#25105;&#20204;&#36824;&#36816;&#34892;&#20102;&#25152;&#26377;&#30340;&#36719;&#20214;&#27169;&#22411;&#65292;&#21253;&#25324;&#30446;&#26631;&#26816;&#27979;&#12289;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#12289;&#35895;&#27468;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;Mycroft&#35821;&#38899;&#36741;&#21161;&#27169;&#22411;&#12290;&#24403;&#36229;&#22768;&#27874;&#36817;&#36317;&#31163;&#20256;&#24863;&#22120;&#26816;&#27979;&#21040;&#20854;&#21069;&#26041;&#26377;&#20219;&#20309;&#38556;&#30861;&#29289;&#26102;&#65292;&#30450;&#20154;&#23558;&#25910;&#21040;&#19968;&#27573;&#20851;&#20110;&#21069;&#26041;&#38556;&#30861;&#29289;&#21450;&#20854;&#36317;&#31163;&#30340;&#38899;&#39057;&#25552;&#31034;&#12290;&#27492;&#26102;&#65292;&#32593;&#32476;&#25668;&#20687;&#22836;&#23558;&#25429;&#25417;&#21069;&#26041;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#26641;&#33683;&#27966;&#19978;&#36816;&#34892;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#12290;&#25429;&#25417;&#21040;&#30340;&#22270;&#20687;&#39318;&#20808;&#36890;&#36807;Tesseract OCR&#27169;&#22359;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#28982;&#21518;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main backbone of our Artificial Eye model is the Raspberry pi3 which is connected to the webcam ,ultrasonic proximity sensor, speaker and we also run all our software models i.e object detection, Optical Character recognition, google text to speech conversion and the Mycroft voice assistance model. At first the ultrasonic proximity sensor will be measuring the distance between itself and any obstacle in front of it .When the Proximity sensor detects any obstacle in front within its specified range, the blind person will hear an audio prompt about an obstacle in his way at a certain distance. At this time the Webcam will capture an image in front of it and the Object detection model and the Optical Character Recognition model will begin to run on the Raspberry pi. The imat of the blind person. The text and the object detected are conveyed to the blind pege captured is first sent through the Tesseract OCR module to detect any texts in the image and then through the Object detection m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21517;&#20026;POnto&#30340;&#39046;&#22495;&#26412;&#20307;&#65292;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#27010;&#24565;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;Polkadot&#29983;&#24577;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#21487;&#20132;&#27969;&#24615;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#21442;&#19982;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.00735</link><description>&lt;p&gt;
&#19968;&#31181;&#30693;&#35782;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378; Polkadot &#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#38598;&#25104;&#21644;&#21487;&#20132;&#27969;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-Oriented Approach to Enhance Integration and Communicability in the Polkadot Ecosystem. (arXiv:2308.00735v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21517;&#20026;POnto&#30340;&#39046;&#22495;&#26412;&#20307;&#65292;&#25552;&#20379;&#32467;&#26500;&#21270;&#30340;&#29983;&#24577;&#31995;&#32479;&#27010;&#24565;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;Polkadot&#29983;&#24577;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#21487;&#20132;&#27969;&#24615;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#21442;&#19982;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Polkadot &#29983;&#24577;&#31995;&#32479;&#26159;&#19968;&#31181;&#20855;&#26377;&#39072;&#35206;&#24615;&#21644;&#39640;&#24230;&#22797;&#26434;&#30340;&#22810;&#38142;&#26550;&#26500;&#65292;&#23545;&#25968;&#25454;&#20998;&#26512;&#21644;&#21487;&#20132;&#27969;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#26816;&#32034;&#21644;&#20998;&#26512;&#36328;&#24179;&#34892;&#38142;&#21644;&#24212;&#29992;&#30340;&#25968;&#25454;&#65292;&#20351;&#24471;&#19968;&#33324;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#38590;&#20197;&#19968;&#33268;&#22320;&#35775;&#38382;&#29983;&#24577;&#31995;&#32479;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#21517;&#20026; POnto&#65288;Polkadot Ontology&#65289;&#30340;&#39046;&#22495;&#26412;&#20307;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;POnto &#25552;&#20379;&#20102;&#29983;&#24577;&#31995;&#32479;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#24179;&#21488;&#30340;&#24418;&#24335;&#21270;&#29702;&#35299;&#12290;&#25552;&#20986;&#30340;&#30693;&#35782;&#23548;&#21521;&#26041;&#27861;&#22686;&#24378;&#20102;&#38598;&#25104;&#21644;&#21487;&#20132;&#27969;&#24615;&#65292;&#20351;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#33021;&#22815;&#21442;&#19982;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#30340;&#24320;&#21457;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#26469;&#33258;Polkado&#30340;&#19987;&#23478;&#21453;&#39304;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Polkadot ecosystem is a disruptive and highly complex multi-chain architecture that poses challenges in terms of data analysis and communicability. Currently, there is a lack of standardized and holistic approaches to retrieve and analyze data across parachains and applications, making it difficult for general users and developers to access ecosystem data consistently. This paper proposes a conceptual framework that includes a domain ontology called POnto (a Polkadot Ontology) to address these challenges. POnto provides a structured representation of the ecosystem's concepts and relationships, enabling a formal understanding of the platform. The proposed knowledge-oriented approach enhances integration and communicability, enabling a wider range of users to participate in the ecosystem and facilitating the development of AI-based applications. The paper presents a case study methodology to validate the proposed framework, which includes expert feedback and insights from the Polkado
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#31867;&#28608;&#27963;&#22270;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#26679;&#26412;&#30340;&#31867;&#28608;&#27963;&#22270;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#36807;&#31243;&#20351;&#24471;&#20998;&#26512;&#24072;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#21644;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00710</link><description>&lt;p&gt;
&#23454;&#29616;&#32858;&#21512;&#30340;&#31867;&#28608;&#27963;&#22270;&#21487;&#29992;&#20110;&#20998;&#26512;&#31867;&#29305;&#24449;&#30340;&#25972;&#20307;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#31867;&#28608;&#27963;&#22270;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20010;&#26679;&#26412;&#30340;&#31867;&#28608;&#27963;&#22270;&#32858;&#21512;&#36215;&#26469;&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#36807;&#31243;&#20351;&#24471;&#20998;&#26512;&#24072;&#21487;&#20197;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#21644;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#39640;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#39118;&#38505;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#26080;&#27861;&#20351;&#29992;&#65292;&#38500;&#38750;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#20851;&#27880;&#20110;&#35299;&#37322;&#31867;&#20284;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#21487;&#35270;&#21270;&#25968;&#25454;&#26679;&#26412;&#20013;&#27599;&#20010;&#29305;&#24449;&#23545;&#20998;&#31867;&#30340;&#36129;&#29486;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#26679;&#26412;&#30340;CAMs&#65292;&#20197;&#23637;&#31034;&#35821;&#20041;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20998;&#31867;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#32858;&#21512;&#20351;&#20998;&#26512;&#24072;&#33021;&#22815;&#36827;&#34892;&#22797;&#26434;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#38075;&#21462;&#21487;&#35270;&#21270;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;CAM&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#20197;&#19968;&#20010;&#26041;&#24418;&#26631;&#35760;&#34920;&#31034;&#27599;&#20010;&#29305;&#24449;&#30340;&#24433;&#21709;&#21147;&#65292;&#26041;&#24418;&#30340;&#39068;&#33394;&#34920;&#31034;&#35813;&#29305;&#24449;&#30340;&#20998;&#31867;&#24433;&#21709;&#21147;&#65292;&#22635;&#20805;&#26041;&#24418;&#30340;&#22823;&#23567;&#34920;&#31034;&#35813;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models achieve remarkable performance in classification tasks. However, models with high complexity can not be used in many risk-sensitive applications unless a comprehensible explanation is presented. Explainable artificial intelligence (xAI) focuses on the research to explain the decision-making of AI systems like DL. We extend a recent method of Class Activation Maps (CAMs) which visualizes the importance of each feature of a data sample contributing to the classification. In this paper, we aggregate CAMs from multiple samples to show a global explanation of the classification for semantically structured data. The aggregation allows the analyst to make sophisticated assumptions and analyze them with further drill-down visualizations. Our visual representation for the global CAM illustrates the impact of each feature with a square glyph containing two indicators. The color of the square indicates the classification impact of this feature. The size of the filled squ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;&#31639;&#27861; (AMBS) &#26469;&#39564;&#35777;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#32473;&#23450;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#30456;&#27604;&#65292;AMBS&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#22312;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00707</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;
&lt;/p&gt;
&lt;p&gt;
Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109;&#31639;&#27861; (AMBS) &#26469;&#39564;&#35777;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#32473;&#23450;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#30456;&#27604;&#65292;AMBS&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#22312;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064; (RL) &#22312;&#21508;&#20010;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558; RL &#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#26368;&#22823;&#21270;&#26631;&#20934; RL &#30446;&#26631;&#19981;&#33021;&#20445;&#35777;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#27169;&#22411;&#23631;&#34109; (AMBS)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29702;&#24615;&#21069;&#30651;&#23631;&#34109;&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#23398;&#20064;&#30340; RL &#31574;&#30053;&#30456;&#23545;&#20110;&#19968;&#32452;&#32473;&#23450;&#30340;&#23433;&#20840;&#32422;&#26463;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#23631;&#34109;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#23545;&#31995;&#32479;&#30340;&#23433;&#20840;&#30456;&#20851;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20026; AMBS &#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#22312;&#19968;&#32452;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340; Atari &#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;2015-2020&#24180;&#38388;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25991;&#29486;&#36235;&#21183;&#65292;&#21457;&#29616;&#21830;&#19994;&#26399;&#21002;&#22312;&#24341;&#29992;&#20998;&#25968;&#21644;&#21457;&#34920;&#25968;&#37327;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#21508;&#22269;&#30340;&#20986;&#29256;&#24773;&#20917;&#21644;&#21360;&#24230;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00705</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25991;&#29486;&#30740;&#31350;&#65306;&#20840;&#29699;&#27010;&#20917;&#19982;&#21360;&#24230;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
A Bibliographic Study on Artificial Intelligence Research: Global Panorama and Indian Appearance. (arXiv:2308.00705v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;2015-2020&#24180;&#38388;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25991;&#29486;&#36235;&#21183;&#65292;&#21457;&#29616;&#21830;&#19994;&#26399;&#21002;&#22312;&#24341;&#29992;&#20998;&#25968;&#21644;&#21457;&#34920;&#25968;&#37327;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#21508;&#22269;&#30340;&#20986;&#29256;&#24773;&#20917;&#21644;&#21360;&#24230;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#29486;&#35745;&#37327;&#23398;&#30340;&#31185;&#23398;&#26144;&#23556;&#26041;&#27861;&#65292;&#35782;&#21035;&#21644;&#35780;&#20272;&#20102;2015-2020&#24180;&#38388;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25991;&#29486;&#36235;&#21183;&#12290;&#25152;&#38656;&#25968;&#25454;&#26469;&#33258;Scopus&#25968;&#25454;&#24211;&#12290;&#36890;&#36807;&#25163;&#21160;&#21644;&#20351;&#29992;OpenRefine&#24037;&#20855;&#36827;&#34892;&#24517;&#35201;&#30340;&#25968;&#25454;&#36716;&#25442;&#65292;&#20351;&#24471;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#20998;&#26512;&#12290;&#20026;&#20102;&#30830;&#23450;&#36235;&#21183;&#21644;&#36827;&#34892;&#26144;&#23556;&#25216;&#26415;&#65292;&#26681;&#25454;&#20854;&#24341;&#29992;&#20998;&#25968;&#39537;&#21160;&#30340;&#25490;&#21517;&#65292;&#36873;&#25321;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20116;&#20010;&#24320;&#25918;&#33719;&#21462;&#21644;&#21830;&#19994;&#26399;&#21002;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#35813;&#26399;&#38388;&#21457;&#34920;&#30340;6880&#31687;&#25991;&#31456;&#12290;&#36235;&#21183;&#21253;&#25324;&#21508;&#22269;&#30340;&#20986;&#29256;&#29289;&#65292;&#24180;&#24230;&#20986;&#29256;&#29289;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20027;&#39064;&#35789;&#65292;&#34987;&#24341;&#29992;&#27425;&#25968;&#26368;&#22810;&#30340;&#25991;&#31456;&#65292;&#37325;&#35201;&#20316;&#32773;&#65292;&#20027;&#35201;&#26426;&#26500;&#65292;&#24037;&#19994;&#30028;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#21442;&#19982;&#20197;&#21450;&#21360;&#24230;&#30340;&#20986;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24320;&#25918;&#33719;&#21462;&#26399;&#21002;&#30456;&#27604;&#65292;&#21830;&#19994;&#26399;&#21002;&#22312;&#22810;&#24180;&#30340;&#24341;&#29992;&#20998;&#25968;&#21644;&#21457;&#34920;&#25991;&#31456;&#25968;&#37327;&#19978;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study identifies and assesses the bibliographic trend in Artificial Intelligence (AI) research for the years 2015-2020 using the science mapping method of bibliometric study. The required data has been collected from the Scopus database. To make the collected data analysis-ready, essential data transformation was performed manually and with the help of a tool viz. OpenRefine. For determining the trend and performing the mapping techniques, top five open access and commercial journals of AI have been chosen based on their citescore driven ranking. The work includes 6880 articles published in the specified period for analysis. The trend is based on Country-wise publications, year-wise publications, topical terms in AI, top-cited articles, prominent authors, major institutions, involvement of industries in AI and Indian appearance. The results show that compared to open access journals; commercial journals have a higher citescore and number of articles published over the years
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00352</link><description>&lt;p&gt;
MetaGPT: &#20803;&#32534;&#31243;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00352
&lt;/p&gt;
&lt;p&gt;
MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#65292;&#33258;&#21160;&#20219;&#21153;&#35299;&#20915;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#32570;&#20047;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#25506;&#32034;&#21644;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#24187;&#35273;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#26102;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#23548;&#33268;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#37319;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#20316;&#20026;&#20803;&#32534;&#31243;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaGPT&#39318;&#20808;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOPs&#65289;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#12290;&#28982;&#21518;&#65292;&#23427;&#36827;&#19968;&#27493;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#24179;&#34892;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MetaGPT&#21033;&#29992;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>AsdKB&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#30142;&#30149;&#21644;&#35786;&#26029;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.16773</link><description>&lt;p&gt;
AsdKB: &#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder. (arXiv:2307.16773v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16773
&lt;/p&gt;
&lt;p&gt;
AsdKB&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#30142;&#30149;&#21644;&#35786;&#26029;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20415;&#25463;&#22320;&#33719;&#21462;&#26377;&#20851;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#30693;&#35782;&#24182;&#24110;&#21161;&#20854;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AsdKB&#65292;&#19968;&#20010;&#20851;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#12290;&#35813;&#30693;&#35782;&#24211;&#24314;&#31435;&#22312;&#22810;&#31181;&#26469;&#28304;&#30340;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;1&#65289;&#20174;SNOMED CT&#21644;ICD-10&#30340;&#20020;&#24202;&#25551;&#36848;&#20013;&#33719;&#24471;&#30340;&#30142;&#30149;&#30693;&#35782;&#65292;2&#65289;&#20174;DSM-5&#21644;&#31038;&#20250;&#32452;&#32455;&#21644;&#21307;&#23398;&#30740;&#31350;&#26426;&#26500;&#25512;&#33616;&#30340;&#19981;&#21516;&#31579;&#36873;&#24037;&#20855;&#20013;&#33719;&#24471;&#30340;&#35786;&#26029;&#30693;&#35782;&#65292;&#20197;&#21450;3&#65289;&#26469;&#33258;&#32593;&#32476;&#19978;&#19987;&#19994;&#21307;&#29983;&#21644;&#21307;&#38498;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;AsdKB&#21253;&#21547;&#26412;&#20307;&#30693;&#35782;&#21644;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807; https://w3id.org/asdkb/ &#20316;&#20026;&#38142;&#25509;&#25968;&#25454;&#36827;&#34892;&#35775;&#38382;&#12290;AsdKB&#30340;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#26469;&#36827;&#34892;&#28436;&#31034;&#65292;&#35813;&#21407;&#22411;&#21487;&#20197;&#36890;&#36807;&#27492;http URL&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
To easily obtain the knowledge about autism spectrum disorder and help its early screening and diagnosis, we create AsdKB, a Chinese knowledge base on autism spectrum disorder. The knowledge base is built on top of various sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical descriptions on mental and behavioural disorders, 2) the diagnostic knowledge from DSM-5 and different screening tools recommended by social organizations and medical institutes, and 3) the expert knowledge on professional physicians and hospitals from the Web. AsdKB contains both ontological and factual knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The potential applications of AsdKB are question answering, auxiliary diagnosis, and expert recommendation, and we illustrate them with a prototype which can be accessed at this http URL
&lt;/p&gt;</description></item><item><title>LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.16648</link><description>&lt;p&gt;
LLMs4OL: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16648
&lt;/p&gt;
&lt;p&gt;
LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLMs4OL&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26412;&#20307;&#23398;&#20064;&#65288;OL&#65289;&#12290;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30693;&#35782;&#39046;&#22495;&#20013;&#25429;&#25417;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;LLMs4OL&#33539;&#24335;&#30740;&#31350;&#20102;&#20197;&#19979;&#20551;&#35774;&#65306;\textit{LLMs&#33021;&#21542;&#26377;&#25928;&#24212;&#29992;&#23427;&#20204;&#30340;&#35821;&#35328;&#27169;&#24335;&#25429;&#25417;&#33021;&#21147;&#21040;OL&#20013;&#65292;&#36825;&#28041;&#21450;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;?} &#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLM&#27169;&#22411;&#26063;&#32676;&#65292;&#38024;&#23545;&#19977;&#20010;&#20027;&#35201;&#30340;OL&#20219;&#21153;&#65306;&#26415;&#35821;&#31867;&#22411;&#21010;&#20998;&#12289;&#23618;&#32423;&#21457;&#29616;&#21644;&#38750;&#23618;&#32423;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#28085;&#30422;&#20102;&#26412;&#20307;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#21253;&#25324;WordNet&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#30693;&#35782;&#12289;GeoNames&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#21644;UMLS&#20013;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65288;SEED&#65289;&#32467;&#21512;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15801</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Primitive Skill-based Robot Learning from Human Evaluative Feedback. (arXiv:2307.15801v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15801
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35780;&#20215;&#21453;&#39304;&#30340;&#21407;&#22987;&#25216;&#33021;&#23548;&#21521;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65288;SEED&#65289;&#32467;&#21512;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#38271;&#26102;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#26102;&#38754;&#20020;&#30528;&#26679;&#26412;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;SEED&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21644;&#22522;&#20110;&#21407;&#22987;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#21644;&#38271;&#26102;&#31243;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#36890;&#36807;&#32467;&#21512;&#23427;&#20204;&#65292;SEED&#20943;&#23569;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#25152;&#38656;&#30340;&#20154;&#21147;&#25237;&#20837;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#21442;&#25968;&#21270;&#25216;&#33021;&#25552;&#20379;&#20102;&#23545;&#20195;&#29702;&#30340;&#39640;&#32423;&#24847;&#22270;&#30340;&#28165;&#26224;&#35270;&#22270;&#65292;&#20801;&#35768;&#20154;&#31867;&#22312;&#25191;&#34892;&#20043;&#21069;&#35780;&#20272;&#25216;&#33021;&#36873;&#25321;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#12290;&#20026;&#20102;&#35780;&#20272;SEED&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms face significant challenges when dealing with long-horizon robot manipulation tasks in real-world environments due to sample inefficiency and safety issues. To overcome these challenges, we propose a novel framework, SEED, which leverages two approaches: reinforcement learning from human feedback (RLHF) and primitive skill-based reinforcement learning. Both approaches are particularly effective in addressing sparse reward issues and the complexities involved in long-horizon tasks. By combining them, SEED reduces the human effort required in RLHF and increases safety in training robot manipulation with RL in real-world settings. Additionally, parameterized skills provide a clear view of the agent's high-level intentions, allowing humans to evaluate skill choices before they are executed. This feature makes the training process even safer and more efficient. To evaluate the performance of SEED, we conducted extensive experiments on five manipulation
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14510</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#27880;&#24847;&#21147;: &#29992;&#20110;&#24378;&#20581;&#30340;&#27169;&#25311;-&#30495;&#23454;&#35302;&#35273;&#25511;&#21046;&#30340;&#35302;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention of Robot Touch: Tactile Saliency Prediction for Robust Sim-to-Real Tactile Control. (arXiv:2307.14510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#30340;&#35302;&#35273;&#20256;&#24863;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#23616;&#37096;&#25509;&#35302;&#30340;&#20934;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#26679;&#30340;&#20219;&#21153;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#35302;&#35273;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#26426;&#22120;&#20154;&#35302;&#35273;&#30340;&#8220;&#35302;&#35273;&#26174;&#33879;&#24615;&#8221;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20154;&#31867;&#35302;&#35273;&#27880;&#24847;&#26426;&#21046;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35270;&#35273;&#26174;&#33879;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;&#31867;&#20284;&#20110;&#35270;&#35273;&#26174;&#33879;&#24615;&#65292;&#36825;&#20010;&#27010;&#24565;&#28041;&#21450;&#21040;&#36890;&#36807;&#35302;&#35273;&#20256;&#24863;&#22120;&#25429;&#25417;&#21040;&#30340;&#35302;&#35273;&#22270;&#20687;&#20013;&#35782;&#21035;&#20851;&#38190;&#20449;&#24687;&#12290;&#34429;&#28982;&#35270;&#35273;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20154;&#31867;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#30001;&#20110;&#35302;&#35273;&#22270;&#20687;&#30340;&#21453;&#30452;&#35273;&#27169;&#24335;&#65292;&#25163;&#21160;&#26631;&#35760;&#35302;&#35273;&#22270;&#20687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#32593;&#32476;&#32452;&#25104;&#30340;&#26032;&#26041;&#27861;: 1) &#25509;&#35302;&#28145;&#24230;&#32593;&#32476;&#65288;ConDepNet&#65289;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#25509;&#35302;&#28145;&#24230;&#22320;&#22270;&#20197;&#23450;&#20301;&#30495;&#23454;&#35302;&#35273;&#20013;&#30340;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution tactile sensing can provide accurate information about local contact in contact-rich robotic tasks. However, the deployment of such tasks in unstructured environments remains under-investigated. To improve the robustness of tactile robot control in unstructured environments, we propose and study a new concept: \textit{tactile saliency} for robot touch, inspired by the human touch attention mechanism from neuroscience and the visual saliency prediction problem from computer vision. In analogy to visual saliency, this concept involves identifying key information in tactile images captured by a tactile sensor. While visual saliency datasets are commonly annotated by humans, manually labelling tactile images is challenging due to their counterintuitive patterns. To address this challenge, we propose a novel approach comprised of three interrelated networks: 1) a Contact Depth Network (ConDepNet), which generates a contact depth map to localize deformation in a real tactile 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.03393</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#23581;&#35797;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#23558;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#36890;&#36807;&#28023;&#37327;&#30693;&#35782;&#26469;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#29983;&#25104;&#39044;&#27979;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22240;&#20854;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20197;&#25991;&#26412;&#33410;&#28857;&#23646;&#24615;&#20026;&#20027;&#30340;&#22270;&#23398;&#20064;&#26368;&#27969;&#34892;&#30340;&#27969;&#31243;&#20027;&#35201;&#20381;&#36182;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#21033;&#29992;&#27973;&#23618;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#21021;&#22987;&#33410;&#28857;&#34920;&#31034;&#65292;&#20294;&#23384;&#22312;&#36890;&#29992;&#30693;&#35782;&#21644;&#28145;&#21051;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#20855;&#26377;&#24191;&#27867;&#30340;&#24120;&#35782;&#21644;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24050;&#32463;&#39072;&#35206;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20004;&#31181;&#21487;&#33021;&#30340;&#27969;&#31243;&#65306;LLMs&#20316;&#20026;&#22686;&#24378;&#22120;&#21644;LLMs&#20316;&#20026;&#39044;&#27979;&#22120;&#12290;&#21069;&#32773;&#21033;&#29992;LLMs&#36890;&#36807;&#20854;&#28023;&#37327;&#30693;&#35782;&#22686;&#24378;&#33410;&#28857;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;GNNs&#29983;&#25104;&#39044;&#27979;&#12290;&#21518;&#32773;&#35797;&#22270;&#30452;&#25509;&#20351;&#29992;LLMs&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>TeleViT&#26159;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23395;&#33410;&#24615;&#37326;&#28779;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#65292;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10940</link><description>&lt;p&gt;
TeleViT: &#30005;&#32852;&#39537;&#21160;&#30340;Transformer&#25913;&#36827;&#20102;&#23395;&#33410;&#24615;&#37326;&#28779;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting. (arXiv:2306.10940v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10940
&lt;/p&gt;
&lt;p&gt;
TeleViT&#26159;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#23395;&#33410;&#24615;&#37326;&#28779;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#65292;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#21152;&#21095;&#65292;&#37326;&#28779;&#38382;&#39064;&#26085;&#30410;&#24694;&#21270;&#65292;&#38656;&#35201;&#37319;&#21462;&#20808;&#36827;&#30340;&#20027;&#21160;&#25514;&#26045;&#36827;&#34892;&#26377;&#25928;&#30340;&#32531;&#35299;&#12290;&#25552;&#21069;&#20960;&#21608;&#29978;&#33267;&#20960;&#20010;&#26376;&#39044;&#27979;&#37326;&#28779;&#23545;&#20110;&#35745;&#21010;&#26862;&#26519;&#29123;&#26009;&#31649;&#29702;&#12289;&#36164;&#28304;&#37319;&#36141;&#21644;&#37197;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26399;&#39044;&#27979;&#65292;&#24517;&#39035;&#37319;&#29992;&#33021;&#22815;&#32771;&#34385;&#22320;&#29699;&#31995;&#32479;&#22266;&#26377;&#30340;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;&#35760;&#24518;&#25928;&#24212;&#21644;&#30005;&#32852;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30005;&#32852;&#39537;&#21160;&#30340;&#35270;&#35273;Transformer&#65288;TeleViT&#65289;&#65292;&#33021;&#22815;&#23558;&#22320;&#29699;&#35270;&#20026;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#31995;&#32479;&#65292;&#23558;&#31934;&#32454;&#30340;&#23616;&#37096;&#23610;&#24230;&#36755;&#20837;&#19982;&#20840;&#29699;&#23610;&#24230;&#36755;&#20837;&#65288;&#22914;&#27668;&#20505;&#25351;&#25968;&#21644;&#31895;&#31890;&#24230;&#30340;&#20840;&#29699;&#21464;&#37327;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;TeleViT&#22312;&#20934;&#30830;&#39044;&#27979;&#21508;&#31181;&#39044;&#27979;&#31383;&#21475;&#19979;&#30340;&#20840;&#29699;&#28903;&#27585;&#38754;&#31215;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#25552;&#21069;&#22235;&#20010;&#26376;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfires are increasingly exacerbated as a result of climate change, necessitating advanced proactive measures for effective mitigation. It is important to forecast wildfires weeks and months in advance to plan forest fuel management, resource procurement and allocation. To achieve such accurate long-term forecasts at a global scale, it is crucial to employ models that account for the Earth system's inherent spatio-temporal interactions, such as memory effects and teleconnections. We propose a teleconnection-driven vision transformer (TeleViT), capable of treating the Earth as one interconnected system, integrating fine-grained local-scale inputs with global-scale inputs, such as climate indices and coarse-grained global variables. Through comprehensive experimentation, we demonstrate the superiority of TeleViT in accurately predicting global burned area patterns for various forecasting windows, up to four months in advance. The gain is especially pronounced in larger forecasting wind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05150</link><description>&lt;p&gt;
&#26114;&#36149;&#23884;&#22871;&#28784;&#30418;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#28784;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#30001;&#40657;&#31665;&#21644;&#30333;&#31665;&#20989;&#25968;&#32452;&#25104;&#30340;&#23884;&#22871;&#20989;&#25968;&#12290;&#32473;&#20986;&#20102;&#36825;&#31181;&#28784;&#30418;&#38382;&#39064;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#28784;&#30418;&#20248;&#21270;&#20844;&#24335;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#40657;&#31665;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30456;&#20284;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#20294;&#20056;&#20197;&#20381;&#36182;&#20110;&#25152;&#32771;&#34385;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#30340;&#24120;&#25968;&#20056;&#39033;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#24773;&#20917;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;&#24120;&#29992;&#30340;&#26680;&#20989;&#25968;&#65292;&#21518;&#24724;&#36793;&#30028;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#21040;&#26368;&#20248;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#40657;&#31665;&#20248;&#21270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#28784;&#30418;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#30740;&#31350;&#36719;&#27877;&#33740;&#31867;&#26377;&#26426;&#20307;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#32467;&#21512;&#34394;&#25311;&#27969;&#20307;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#36890;&#36807;&#27979;&#35797;&#26234;&#33021;&#30340;&#26041;&#24335;&#30740;&#31350;&#26377;&#26426;&#20307;&#30340;&#26234;&#33021;&#34892;&#20026;&#65292;&#22312;&#26410;&#26469;&#21487;&#20197;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.13425</link><description>&lt;p&gt;
EINCASM: &#36719;&#27877;&#33740;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#30340;&#26032;&#20852;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EINCASM: Emergent Intelligence in Neural Cellular Automaton Slime Molds. (arXiv:2305.13425v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#30740;&#31350;&#36719;&#27877;&#33740;&#31867;&#26377;&#26426;&#20307;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#32467;&#21512;&#34394;&#25311;&#27969;&#20307;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#36890;&#36807;&#27979;&#35797;&#26234;&#33021;&#30340;&#26041;&#24335;&#30740;&#31350;&#26377;&#26426;&#20307;&#30340;&#26234;&#33021;&#34892;&#20026;&#65292;&#22312;&#26410;&#26469;&#21487;&#20197;&#36827;&#19968;&#27493;&#28145;&#20837;&#30740;&#31350;&#36825;&#20123;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EINCASM&#65292;&#19968;&#20010;&#37319;&#29992;&#26032;&#39062;&#26694;&#26550;&#30740;&#31350;&#31867;&#20284;&#20110;&#36719;&#27877;&#33740;&#30340;&#26377;&#26426;&#20307;&#20013;&#26032;&#20852;&#26234;&#33021;&#30340;&#21407;&#22411;&#31995;&#32479;&#12290;EINCASM&#36890;&#36807;NEAT&#36827;&#21270;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#65292;&#20197;&#26368;&#22823;&#21270;&#21463;&#33829;&#20859;&#21644;&#33021;&#37327;&#25104;&#26412;&#25152;&#38480;&#21046;&#30340;&#32454;&#32990;&#29983;&#38271;&#12290;&#36825;&#20123;&#26377;&#26426;&#20307;&#21033;&#29992;&#34394;&#25311;&#27969;&#20307;&#26469;&#36816;&#36755;&#33829;&#20859;&#29289;&#36136;&#21644;&#21270;&#23398;&#20449;&#21495;&#65292;&#20197;&#21327;&#35843;&#22312;&#22797;&#26434;&#12289;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#30340;&#29983;&#38271;&#21644;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#30740;&#31350;&#35868;&#39064;&#12289;&#29289;&#29702;&#12289;&#36890;&#20449;&#12289;&#31454;&#20105;&#21644;&#21160;&#24577;&#24320;&#25918;&#24335;&#29615;&#22659;&#22914;&#20309;&#20419;&#36827;&#26234;&#33021;&#34892;&#20026;&#30340;&#20986;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#26377;&#26426;&#20307;&#26234;&#33021;&#24615;&#30340;&#21021;&#27493;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26356;&#24378;&#22823;&#30340;&#31995;&#32479;&#20351;&#29992;EINCASM&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20998;&#24067;&#24335;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents EINCASM, a prototype system employing a novel framework for studying emergent intelligence in organisms resembling slime molds. EINCASM evolves neural cellular automata with NEAT to maximize cell growth constrained by nutrient and energy costs. These organisms capitalize physically simulated fluid to transport nutrients and chemical-like signals to orchestrate growth and adaptation to complex, changing environments. Our framework builds the foundation for studying how the presence of puzzles, physics, communication, competition and dynamic open-ended environments contribute to the emergence of intelligent behavior. We propose preliminary tests for intelligence in such organisms and suggest future work for more powerful systems employing EINCASM to better understand intelligence in distributed dynamical systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#23545;&#26410;&#26469;&#30340;&#25910;&#20837;&#21069;&#26223;&#20445;&#25345;&#24754;&#35266;&#24577;&#24230;&#65292;&#36825;&#31181;&#24433;&#21709;&#24191;&#27867;&#23384;&#22312;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20013;&#12290;&#36825;&#20010;&#30740;&#31350;&#32473;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23398;&#29983;&#30340;&#25285;&#24551;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#23398;&#29983;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2305.11900</link><description>&lt;p&gt;
ChatGPT&#19982;&#21171;&#21160;&#21147;&#24066;&#22330;&#65306;&#25581;&#31034;AI&#35752;&#35770;&#23545;&#23398;&#29983;&#25910;&#20837;&#39044;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on Students' Earnings Expectations. (arXiv:2305.11900v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#23545;&#26410;&#26469;&#30340;&#25910;&#20837;&#21069;&#26223;&#20445;&#25345;&#24754;&#35266;&#24577;&#24230;&#65292;&#36825;&#31181;&#24433;&#21709;&#24191;&#27867;&#23384;&#22312;&#20110;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20013;&#12290;&#36825;&#20010;&#30740;&#31350;&#32473;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#20250;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23398;&#29983;&#30340;&#25285;&#24551;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#23398;&#29983;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36127;&#38754;&#21644;&#27491;&#38754; ChatGPT&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35752;&#35770;&#23545;&#32654;&#22269;&#23398;&#29983;&#39044;&#26399;&#21171;&#21160;&#24066;&#22330;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25509;&#35302;AI&#35752;&#35770;&#21518;&#65292;&#23398;&#29983;&#20449;&#24515;&#20250;&#38477;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#38405;&#35835;&#36127;&#38754;&#24773;&#32490;&#30340;&#35752;&#35770;&#25688;&#24405;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#26356;&#21152;&#26126;&#26174;&#12290;&#19982;STEM&#19987;&#19994;&#19981;&#21516;&#65292;&#38750;STEM&#39046;&#22495;&#30340;&#23398;&#29983;&#34920;&#29616;&#20986;&#19981;&#23545;&#31216;&#21644;&#24754;&#35266;&#30340;&#20449;&#20208;&#21464;&#21270;&#65292;&#34920;&#26126;&#20182;&#20204;&#21487;&#33021;&#24863;&#21463;&#21040;&#26032;&#20852;AI&#25216;&#26415;&#30340;&#24433;&#21709;&#26356;&#24378;&#12290;&#23545;&#20110;&#26410;&#26469;&#25910;&#20837;&#30340;&#24754;&#35266;&#20449;&#20208;&#26356;&#26032;&#20063;&#26222;&#36941;&#23384;&#22312;&#20110;&#21508;&#20010;&#24615;&#21035;&#21644;GPA&#27700;&#24179;&#20043;&#38388;&#65292;&#36825;&#34920;&#26126;&#25152;&#26377;&#23398;&#29983;&#32676;&#20307;&#37117;&#23384;&#22312;&#24191;&#27867;&#30340;AI&#25285;&#24551;&#12290;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#31649;&#29702;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21487;&#20197;&#23450;&#26399;&#19982;&#23398;&#29983;&#20114;&#21160;&#20197;&#35299;&#20915;&#20182;&#20204;&#30340;&#25285;&#24551;&#65292;&#24182;&#25913;&#36827;&#25945;&#32946;&#35838;&#31243;&#65292;&#20351;&#20182;&#20204;&#26356;&#22909;&#22320;&#20934;&#22791;&#26410;&#26469;&#65292;&#36825;&#20010;&#26410;&#26469;&#24517;&#28982;&#20250;&#34987;AI&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the causal impact of negatively and positively framed ChatGPT Artificial Intelligence (AI) discussions on US students' anticipated labor market outcomes. Our findings reveal students reduce their confidence regarding their future earnings prospects after exposure to AI debates, and this effect is more pronounced after reading discussion excerpts with a negative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric and pessimistic belief changes, suggesting that they might feel more vulnerable to emerging AI technologies. Pessimistic belief updates regarding future earnings are also prevalent across gender and GPA levels, indicating widespread AI concerns among all student subgroups. Educators, administrators, and policymakers may regularly engage with students to address their concerns and enhance educational curricula to better prepare them for a future that will be inevitably shaped by AI.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04312</link><description>&lt;p&gt;
&#20154;&#36824;&#26159;&#26426;&#22120;&#65306;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#30340;&#26085;&#24120;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Human or Machine: Reflections on Turing-Inspired Testing for the Everyday. (arXiv:2305.04312v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#30340;&#24320;&#21019;&#24615;&#35770;&#25991;&#12298;&#35745;&#31639;&#26426;&#22120;&#26800;&#19982;&#26234;&#33021;&#12299;&#20013;&#65292;&#33406;&#20262;&#183;&#22270;&#28789;&#24341;&#20837;&#20102;&#8220;&#27169;&#20223;&#28216;&#25103;&#8221;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#26234;&#33021;&#30340;&#27010;&#24565;&#12290;&#22270;&#28789;&#27979;&#35797;&#33258;&#37027;&#26102;&#20197;&#26469;&#19968;&#30452;&#26159;&#24191;&#27867;&#35752;&#35770;&#12289;&#23436;&#21892;&#21644;&#25193;&#23637;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#22238;&#36991;&#20102;&#20851;&#20110;&#26576;&#20010;&#29305;&#23450;&#26426;&#22120;&#26159;&#21542;&#33021;&#34987;&#26631;&#35760;&#20026;&#26234;&#33021;&#25110;&#33021;&#21542;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#21305;&#37197;&#20154;&#31867;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20294;&#21463;&#22270;&#28789;&#21551;&#21457;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30830;&#23450;&#26159;&#21542;&#27491;&#22312;&#19982;&#19968;&#20010;&#20154;&#25110;&#19968;&#20010;&#26426;&#22120;&#36827;&#34892;&#20132;&#20114;&#36825;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#21450;&#20854;&#21487;&#38752;&#31572;&#26696;&#30340;&#24212;&#29992;&#24863;&#21040;&#24863;&#20852;&#36259;&#65292;&#24182;&#24076;&#26395;&#21453;&#24605;&#20854;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#22270;&#28789;&#30340;&#21407;&#22987;&#27979;&#35797;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#24605;&#32500;&#23454;&#39564;&#65292;&#20294;&#26412;&#25991;&#35752;&#35770;&#30340;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#20855;&#26377;&#26126;&#26174;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#34429;&#28982;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#21019;&#36896;&#20986;&#33021;&#22815;&#32988;&#20219;&#25152;&#26377;&#30340;&#20154;&#31867;&#24037;&#20316;&#30340;&#26426;&#22120;&#20063;&#26410;&#21487;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
In his seminal paper "Computing Machinery and Intelligence", Alan Turing introduced the "imitation game" as part of exploring the concept of machine intelligence. The Turing Test has since been the subject of much analysis, debate, refinement and extension. Here we sidestep the question of whether a particular machine can be labeled intelligent, or can be said to match human capabilities in a given context. Instead, but inspired by Turing, we draw attention to the seemingly simpler challenge of determining whether one is interacting with a human or with a machine, in the context of everyday life. We are interested in reflecting upon the importance of this Human-or-Machine question and the use one may make of a reliable answer thereto. Whereas Turing's original test is widely considered to be more of a thought experiment, the Human-or-Machine question as discussed here has obvious practical significance. And while the jury is still not in regarding the possibility of machines that can m
&lt;/p&gt;</description></item><item><title>ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.00365</link><description>&lt;p&gt;
ReLBOT&#65306;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#26368;&#23567;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#24378;&#21270;&#23398;&#20064;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00365
&lt;/p&gt;
&lt;p&gt;
ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#12290;&#24403;&#26234;&#33021;&#24314;&#31569;&#25237;&#20837;&#20351;&#29992;&#26102;&#65292;&#27809;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#23384;&#22312;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#24403;RL&#20195;&#29702;&#26368;&#21021;&#25506;&#32034;&#20854;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#23427;&#21487;&#33021;&#20250;&#32473;&#24314;&#31569;&#23621;&#27665;&#24102;&#26469;&#37325;&#22823;&#19981;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLBOT&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#32467;&#21512;&#28145;&#24230;RL&#65292;&#20174;&#29616;&#26377;&#30340;&#20248;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#30693;&#35782;&#21040;&#26032;&#25237;&#20837;&#20351;&#29992;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28909;&#36523;&#26399;&#23545;&#24314;&#31569;&#29289;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#26524;&#65292;&#28909;&#36523;&#26399;&#30340;&#25345;&#32493;&#26102;&#38388;&#21487;&#25552;&#39640;6.2&#20493;&#65292;&#24182;&#19988;&#39044;&#27979;&#26041;&#24046;&#21487;&#25552;&#39640;132&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;POMDP&#35268;&#21010;&#30340;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#23548;&#20986;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#20043;&#38388;&#30340;&#20215;&#20540;&#20989;&#25968;&#36793;&#30028;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.02139</link><description>&lt;p&gt;
&#25968;&#25454;&#20851;&#32852;&#24863;&#30693;&#30340;POMDP&#35268;&#21010;&#19982;&#20551;&#35774;&#21098;&#26525;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Data Association Aware POMDP Planning with Hypothesis Pruning Performance Guarantees. (arXiv:2303.02139v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02139
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;POMDP&#35268;&#21010;&#30340;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#23548;&#20986;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#20043;&#38388;&#30340;&#20215;&#20540;&#20989;&#25968;&#36793;&#30028;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20943;&#26525;&#20551;&#35774;&#23376;&#38598;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#24182;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36816;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#35201;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#32780;&#36825;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340; POMDP &#27169;&#22411;&#20381;&#36182;&#20110;&#23436;&#20840;&#30693;&#35782;&#35266;&#27979;&#28304;&#30340;&#20551;&#35774;&#65292;&#21363;&#23436;&#20840;&#21487;&#35266;&#27979;&#25968;&#25454;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#32500;&#25252;&#22810;&#20010;&#25968;&#25454;&#20851;&#32852;&#20551;&#35774;&#65292;&#34920;&#31034;&#20026;&#20449;&#24565;&#28151;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#32852;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#20551;&#35774;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21098;&#26525;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25968;&#25454;&#20851;&#32852;&#30340;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#22522;&#20110;&#23436;&#25972;&#20551;&#35774;&#38598;&#19982;&#22522;&#20110;&#20551;&#35774;&#21098;&#26525;&#23376;&#38598;&#30340;&#20215;&#20540;&#20989;&#25968;&#20043;&#38388;&#23548;&#20986;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#20351;&#29992;&#20462;&#21098;&#30340;&#20551;&#35774;&#23376;&#38598;&#25152;&#36896;&#25104;&#30340;&#26368;&#22823;&#25439;&#22833;&#30340;&#32039;&#23494;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#65292;&#21516;&#26102;&#20445;&#25345;&#21512;&#29702;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents that operate in the real world must often deal with partial observability, which is commonly modeled as partially observable Markov decision processes (POMDPs). However, traditional POMDP models rely on the assumption of complete knowledge of the observation source, known as fully observable data association. To address this limitation, we propose a planning algorithm that maintains multiple data association hypotheses, represented as a belief mixture, where each component corresponds to a different data association hypothesis. However, this method can lead to an exponential growth in the number of hypotheses, resulting in significant computational overhead. To overcome this challenge, we introduce a pruning-based approach for planning with ambiguous data associations. Our key contribution is to derive bounds between the value function based on the complete set of hypotheses and the value function based on a pruned-subset of the hypotheses, enabling us to establish a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#29616;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01584</link><description>&lt;p&gt;
&#36827;&#21270;&#22686;&#24378;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27604;&#36739;&#20102;&#20960;&#31181;&#29616;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#25216;&#26415;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#36741;&#21161;&#38454;&#27573;&#65288;&#20063;&#31216;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#65289;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24433;&#21709;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#23545;&#32422;&#26463;&#26465;&#20214;&#19979;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#65292;&#24182;&#27979;&#37327;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20013;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#26579;&#33394;&#20307;&#20013;&#32534;&#30721;&#19981;&#21516;&#32452;&#21512;&#30340;&#22686;&#24378;&#25805;&#20316;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#21270;&#20248;&#21270;&#26426;&#21046;&#23547;&#27714;&#26368;&#20248;&#30340;&#22686;&#24378;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#37322;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and expla
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22270;&#24418;&#27880;&#24847;&#21147;&#22810;&#26234;&#33021;&#20307;&#33322;&#31354;&#26426;&#38431;&#33258;&#27835;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#31574;&#30053;&#65292;&#32771;&#34385;&#20195;&#29702;&#30340;&#24322;&#36136;&#24615;&#21644;&#33258;&#31169;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#21160;&#32593;&#32476;&#20013;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#35266;&#23519;&#19981;&#30830;&#23450;&#24615;&#30340;&#24314;&#27169;&#12290;&#36890;&#36807;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#30340;&#20998;&#25955;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2302.07337</link><description>&lt;p&gt;
&#22270;&#24418;&#27880;&#24847;&#21147;&#22810;&#26234;&#33021;&#20307;&#33322;&#31354;&#26426;&#38431;&#33258;&#27835;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility. (arXiv:2302.07337v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22270;&#24418;&#27880;&#24847;&#21147;&#22810;&#26234;&#33021;&#20307;&#33322;&#31354;&#26426;&#38431;&#33258;&#27835;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#31574;&#30053;&#65292;&#32771;&#34385;&#20195;&#29702;&#30340;&#24322;&#36136;&#24615;&#21644;&#33258;&#31169;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#21160;&#32593;&#32476;&#20013;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#35266;&#23519;&#19981;&#30830;&#23450;&#24615;&#30340;&#24314;&#27169;&#12290;&#36890;&#36807;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#30340;&#20998;&#25955;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#21160;&#24615;&#27491;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#39072;&#35206;&#24615;&#22478;&#24066;&#20132;&#36890;&#27169;&#24335;&#65292;&#29992;&#20110;&#36135;&#29289;&#21644;&#20056;&#23458;&#30340;&#31227;&#21160;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#33258;&#20027;&#26426;&#21160;&#25216;&#26415;&#26469;&#36866;&#24212;&#24555;&#36895;&#22686;&#38271;&#30340;&#26426;&#21160;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26426;&#21160;&#36710;&#38431;&#30340;&#24322;&#36136;&#24615;&#22686;&#21152;&#12289;&#26102;&#38388;&#21464;&#21270;&#30340;&#38656;&#27714;&#27169;&#24335;&#12289;&#26381;&#21153;&#21306;&#22495;&#30340;&#25193;&#23637;&#21644;&#36890;&#20449;&#38480;&#21046;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20808;&#36827;&#33322;&#31354;&#26426;&#21160;&#21327;&#35843;&#28216;&#25103;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32771;&#34385;&#20114;&#21160;&#20195;&#29702;&#30340;&#24322;&#36136;&#24615;&#21644;&#21830;&#19994;&#26426;&#21160;&#36710;&#38431;&#22266;&#26377;&#30340;&#33258;&#31169;&#24615;&#26469;&#21327;&#35843;&#19968;&#25903;&#31354;&#20013;&#39134;&#34892;&#22120;&#26426;&#38431;&#12290;&#20026;&#20102;&#23545;&#26426;&#21160;&#32593;&#32476;&#20013;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#35266;&#23519;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#22270;&#24418;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;HetGAT Enc-Dec&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#35813;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#30340;&#20998;&#25955;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous mobility is emerging as a new disruptive mode of urban transportation for moving cargo and passengers. However, designing scalable autonomous fleet coordination schemes to accommodate fast-growing mobility systems is challenging primarily due to the increasing heterogeneity of the fleets, time-varying demand patterns, service area expansions, and communication limitations. We introduce the concept of partially observable advanced air mobility games to coordinate a fleet of aerial vehicles by accounting for the heterogeneity of the interacting agents and the self-interested nature inherent to commercial mobility fleets. To model the complex interactions among the agents and the observation uncertainty in the mobility networks, we propose a novel heterogeneous graph attention encoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We train the policy by leveraging deep multi-agent reinforcement learning, allowing decentralized decision-making for the agents us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2212.05206</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#26032;&#20852;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20687;GPT-3&#36825;&#26679;&#30340;LLMs&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30452;&#35273;&#24778;&#20154;&#30456;&#20284;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30001;&#27492;&#24102;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#38519;&#20837;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35748;&#30693;&#21453;&#24605;&#27979;&#35797;&#65288;CRT&#65289;&#20197;&#21450;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#38169;&#35273;&#26469;&#25506;&#32034;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#30740;&#31350;LLMs&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20182;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.08549</link><description>&lt;p&gt;
&#22269;&#38469;&#31354;&#38388;&#31449;&#33258;&#21160;&#32039;&#24613;&#26080;&#23576;&#35299;&#20915;&#26041;&#26696;: &#24102;&#26377;Bi-GRU&#30340;(AED-ISS)
&lt;/p&gt;
&lt;p&gt;
Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#39063;&#31890;&#29289;&#23545;&#20202;&#22120;&#30340;&#21361;&#23475;&#38382;&#39064;&#65292;&#36890;&#36807;Bi-GRU&#31639;&#27861;&#26500;&#24314;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#65292;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#26377;&#28508;&#21147;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;PM2.5&#25110;PM0.3&#38382;&#39064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#39063;&#31890;&#29289;&#19981;&#20165;&#23545;&#29615;&#22659;&#21644;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#65292;&#32780;&#19988;&#23545;&#22269;&#38469;&#31354;&#38388;&#31449;&#19978;&#30340;&#20202;&#22120;&#20063;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22242;&#38431;&#26088;&#22312;&#23558;&#21508;&#31181;&#39063;&#31890;&#29289;&#27987;&#24230;&#19982;&#30913;&#22330;&#12289;&#28287;&#24230;&#12289;&#21152;&#36895;&#24230;&#12289;&#28201;&#24230;&#12289;&#21387;&#21147;&#21644;CO2&#27987;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;(EWS)&#65292;&#33021;&#22815;&#39044;&#27979;&#39063;&#31890;&#29289;&#27700;&#24179;&#65292;&#24182;&#20026;&#23431;&#33322;&#21592;&#25552;&#20379;&#20805;&#36275;&#30340;&#21453;&#24212;&#26102;&#38388;&#65292;&#20197;&#20445;&#25252;&#20182;&#20204;&#22312;&#26576;&#20123;&#23454;&#39564;&#20013;&#30340;&#20202;&#22120;&#65292;&#25110;&#32773;&#25552;&#39640;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65307;&#27492;&#22806;&#65292;&#25152;&#26500;&#24314;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21457;&#23637;&#20026;&#19982;&#28779;&#28798;&#30456;&#20851;&#30340;&#36965;&#24863;&#28895;&#38654;&#25253;&#35686;&#35013;&#32622;&#30340;&#21407;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23454;&#29616;Bi-GRU(&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;)&#31639;&#27861;&#65292;&#25910;&#38598;&#36807;&#21435;90&#20998;&#38047;&#30340;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#36229;&#36807;2.5&#24494;&#31859;&#30340;&#39063;&#31890;&#29289;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rising attention for the issue of PM2.5 or PM0.3, particulate matters have become not only a potential threat to both the environment and human, but also a harming existence to instruments onboard International Space Station (ISS). Our team is aiming to relate various concentration of particulate matters to magnetic fields, humidity, acceleration, temperature, pressure and CO2 concentration. Our goal is to establish an early warning system (EWS), which is able to forecast the levels of particulate matters and provides ample reaction time for astronauts to protect their instruments in some experiments or increase the accuracy of the measurements; In addition, the constructed model can be further developed into a prototype of a remote-sensing smoke alarm for applications related to fires. In this article, we will implement the Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for past 90 minutes and predict the levels of particulates which over 2.5 micromete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2210.07420</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#35745;&#21010;&#31283;&#20581;&#30340;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26434;&#20081;&#38382;&#39064;&#65292;&#22810;&#20010;&#21018;&#24615;&#20984;&#22810;&#36793;&#24418;&#29289;&#20307;&#38543;&#26426;&#25918;&#32622;&#22312;&#19968;&#20010;&#24179;&#38754;&#34920;&#38754;&#19978;&#65292;&#24517;&#39035;&#20351;&#29992;&#21333;&#20010;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#25235;&#21462;&#26041;&#24335;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36816;&#36755;&#21040;&#35013;&#31665;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#25705;&#25830;&#26469;&#22686;&#21152;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#35745;&#21010;&#31283;&#20581;&#30340;&#22810;&#29289;&#20307;&#25235;&#21462;&#12290;&#22312;&#29289;&#29702;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#20110;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#21457;&#29616;&#25104;&#21151;&#29575;&#22686;&#21152;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;&#19982;&#21333;&#20010;&#29289;&#20307;&#25235;&#21462;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;3.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#22312;&#22270;&#20687;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#29983;&#25104;&#26080;&#25928;&#35270;&#22270;&#21644;&#19981;&#21487;&#38752;&#30456;&#20284;&#24615;&#23545;&#30340;&#38480;&#21046;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.13964</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13964
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#22312;&#22270;&#20687;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#29983;&#25104;&#26080;&#25928;&#35270;&#22270;&#21644;&#19981;&#21487;&#38752;&#30456;&#20284;&#24615;&#23545;&#30340;&#38480;&#21046;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#25490;&#24207;&#30340;&#22270;&#24418;&#36719;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;Graph Contrastive Learning&#65292;GCL&#65289;&#24050;&#25104;&#20026;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20027;&#35201;&#20511;&#37492;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#30340;&#21407;&#29702;&#65306;&#36890;&#36807;&#25351;&#23450;&#23436;&#20840;&#30456;&#20284;&#30340;&#23545;&#26469;&#24314;&#27169;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#65292;&#36825;&#31181;&#33539;&#24335;&#36935;&#21040;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#38480;&#21046;&#65306;&#65288;1&#65289;&#29983;&#25104;&#30340;&#35270;&#22270;&#30340;&#26377;&#25928;&#24615;&#26080;&#27861;&#24471;&#21040;&#20445;&#35777;&#65306;&#22270;&#25200;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#36829;&#21453;&#35821;&#20041;&#21644;&#22270;&#25968;&#25454;&#20869;&#22312;&#25299;&#25169;&#24615;&#36136;&#30340;&#26080;&#25928;&#35270;&#22270;&#65307;&#65288;2&#65289;&#22312;&#22270;&#35270;&#22270;&#20013;&#25351;&#23450;&#23436;&#20840;&#30456;&#20284;&#30340;&#23545;&#26159;&#19981;&#21487;&#38752;&#30340;&#65306;&#23545;&#20110;&#25277;&#35937;&#21644;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#22270;&#25968;&#25454;&#65292;&#20154;&#20204;&#24456;&#38590;&#30452;&#35266;&#22320;&#20915;&#23450;&#32477;&#23545;&#30340;&#30456;&#20284;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;GCL&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#65306;GCL&#26159;&#21542;&#33021;&#26356;&#26377;&#25928;&#22320;&#36866;&#24212;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#21407;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has emerged as a promising approach in the realm of graph self-supervised learning. Prevailing GCL methods mainly derive from the principles of contrastive learning in the field of computer vision: modeling invariance by specifying absolutely similar pairs. However, when applied to graph data, this paradigm encounters two significant limitations: (1) the validity of the generated views cannot be guaranteed: graph perturbation may produce invalid views against semantics and intrinsic topology of graph data; (2) specifying absolutely similar pairs in the graph views is unreliable: for abstract and non-Euclidean graph data, it is difficult for humans to decide the absolute similarity and dissimilarity intuitively. Despite the notable performance of current GCL methods, these challenges necessitate a reevaluation: Could GCL be more effectively tailored to the intrinsic properties of graphs, rather than merely adopting principles from computer vision? In res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.02231</link><description>&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20559;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#21033;&#30410;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#36712;&#36857;&#27573;&#23545;&#20043;&#38388;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#24120;&#20551;&#35774;&#36825;&#20123;&#20154;&#31867;&#20559;&#22909;&#20165;&#30001;&#37096;&#20998;&#22238;&#25253;&#26469;&#20915;&#23450;&#65292;&#21363;&#27599;&#20010;&#36712;&#36857;&#27573;&#19978;&#30340;&#22870;&#21169;&#24635;&#21644;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20551;&#35774;&#23384;&#22312;&#32570;&#38519;&#65292;&#25552;&#20986;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#30001;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#26469;&#20915;&#23450;&#65292;&#36951;&#25022;&#26159;&#19968;&#31181;&#34913;&#37327;&#36712;&#36857;&#27573;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#20559;&#31163;&#31243;&#24230;&#30340;&#24230;&#37327;&#12290;&#22312;&#26681;&#25454;&#36951;&#25022;&#29983;&#25104;&#30340;&#26080;&#31351;&#22810;&#20010;&#20559;&#22909;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#35782;&#21035;&#21040;&#19982;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#31561;&#20215;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20197;&#21069;&#30340;&#37096;&#20998;&#22238;&#25253;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#32570;&#20047;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#23646;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#26469;&#35299;&#20915;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20851;&#31995;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13492</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#23398;&#20064;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sparse Graph Learning from Spatiotemporal Time Series. (arXiv:2205.13492v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#26469;&#35299;&#20915;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#20851;&#31995;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#26480;&#20986;&#25104;&#26524;&#34920;&#26126;&#65292;&#20851;&#31995;&#32422;&#26463;&#20026;&#31070;&#32463;&#39044;&#27979;&#26550;&#26500;&#24341;&#20837;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#34920;&#24449;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#20851;&#31995;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#20174;&#32780;&#20351;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22312;&#21518;&#32493;&#22788;&#29702;&#38454;&#27573;&#20013;&#20351;&#29992;&#21738;&#20010;&#20851;&#31995;&#22270;&#25104;&#20026;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26377;&#29702;&#35770;&#20381;&#25454;&#30340;&#27010;&#29575;&#35780;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20219;&#21153;&#25972;&#20010;&#36807;&#31243;&#30340;&#24615;&#33021;&#26469;&#23398;&#20064;&#20851;&#31995;&#20381;&#36182;&#30340;&#22270;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35780;&#20998;&#26799;&#24230;&#20272;&#35745;&#30340;&#24041;&#22266;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26799;&#24230;&#20272;&#35745;&#22120;&#35843;&#25972;&#20026;&#22270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outstanding achievements of graph neural networks for spatiotemporal time series analysis show that relational constraints introduce an effective inductive bias into neural forecasting architectures. Often, however, the relational information characterizing the underlying data-generating process is unavailable and the practitioner is left with the problem of inferring from data which relational graph to use in the subsequent processing stages. We propose novel, principled - yet practical - probabilistic score-based methods that learn the relational dependencies as distributions over graphs while maximizing end-to-end the performance at task. The proposed graph learning framework is based on consolidated variance reduction techniques for Monte Carlo score-based gradient estimation, is theoretically grounded, and, as we show, effective in practice. In this paper, we focus on the time series forecasting problem and show that, by tailoring the gradient estimators to the graph learning prob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#20351;&#29992;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#20316;&#20026;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#20107;&#20214;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12490</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#25552;&#39640;&#20107;&#20214;&#25277;&#21462;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improve Event Extraction via Self-Training with Gradient Guidance. (arXiv:2205.12490v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#20351;&#29992;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#20316;&#20026;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#20107;&#20214;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#19968;&#30452;&#26159;&#38480;&#21046;&#20107;&#20214;&#25277;&#21462;&#36827;&#23637;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#21453;&#39304;&#65288;STF&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#19982;&#30456;&#21516;&#21477;&#23376;&#30340;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#36827;&#34892;&#27604;&#36739;&#65292;&#20026;&#27599;&#20010;&#26032;&#20107;&#20214;&#39044;&#27979;&#33719;&#21462;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;STF&#21253;&#25324;&#65288;1&#65289;&#22312;&#29616;&#26377;&#20107;&#20214;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#20107;&#20214;&#25277;&#21462;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20197;&#39044;&#27979;&#26032;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#20266;&#35757;&#32451;&#26679;&#26412;&#65292;&#21644;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#20110;&#27599;&#20010;&#26032;&#39044;&#27979;&#30340;&#20107;&#20214;&#35302;&#21457;&#22120;&#12289;&#19968;&#20010;&#21442;&#25968;&#12289;&#23427;&#30340;&#21442;&#25968;&#35282;&#33394;&#20197;&#21450;&#23427;&#20204;&#22312;AMR&#22270;&#20013;&#30340;&#36335;&#24452;&#36827;&#34892;&#20272;&#35745;&#65292;&#20197;&#34920;&#31034;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#20860;&#23481;&#24615;&#20998;&#25968;&#36827;&#19968;&#27493;&#20316;&#20026;&#21453;&#39304;&#65292;&#40723;&#21169;&#25110;&#38459;&#27490;&#27169;&#22411;&#22312;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#20266;&#26631;&#31614;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions as pseudo training samples, and (2) a novel scoring model that takes in each new predicted event trigger, an argument, its argument role, as well as their paths in the AMR graph to estimate a compatibility score indicating the correctness of the pseudo label. The compatibility scores further act as feedback to encourage or discourage the model learning on the pseudo labels during self-training. Experimental results on three benchmark da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#24322;&#27493;&#36873;&#39033;&#25191;&#34892;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.15925</link><description>&lt;p&gt;
&#24322;&#27493;&#12289;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#24322;&#27493;&#36873;&#39033;&#25191;&#34892;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#36827;&#34892;&#21327;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20840;&#23616;&#29366;&#24577;&#30340;&#20013;&#22830;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#20302;&#32423;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#22312;&#20855;&#26377;&#22823;&#35268;&#27169;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#65292;&#23558;MAPG&#26041;&#27861;&#25193;&#23637;&#20026;&#20351;&#29992;&#26356;&#39640;&#32423;&#21035;&#30340;&#21160;&#20316;&#65288;&#20063;&#31216;&#20026;&#36873;&#39033;&#65289;&#20197;&#25552;&#39640;&#31574;&#30053;&#25628;&#32034;&#25928;&#29575;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#26426;&#22120;&#20154;&#36873;&#39033;&#25191;&#34892;&#36890;&#24120;&#26159;&#24322;&#27493;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#27493;&#39588;&#36873;&#25321;&#24182;&#23436;&#25104;&#23427;&#20204;&#30340;&#36873;&#39033;&#12290;&#36825;&#20351;&#24471;MAPG&#26041;&#27861;&#24456;&#38590;&#25512;&#23548;&#20986;&#19968;&#20010;&#20013;&#22830;&#31574;&#30053;&#24182;&#35780;&#20272;&#20854;&#26799;&#24230;&#65292;&#22240;&#20026;&#20013;&#22830;&#31574;&#30053;&#24635;&#26159;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#36873;&#25321;&#26032;&#30340;&#36873;&#39033;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#20195;&#34920;&#24615;&#30340;&#22522;&#20110;&#36873;&#39033;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#20256;&#36755;&#25110;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#38480;&#21046;&#35813;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05877</link><description>&lt;p&gt;
&#12298;&#34394;&#26500;&#30340;&#32763;&#36716;&#65306;&#26080;&#25968;&#25454;&#30340;&#20013;&#27602;&#32852;&#37030;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#20256;&#36755;&#25110;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25915;&#20987;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#65292;&#24182;&#38480;&#21046;&#35813;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#32852;&#37030;&#23398;&#20064;&#30340;&#25915;&#20987;&#21487;&#20197;&#20005;&#37325;&#38477;&#20302;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#38480;&#21046;&#36825;&#31181;&#26032;&#20852;&#23398;&#20064;&#27169;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#27169;&#24335;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38750;&#23450;&#21521;&#25915;&#20987;&#23545;&#35768;&#22810;&#22330;&#26223;&#26469;&#35828;&#37117;&#19981;&#23454;&#38469;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#25915;&#20987;&#32773;&#30693;&#36947;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#27599;&#20010;&#26356;&#26032;&#65292;&#25110;&#32773;&#25915;&#20987;&#32773;&#25317;&#26377;&#22823;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#26469;&#27169;&#20223;&#33391;&#24615;&#21442;&#19982;&#26041;&#30340;&#26356;&#26032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#38750;&#23450;&#21521;&#25915;&#20987;&#65288;DFA&#65289;&#65292;&#23427;&#36890;&#36807;&#21512;&#25104;&#24694;&#24847;&#25968;&#25454;&#26469;&#21046;&#36896;&#23545;&#25239;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#31363;&#21548;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#20256;&#36755;&#65292;&#20063;&#26080;&#38656;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;DFA&#30340;&#21464;&#20307;&#65292;&#21363;DFA-R&#21644;DFA-G&#65292;&#23427;&#20204;&#22312;&#22914;&#20309;&#26435;&#34913;&#38544;&#34109;&#24615;&#21644;&#25928;&#26524;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DFA-R&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#19968;&#20010;&#24694;&#24847;&#25968;&#25454;&#23618;&#26469;&#26368;&#23567;&#21270;&#20840;&#23616;&#27169;&#22411;&#25152;&#26377;&#36755;&#20986;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#32780;DFA-G&#21017;&#36890;&#36807;&#20132;&#20114;&#24335;&#35757;&#32451;&#24694;&#24847;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attacks on Federated Learning (FL) can severely reduce the quality of the generated models and limit the usefulness of this emerging learning paradigm that enables on-premise decentralized learning. However, existing untargeted attacks are not practical for many scenarios as they assume that i) the attacker knows every update of benign clients, or ii) the attacker has a large dataset to locally train updates imitating benign parties. In this paper, we propose a data-free untargeted attack (DFA) that synthesizes malicious data to craft adversarial models without eavesdropping on the transmission of benign clients at all or requiring a large quantity of task-specific training data. We design two variants of DFA, namely DFA-R and DFA-G, which differ in how they trade off stealthiness and effectiveness. Specifically, DFA-R iteratively optimizes a malicious data layer to minimize the prediction confidence of all outputs of the global model, whereas DFA-G interactively trains a malicious dat
&lt;/p&gt;</description></item><item><title>&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;Successor Representations (SR)&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#26469;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2110.15701</link><description>&lt;p&gt;
&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15701
&lt;/p&gt;
&lt;p&gt;
&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;Successor Representations (SR)&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#26469;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#28304;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#32487;&#25215;&#34920;&#31034;&#65288;SR&#65289;&#21450;&#20854;&#25193;&#23637;&#30340;&#32487;&#25215;&#29305;&#24449;&#65288;SF&#65289;&#26159;&#22312;&#22870;&#21169;&#20989;&#25968;&#22312;&#20219;&#21153;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#39046;&#22495;&#20013;&#26174;&#33879;&#30340;&#36801;&#31227;&#26426;&#21046;&#12290;&#23427;&#20204;&#37325;&#26032;&#35780;&#20272;&#20808;&#21069;&#23398;&#20064;&#31574;&#30053;&#22312;&#26032;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#39044;&#26399;&#22238;&#25253;&#65292;&#20197;&#20256;&#36882;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;SF&#26694;&#26550;&#36890;&#36807;&#23558;&#22870;&#21169;&#32447;&#24615;&#20998;&#35299;&#20026;&#32487;&#25215;&#29305;&#24449;&#21644;&#22870;&#21169;&#26435;&#37325;&#21521;&#37327;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;SR&#65292;&#24182;&#20801;&#35768;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;&#20294;&#36825;&#26679;&#20570;&#30340;&#20195;&#20215;&#26159;&#22870;&#21169;&#20989;&#25968;&#19982;&#32487;&#25215;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#23384;&#22312;&#36825;&#31181;&#32447;&#24615;&#20851;&#31995;&#30340;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SR&#34920;&#36798;&#26041;&#24335;&#65292;&#21363;&#23398;&#20064;&#32487;&#25215;&#29305;&#24449;&#30340;&#32047;&#31215;&#25240;&#25187;&#27010;&#29575;&#65292;&#31216;&#20026;&#32487;&#25215;&#29305;&#24449;&#34920;&#31034;&#65288;SFR&#65289;&#12290;&#20851;&#38190;&#26159;&#65292;SFR&#21487;&#20197;&#37325;&#26032;&#35780;&#20272;&#31574;&#30053;&#30340;&#39044;&#26399;&#22238;&#25253;
&lt;/p&gt;
&lt;p&gt;
Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#21644;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#30340;&#20869;&#37096;&#24046;&#24322;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#24182;&#36991;&#20813;&#20102;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#21644;&#39069;&#22806;&#30340;&#27880;&#37322;&#20803;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.10401</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification. (arXiv:2104.10401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27880;&#24847;&#21147;&#36719;&#20998;&#21306;&#32593;&#32476;&#30340;&#36710;&#36742;&#20877;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#21644;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#30340;&#20869;&#37096;&#24046;&#24322;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#24182;&#36991;&#20813;&#20102;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#21644;&#39069;&#22806;&#30340;&#27880;&#37322;&#20803;&#25968;&#25454;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20877;&#35782;&#21035;&#36890;&#36807;&#21306;&#20998;&#30456;&#21516;&#21644;&#19981;&#21516;&#36710;&#36742;&#30340;&#22270;&#20687;&#26469;&#23454;&#29616;&#35782;&#21035;&#12290;&#30001;&#20110;&#19981;&#21516;&#35270;&#35282;&#19979;&#30456;&#21516;&#36710;&#36742;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20869;&#37096;&#24046;&#24322;&#20197;&#21450;&#30456;&#20284;&#36710;&#36742;&#20043;&#38388;&#23384;&#22312;&#32454;&#24494;&#30340;&#22806;&#37096;&#24046;&#24322;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#31354;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#35270;&#35282;&#24863;&#30693;&#25110;&#37096;&#20998;&#29305;&#23450;&#29305;&#24449;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#22024;&#26434;&#30340;&#27880;&#24847;&#21147;&#22270;&#25110;&#38656;&#35201;&#26114;&#36149;&#30340;&#39069;&#22806;&#27880;&#37322;&#20803;&#25968;&#25454;&#65288;&#22914;&#20851;&#38190;&#28857;&#65289;&#26469;&#25552;&#39640;&#36136;&#37327;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#27934;&#23519;&#21147;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#23450;&#35270;&#35282;&#25110;&#36710;&#36742;&#37096;&#20214;&#30340;&#22810;&#27880;&#24847;&#21147;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#27880;&#24847;&#21147;&#20998;&#25903;&#30340;&#25968;&#37327;&#21644;&#24615;&#36136;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20877;&#35782;&#21035;&#20219;&#21153;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#36719;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#36710;&#36742;&#20877;&#35782;&#21035;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle re-identification helps in distinguishing between images of the same and other vehicles. It is a challenging process because of significant intra-instance differences between identical vehicles from different views and subtle inter-instance differences between similar vehicles. To solve this issue, researchers have extracted view-aware or part-specific features via spatial attention mechanisms, which usually result in noisy attention maps or otherwise require expensive additional annotation for metadata, such as key points, to improve the quality. Meanwhile, based on the researchers' insights, various handcrafted multi-attention architectures for specific viewpoints or vehicle parts have been proposed. However, this approach does not guarantee that the number and nature of attention branches will be optimal for real-world re-identification tasks. To address these problems, we proposed a new vehicle re-identification network based on a multiple soft attention mechanism for captu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2010.08657</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.08657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#38754;&#23545;&#19968;&#31995;&#21015;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#23398;&#20064;&#26032;&#31867;&#21035;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#31867;&#21035;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24120;&#24120;&#20250;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#25928;&#30340;&#26041;&#27861;&#21033;&#29992;&#23384;&#20648;&#22312;&#19968;&#20010;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#21516;&#26102;&#25193;&#23637;&#26368;&#32456;&#20998;&#31867;&#22120;&#33410;&#28857;&#20197;&#23481;&#32435;&#26032;&#30340;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#22266;&#23450;&#20998;&#31867;&#22120;&#26367;&#20195;&#20102;&#25193;&#23637;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#38454;&#27573;&#24320;&#22987;&#23601;&#21463;&#21040;&#20998;&#31867;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#19982;&#26631;&#20934;&#25193;&#23637;&#20998;&#31867;&#22120;&#30456;&#21453;&#65292;&#36825;&#26679;&#20570;&#26377;&#20197;&#19979;&#22909;&#22788;&#65306;(a)&#26410;&#26469;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#30340;&#19968;&#24320;&#22987;&#23601;&#33021;&#30475;&#21040;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#30340;&#27491;&#26679;&#26412;&#65307;(b)&#33021;&#22815;&#23398;&#20064;&#19981;&#38543;&#30528;&#26032;&#31867;&#21035;&#30340;&#21152;&#20837;&#32780;&#25913;&#21464;&#20854;&#20960;&#20309;&#37197;&#32622;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
&lt;/p&gt;</description></item></channel></rss>