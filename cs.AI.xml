<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SHACIRA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#39640;&#27700;&#24179;&#21387;&#32553;&#65292;&#36890;&#36807;&#37327;&#21270;&#28508;&#22312;&#26435;&#37325;&#21644;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#21387;&#32553;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#30340;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15848</link><description>&lt;p&gt;
SHACIRA: &#21487;&#25193;&#23637;&#30340;&#21704;&#24076;&#32593;&#26684;&#21387;&#32553;&#25216;&#26415;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations. (arXiv:2309.15848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15848
&lt;/p&gt;
&lt;p&gt;
SHACIRA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#39640;&#27700;&#24179;&#21387;&#32553;&#65292;&#36890;&#36807;&#37327;&#21270;&#28508;&#22312;&#26435;&#37325;&#21644;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#21387;&#32553;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#19978;&#30340;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#25110;&#31070;&#32463;&#22330;&#24050;&#25104;&#20026;&#32534;&#30721;&#22810;&#23186;&#20307;&#20449;&#21495;&#65288;&#22914;&#22270;&#20687;&#21644;&#36752;&#23556;&#22330;&#65289;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#30001;Instant-NGP&#25552;&#20986;&#30340;&#21487;&#23398;&#20064;&#29305;&#24449;&#32593;&#26684;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;INR&#26041;&#38754;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#23427;&#36890;&#36807;&#29992;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#26597;&#25214;&#34920;&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#19968;&#20010;&#26356;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#20195;&#19968;&#20010;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#32593;&#26684;&#30340;&#20869;&#23384;&#28040;&#32791;&#24456;&#22823;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23384;&#20648;&#21644;&#27969;&#23186;&#20307;&#24212;&#29992;&#30340;&#29942;&#39048;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SHACIRA&#65292;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36825;&#20123;&#29305;&#24449;&#32593;&#26684;&#36827;&#34892;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#20462;&#21098;/&#37327;&#21270;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#37327;&#21270;&#30340;&#28508;&#22312;&#26435;&#37325;&#23545;&#29305;&#24449;&#32593;&#26684;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#24212;&#29992;&#29109;&#27491;&#21017;&#21270;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#39640;&#27700;&#24179;&#21387;&#32553;&#12290;&#23545;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datase
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#12290;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#65306;&#30446;&#21069;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65311;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#26080;&#25928;&#65292;LLM&#26159;&#21542;&#33021;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#38450;&#24481;&#25163;&#27573;&#65311;&#22914;&#26524;&#21069;&#20004;&#31181;&#31574;&#30053;&#22833;&#36133;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#65311;</title><link>http://arxiv.org/abs/2309.15847</link><description>&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65306;&#22312;LLM&#26102;&#20195;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Disinformation Detection: An Evolving Challenge in the Age of LLMs. (arXiv:2309.15847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15847
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25152;&#24102;&#26469;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#12290;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#65306;&#30446;&#21069;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65311;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#26080;&#25928;&#65292;LLM&#26159;&#21542;&#33021;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#38450;&#24481;&#25163;&#27573;&#65311;&#22914;&#26524;&#21069;&#20004;&#31181;&#31574;&#30053;&#22833;&#36133;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#19968;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#20652;&#29983;&#20102;&#21464;&#38761;&#24615;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#23041;&#32961;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#28389;&#29992;LLM&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#32773;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#39640;&#24230;&#26377;&#35828;&#26381;&#21147;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#65292;&#25361;&#25112;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31995;&#32479;&#12290;&#27492;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#29616;&#26377;&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#25216;&#26415;&#33021;&#22815;&#21487;&#38752;&#22320;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#31243;&#24230;&#26159;&#22810;&#23569;&#65311; &#65288;2&#65289;&#22914;&#26524;&#20256;&#32479;&#25216;&#26415;&#35777;&#26126;&#25928;&#26524;&#36739;&#24046;&#65292;LLM&#26412;&#36523;&#26159;&#21542;&#21487;&#20197;&#34987;&#21033;&#29992;&#20316;&#20026;&#23545;&#25239;&#20808;&#36827;&#34394;&#20551;&#20449;&#24687;&#30340;&#24378;&#22823;&#38450;&#24481;&#65311; &#65288;3&#65289;&#22914;&#26524;&#36825;&#20004;&#31181;&#31574;&#30053;&#37117;&#22833;&#25928;&#65292;&#21487;&#20197;&#25552;&#20986;&#20160;&#20040;&#26032;&#30340;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#23041;&#32961;&#65311;&#23545;&#20110;&#34394;&#20551;&#20449;&#24687;&#30340;&#24418;&#25104;&#21644;&#26816;&#27979;&#36827;&#34892;&#20102;&#25972;&#20307;&#25506;&#32034;&#26469;&#25512;&#21160;&#36825;&#19968;&#34892;
&lt;/p&gt;
&lt;p&gt;
The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15840</link><description>&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;AI&#35854;&#35328;&#65306;&#36890;&#36807;&#38382;&#26080;&#20851;&#38382;&#39064;&#22312;&#40657;&#30418;LLMs&#20013;&#36827;&#34892;&#35854;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. (arXiv:2309.15840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#39640;&#31934;&#30830;&#24230;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#26080;&#20851;&#30340;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#19968;&#20010;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#65292;&#35813;&#26816;&#27979;&#22120;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#8220;&#35828;&#35854;&#8221;&#65292;&#20063;&#23601;&#26159;&#22312;&#26126;&#30693;&#36947;&#30495;&#30456;&#30340;&#24773;&#20917;&#19979;&#36755;&#20986;&#34394;&#20551;&#38472;&#36848;&#12290;&#24403;&#25351;&#31034;&#36755;&#20986;&#38169;&#35823;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#8220;&#35828;&#35854;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35854;&#35328;&#26816;&#27979;&#22120;&#65292;&#26082;&#19981;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#28608;&#27963;&#65288;&#40657;&#30418;&#65289;&#65292;&#20063;&#19981;&#38656;&#35201;&#20107;&#23454;&#38382;&#39064;&#30340;&#30495;&#30456;&#30693;&#35782;&#12290;&#36825;&#20010;&#26816;&#27979;&#22120;&#36890;&#36807;&#22312;&#24576;&#30097;&#26377;&#35854;&#35328;&#30340;&#24773;&#20917;&#19979;&#38382;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#26080;&#20851;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#23558;LLM&#30340;&#26159;/&#21542;&#31572;&#26696;&#36755;&#20837;&#21040;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#20013;&#26469;&#24037;&#20316;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#36825;&#20010;&#35854;&#35328;&#26816;&#27979;&#22120;&#38750;&#24120;&#20934;&#30830;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#22320;&#36890;&#29992;&#12290;&#24403;&#22312;&#21333;&#19968;&#24773;&#22659;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451; - &#20419;&#20351;GPT-3.5&#22312;&#20107;&#23454;&#38382;&#39064;&#19978;&#25746;&#35854; - &#35813;&#26816;&#27979;&#22120;&#21487;&#20197;&#25512;&#24191;&#21040;&#20197;&#19979;&#24773;&#20917;&#65306;&#65288;1&#65289;&#20854;&#20182;LLM&#26550;&#26500;&#65292;&#65288;2&#65289;&#32454;&#35843;&#20026;&#35828;&#35854;&#30340;LLMs&#65292;&#65288;3&#65289;&#35844;&#23194;&#30340;&#35854;&#35328;&#65292;&#21644;&#65288;4&#65289;&#20986;&#29616;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#35854;&#35328;&#65292;&#27604;&#22914;&#38144;&#21806;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29305;&#27530;&#30340;&#19982;&#35854;&#35328;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can "lie", which we define as outputting false statements despite "knowing" the truth in a demonstrable sense. LLMs might "lie", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural pa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20799;&#31461;&#22312;AI&#38382;&#39064;&#26500;&#24605;&#20013;&#30340;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20799;&#31461;&#22914;&#20309;&#21521;&#33258;&#24049;&#35774;&#35745;&#30340;AI&#31995;&#32479;&#20013;&#34701;&#20837;&#20215;&#20540;&#35266;&#65292;&#20026;&#26085;&#24120;&#27963;&#21160;&#25552;&#20379;&#25903;&#25345;&#65292;&#21253;&#25324;&#23545;&#39640;&#32423;&#31995;&#32479;&#26234;&#33021;&#30340;&#38656;&#27714;&#21644;&#23545;&#24773;&#24863;&#26816;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.15839</link><description>&lt;p&gt;
&#26816;&#35270;&#20799;&#31461;&#22312;AI&#38382;&#39064;&#26500;&#24605;&#20013;&#20307;&#29616;&#30340;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Examining the Values Reflected by Children during AI Problem Formulation. (arXiv:2309.15839v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15839
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20799;&#31461;&#22312;AI&#38382;&#39064;&#26500;&#24605;&#20013;&#30340;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20799;&#31461;&#22914;&#20309;&#21521;&#33258;&#24049;&#35774;&#35745;&#30340;AI&#31995;&#32479;&#20013;&#34701;&#20837;&#20215;&#20540;&#35266;&#65292;&#20026;&#26085;&#24120;&#27963;&#21160;&#25552;&#20379;&#25903;&#25345;&#65292;&#21253;&#25324;&#23545;&#39640;&#32423;&#31995;&#32479;&#26234;&#33021;&#30340;&#38656;&#27714;&#21644;&#23545;&#24773;&#24863;&#26816;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20799;&#31461;&#22312;&#35832;&#22914;&#21487;&#25945;&#25480;&#26426;&#22120;&#31561;AI&#30028;&#38754;&#35774;&#35745;&#20013;&#30340;&#35774;&#35745;&#36807;&#31243;&#21644;&#20215;&#20540;&#35266;&#65292;&#26377;&#21161;&#20110;&#22686;&#21152;&#36825;&#20123;&#27963;&#21160;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#25351;&#23548;&#26410;&#26469;&#25216;&#26415;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#19968;&#20010;&#25913;&#32534;&#30340;&#25925;&#20107;&#26495;&#35774;&#35745;&#20250;&#35805;&#65292;&#19968;&#20010;&#30001;5&#21517;&#24180;&#40836;&#22312;7-13&#23681;&#30340;&#20799;&#31461;&#21644;&#25104;&#20154;&#20849;&#21516;&#35774;&#35745;&#32773;&#32452;&#25104;&#30340;&#22242;&#38431;&#65292;&#21442;&#19982;&#20102;AI&#38382;&#39064;&#26500;&#24605;&#27963;&#21160;&#65292;&#20182;&#20204;&#24819;&#35937;&#20102;&#33258;&#24049;&#30340;&#21487;&#25945;&#25480;&#26426;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21033;&#29992;&#20102;&#19968;&#20010;&#24050;&#32463;&#24314;&#31435;&#30340;&#24515;&#29702;&#20215;&#20540;&#26694;&#26550;&#65288;Rokeach&#20215;&#20540;&#35843;&#26597;&#65289;, &#25581;&#31034;&#20102;&#20799;&#31461;&#22914;&#20309;&#22312;&#20182;&#20204;&#33258;&#24049;&#35774;&#35745;&#30340;AI&#31995;&#32479;&#20013;&#27010;&#24565;&#21270;&#21644;&#34701;&#20837;&#20182;&#20204;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#25903;&#25345;&#20182;&#20204;&#30340;&#26085;&#24120;&#27963;&#21160;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#25552;&#20986;&#30340;&#24819;&#27861;&#38656;&#35201;&#20808;&#36827;&#30340;&#31995;&#32479;&#26234;&#33021;&#65292;&#20363;&#22914;&#24773;&#24863;&#26816;&#27979;&#21644;&#29702;&#35299;&#29992;&#25143;&#30340;&#31038;&#20132;&#20851;&#31995;&#12290;&#24213;&#23618;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#31181;&#27169;&#24335;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#25968;&#25454;&#25110;&#39044;&#27979;&#36127;&#38754;&#20107;&#20214;&#26469;&#20462;&#22797;&#20219;&#20309;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how children design and what they value in AI interfaces that allow them to explicitly train their models such as teachable machines, could help increase such activities' impact and guide the design of future technologies. In a co-design session using a modified storyboard, a team of 5 children (aged 7-13 years) and adult co-designers, engaged in AI problem formulation activities where they imagine their own teachable machines. Our findings, leveraging an established psychological value framework (the Rokeach Value Survey), illuminate how children conceptualize and embed their values in AI systems that they themselves devise to support their everyday activities. Specifically, we find that children's proposed ideas require advanced system intelligence, e.g. emotion detection and understanding the social relationships of a user. The underlying models could be trained under multiple modalities and any errors would be fixed by adding more data or by anticipating negative exam
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;OrthoPlanes&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#22270;&#20013;&#32534;&#30721;&#32454;&#31890;&#24230;&#30340;3D&#20449;&#24687;&#26469;&#29983;&#25104;&#30495;&#23454;&#19988;&#35270;&#22270;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;OrthoPlanes&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35270;&#35282;&#21644;&#21512;&#25104;&#20855;&#26377;&#39640;&#24230;&#31354;&#38388;&#33258;&#30001;&#24230;&#30340;&#20851;&#33410;&#23545;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;SHHQ&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15830</link><description>&lt;p&gt;
OrthoPlanes: &#19968;&#31181;&#25913;&#36827; GANs &#19977;&#32500;&#24863;&#30693;&#33021;&#21147;&#30340;&#26032;&#22411;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs. (arXiv:2309.15830v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26041;&#27861;OrthoPlanes&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#22270;&#20013;&#32534;&#30721;&#32454;&#31890;&#24230;&#30340;3D&#20449;&#24687;&#26469;&#29983;&#25104;&#30495;&#23454;&#19988;&#35270;&#22270;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;OrthoPlanes&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35270;&#35282;&#21644;&#21512;&#25104;&#20855;&#26377;&#39640;&#24230;&#31354;&#38388;&#33258;&#30001;&#24230;&#30340;&#20851;&#33410;&#23545;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;SHHQ&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;2D&#22270;&#20687;&#38598;&#21512;&#20013;&#29983;&#25104;&#20855;&#26377;&#31934;&#32454;&#20960;&#20309;&#32467;&#26500;&#30340;&#30495;&#23454;&#19988;&#35270;&#22270;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#26174;&#24335;-&#38544;&#24335;&#34920;&#31034;&#65292;&#31216;&#20026;OrthoPlanes&#65292;&#23427;&#22312;&#29305;&#24449;&#22270;&#20013;&#32534;&#30721;&#20102;&#32454;&#31890;&#24230;&#30340;3D&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;2D StyleGANs&#39640;&#25928;&#29983;&#25104;&#12290;&#19982;&#20197;&#21069;&#30340;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28165;&#26224;&#26126;&#30830;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#39640;&#24230;&#31354;&#38388;&#33258;&#30001;&#24230;&#30340;&#20851;&#33410;&#23545;&#35937;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FFHQ&#21644;SHHQ&#25968;&#25454;&#38598;&#19978;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for generating realistic and view-consistent images with fine geometry from 2D image collections. Our method proposes a hybrid explicit-implicit representation called \textbf{OrthoPlanes}, which encodes fine-grained 3D information in feature maps that can be efficiently generated by modifying 2D StyleGANs. Compared to previous representations, our method has better scalability and expressiveness with clear and explicit information. As a result, our method can handle more challenging view-angles and synthesize articulated objects with high spatial degree of freedom. Experiments demonstrate that our method achieves state-of-the-art results on FFHQ and SHHQ datasets, both quantitatively and qualitatively. Project page: \url{https://orthoplanes.github.io/}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15817</link><description>&lt;p&gt;
&#20351;&#29992;LM&#27169;&#25311;&#27801;&#30418;&#35782;&#21035;LM&#20195;&#29702;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;LM&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#21644;&#24320;&#21457;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27979;&#35797;LM&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#21644;&#23547;&#25214;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20195;&#29702;&#21644;&#24037;&#20855;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20363;&#22914;ChatGPT&#25554;&#20214;&#65292;&#20351;&#24471;&#20195;&#29702;&#20855;&#22791;&#20102;&#20016;&#23500;&#30340;&#21151;&#33021;&#65292;&#20294;&#20063;&#25918;&#22823;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22914;&#27844;&#38706;&#31169;&#20154;&#25968;&#25454;&#25110;&#24341;&#21457;&#36130;&#21153;&#25439;&#22833;&#12290;&#35782;&#21035;&#36825;&#20123;&#39118;&#38505;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#23454;&#26045;&#24037;&#20855;&#65292;&#25163;&#21160;&#35774;&#32622;&#27599;&#20010;&#27979;&#35797;&#22330;&#26223;&#30340;&#29615;&#22659;&#65292;&#24182;&#25214;&#21040;&#39118;&#38505;&#26696;&#20363;&#12290;&#38543;&#30528;&#24037;&#20855;&#21644;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#27979;&#35797;&#36825;&#20123;&#20195;&#29702;&#30340;&#39640;&#25104;&#26412;&#23558;&#20351;&#23547;&#25214;&#39640;&#39118;&#38505;&#12289;&#38271;&#23614;&#39118;&#38505;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToolEmu&#65306;&#19968;&#20010;&#20351;&#29992;LM&#26469;&#27169;&#25311;&#24037;&#20855;&#25191;&#34892;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23454;&#20363;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;LM&#20195;&#29702;&#36827;&#34892;&#21508;&#31181;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#27979;&#35797;&#12290;&#38500;&#20102;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#26816;&#26597;&#20195;&#29702;&#30340;&#22833;&#36133;&#24182;&#37327;&#21270;&#30456;&#20851;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#27979;&#35797;&#20102;&#24037;&#20855;&#27169;&#25311;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#24182;&#21457;&#29616;&#20102;6&#20010;...
&lt;/p&gt;
&lt;p&gt;
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#65292;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65292;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#39044;&#35745;&#35813;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#39564;&#35777;&#21518;&#65292;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15803</link><description>&lt;p&gt;
ANNCRIPS:&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#21644;&#29983;&#23384;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ANNCRIPS: Artificial Neural Networks for Cancer Research In Prediction &amp; Survival. (arXiv:2309.15803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#65292;&#38477;&#20302;&#20551;&#38451;&#24615;&#29575;&#65292;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#12290;&#39044;&#35745;&#35813;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#39564;&#35777;&#21518;&#65292;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#26159;50&#23681;&#21450;&#20197;&#19978;&#30007;&#24615;&#20013;&#24120;&#35265;&#30340;&#24694;&#24615;&#32959;&#30244;&#12290;&#30446;&#21069;&#30340;&#35786;&#26029;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#34880;&#28082;&#27979;&#35797;&#12289;&#21069;&#21015;&#33146;&#29305;&#24322;&#24615;&#25239;&#21407;(PSA)&#27700;&#24179;&#21644;&#30452;&#32928;&#25351;&#26816;(DRE)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#30340;&#20551;&#38451;&#24615;&#29575;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#30340;&#26234;&#33021;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23637;&#31034;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#26089;&#26399;&#21457;&#29616;&#21069;&#21015;&#33146;&#30284;&#12289;&#20419;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21450;&#26102;&#24178;&#39044;&#30340;&#26032;&#22411;&#25968;&#23398;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30340;&#23454;&#26045;&#23637;&#31034;&#20102;&#38477;&#20302;&#20551;&#38451;&#24615;&#21457;&#29983;&#29575;&#12289;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39044;&#35265;&#22312;&#36827;&#19968;&#27493;&#25913;&#36827;&#12289;&#24191;&#27867;&#27979;&#35797;&#21644;&#39564;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#30340;&#12289;&#24066;&#22330;&#21270;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prostate cancer is a prevalent malignancy among men aged 50 and older. Current diagnostic methods primarily rely on blood tests, PSA:Prostate-Specific Antigen levels, and Digital Rectal Examinations (DRE). However, these methods suffer from a significant rate of false positive results. This study focuses on the development and validation of an intelligent mathematical model utilizing Artificial Neural Networks (ANNs) to enhance the early detection of prostate cancer. The primary objective of this research paper is to present a novel mathematical model designed to aid in the early detection of prostate cancer, facilitating prompt intervention by healthcare professionals. The model's implementation demonstrates promising potential in reducing the incidence of false positives, thereby improving patient outcomes. Furthermore, we envision that, with further refinement, extensive testing, and validation, this model can evolve into a robust, marketable solution for prostate cancer detection. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;IBM Watson&#21644;Google AlphaGo&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;AI&#25216;&#26415;&#26377;&#21161;&#20110;&#23454;&#29616;&#26234;&#33021;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.15768</link><description>&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#26696;&#20363;&#30740;&#31350;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AI in Software Engineering: Case Studies and Prospects. (arXiv:2309.15768v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;IBM Watson&#21644;Google AlphaGo&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;AI&#25216;&#26415;&#26377;&#21161;&#20110;&#23454;&#29616;&#26234;&#33021;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#21508;&#20010;&#38454;&#27573;&#24212;&#29992;AI&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#36719;&#20214;&#20135;&#21697;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20063;&#26377;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;SE&#21644;AI&#30340;&#20132;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;SE&#21644;AI&#20043;&#38388;&#30340;&#20851;&#31995;&#38750;&#24120;&#24494;&#24369;&#65307;&#28982;&#32780;&#65292;&#19968;&#31181;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#24050;&#32463;&#22312;&#21478;&#19968;&#31181;&#39046;&#22495;&#20013;&#34987;&#37319;&#29992;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#36719;&#20214;&#20135;&#21697;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#26377;&#26234;&#33021;&#34892;&#20026;&#12290;&#26412;&#25991;&#20998;&#26512;&#12289;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;AI&#25216;&#26415;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24615;&#38382;&#39064;&#30340;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21363;IBM Watson&#21644;Google AlphaGo&#12290;&#22522;&#20110;&#23545;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#30340;&#20998;&#26512;&#65292;&#20351;&#29992;&#35832;&#22914;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;AI&#25216;&#26415;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#26377;&#21161;&#20110;&#26234;&#33021;&#21270;&#31995;&#32479;&#12290;Watson&#37319;&#29992;&#8220;&#20915;&#31574;&#25903;&#25345;&#8221;&#31574;&#30053;&#26469;&#24110;&#21161;&#20154;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and software engineering (SE) are two important areas in computer science. In recent years, researchers are trying to apply AI techniques in various stages of software development to improve the overall quality of software products. Moreover, there are also some researchers focus on the intersection between SE and AI. In fact, the relationship between SE and AI is very weak; however, methods and techniques in one area have been adopted in another area. More and more software products are capable of performing intelligent behaviour like human beings. In this paper, two cases studies which are IBM Watson and Google AlphaGo that use different AI techniques in solving real world challenging problems have been analysed, evaluated and compared. Based on the analysis of both case studies, using AI techniques such as deep learning and machine learning in software systems contributes to intelligent systems. Watson adopts 'decision making support' strategy to help hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20020;&#24202;&#23545;&#35805;&#30340;&#25688;&#35201;&#12290;&#36890;&#36807;&#36866;&#37197;&#22120;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15739</link><description>&lt;p&gt;
&#20307;&#39564;&#21644;&#35777;&#25454;&#26159;&#20986;&#33394;&#25688;&#35201;&#26426;&#22120;&#20154;&#30340;&#30524;&#30555;&#65281;&#26397;&#30528;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization. (arXiv:2309.15739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#34701;&#20837;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20020;&#24202;&#23545;&#35805;&#30340;&#25688;&#35201;&#12290;&#36890;&#36807;&#36866;&#37197;&#22120;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36828;&#31243;&#21307;&#30103;&#30340;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#21307;&#30103;&#20174;&#19994;&#32773;&#27491;&#20849;&#21516;&#21162;&#21147;&#24320;&#21457;&#21508;&#31181;&#25216;&#26415;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#21307;&#30103;&#25805;&#20316;&#65292;&#22914;&#35786;&#26029;&#25253;&#21578;&#29983;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#26681;&#25454;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#30340;&#20132;&#20114;&#65288;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#65289;&#29983;&#25104;&#23545;&#35805;&#30340;&#31616;&#27905;&#27010;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#21307;&#23398;&#39046;&#22495;&#35782;&#21035;&#21644;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#65288;MM-CliConSummation&#65289;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#36866;&#37197;&#22120;&#26469;&#27880;&#20837;&#30693;&#35782;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#32479;&#19968;&#34701;&#21512;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#24102;&#26377;&#24847;&#22270;&#12289;&#30151;&#29366;&#21644;&#25688;&#35201;&#27880;&#37322;&#30340;&#22810;&#27169;&#24577;&#22810;&#24847;&#22270;&#20020;&#24202;&#23545;&#35805;&#25688;&#35201;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#22823;&#37327;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#24471;&#20986;&#20197;&#19979;&#21457;&#29616;&#65306;(a)&#20851;&#38190;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
With the advancement of telemedicine, both researchers and medical practitioners are working hand-in-hand to develop various techniques to automate various medical operations, such as diagnosis report generation. In this paper, we first present a multi-modal clinical conversation summary generation task that takes a clinician-patient interaction (both textual and visual information) and generates a succinct synopsis of the conversation. We propose a knowledge-infused, multi-modal, multi-tasking medical domain identification and clinical conversation summary generation (MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and visual features and unify the fused feature vector using a gated mechanism. Furthermore, we developed a multi-modal, multi-intent clinical conversation summarization corpus annotated with intent, symptom, and summary. The extensive set of experiments, both quantitatively and qualitatively, led to the following findings: (a) critical significan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#35299;&#30721;&#22120;MindGPT&#65292;&#23427;&#33021;&#22815;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#26512;&#25152;&#35265;&#30340;&#35270;&#35273;&#21050;&#28608;&#24182;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2309.15729</link><description>&lt;p&gt;
MindGPT&#65306;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#35299;&#35835;&#25152;&#35265;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
MindGPT: Interpreting What You See with Non-invasive Brain Recordings. (arXiv:2309.15729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#35299;&#30721;&#22120;MindGPT&#65292;&#23427;&#33021;&#22815;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#26512;&#25152;&#35265;&#30340;&#35270;&#35273;&#21050;&#28608;&#24182;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#33041;&#35760;&#24405;&#35299;&#35835;&#25152;&#35265;&#30340;&#35270;&#35273;&#20869;&#23481;&#20855;&#26377;&#37325;&#35201;&#30340;&#31185;&#23398;&#21644;&#23454;&#36341;&#20215;&#20540;&#12290;&#24050;&#32463;&#20570;&#20986;&#20102;&#21162;&#21147;&#26469;&#20174;&#33041;&#20449;&#21495;&#20013;&#24674;&#22797;&#25152;&#35265;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#25110;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#30495;&#23454;&#21453;&#26144;&#35270;&#35273;&#20869;&#23481;&#12290;&#19982;&#37325;&#24314;&#20687;&#32032;&#32423;&#35270;&#35273;&#22270;&#20687;&#30456;&#27604;&#65292;&#35762;&#35805;&#26159;&#19968;&#31181;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#35270;&#35273;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MindGPT&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#23427;&#23558;&#24863;&#30693;&#21040;&#30340;&#35270;&#35273;&#21050;&#28608;&#35299;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#36890;&#36807;fMRI&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#35270;&#35273;&#24341;&#23548;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#21327;&#21516;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23558;&#28508;&#22312;&#30340;&#31070;&#32463;&#34920;&#31034;&#24341;&#23548;&#21040;&#25152;&#38656;&#30340;&#35821;&#35328;&#35821;&#20041;&#26041;&#21521;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21457;&#29616;MindGPT&#30340;&#31070;&#32463;&#34920;&#31034;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed as MindGPT, which interprets perceived visual stimuli into natural languages from fMRI signals. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism, which permits us to guide latent neural representations towards a desired language semantic direction in an end-to-end manner by the collaborative use of the large language model GPT. By doing so, we found that the neural representations of the MindGPT are explainable, which can be used 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#19981;&#21516;&#38454;&#27573;&#21644;&#35282;&#33394;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.15723</link><description>&lt;p&gt;
&#25105;&#20204;&#30446;&#21069;&#22788;&#20110;&#21738;&#20010;&#38454;&#27573;&#65311;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#29702;&#35299;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration. (arXiv:2309.15723v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20851;&#20110;&#19981;&#21516;&#38454;&#27573;&#21644;&#35282;&#33394;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21465;&#20107;&#22312;&#20256;&#36798;&#25968;&#25454;&#27934;&#23519;&#21147;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#38656;&#35201;&#20154;&#31867;&#21019;&#20316;&#32773;&#20855;&#22791;&#22810;&#26679;&#21270;&#30340;&#25216;&#33021;&#21644;&#30456;&#24403;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24191;&#27867;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#21644;&#22686;&#24378;&#20154;&#31867;&#22312;&#25968;&#25454;&#21465;&#20107;&#20013;&#30340;&#20316;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#26469;&#29702;&#35299;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#26469;&#30475;&#25968;&#25454;&#21465;&#20107;&#24037;&#20855;&#65292;&#36825;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#29616;&#26377;&#30340;&#21327;&#20316;&#24037;&#20855;&#35774;&#35745;&#30340;&#24605;&#32771;&#65292;&#20197;&#20419;&#36827;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#24182;&#20943;&#23569;&#20182;&#20204;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#29616;&#26377;&#24037;&#20855;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#26469;&#30475;&#65306;&#20854;&#26381;&#21153;&#20110;&#21465;&#20107;&#24037;&#20316;&#27969;&#31243;&#30340;&#38454;&#27573;&#65292;&#21253;&#25324;&#20998;&#26512;&#12289;&#35268;&#21010;&#12289;&#23454;&#26045;&#21644;&#27807;&#36890;&#65307;&#20197;&#21450;&#22312;&#27599;&#20010;&#38454;&#27573;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35282;&#33394;&#65292;&#22914;&#21019;&#20316;&#32773;&#12289;&#21161;&#25163;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#23457;&#21592;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#29616;&#26377;&#24037;&#20855;&#20013;&#30340;&#20849;&#21516;&#21327;&#20316;&#27169;&#24335;&#65292;&#24182;&#24635;&#32467;&#20102;&#20174;&#36825;&#20123;&#27169;&#24335;&#20013;&#24471;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these pattern
&lt;/p&gt;</description></item><item><title>Model Share AI&#26159;&#19968;&#20010;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;&#30340;&#38598;&#25104;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#12289;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#21644;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#37096;&#32626;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15719</link><description>&lt;p&gt;
Model Share AI: &#19968;&#20010;&#38598;&#25104;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python. (arXiv:2309.15719v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15719
&lt;/p&gt;
&lt;p&gt;
Model Share AI&#26159;&#19968;&#20010;&#29992;&#20110;&#21327;&#20316;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#26469;&#28304;&#36861;&#36394;&#21644;&#37096;&#32626;&#30340;&#38598;&#25104;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#12289;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#21644;&#33258;&#21160;&#21270;&#30340;&#27169;&#22411;&#37096;&#32626;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#21644;&#34892;&#19994;&#65292;&#20294;&#35768;&#22810;ML&#39033;&#30446;&#20174;&#27010;&#24565;&#39564;&#35777;&#38454;&#27573;&#23601;&#26080;&#27861;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Model Share AI&#65288;AIMS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#31616;&#21270;&#21327;&#20316;&#27169;&#22411;&#24320;&#21457;&#12289;&#27169;&#22411;&#26469;&#28304;&#36861;&#36394;&#21644;&#27169;&#22411;&#37096;&#32626;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#20854;&#20182;&#21151;&#33021;&#65292;&#20197;&#26368;&#22823;&#21270;ML&#30740;&#31350;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;AIMS&#20855;&#26377;&#21327;&#20316;&#39033;&#30446;&#31354;&#38388;&#21644;&#26631;&#20934;&#21270;&#30340;&#27169;&#22411;&#35780;&#20272;&#27969;&#31243;&#65292;&#26681;&#25454;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35780;&#20272;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#23545;&#27169;&#22411;&#25552;&#20132;&#36827;&#34892;&#25490;&#21517;&#65292;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#24320;&#21457;&#21644;&#20247;&#21253;&#12290;&#33258;&#21160;&#25429;&#33719;&#27169;&#22411;&#24615;&#33021;&#21644;&#21508;&#31181;&#27169;&#22411;&#20803;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26469;&#28304;&#36861;&#36394;&#24182;&#20801;&#35768;&#29992;&#25143;&#23398;&#20064;&#21644;&#20511;&#37492;&#20043;&#21069;&#30340;&#25552;&#20132;&#12290;&#27492;&#22806;&#65292;AIMS&#20801;&#35768;&#29992;&#25143;&#23558;&#22312;Scikit-Learn&#12289;TensorFlow Keras&#12289;PyTorch&#21644;ONNX&#20013;&#26500;&#24314;&#30340;ML&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#26102;&#30340;REST API&#21644;au
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has the potential to revolutionize a wide range of research areas and industries, but many ML projects never progress past the proof-of-concept stage. To address this issue, we introduce Model Share AI (AIMS), an easy-to-use MLOps platform designed to streamline collaborative model development, model provenance tracking, and model deployment, as well as a host of other functions aiming to maximize the real-world impact of ML research. AIMS features collaborative project spaces and a standardized model evaluation process that ranks model submissions based on their performance on unseen evaluation data, enabling collaborative model development and crowd-sourcing. Model performance and various model metadata are automatically captured to facilitate provenance tracking and allow users to learn from and build on previous submissions. Additionally, AIMS allows users to deploy ML models built in Scikit-Learn, TensorFlow Keras, PyTorch, and ONNX into live REST APIs and au
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.15714</link><description>&lt;p&gt;
ChatGPT-BCI&#65306;&#20351;&#29992;GPT&#12289;EEG&#21644;&#30524;&#21160;&#29983;&#29289;&#26631;&#35760;&#22120;&#22312;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#36827;&#34892;&#21333;&#35789;&#32423;&#31070;&#32463;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#29702;&#35299;&#35821;&#20041;&#35821;&#35328;&#24847;&#20041;&#30340;&#33021;&#21147;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#12290;&#36825;&#38656;&#35201;&#36328;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLMs&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22914;&#20309;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#31243;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#25552;&#20379;&#20851;&#20110;&#20010;&#20307;&#31070;&#32463;&#29366;&#24577;&#22312;&#35821;&#20041;&#20851;&#31995;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#25913;&#36827;&#20102;&#19982;&#20851;&#38190;&#23383;&#39640;&#30456;&#20851;&#24230;&#21644;&#20302;&#30456;&#20851;&#24230;&#30340;&#21333;&#35789;&#38405;&#35835;&#36807;&#31243;&#20013;&#19982;&#27880;&#35270;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#22312;12&#21517;&#21463;&#35797;&#32773;&#20013;&#65292;&#27492;&#21333;&#35789;&#32423;&#21035;&#20998;&#31867;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent explosion of large language models (LLMs), such as Generative Pretrained Transformers (GPT), the need to understand the ability of humans and machines to comprehend semantic language meaning has entered a new phase. This requires interdisciplinary research that bridges the fields of cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15649</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#21644;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;ASR&#21518;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#37325;&#26032;&#35780;&#20998;&#21644;&#38169;&#35823;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#25351;&#20196;&#25552;&#31034;&#35753;LLMs&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#65292;&#21253;&#25324;&#38646;-shot&#21644;&#23569;-shot&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#28608;&#27963;&#25552;&#31034;&#65288;TAP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#25351;&#20196;&#21644;&#28436;&#31034;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#22806;&#30340;&#20219;&#21153;&#65288;ATIS&#21644;WSJ&#65289;&#19978;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31532;&#19968;&#27425;&#25195;&#25551;&#31995;&#32479;&#21644;&#37325;&#26032;&#35780;&#20998;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#36890;&#36807;&#20923;&#32467;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#21487;&#20197;&#36798;&#21040;&#19982;&#39046;&#22495;&#35843;&#20248;&#30340;LMs&#37325;&#26032;&#35780;&#20998;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#25552;&#31034;&#25216;&#26415;&#19982;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20302;&#20110;N-best Oracle&#27700;&#24179;&#30340;&#38169;&#35823;&#29575;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26500;&#24314;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#23545;&#20914;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#26469;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#33633;&#26102;&#26399;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15640</link><description>&lt;p&gt;
&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#23545;&#20914;&#30340;&#23545;&#20914;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices. (arXiv:2309.15640v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26500;&#24314;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#30340;&#23545;&#20914;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#26469;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#33633;&#26102;&#26399;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37329;&#34701;&#24066;&#22330;&#21463;&#37329;&#34701;&#21160;&#33633;&#24433;&#21709;&#26102;&#23545;&#20914;&#39118;&#38505;&#36164;&#20135;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22810;&#20803;&#31639;&#27861;&#25237;&#36164;&#31574;&#30053;&#65288;AIS&#65289;&#30340;&#20998;&#25955;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#22312;&#21333;&#20010;&#36164;&#20135;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#65292;&#32780;&#26159;&#22312;&#22522;&#20110;&#36825;&#20123;&#36164;&#20135;&#30340;&#20215;&#26684;&#30340;&#32423;&#21035;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#37319;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#27169;&#22411;&#65288;LSTM - &#38271;&#30701;&#26399;&#35760;&#24518;&#12289;ARIMA-GARCH - &#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343; - &#24191;&#20041;&#33258;&#22238;&#24402;&#26465;&#20214;&#24322;&#26041;&#24046;&#12289;&#21160;&#37327;&#21644;&#21453;&#21521;&#20132;&#26131;&#65289;&#26469;&#29983;&#25104;&#20215;&#26684;&#39044;&#27979;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20135;&#29983;&#21333;&#20010;&#21644;&#22797;&#21512;&#30340;AIS&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#39564;&#35777;&#30001;&#21508;&#31181;&#36164;&#20135;&#65288;&#33021;&#28304;&#21830;&#21697;&#12289;&#36149;&#37329;&#23646;&#12289;&#21152;&#23494;&#36135;&#24065;&#25110;&#36719;&#21830;&#21697;&#65289;&#32452;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25237;&#36164;&#31574;&#30053;&#22312;&#23545;&#20914;&#29992;&#20110;&#32929;&#31080;&#25351;&#25968;&#65288;S&amp;P 500&#25351;&#25968;&#65289;&#30340;&#32452;&#21512;AIS&#20013;&#30340;&#22810;&#26679;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils. We introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies (AIS) built based on the prices of these assets. We employ four types of diverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH Autoregressive Integrated Moving Average - Generalized Autoregressive Conditional Heteroskedasticity, momentum, and contrarian) to generate price forecasts, which are then used to produce investment signals in single and complex AIS. In such a way, we are able to verify the diversification potential of different types of investment strategies consisting of various assets (energy commodities, precious metals, cryptocurrencies, or soft commodities) in hedging ensemble AIS built for equity indices (S&amp;P 500 index). Empirical data used in this study cov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#31185;&#23398;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#24863;&#30693;&#27169;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20102;&#26368;&#26032;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#21644;&#29702;&#35299;&#20869;&#37096;&#29366;&#24577;&#12289;&#29615;&#22659;&#12289;&#29289;&#20307;&#21644;&#20154;&#31867;&#27963;&#21160;&#12290;&#26368;&#37325;&#35201;&#30340;&#21019;&#26032;&#26159;&#22312;&#20869;&#37096;&#29366;&#24577;&#20272;&#35745;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#21644;&#26368;&#22823;&#21518;&#39564;&#20844;&#24335;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;&#22806;&#37096;&#29615;&#22659;&#29702;&#35299;&#20013;&#21033;&#29992;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15616</link><description>&lt;p&gt;
&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Perception for Humanoid Robots. (arXiv:2309.15616v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#31185;&#23398;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#24863;&#30693;&#27169;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#30740;&#31350;&#20102;&#26368;&#26032;&#30340;&#26041;&#27861;&#26469;&#24863;&#30693;&#21644;&#29702;&#35299;&#20869;&#37096;&#29366;&#24577;&#12289;&#29615;&#22659;&#12289;&#29289;&#20307;&#21644;&#20154;&#31867;&#27963;&#21160;&#12290;&#26368;&#37325;&#35201;&#30340;&#21019;&#26032;&#26159;&#22312;&#20869;&#37096;&#29366;&#24577;&#20272;&#35745;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#21644;&#26368;&#22823;&#21518;&#39564;&#20844;&#24335;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;&#22806;&#37096;&#29615;&#22659;&#29702;&#35299;&#20013;&#21033;&#29992;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#23457;&#30446;&#30340;&#65306;&#22312;&#20154;&#24418;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#24863;&#30693;&#22312;&#20351;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#21644;&#29615;&#22659;&#26080;&#32541;&#20114;&#21160;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#31185;&#23398;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#24863;&#30693;&#27169;&#24335;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#65292;&#36890;&#36807;&#25506;&#32034;&#26368;&#26032;&#30340;&#24863;&#30693;&#21644;&#29702;&#35299;&#20869;&#37096;&#29366;&#24577;&#12289;&#29615;&#22659;&#12289;&#29289;&#20307;&#21644;&#20154;&#31867;&#27963;&#21160;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#26368;&#26032;&#21457;&#29616;&#65306;&#20869;&#37096;&#29366;&#24577;&#20272;&#35745;&#24191;&#27867;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#28388;&#27874;&#26041;&#27861;&#21644;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#20844;&#24335;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#21033;&#29992;&#26412;&#20307;&#24863;&#30693;&#12290;&#22312;&#22806;&#37096;&#29615;&#22659;&#29702;&#35299;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#23545;&#21160;&#24577;&#12289;&#19981;&#21487;&#39044;&#35265;&#30340;&#29615;&#22659;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#26412;&#30740;&#31350;&#20013;&#35752;&#35770;&#30340;&#26032;&#19968;&#25209;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#26426;&#22120;&#23398;&#20064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose of Review: The field of humanoid robotics, perception plays a fundamental role in enabling robots to interact seamlessly with humans and their surroundings, leading to improved safety, efficiency, and user experience. This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.  Recent Findings: Internal state estimation makes extensive use of Bayesian filtering methods and optimization techniques based on maximum a-posteriori formulation by utilizing proprioceptive sensing. In the area of external environment understanding, with an emphasis on robustness and adaptability to dynamic, unforeseen environmental changes, the new slew of research discussed in this study have focused largely on multi-sensor fusion and machine learning in contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15609</link><description>&lt;p&gt;
&#21457;&#23637;&#22269;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#30340;&#33258;&#21160;&#36880;&#23383;&#36716;&#24405;&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution. (arXiv:2309.15609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#65292;&#35299;&#20915;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#26367;&#20195;&#21644;&#25913;&#21892;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21019;&#24314;&#20840;&#33258;&#21160;&#30340;&#20250;&#35758;&#35760;&#24405;&#21644;&#23545;&#23427;&#20204;&#36827;&#34892;&#22810;&#31181;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#24037;&#20855;&#26159;&#22312;&#19990;&#30028;&#30693;&#35782;&#20135;&#26435;&#32452;&#32455;&#65288;WIPO&#65289;&#24320;&#21457;&#30340;&#12289;&#20351;&#29992;&#20854;&#20869;&#37096;&#24320;&#21457;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#65288;S2T&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#32452;&#20214;&#30340;&#31995;&#32479;&#12290;&#38500;&#20102;&#25551;&#36848;&#25968;&#25454;&#25910;&#38598;&#21644;&#20248;&#21270;&#36807;&#31243;&#65292;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#22806;&#65292;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#25216;&#26415;&#32452;&#20214;&#30340;&#26550;&#26500;&#21644;&#28436;&#21464;&#65292;&#24182;&#31361;&#20986;&#20102;&#29992;&#25143;&#26041;&#38754;&#30340;&#21830;&#19994;&#24433;&#21709;&#21644;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#31995;&#32479;&#22312;&#28436;&#36827;&#21644;&#37319;&#29992;&#36807;&#31243;&#20013;&#30340;&#29305;&#27530;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#22914;&#20309;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#20135;&#21697;&#65292;&#24182;&#21462;&#20195;&#20102;&#20250;&#35758;&#31649;&#29702;&#25991;&#26723;&#20013;&#29616;&#26377;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;RCC-8&#20013;&#30340;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32463;&#20856;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.15577</link><description>&lt;p&gt;
&#23545;ChatGPT-4&#22312;RCC-8&#20013;&#30340;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8. (arXiv:2309.15577v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;RCC-8&#20013;&#30340;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32463;&#20856;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#65288;QSR&#65289;&#26159;&#24120;&#35782;&#25512;&#29702;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#19988;&#20855;&#26377;&#20174;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#21040;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#25552;&#20986;&#20102;&#35768;&#22810;&#22768;&#31216;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#29305;&#23450;LLM&#22312;RCC-8&#30340;&#32463;&#20856;&#23450;&#24615;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense Reasoning and has multiple applications ranging from Geographical Information Systems to Robotics and Computer Vision. Recently many claims have been made for the capabilities of Large Language Models (LLMs). In this paper we investigate the extent to which one particular LLM can perform classical qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20182;&#20204;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15554</link><description>&lt;p&gt;
&#30452;&#25509;&#27169;&#22411;&#29992;&#20110;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#65306;FBK&#22312;IWSLT2023&#20013;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023. (arXiv:2309.15554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20182;&#20204;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#23454;&#29616;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;FBK&#22312;IWSLT 2023&#35780;&#20272;&#27963;&#21160;&#30340;&#21516;&#26102;&#32763;&#35793;&#21644;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#21442;&#19982;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20851;&#27880;&#20110;&#20351;&#29992;&#30452;&#25509;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20004;&#20010;&#20219;&#21153;&#65306;&#23545;&#20110;&#21516;&#26102;&#32763;&#35793;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31163;&#32447;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#31574;&#30053;&#26469;&#36827;&#34892;&#23454;&#26102;&#25512;&#29702;&#65307;&#23545;&#20110;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;ST&#27169;&#22411;&#35843;&#25972;&#20026;&#29983;&#25104;&#31526;&#21512;&#35268;&#33539;&#30340;&#23383;&#24149;&#65292;&#24182;&#21033;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#29983;&#25104;&#19982;&#38899;&#35270;&#39057;&#20869;&#23481;&#21516;&#27493;&#25152;&#38656;&#30340;&#26102;&#38388;&#25139;&#12290;&#25105;&#20204;&#30340;&#33521;&#24503;SimulST&#31995;&#32479;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#26041;&#38754;&#27604;2021&#24180;&#21644;2022&#24180;&#30340;&#20219;&#21153;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#31995;&#32479;&#26377;&#25152;&#20943;&#23569;&#65292;&#24182;&#33719;&#24471;&#20102;&#39640;&#36798;3.5 BLEU&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#23383;&#24149;&#29983;&#25104;&#31995;&#32479;&#22312;&#33521;&#24503;&#21644;&#33521;&#35199;&#25991;&#23545;&#20013;&#20248;&#20110;&#22522;&#20110;&#30452;&#25509;&#31995;&#32479;&#30340;&#21807;&#19968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;3.7&#21644;1.7&#30340;SubER&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2309.15551</link><description>&lt;p&gt;
&#20351;&#29992;DeepRepViz&#26469;&#35782;&#21035;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identifying confounders in deep-learning-based model predictions using DeepRepViz. (arXiv:2309.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DeepRepViz&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20013;&#35782;&#21035;&#28151;&#28102;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#24230;&#37327;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24102;&#26469;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#25581;&#31034;&#22823;&#33041;&#12289;&#22823;&#33041;&#30149;&#29702;&#21644;&#24515;&#29702;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#21442;&#19982;&#32773;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#24433;&#20687;&#20266;&#24433;&#31561;&#22806;&#37096;&#30340;&#8220;&#28151;&#28102;&#22240;&#32032;&#8221;&#21464;&#37327;&#21487;&#33021;&#20250;&#20559;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#23398;&#20064;&#30456;&#20851;&#30340;&#33041;-&#34920;&#22411;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DeepRepViz&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#31995;&#32479;&#22320;&#26816;&#27979;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;(1)&#24230;&#37327;&#21487;&#33021;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#25351;&#26631;&#21644;(2)&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#23450;&#24615;&#26816;&#26597;DL&#27169;&#22411;&#23398;&#20064;&#20869;&#23481;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;DeepRepViz&#19982;DL&#27169;&#22411;&#32467;&#21512;&#30340;&#30410;&#22788;&#12290;&#20363;&#22914;&#65292;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#24615;&#21035;&#26159;DL&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#36807;&#28388;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#22270;&#20687;&#24211;&#20013;&#25552;&#21462;&#21355;&#26143;&#22270;&#20687;&#12290;&#36825;&#23548;&#33268;&#20102;&#21457;&#24067;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;LAION-EO&#12290;</title><link>http://arxiv.org/abs/2309.15535</link><description>&lt;p&gt;
&#20174;LAION-5B&#21040;LAION-EO&#65306;&#20351;&#29992;&#38170;&#23450;&#25968;&#25454;&#38598;&#36807;&#28388;&#25968;&#21313;&#20159;&#24352;&#22270;&#29255;&#36827;&#34892;&#21355;&#26143;&#22270;&#20687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction. (arXiv:2309.15535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#36807;&#28388;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#22270;&#20687;&#24211;&#20013;&#25552;&#21462;&#21355;&#26143;&#22270;&#20687;&#12290;&#36825;&#23548;&#33268;&#20102;&#21457;&#24067;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;LAION-EO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22914;LAION-5B&#65292;&#21253;&#21547;&#22312;&#32447;&#20849;&#20139;&#30340;&#21508;&#31181;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25552;&#21462;&#22823;&#22411;&#22270;&#20687;&#24211;&#30340;&#39046;&#22495;&#29305;&#23450;&#23376;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#23450;&#25968;&#25454;&#38598;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#32467;&#21512;&#36827;&#19968;&#27493;&#30340;&#36807;&#28388;&#65292;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;LAION-EO&#30340;&#21457;&#24067;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#32593;&#32476;&#20013;&#33719;&#21462;&#30340;&#39640;&#65288;&#36880;&#20687;&#32032;&#65289;&#20998;&#36776;&#29575;&#30340;&#25991;&#26412;&#21644;&#21355;&#26143;&#22270;&#20687;&#23545;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#37319;&#38598;&#36807;&#31243;&#20197;&#21450;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large datasets, such as LAION-5B, contain a diverse distribution of images shared online. However, extraction of domain-specific subsets of large image corpora is challenging. The extraction approach based on an anchor dataset, combined with further filtering, is proposed here and demonstrated for the domain of satellite imagery. This results in the release of LAION-EO, a dataset sourced from the web containing pairs of text and satellite images in high (pixel-wise) resolution. The paper outlines the acquisition procedure as well as some of the features of the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#27969;&#34892;&#30340;&#21487;&#22797;&#21046;&#24615;&#24179;&#21488;&#20013;&#27809;&#26377;&#19968;&#20010;&#23436;&#20840;&#25972;&#21512;&#20102;&#24517;&#35201;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#65292;&#20294;Kaggle&#21644;Codalab&#22312;&#23454;&#26045;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#12290;&#26412;&#30740;&#31350;&#36824;&#26681;&#25454;&#29992;&#25143;&#30340;&#19981;&#21516;&#24773;&#20917;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#25972;&#21512;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15525</link><description>&lt;p&gt;
&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#21487;&#22797;&#21046;&#24615;&#30340;&#24179;&#21488;&#30340;&#32593;&#32476;&#23433;&#20840;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Cyber Security Requirements for Platforms Enhancing AI Reproducibility. (arXiv:2309.15525v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#27969;&#34892;&#30340;&#21487;&#22797;&#21046;&#24615;&#24179;&#21488;&#20013;&#27809;&#26377;&#19968;&#20010;&#23436;&#20840;&#25972;&#21512;&#20102;&#24517;&#35201;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#65292;&#20294;Kaggle&#21644;Codalab&#22312;&#23454;&#26045;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#12290;&#26412;&#30740;&#31350;&#36824;&#26681;&#25454;&#29992;&#25143;&#30340;&#19981;&#21516;&#24773;&#20917;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#25972;&#21512;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30740;&#31350;&#36234;&#26469;&#36234;&#20381;&#36182;&#35745;&#31639;&#26041;&#27861;&#65292;&#36825;&#32473;&#30830;&#20445;&#30740;&#31350;&#21487;&#22797;&#21046;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20174;&#32593;&#32476;&#23433;&#20840;&#35282;&#24230;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#21487;&#22797;&#21046;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#19982;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30456;&#20851;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#21487;&#22797;&#21046;&#24615;&#24179;&#21488;&#65306;Floydhub&#12289;BEAT&#12289;Codalab&#12289;Kaggle&#21644;OpenML&#12290;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#20123;&#24179;&#21488;&#37117;&#27809;&#26377;&#23436;&#20840;&#25972;&#21512;&#24517;&#35201;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#65292;&#36825;&#20123;&#25514;&#26045;&#23545;&#20110;&#31283;&#20581;&#30340;&#21487;&#22797;&#21046;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;Kaggle&#21644;Codalab&#22312;&#23454;&#26045;&#28085;&#30422;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#12289;&#26131;&#29992;&#24615;&#21644;&#20449;&#20219;&#31561;&#26041;&#38754;&#30340;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#19981;&#21516;&#30340;&#29992;&#25143;&#24773;&#20917;&#25552;&#20379;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#24314;&#35758;&#65292;&#21253;&#25324;&#20010;&#20154;&#30740;&#31350;&#32773;&#12289;&#23567;&#22411;&#23454;&#39564;&#23460;&#21644;&#22823;&#22411;&#20225;&#19994;&#12290;&#30740;&#31350;&#24378;&#35843;&#25972;&#21512;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific research is increasingly reliant on computational methods, posing challenges for ensuring research reproducibility. This study focuses on the field of artificial intelligence (AI) and introduces a new framework for evaluating AI platforms for reproducibility from a cyber security standpoint to address the security challenges associated with AI research. Using this framework, five popular AI reproducibility platforms; Floydhub, BEAT, Codalab, Kaggle, and OpenML were assessed. The analysis revealed that none of these platforms fully incorporates the necessary cyber security measures essential for robust reproducibility. Kaggle and Codalab, however, performed better in terms of implementing cyber security measures covering aspects like security, privacy, usability, and trust. Consequently, the study provides tailored recommendations for different user scenarios, including individual researchers, small laboratories, and large corporations. It emphasizes the importance of integra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#20316;&#32773;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.15522</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#20869;&#37096;&#34920;&#31034;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Internal Representations for Domain Generalization. (arXiv:2309.15522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#20316;&#32773;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30740;&#31350;&#30340;&#25104;&#26524;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#25105;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#30740;&#31350;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;&#20102;&#25345;&#32493;&#23398;&#20064;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#25152;&#24102;&#26469;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#36890;&#36807;&#24635;&#32467;&#25105;&#36807;&#21435;&#21644;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340;&#36129;&#29486;&#65292;&#26412;&#25991;&#26088;&#22312;&#21576;&#29616;&#25105;&#30740;&#31350;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#20026;&#26410;&#26469;&#39046;&#22495;&#30340;&#25506;&#32034;&#21644;&#36827;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#30340;&#30740;&#31350;&#28041;&#21450;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#21508;&#31181;&#35774;&#32622;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#25105;&#24076;&#26395;&#36825;&#20010;&#35843;&#26597;&#25552;&#20379;&#32473;&#37027;&#20123;&#24076;&#26395;&#19987;&#27880;&#20110;&#31867;&#20284;&#30740;&#31350;&#26041;&#21521;&#30340;&#30740;&#31350;&#32773;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper which is part of the New Faculty Highlights Invited Speaker Program of AAAI'23, serves as a comprehensive survey of my research in transfer learning by utilizing embedding spaces. The work reviewed in this paper specifically revolves around the inherent challenges associated with continual learning and limited availability of labeled data. By providing an overview of my past and ongoing contributions, this paper aims to present a holistic understanding of my research, paving the way for future explorations and advancements in the field. My research delves into the various settings of transfer learning, including, few-shot learning, zero-shot learning, continual learning, domain adaptation, and distributed learning. I hope this survey provides a forward-looking perspective for researchers who would like to focus on similar research directions.
&lt;/p&gt;</description></item><item><title>Raij\=u&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#28183;&#36879;&#27979;&#35797;&#21592;&#24555;&#36895;&#23454;&#26045;&#32593;&#32476;&#31995;&#32479;&#30340;&#21518;&#28183;&#36879;&#36807;&#31243;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#30340;&#23433;&#20840;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15518</link><description>&lt;p&gt;
Raij\=u: &#24378;&#21270;&#23398;&#20064;&#25351;&#23548;&#30340;&#21518;&#28183;&#36879;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#32593;&#32476;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Raij\=u: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems. (arXiv:2309.15518v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15518
&lt;/p&gt;
&lt;p&gt;
Raij\=u&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#28183;&#36879;&#27979;&#35797;&#21592;&#24555;&#36895;&#23454;&#26045;&#32593;&#32476;&#31995;&#32479;&#30340;&#21518;&#28183;&#36879;&#36807;&#31243;&#65292;&#20197;&#35780;&#20272;&#31995;&#32479;&#30340;&#23433;&#20840;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32593;&#32476;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24517;&#39035;&#35843;&#26597;&#25915;&#20987;&#32773;&#22312;&#25104;&#21151;&#20837;&#20405;&#21518;&#30340;&#34892;&#20026;&#65292;&#36825;&#34987;&#31216;&#20026;&#21518;&#28183;&#36879;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#39640;&#25928;&#30340;&#24037;&#20855;&#25903;&#25345;&#21518;&#28183;&#36879;&#23454;&#26045;&#65292;&#20294;&#27809;&#26377;&#24212;&#29992;&#31243;&#24207;&#33021;&#22815;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#30340;&#22823;&#37096;&#20998;&#27493;&#39588;&#30001;&#28145;&#20837;&#20102;&#35299;&#23433;&#20840;&#30340;&#19987;&#23478;&#23436;&#25104;&#65292;&#34987;&#31216;&#20026;&#28183;&#36879;&#27979;&#35797;&#21592;&#25110;pen-testers&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;Raij\=u&#26694;&#26550;&#65292;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;pen-testers&#24555;&#36895;&#23454;&#26045;&#32593;&#32476;&#31995;&#32479;&#23433;&#20840;&#35780;&#20272;&#30340;&#21518;&#28183;&#36879;&#36807;&#31243;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20004;&#20010;RL&#31639;&#27861;&#65292;Advantage Actor-Critic&#65288;A2C&#65289;&#21644;Proximal Policy Optimization&#65288;PPO&#65289;&#65292;&#26469;&#35757;&#32451;&#33021;&#22815;&#36827;&#34892;&#26234;&#33021;&#21160;&#20316;&#30340;&#19987;&#29992;&#20195;&#29702;&#65292;&#36825;&#20123;&#21160;&#20316;&#26159;Metasploit&#27169;&#22359;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#36215;&#29305;&#26435;&#21319;&#32423;&#12289;&#25910;&#38598;hashdump&#21644;&#27178;&#21521;&#31227;&#21160;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
In order to assess the risks of a network system, it is important to investigate the behaviors of attackers after successful exploitation, which is called post-exploitation. Although there are various efficient tools supporting post-exploitation implementation, no application can automate this process. Most of the steps of this process are completed by experts who have profound knowledge of security, known as penetration testers or pen-testers. To this end, our study proposes the Raij\=u framework, a Reinforcement Learning (RL)-driven automation approach that assists pen-testers in quickly implementing the process of post-exploitation for security-level evaluation in network systems. We implement two RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO), to train specialized agents capable of making intelligent actions, which are Metasploit modules to automatically launch attacks of privileges escalation, gathering hashdump, and lateral movement. By leverag
&lt;/p&gt;</description></item><item><title>&#27531;&#24046;&#35843;&#24230;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#31227;&#38500;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.15517</link><description>&lt;p&gt;
&#27531;&#24046;&#35843;&#24230;&#65306;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Scheduling: A New Reinforcement Learning Approach to Solving Job Shop Scheduling Problem. (arXiv:2309.15517v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15517
&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#35843;&#24230;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#31227;&#38500;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSP&#65289;&#26159;&#19968;&#31181;&#22312;&#21046;&#36896;&#19994;&#31561;&#34892;&#19994;&#24191;&#27867;&#24212;&#29992;&#30340;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#65292;&#32780;&#28789;&#27963;&#30340;JSP&#65288;FJSP&#65289;&#21017;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21464;&#20307;&#12290;&#30001;&#20110;&#23427;&#20204;&#26159;NP-hard&#38382;&#39064;&#65292;&#24456;&#38590;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#25214;&#21040;&#25152;&#26377;&#24773;&#20917;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#27492;&#24320;&#21457;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;JSP/FJSP&#38382;&#39064;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24456;&#22810;&#26500;&#24314;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27531;&#24046;&#35843;&#24230;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;JSP/FJSP&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#26032;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#31227;&#38500;&#20102;&#19981;&#30456;&#20851;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#20363;&#22914;&#24050;&#23436;&#25104;&#30340;&#26426;&#22120;&#21644;&#20316;&#19994;&#65292;&#20197;&#20415;&#29366;&#24577;&#20165;&#21253;&#21547;&#21097;&#19979;&#30340;&#65288;&#25110;&#30456;&#20851;&#30340;&#65289;&#26426;&#22120;&#21644;&#20316;&#19994;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#30693;&#21517;&#30340;&#26500;&#24314;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Job-shop scheduling problem (JSP) is a mathematical optimization problem widely used in industries like manufacturing, and flexible JSP (FJSP) is also a common variant. Since they are NP-hard, it is intractable to find the optimal solution for all cases within reasonable times. Thus, it becomes important to develop efficient heuristics to solve JSP/FJSP. A kind of method of solving scheduling problems is construction heuristics, which constructs scheduling solutions via heuristics. Recently, many methods for construction heuristics leverage deep reinforcement learning (DRL) with graph neural networks (GNN). In this paper, we propose a new approach, named residual scheduling, to solving JSP/FJSP. In this new approach, we remove irrelevant machines and jobs such as those finished, such that the states include the remaining (or relevant) machines and jobs only. Our experiments show that our approach reaches state-of-the-art (SOTA) among all known construction heuristics on most well-known
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15512</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#20811;&#38534;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#25991;&#26412;-&#35821;&#38899;&#23545;&#12290;&#26368;&#23567;&#30417;&#30563;&#30340;&#35821;&#38899;&#21512;&#25104;&#36890;&#36807;&#32452;&#21512;&#20004;&#31181;&#31867;&#22411;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#65288;&#35821;&#20041;&#21644;&#22768;&#23398;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#26368;&#23569;&#30417;&#30563;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#65292;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#12290;&#33258;&#22238;&#24402;&#26694;&#26550;&#20855;&#26377;&#20856;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#21487;&#25511;&#24615;&#38382;&#39064;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#21463;&#21040;&#25345;&#32493;&#39044;&#27979;&#27169;&#22411;&#24341;&#36215;&#30340;&#38901;&#24459;&#24179;&#22343;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#27169;&#22359;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic \&amp; acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#34892;&#20026;&#25104;&#26412;&#24378;&#21270;&#23398;&#20064;&#65288;ABC-RL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#34892;&#20026;&#38480;&#21046;&#20316;&#20026;&#25104;&#26412;&#20449;&#21495;&#65292;&#24182;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#35757;&#32451;&#20986;&#20154;&#31867;&#21270;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38750;&#33258;&#28982;&#34892;&#20026;&#38382;&#39064;&#65292;&#20351;&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#26356;&#20687;&#20154;&#31867;&#65292;&#24182;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15484</link><description>&lt;p&gt;
&#36808;&#21521;&#20154;&#31867;&#21270;&#24378;&#21270;&#23398;&#20064;&#65306;&#36890;&#36807;3D&#28216;&#25103;&#20013;&#30340;&#33258;&#36866;&#24212;&#34892;&#20026;&#25104;&#26412;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#33258;&#28982;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games. (arXiv:2309.15484v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#34892;&#20026;&#25104;&#26412;&#24378;&#21270;&#23398;&#20064;&#65288;ABC-RL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#34892;&#20026;&#38480;&#21046;&#20316;&#20026;&#25104;&#26412;&#20449;&#21495;&#65292;&#24182;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#35757;&#32451;&#20986;&#20154;&#31867;&#21270;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38750;&#33258;&#28982;&#34892;&#20026;&#38382;&#39064;&#65292;&#20351;&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#26356;&#20687;&#20154;&#31867;&#65292;&#24182;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#34892;&#20026;&#25104;&#26412;&#24378;&#21270;&#23398;&#20064;&#65288;ABC-RL&#65289;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#31454;&#20105;&#21147;&#24378;&#20154;&#31867;&#21270;&#26234;&#33021;&#20307;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#21508;&#31181;&#35270;&#39057;&#28216;&#25103;&#20013;&#26368;&#36817;&#23454;&#29616;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#20854;&#20013;&#19968;&#20123;&#19981;&#21463;&#38480;&#21046;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#34920;&#29616;&#20986;&#19981;&#31526;&#21512;&#20154;&#31867;&#34892;&#20026;&#30340;&#21160;&#20316;&#65292;&#20363;&#22914;&#25671;&#26179;&#21644;&#26059;&#36716;&#65292;&#23548;&#33268;&#22855;&#29305;&#30340;&#28216;&#25103;&#20307;&#39564;&#12290;&#20026;&#20102;&#20687;&#20154;&#31867;&#37027;&#26679;&#34892;&#20026;&#65292;&#24182;&#20445;&#25345;&#30456;&#20284;&#30340;&#34920;&#29616;&#65292;ABC-RL&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22686;&#21152;&#20102;&#34892;&#20026;&#38480;&#21046;&#20316;&#20026;&#25104;&#26412;&#20449;&#21495;&#65292;&#24182;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#12290;&#19982;&#20256;&#32479;&#30340;&#21463;&#38480;&#31574;&#30053;&#20248;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#26368;&#23567;&#21270;&#34892;&#20026;&#25104;&#26412;&#65292;&#21516;&#26102;&#32422;&#26463;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25289;&#26684;&#26391;&#26085;&#35843;&#25972;&#30340;&#36817;&#20284;&#65292;&#22788;&#29702;&#24615;&#33021;&#19982;&#20154;&#31867;&#21270;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new approach called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL) for training a human-like agent with competitive strength. While deep reinforcement learning agents have recently achieved superhuman performance in various video games, some of these unconstrained agents may exhibit actions, such as shaking and spinning, that are not typically observed in human behavior, resulting in peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between the performance and the human-like behavior. Through experiments co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;&#30340;&#36328;&#32423;&#21035;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;DL&#27169;&#22411;&#21644;&#31995;&#32479;&#35843;&#24230;&#65292;&#25913;&#21892;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#25512;&#21160;AIoT&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.15467</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#32423;&#21035;&#20248;&#21270;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey. (arXiv:2309.15467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;AIoT&#31995;&#32479;&#30340;&#36328;&#32423;&#21035;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;DL&#27169;&#22411;&#21644;&#31995;&#32479;&#35843;&#24230;&#65292;&#25913;&#21892;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#25512;&#21160;AIoT&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#30340;&#24191;&#27867;&#20351;&#29992;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#65292;&#20154;&#24037;&#26234;&#33021;&#29289;&#32852;&#32593;&#65288;AIoT&#65292;AI+IoT&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#24471;&#21040;&#20102;&#25512;&#21160;&#12290;DL&#27169;&#22411;&#36164;&#28304;&#23494;&#38598;&#65292;&#22240;&#27492;&#29616;&#26377;&#30740;&#31350;&#21162;&#21147;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22522;&#30784;&#35774;&#26045;&#19978;&#23454;&#29616;AIoT&#23454;&#26102;&#25512;&#29702;&#21644;&#20302;&#25104;&#26412;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#36164;&#28304;&#21451;&#22909;&#30340;DL&#27169;&#22411;&#21644;&#27169;&#22411;&#33258;&#36866;&#24212;&#31995;&#32479;&#35843;&#24230;&#30340;&#31639;&#27861;-&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;&#25913;&#36827;&#20102;&#36816;&#34892;&#26102;&#36164;&#28304;&#21487;&#29992;&#24615;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#30001;&#29420;&#31435;&#32423;&#21035;&#35774;&#23450;&#30340;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field of artificial intelligence of things (AIoT, AI+IoT) is driven by the widespread use of intelligent infrastructures and the impressive success of deep learning (DL). With the deployment of DL on various intelligent infrastructures featuring rich sensors and weak DL computing capabilities, a diverse range of AIoT applications has become possible. However, DL models are notoriously resource-intensive. Existing research strives to realize near-/realtime inference of AIoT live data and low-cost training using AIoT datasets on resource-scare infrastructures. Accordingly, the accuracy and responsiveness of DL models are bounded by resource availability. To this end, the algorithm-system co-design that jointly optimizes the resource-friendly DL models and model-adaptive system scheduling improves the runtime resource availability and thus pushes the performance boundary set by the standalone level. Unlike previous surveys on resource-friendly DL models or hand-crafted DL com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.15458</link><description>&lt;p&gt;
LogicMP: &#19968;&#31181;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints. (arXiv:2309.15458v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LogicMP&#30340;&#26032;&#39062;&#31070;&#32463;&#23618;&#65292;&#35813;&#23618;&#36890;&#36807;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#32534;&#30721;&#36827;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#36890;&#36807;&#26377;&#25928;&#32531;&#35299;&#19968;&#38454;&#36923;&#36753;&#27169;&#22411;&#30340;&#25512;&#26029;&#22256;&#38590;&#65292;LogicMP&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#24314;&#27169;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#20197;&#28385;&#36275;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#23618;LogicMP&#65292;&#20854;&#23618;&#23545;MLN&#36827;&#34892;&#22343;&#22330;&#21464;&#20998;&#25512;&#26029;&#12290;&#23427;&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#32534;&#30721;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22359;&#21270;&#21644;&#25928;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MLN&#20013;&#30340;&#32467;&#26500;&#21644;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#33391;&#22909;&#12289;&#39640;&#25928;&#30340;&#22343;&#22330;&#36845;&#20195;&#33021;&#22815;&#26377;&#25928;&#32531;&#35299;MLN&#25512;&#26029;&#30340;&#22256;&#38590;&#65292;&#23558;&#25512;&#26029;&#20174;&#39034;&#24207;&#35745;&#31639;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#24352;&#37327;&#25805;&#20316;&#12290;&#22312;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#19977;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LogicMP&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#37117;&#20248;&#20110;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21387;&#32553;&#35270;&#39057;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#30340;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#35299;&#30721;&#35270;&#39057;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#21387;&#32553;&#22495;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#12290;&#20351;&#29992;&#36731;&#37327;&#32423;ConvNets&#25552;&#21462;GOP&#20013;&#30340;P&#24103;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#30340;&#31354;&#38388;-&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#29305;&#24449;&#32454;&#21270;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20107;&#20214;&#36793;&#30028;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15431</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#21387;&#32553;&#35270;&#39057;&#27969;&#23398;&#20064;&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Local Compressed Video Stream Learning for Generic Event Boundary Detection. (arXiv:2309.15431v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21387;&#32553;&#35270;&#39057;&#27969;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#30340;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#35299;&#30721;&#35270;&#39057;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#21387;&#32553;&#22495;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#12290;&#20351;&#29992;&#36731;&#37327;&#32423;ConvNets&#25552;&#21462;GOP&#20013;&#30340;P&#24103;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#30340;&#31354;&#38388;-&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#29305;&#24449;&#32454;&#21270;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20107;&#20214;&#36793;&#30028;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#26816;&#27979;&#26088;&#22312;&#23558;&#35270;&#39057;&#20998;&#27573;&#20026;&#22359;&#65292;&#23450;&#20301;&#36890;&#29992;&#19988;&#26080;&#20998;&#31867;&#30340;&#20107;&#20214;&#36793;&#30028;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#23558;&#35270;&#39057;&#24103;&#35299;&#30721;&#21518;&#25165;&#23558;&#20854;&#36755;&#20837;&#32593;&#32476;&#65292;&#36825;&#20854;&#20013;&#23384;&#22312;&#36739;&#22823;&#30340;&#26102;&#31354;&#20887;&#20313;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#21387;&#32553;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20107;&#20214;&#36793;&#30028;&#26816;&#27979;&#65292;&#23427;&#23436;&#20840;&#22522;&#20110;&#21387;&#32553;&#22495;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;RGB&#12289;&#36816;&#21160;&#21521;&#37327;&#12289;&#27531;&#24046;&#21644;&#22270;&#20687;&#32452;&#65288;GOP&#65289;&#32467;&#26500;&#65292;&#32780;&#26080;&#38656;&#23436;&#20840;&#35299;&#30721;&#35270;&#39057;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;ConvNets&#25552;&#21462;GOP&#20013;&#30340;P&#24103;&#30340;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31354;&#38388;-&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;SCAM&#65289;&#65292;&#26681;&#25454;&#21452;&#21521;&#20449;&#24687;&#27969;&#20351;&#29992;&#21387;&#32553;&#20449;&#24687;&#26469;&#32454;&#21270;P&#24103;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20026;&#20102;&#23398;&#20064;&#36866;&#21512;&#36793;&#30028;&#26816;&#27979;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#26368;&#23567;&#20999;&#21106;&#65288;JLC&#65289;&#21644;&#20013;&#24515;&#20999;&#21106;&#65288;CFC&#65289;&#26469;&#25191;&#34892;&#20107;&#20214;&#36793;&#30028;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic event boundary detection aims to localize the generic, taxonomy-free event boundaries that segment videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which contains significant spatio-temporal redundancy and demands considerable computational power and storage space. To remedy these issues, we propose a novel compressed video representation learning method for event boundary detection that is fully end-to-end leveraging rich information in the compressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we use lightweight ConvNets to extract features of the P-frames in the GOPs and spatial-channel attention module (SCAM) is designed to refine the feature representations of the P-frames based on the compressed information with bidirectional information flow. To learn a suitable representation for boundary detection, we co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15427</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#31070;&#32463;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Prompting with Large Language Models. (arXiv:2309.15427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#65292;&#20197;&#24357;&#34917;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#20934;&#30830;&#25429;&#25417;&#21644;&#36820;&#22238;&#22522;&#20110;&#30693;&#35782;&#30340;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21644;&#23450;&#21046;&#27169;&#22411;&#26550;&#26500;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#26159;&#23558;&#27492;&#24212;&#29992;&#20110;LLMs&#23384;&#22312;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#24182;&#36991;&#20813;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#33258;&#23450;&#20041;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#31070;&#32463;&#25552;&#31034;&#65288;GNP&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#35757;&#32451;&#30340;LLMs&#20174;&#30693;&#35782;&#22270;&#20013;&#23398;&#20064;&#26377;&#30410;&#30340;&#30693;&#35782;&#12290;GNP&#21253;&#25324;&#21508;&#31181;&#35774;&#35745;&#65292;&#21253;&#25324;&#26631;&#20934;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#12289;&#36328;&#27169;&#24577;&#27719;&#32858;&#27169;&#22359;&#12289;&#22495;&#25237;&#24433;&#22120;&#21644;&#33258;&#30417;&#30563;&#38142;&#25509;&#39044;&#27979;&#30446;&#26631;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;GNP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#35757;&#32451;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.15420</link><description>&lt;p&gt;
&#22833;&#36133;&#27169;&#24335;&#19977;&#20803;&#32452;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Triad of Failure Modes and a Possible Way Out. (arXiv:2309.15420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#35757;&#32451;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32858;&#31867;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#34920;&#31034;&#23849;&#28291;&#12289;&#32858;&#31867;&#23849;&#28291;&#21644;&#23545;&#32858;&#31867;&#20998;&#37197;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#31561;&#19977;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#36825;&#20010;&#30446;&#26631;&#20989;&#25968;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26500;&#25104;&#65306;&#65288;i&#65289;&#24809;&#32602;&#34920;&#31034;&#23849;&#28291;&#30340;&#29983;&#25104;&#39033;&#65292;&#65288;ii&#65289;&#20419;&#36827;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#30340;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#26631;&#31614;&#32622;&#25442;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#24809;&#32602;&#32858;&#31867;&#23849;&#28291;&#30340;&#22343;&#21248;&#24615;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#21487;&#20197;&#20174;&#36125;&#21494;&#26031;&#30340;&#35282;&#24230;&#35299;&#37322;&#20026;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#19979;&#30028;&#12290;&#20854;&#27425;&#65292;&#23427;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#20027;&#24178;&#32467;&#26500;&#65292;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#65292;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#23618;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#38750;&#24120;&#36866;&#21512;
&lt;/p&gt;
&lt;p&gt;
We present a novel objective function for cluster-based self-supervised learning (SSL) that is designed to circumvent the triad of failure modes, namely representation collapse, cluster collapse, and the problem of invariance to permutations of cluster assignments. This objective consists of three key components: (i) A generative term that penalizes representation collapse, (ii) a term that promotes invariance to data augmentations, thereby addressing the issue of label permutations and (ii) a uniformity term that penalizes cluster collapse. Additionally, our proposed objective possesses two notable advantages. Firstly, it can be interpreted from a Bayesian perspective as a lower bound on the data log-likelihood. Secondly, it enables the training of a standard backbone architecture without the need for asymmetric elements like stop gradients, momentum encoders, or specialized clustering layers. Due to its simplicity and theoretical foundation, our proposed objective is well-suited for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#23610;&#24230;&#24863;&#30693;&#26694;&#26550;GeoAgent&#65292;&#29992;&#20110;&#35299;&#20915;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#20043;&#22806;&#30340;&#36866;&#24403;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#21464;&#30340;&#22320;&#29702;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15372</link><description>&lt;p&gt;
&#36229;&#36234;&#34917;&#19969;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#30340;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning. (arXiv:2309.15372v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#23610;&#24230;&#24863;&#30693;&#26694;&#26550;GeoAgent&#65292;&#29992;&#20110;&#35299;&#20915;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#20013;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#20043;&#22806;&#30340;&#36866;&#24403;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#21464;&#30340;&#22320;&#29702;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#22312;&#25429;&#25417;&#28369;&#21160;&#31383;&#21475;&#20043;&#22806;&#30340;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20010;&#32570;&#28857;&#22312;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#21464;&#30340;&#22320;&#29702;&#23545;&#35937;&#26102;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#20998;&#21106;&#32467;&#26524;&#20013;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#23610;&#24230;&#24863;&#30693;&#26694;&#26550;GeoAgent&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#22320;&#29702;&#23545;&#35937;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#22270;&#20687;&#34917;&#19969;&#20043;&#22806;&#30340;&#36866;&#24403;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22312;GeoAgent&#20013;&#65292;&#27599;&#20010;&#22270;&#20687;&#34917;&#19969;&#30340;&#29366;&#24577;&#36890;&#36807;&#20840;&#23616;&#32553;&#30053;&#22270;&#21644;&#20301;&#32622;&#25513;&#30721;&#26469;&#34920;&#31034;&#12290;&#20840;&#23616;&#32553;&#30053;&#22270;&#25552;&#20379;&#20102;&#34917;&#19969;&#20043;&#22806;&#30340;&#19978;&#19979;&#25991;&#65292;&#20301;&#32622;&#25513;&#30721;&#25351;&#23548;&#20102;&#24863;&#30693;&#21040;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#23610;&#24230;&#36873;&#25321;&#21160;&#20316;&#26159;&#36890;&#36807;&#23610;&#24230;&#25511;&#21046;Agent(SCA)&#25191;&#34892;&#30340;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#32034;&#24341;&#27169;&#22359;&#26469;&#22686;&#24378;Agent&#21306;&#20998;&#24403;&#21069;&#22270;&#20687;&#34917;&#19969;&#20301;&#32622;&#30340;&#33021;&#21147;&#12290;&#35813;&#21160;&#20316;&#20999;&#25442;&#20102;&#21452;&#20998;&#25903;&#20998;&#21106;&#32593;&#32476;&#30340;&#34917;&#19969;&#23610;&#24230;&#21644;&#19978;&#19979;&#25991;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
In remote sensing imagery analysis, patch-based methods have limitations in capturing information beyond the sliding window. This shortcoming poses a significant challenge in processing complex and variable geo-objects, which results in semantic inconsistency in segmentation results. To address this challenge, we propose a dynamic scale perception framework, named GeoAgent, which adaptively captures appropriate scale context information outside the image patch based on the different geo-objects. In GeoAgent, each image patch's states are represented by a global thumbnail and a location mask. The global thumbnail provides context beyond the patch, and the location mask guides the perceived spatial relationships. The scale-selection actions are performed through a Scale Control Agent (SCA). A feature indexing module is proposed to enhance the ability of the agent to distinguish the current image patch's location. The action switches the patch scale and context branch of a dual-branch seg
&lt;/p&gt;</description></item><item><title>C3Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23884;&#20837;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#24182;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15334</link><description>&lt;p&gt;
C3Net: &#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#21407;&#23376;&#38388;&#21183;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems. (arXiv:2309.15334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15334
&lt;/p&gt;
&lt;p&gt;
C3Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#36136;&#31995;&#32479;&#20013;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#23884;&#20837;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#24182;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28342;&#36136;&#19982;&#20854;&#29615;&#22659;&#30340;&#30456;&#20114;&#20316;&#29992;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#23376;&#31867;&#22411;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#30340;&#23884;&#20837;&#21644;&#36981;&#24490;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#21407;&#23376;&#38388;&#21183;&#12290;&#35813;&#26550;&#26500;&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#31995;&#32479;&#20013;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#65292;&#21253;&#25324;&#28342;&#35299;&#22312;&#19981;&#21516;&#28342;&#21058;&#20013;&#12289;1-&#36763;&#37255;-&#27700;&#20998;&#37197;&#21644;PAMPA&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#28342;&#35299;&#33258;&#30001;&#33021;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#20174;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#28342;&#36136;&#20013;&#27599;&#20010;&#21407;&#23376;&#30340;&#21407;&#23376;&#38388;&#21183;&#20801;&#35768;&#36827;&#34892;&#31526;&#21512;&#21270;&#23398;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#36719;&#20214;&#21487;&#22312;https://github.com/SehanLee/C3Net&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the interactions of a solute with its environment is of fundamental importance in chemistry and biology. In this work, we propose a deep neural network architecture for atom type embeddings in its molecular context and interatomic potential that follows fundamental physical laws. The architecture is applied to predict physicochemical properties in heterogeneous systems including solvation in diverse solvents, 1-octanol-water partitioning, and PAMPA with a single set of network weights. We show that our architecture is generalized well to the physicochemical properties and outperforms state-of-the-art approaches based on quantum mechanics and neural networks in the task of solvation free energy prediction. The interatomic potentials at each atom in a solute obtained from the model allow quantitative analysis of the physicochemical properties at atomic resolution consistent with chemical and physical reasoning. The software is available at https://github.com/SehanLee/C3Net.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.15317</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
joint prediction and denoising for large-scale multilingual self-supervised learning. (arXiv:2309.15317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15317
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;WavLabLM&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#65292;&#20351;&#20854;&#22312;ML-SUPERB&#19978;&#36798;&#21040;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#30001;&#20110;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#25152;&#38656;&#30340;&#36153;&#29992;&#21644;&#22797;&#26434;&#24615;&#32780;&#32463;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36825;&#36827;&#19968;&#27493;&#24433;&#21709;&#20102;SSL&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#30001;&#20110;&#36164;&#28304;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;SSL&#24050;&#32463;&#20165;&#38480;&#20110;&#23569;&#25968;&#30740;&#31350;&#22242;&#38431;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#26356;&#22810;&#30340;&#30740;&#31350;&#22242;&#38431;&#33021;&#22815;&#21152;&#20837;SSL&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WavLabLM&#65292;&#23558;WavLM&#30340;&#32852;&#21512;&#39044;&#27979;&#21644;&#21435;&#22122;&#25193;&#23637;&#21040;136&#31181;&#35821;&#35328;&#30340;40k&#23567;&#26102;&#25968;&#25454;&#12290;&#20026;&#20102;&#26500;&#24314;WavLabLM&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#25968;&#25454;&#30340;&#35821;&#35328;&#22833;&#34913;&#38382;&#39064;&#12290;WavLabLM&#22312;ML-SUPERB&#19978;&#23454;&#29616;&#20102;&#19982;XLS-R&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#19981;&#21040;10%&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;SSL&#22312;&#23398;&#26415;&#39640;&#24615;&#33021;&#35745;&#31639;&#19978;&#21487;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;vanilla HuBERT Base&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#65292;&#20165;&#20351;&#29992;3%&#30340;&#25968;&#25454;&#12289;4&#20010;GPU&#21644;&#26377;&#38480;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#23601;&#33021;&#20445;&#25345;94%&#30340;XLS-R&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than 10% of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain 94% of XLS-R's performance with only 3% of the data, 4 GPUs, and limited trials. 
&lt;/p&gt;</description></item><item><title>MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15312</link><description>&lt;p&gt;
MAPTree: &#29992;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#20987;&#36133;&#8220;&#26368;&#20248;&#8221;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15312
&lt;/p&gt;
&lt;p&gt;
MAPTree&#26159;&#19968;&#31181;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23545;&#20915;&#31574;&#26641;&#36827;&#34892;&#24402;&#32435;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;AND/OR&#25628;&#32034;&#23454;&#29616;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#24674;&#22797;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MAPTree&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#20197;&#26356;&#23567;&#30340;&#26641;&#26469;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;MAPTree&#36824;&#23637;&#31034;&#20986;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#20173;&#28982;&#26159;&#24403;&#20170;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#24320;&#31665;&#21363;&#29992;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20915;&#31574;&#26641;&#24402;&#32435;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20915;&#31574;&#26641;&#30340;&#26368;&#22823;&#21518;&#39564;&#25512;&#29702;&#19982;AND/OR&#25628;&#32034;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MAPTree&#30340;AND/OR&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#19990;&#30028;&#22330;&#26223;&#20013;&#23637;&#31034;&#26368;&#22823;&#21518;&#39564;&#26641;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;16&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#35201;&#20040;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#35201;&#20040;&#22312;&#24615;&#33021;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#23567;&#30340;&#26641;&#12290;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#65292;MAPTree&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24378;&#30340;&#25239;&#22122;&#22768;&#33021;&#21147;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;MAPTree&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#22320;&#24674;&#22797;&#20986;&#26368;&#22823;&#21518;&#39564;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#65292;&#26088;&#22312;&#22686;&#24378;&#24863;&#30693;&#24773;&#24863;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#22791;&#34920;&#36798;&#34892;&#20026;&#30340;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#20013;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.15311</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#23545;&#20110;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents. (arXiv:2309.15311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#65292;&#26088;&#22312;&#22686;&#24378;&#24863;&#30693;&#24773;&#24863;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#22791;&#34920;&#36798;&#34892;&#20026;&#30340;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#20013;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#26377;&#20851;&#23545;&#20110;&#20855;&#36523;&#34394;&#25311;&#26234;&#33021;&#20307;&#24773;&#24863;&#24863;&#30693;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#34394;&#25311;&#35282;&#33394;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#20256;&#36798;&#24773;&#24863;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#34920;&#36798;&#34892;&#20026;&#30340;&#33258;&#20027;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#21512;&#25104;&#27599;&#31181;&#27169;&#24577;&#30340;&#23545;&#35805;&#34892;&#20026;&#38750;&#24120;&#34920;&#36798;&#24615;&#65292;&#20687;&#30495;&#23454;&#20154;&#31867;&#34892;&#20026;&#19968;&#26679;&#20855;&#26377;&#34920;&#36798;&#24615;&#30340;&#22256;&#38590;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#24773;&#24863;&#34987;&#29420;&#31435;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#29983;&#25104;&#22312;&#25152;&#26377;&#27169;&#24577;&#19978;&#20855;&#26377;&#19968;&#33268;&#24773;&#24863;&#30340;&#22810;&#27169;&#24577;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65288;&#19968;&#33268;&#24615;&#24773;&#24863;&#22810;&#27169;&#24577;&#34892;&#20026;&#29983;&#25104;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#26469;&#22686;&#24378;&#24773;&#24863;&#24863;&#30693;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#25307;&#21215;&#20102;199&#21517;&#21442;&#19982;&#32773;&#65292;&#20197;&#35780;&#20272;&#26222;&#36890;&#20154;&#22914;&#20309;&#21028;&#26029;&#20174;&#22810;&#27169;&#24577;&#29366;&#24577;&#19979;&#24863;&#30693;&#21040;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies regarding the perception of emotions for embodied virtual agents have shown the effectiveness of using virtual characters in conveying emotions through interactions with humans. However, creating an autonomous embodied conversational agent with expressive behaviors presents two major challenges. The first challenge is the difficulty of synthesizing the conversational behaviors for each modality that are as expressive as real human behaviors. The second challenge is that the affects are modeled independently, which makes it difficult to generate multimodal responses with consistent emotions across all modalities. In this work, we propose a conceptual framework, ACTOR (Affect-Consistent mulTimodal behaviOR generation), that aims to increase the perception of affects by generating multimodal behaviors conditioned on a consistent driving affect. We have conducted a user study with 199 participants to assess how the average person judges the affects perceived from multimoda
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2309.15302</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#26080;&#32422;&#26463;&#26426;&#22120;&#20154;&#32463;&#39564;&#20013;&#30340;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#24418;&#35748;&#30693;&#65292;&#21363;&#36776;&#21035;&#21644;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22320;&#24418;&#65292;&#26159;&#26426;&#22120;&#20154;&#22312;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#24517;&#39035;&#20855;&#22791;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#30446;&#21069;&#25552;&#20379;&#26426;&#22120;&#20154;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#26080;&#27861;&#27867;&#21270;&#30340;&#24037;&#31243;&#29305;&#24449;&#21644;&#25104;&#26412;&#20989;&#25968;&#65292;&#25110;&#32773;&#20381;&#36182;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#30340;&#19987;&#23478;&#20154;&#31867;&#31034;&#33539;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#19981;&#21463;&#36825;&#20123;&#38480;&#21046;&#22320;&#20855;&#22791;&#22320;&#24418;&#35748;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#65288;STERLING&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22320;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#26131;&#20110;&#25910;&#38598;&#30340;&#12289;&#26080;&#32422;&#26463;&#65288;&#20363;&#22914;&#65292;&#38750;&#19987;&#23478;&#30340;&#65289;&#21644;&#26410;&#26631;&#35760;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#65292;&#23545;&#25968;&#25454;&#37319;&#38598;&#27809;&#26377;&#39069;&#22806;&#30340;&#38480;&#21046;&#12290;STERLING&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#22320;&#24418;&#30456;&#20851;&#30340;&#34920;&#31034;&#20197;&#29992;&#20110;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#23454;&#29289;&#26426;&#22120;&#20154;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LBP-WHT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;LBP-WHT&#26041;&#27861;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LBP-WHT&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15275</link><description>&lt;p&gt;
Vision Transformer&#36866;&#24212;&#30340;&#39640;&#25928;&#20302;&#31209;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Low-rank Backpropagation for Vision Transformer Adaptation. (arXiv:2309.15275v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LBP-WHT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;LBP-WHT&#26041;&#27861;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LBP-WHT&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#22823;&#65292;&#20351;&#24471;&#20026;&#29305;&#23450;&#38656;&#27714;&#23545;&#36825;&#20123;&#22823;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;ViT&#30340;&#32447;&#24615;&#23618;&#20013;&#38656;&#35201;&#30340;&#35745;&#31639;&#37327;&#22823;&#30340;&#30697;&#38453;&#20056;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Walsh-Hadamard&#21464;&#25442;&#30340;&#20302;&#31209;&#21453;&#21521;&#20256;&#25773;&#65288;LBP-WHT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;LBP-WHT&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20302;&#31209;&#31354;&#38388;&#65292;&#24182;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#36866;&#24212;ViT&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#65292;&#22240;&#20026;&#20302;&#31209;&#31354;&#38388;&#20013;&#30340;&#30697;&#38453;&#20056;&#27861;&#36739;&#23569;&#21344;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#65288;ViT&#12289;&#28151;&#21512;&#21367;&#31215;-ViT&#27169;&#22411;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#23545;CIFAR100&#19978;&#30340;EfficientFormer-L1&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#26102;&#65292;&#25105;&#20204;&#30340;LBP-WHT&#30456;&#27604;&#26631;&#20934;&#26041;&#27861;&#25552;&#39640;&#20102;10.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT. In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4% higher accuracy than the st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#22312;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#36827;&#34892;&#30446;&#26631;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.15274</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#38271;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Continual Learning Object Segmentation for Long Video. (arXiv:2309.15274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#22312;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#36827;&#34892;&#30446;&#26631;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#26041;&#27861;&#22312;&#21033;&#29992;&#21069;&#20960;&#24103;&#30340;&#20449;&#24687;&#36827;&#34892;&#24403;&#21069;&#24103;&#20998;&#21106;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#30446;&#26631;&#23545;&#35937;&#20998;&#21106;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#22806;&#35266;&#21464;&#21270;&#65288;&#34920;&#36798;&#28418;&#31227;&#65289;&#25110;&#36974;&#25377;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#22312;&#32447;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#65288;VOS&#65289;&#26041;&#27861;&#38656;&#35201;&#23558;&#25152;&#26377;&#25110;&#22823;&#37096;&#20998;&#21069;&#20960;&#24103;&#65288;&#25110;&#20854;&#25552;&#21462;&#30340;&#20449;&#24687;&#65289;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#65292;&#24182;&#29992;&#20110;&#36830;&#32493;&#24103;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38271;&#35270;&#39057;&#26469;&#35828;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#25152;&#38656;&#30340;&#20869;&#23384;&#22823;&#23567;&#20250;&#26080;&#38480;&#22686;&#38271;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20869;&#23384;&#26377;&#38480;&#19988;&#30446;&#26631;&#23545;&#35937;&#22312;&#35270;&#39057;&#20013;&#37325;&#22797;&#21457;&#29983;&#34920;&#36798;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent state-of-the-art semi-supervised Video Object Segmentation (VOS) methods have shown significant improvements in target object segmentation accuracy when information from preceding frames is used in undertaking segmentation on the current frame. In particular, such memory-based approaches can help a model to more effectively handle appearance changes (representation drift) or occlusions. Ideally, for maximum performance, online VOS methods would need all or most of the preceding frames (or their extracted information) to be stored in memory and be used for online learning in consecutive frames. Such a solution is not feasible for long videos, as the required memory size would grow without bound. On the other hand, these methods can fail when memory is limited and a target object experiences repeated representation drifts throughout a video.  We propose two novel techniques to reduce the memory requirement of online VOS methods while improving modeling accuracy and generalization 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>VPA&#26159;&#39318;&#20010;&#23558;&#35270;&#35273;&#25552;&#31034;&#19982;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#20196;&#29260;&#23454;&#29616;&#20102;&#23436;&#20840;&#27979;&#35797;&#26102;&#38388;&#21644;&#23384;&#20648;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VPA&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15251</link><description>&lt;p&gt;
VPA: &#20840;&#38754;&#27979;&#35797;&#26102;&#38388;&#35270;&#35273;&#25552;&#31034;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
VPA: Fully Test-Time Visual Prompt Adaptation. (arXiv:2309.15251v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15251
&lt;/p&gt;
&lt;p&gt;
VPA&#26159;&#39318;&#20010;&#23558;&#35270;&#35273;&#25552;&#31034;&#19982;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#20196;&#29260;&#23454;&#29616;&#20102;&#23436;&#20840;&#27979;&#35797;&#26102;&#38388;&#21644;&#23384;&#20648;&#39640;&#25928;&#30340;&#36866;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VPA&#33021;&#22815;&#26377;&#25928;&#25552;&#21319;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#36890;&#36807;&#23558;&#25163;&#24037;&#35843;&#25972;&#30340;&#25552;&#31034;&#20316;&#20026;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36866;&#24212;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#25991;&#26412;&#25552;&#31034;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Prompt Adaptation&#65288;VPA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#35270;&#35273;&#25552;&#31034;&#19982;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;VPA&#24341;&#20837;&#20102;&#23569;&#37327;&#30340;&#21487;&#23398;&#20064;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#27979;&#35797;&#26102;&#38388;&#21644;&#23384;&#20648;&#39640;&#25928;&#30340;&#36866;&#24212;&#65292;&#32780;&#19981;&#38656;&#35201;&#28304;&#22495;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#36866;&#24212;&#35774;&#32622;&#19979;&#26816;&#39564;&#20102;VPA&#35774;&#35745;&#65292;&#21253;&#25324;&#21333;&#22270;&#20687;&#12289;&#25209;&#27425;&#22270;&#20687;&#21644;&#20266;&#26631;&#31614;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;VPA&#65292;&#21253;&#25324;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25439;&#22351;&#40065;&#26834;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VPA&#36890;&#36807;3.3%&#26377;&#25928;&#25552;&#39640;&#20102;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual prompt tuning has demonstrated significant performance improvements in adapting natural language processing models to a variety of downstream tasks by treating hand-engineered prompts as trainable parameters. Inspired by the success of textual prompting, several studies have investigated the efficacy of visual prompt tuning. In this work, we present Visual Prompt Adaptation (VPA), the first framework that generalizes visual prompting with test-time adaptation. VPA introduces a small number of learnable tokens, enabling fully test-time and storage-efficient adaptation without necessitating source-domain information. We examine our VPA design under diverse adaptation settings, encompassing single-image, batched-image, and pseudo-label adaptation. We evaluate VPA on multiple tasks, including out-of-distribution (OOD) generalization, corruption robustness, and domain adaptation. Experimental results reveal that VPA effectively enhances OOD generalization by 3.3% across various mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeMAnD&#30340;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#21644;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2309.15245</link><description>&lt;p&gt;
SeMAnD:&#33258;&#30417;&#30563;&#22810;&#27169;&#24335;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SeMAnD: Self-Supervised Anomaly Detection in Multimodal Geospatial Datasets. (arXiv:2309.15245v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeMAnD&#30340;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#31216;&#20026;SeMAnD&#65292;&#29992;&#20110;&#26816;&#27979;&#22810;&#27169;&#24577;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#20013;&#30340;&#20960;&#20309;&#24322;&#24120;&#12290;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#21253;&#25324;&#33719;&#21462;&#21644;&#34893;&#29983;&#30340;&#24322;&#26500;&#25968;&#25454;&#27169;&#24577;&#65292;&#25105;&#20204;&#23558;&#20854;&#36716;&#25442;&#20026;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#12289;&#31867;&#20284;&#22270;&#20687;&#30340;&#24352;&#37327;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#34920;&#31034;&#12289;&#23545;&#40784;&#21644;&#34701;&#21512;&#30340;&#25361;&#25112;&#12290;SeMAnD&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#31216;&#20026;RandPolyAugment&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30690;&#37327;&#20960;&#20309;&#22686;&#24378;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20855;&#26377;&#19977;&#20010;&#32452;&#20214;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#65292;&#28608;&#21169;&#23398;&#20064;&#23545;&#19968;&#31181;&#27169;&#24577;&#20013;&#30340;&#23616;&#37096;&#21464;&#21270;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34920;&#31034;&#65292;&#36825;&#31181;&#21464;&#21270;&#22312;&#20854;&#20182;&#27169;&#24577;&#20013;&#27809;&#26377;&#24471;&#21040;&#35777;&#23454;&#12290;&#22312;&#22320;&#29702;&#31354;&#38388;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#26816;&#27979;&#23616;&#37096;&#32570;&#38519;&#33267;&#20851;&#37325;&#35201;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24322;&#24120;&#65288;&#22914;&#31227;&#20301;&#12289;&#38169;&#35823;&#36830;&#25509;&#12289;&#30072;&#24418;&#25110;&#32570;&#22833;&#30340;&#22810;&#36793;&#24418;&#30690;&#37327;&#20960;&#20309;&#65292;&#22914;&#36947;&#36335;&#12289;&#24314;&#31569;&#29289;&#12289;&#22320;&#34920;&#35206;&#30422;&#31561;&#65289;&#20063;&#26159;&#26377;&#23475;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Self-supervised Anomaly Detection technique, called SeMAnD, to detect geometric anomalies in Multimodal geospatial datasets. Geospatial data comprises of acquired and derived heterogeneous data modalities that we transform to semantically meaningful, image-like tensors to address the challenges of representation, alignment, and fusion of multimodal data. SeMAnD is comprised of (i) a simple data augmentation strategy, called RandPolyAugment, capable of generating diverse augmentations of vector geometries, and (ii) a self-supervised training objective with three components that incentivize learning representations of multimodal data that are discriminative to local changes in one modality which are not corroborated by the other modalities. Detecting local defects is crucial for geospatial anomaly detection where even small anomalies (e.g., shifted, incorrectly connected, malformed, or missing polygonal vector geometries like roads, buildings, landcover, etc.) are detrimenta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.15242</link><description>&lt;p&gt;
PlotMap&#65306;&#29992;&#20110;&#26500;&#24314;&#28216;&#25103;&#19990;&#30028;&#30340;&#33258;&#21160;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PlotMap: Automated Layout Design for Building Game Worlds. (arXiv:2309.15242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#26159;&#24320;&#21457;&#28216;&#25103;&#30340;&#21465;&#20107;&#21644;&#29289;&#29702;&#19990;&#30028;&#30340;&#36807;&#31243;&#65292;&#22312;&#28216;&#25103;&#20307;&#39564;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22791;&#21463;&#22909;&#35780;&#30340;&#29420;&#31435;&#28216;&#25103;&#21644;AAA&#32423;&#35270;&#39057;&#28216;&#25103;&#34987;&#36190;&#36175;&#20854;&#20248;&#31168;&#30340;&#19990;&#30028;&#26500;&#24314;&#65292;&#20854;&#20013;&#28216;&#25103;&#22320;&#22270;&#19982;&#21465;&#20107;&#32039;&#23494;&#34701;&#21512;&#24182;&#25552;&#21319;&#20102;&#28216;&#25103;&#20307;&#39564;&#65292;&#21560;&#24341;&#20102;&#29609;&#23478;&#24182;&#30041;&#19979;&#20102;&#28145;&#21051;&#30340;&#21360;&#35937;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28385;&#36275;&#21508;&#31181;&#32771;&#34385;&#22240;&#32032;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#28216;&#25103;&#26426;&#21046;&#25110;&#22320;&#22270;&#22320;&#24418;&#30340;&#32771;&#34385;&#65292;&#32780;&#24573;&#35270;&#20102;&#25903;&#25345;&#25925;&#20107;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#35774;&#35745;&#20986;&#36866;&#24212;&#29305;&#23450;&#25925;&#20107;&#30340;&#28216;&#25103;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#27969;&#31243;&#20013;&#24341;&#20837;&#19968;&#20010;&#19982;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#26080;&#20851;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#39069;&#22806;&#23618;&#38754;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
World-building, the process of developing both the narrative and physical world of a game, plays a vital role in the game's experience. Critically acclaimed independent and AAA video games are praised for strong world building, with game maps that masterfully intertwine with and elevate the narrative, captivating players and leaving a lasting impression. However, designing game maps that support a desired narrative is challenging, as it requires satisfying complex constraints from various considerations. Most existing map generation methods focus on considerations about gameplay mechanics or map topography, while the need to support the story is typically neglected. As a result, extensive manual adjustment is still required to design a game world that facilitates particular stories. In this work, we approach this problem by introducing an extra layer of plot facility layout design that is independent of the underlying map generation method in a world-building pipeline. Concretely, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#35748;&#20026;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#20013;&#20855;&#26377;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21021;&#32423;&#35774;&#35745;&#24072;&#21487;&#33021;&#20250;&#38754;&#20020;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#31561;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#19968;&#20123;&#24433;&#21709;&#65292;&#21253;&#25324;&#29256;&#26435;&#19982;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.15237</link><description>&lt;p&gt;
&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#19987;&#19994;&#20154;&#21592;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
User Experience Design Professionals' Perceptions of Generative Artificial Intelligence. (arXiv:2309.15237v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15237
&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#35748;&#20026;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22312;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#20013;&#20855;&#26377;&#36741;&#21161;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21021;&#32423;&#35774;&#35745;&#24072;&#21487;&#33021;&#20250;&#38754;&#20020;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#31561;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#19968;&#20123;&#24433;&#21709;&#65292;&#21253;&#25324;&#29256;&#26435;&#19982;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#20013;&#65292;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#24341;&#36215;&#20102;&#23545;&#20854;&#33021;&#21147;&#30340;&#20852;&#22859;&#21644;&#23545;&#26410;&#39044;&#26399;&#21518;&#26524;&#30340;&#25285;&#24551;&#12290;GenAI&#22914;&#20309;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#65288;UXD&#65289;&#23454;&#36341;&#65292;&#24182;&#19988;&#36825;&#20123;&#25285;&#24551;&#26159;&#21542;&#21512;&#29702;&#65311;&#25105;&#20204;&#37319;&#35775;&#20102;20&#20301;UX&#35774;&#35745;&#24072;&#65292;&#20182;&#20204;&#25317;&#26377;&#20016;&#23500;&#30340;&#32463;&#39564;&#65292;&#26469;&#33258;&#21508;&#31181;&#20844;&#21496;&#65288;&#21019;&#19994;&#20844;&#21496;&#21040;&#22823;&#22411;&#20225;&#19994;&#65289;&#12290;&#25105;&#20204;&#35810;&#38382;&#20182;&#20204;&#30340;&#23454;&#36341;&#29305;&#24449;&#65292;&#24182;&#20102;&#35299;&#20182;&#20204;&#30340;&#24577;&#24230;&#12289;&#20851;&#27880;&#28857;&#21644;&#26399;&#26395;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#35774;&#35745;&#24072;&#23545;&#20182;&#20204;&#30340;&#21407;&#21019;&#24615;&#12289;&#21019;&#36896;&#21147;&#21644;&#20849;&#24773;&#33021;&#21147;&#20805;&#28385;&#20449;&#24515;&#65292;&#24182;&#35748;&#20026;GenAI&#30340;&#35282;&#33394;&#26159;&#36741;&#21161;&#24615;&#30340;&#12290;&#20182;&#20204;&#24378;&#35843;&#20102;"&#20139;&#21463;"&#21644;"&#20195;&#29702;"&#36825;&#20004;&#20010;&#29420;&#29305;&#30340;&#20154;&#31867;&#22240;&#32032;&#65292;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#20154;&#31867;&#20173;&#28982;&#26159;"AI&#23545;&#40784;"&#30340;&#20210;&#35009;&#32773;&#12290;&#28982;&#32780;&#65292;&#25216;&#33021;&#36864;&#21270;&#12289;&#24037;&#20316;&#26367;&#20195;&#21644;&#21019;&#36896;&#21147;&#26543;&#31469;&#21487;&#33021;&#23545;&#21021;&#32423;&#35774;&#35745;&#24072;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;-GenAI&#21327;&#20316;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#29256;&#26435;&#21644;&#25152;&#26377;&#26435;&#12289;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;&#20195;&#29702;&#24615;&#65292;&#20197;&#21450;AI&#32032;&#20859;&#21644;&#33719;&#21462;&#26041;&#38754;&#12290;&#36890;&#36807;&#36825;&#20010;&#35270;&#35282;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#21069;&#20154;&#30740;&#31350;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among creative professionals, Generative Artificial Intelligence (GenAI) has sparked excitement over its capabilities and fear over unanticipated consequences. How does GenAI impact User Experience Design (UXD) practice, and are fears warranted? We interviewed 20 UX Designers, with diverse experience and across companies (startups to large enterprises). We probed them to characterize their practices, and sample their attitudes, concerns, and expectations. We found that experienced designers are confident in their originality, creativity, and empathic skills, and find GenAI's role as assistive. They emphasized the unique human factors of "enjoyment" and "agency", where humans remain the arbiters of "AI alignment". However, skill degradation, job replacement, and creativity exhaustion can adversely impact junior designers. We discuss implications for human-GenAI collaboration, specifically copyright and ownership, human creativity and agency, and AI literacy and access. Through the lens 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.15223</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#20998;&#35299;&#26041;&#27861;&#21644;&#20248;&#21270;&#25554;&#20837;&#30697;&#38453;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;BERT&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#31995;&#32479;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#36755;&#20986;&#37325;&#35780;&#20998;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#31532;&#20108;&#27425;&#37325;&#35780;&#20998;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23558;&#39044;&#35757;&#32451;&#38454;&#27573;&#25193;&#23637;&#21644;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#37325;&#35780;&#20998;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;0.08%&#65289;&#26469;&#35757;&#32451;&#37325;&#35780;&#20998;&#30340;BERT&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#36825;&#20123;&#25554;&#20837;&#30340;&#30697;&#38453;&#36890;&#36807;&#30456;&#20851;&#24615;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#21028;&#21035;&#24615;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;Rescore-BERT&#65288;LoRB&#65289;&#20307;&#31995;&#32467;&#26500;&#22312;LibriSpeech&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;5.4&#33267;3.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
&lt;/p&gt;</description></item><item><title>&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15178</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative World Models. (arXiv:2309.15178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15178
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#25215;&#35834;&#22312;&#31163;&#32447;&#39044;&#35757;&#32451;&#38454;&#27573;&#21518;&#65292;&#25552;&#20379;&#33021;&#22815;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#21069;&#21521;-&#21518;&#21521;&#65288;FB&#65289;&#34920;&#31034;&#22312;&#36825;&#20010;&#29702;&#24819;&#30340;&#23454;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#36798;&#21040;&#29305;&#23450;&#20219;&#21153;&#20195;&#29702;&#30340;85%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#23545;&#20110;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#38382;&#39064;&#26080;&#27861;&#26399;&#26395;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;FB&#24615;&#33021;&#22914;&#20309;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#20445;&#23432;&#24615;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23478;&#26063;&#65292;&#22312;&#24635;&#20307;&#19978;&#36798;&#21040;&#20102;150%&#30340;&#26222;&#36890;FB&#24615;&#33021;&#12290;&#26377;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20445;&#23432;&#30340;FB&#31639;&#27861;&#22312;&#27809;&#26377;&#35775;&#38382;&#22870;&#21169;&#26631;&#31614;&#19988;&#38656;&#35201;&#32500;&#25252;&#25152;&#26377;&#20219;&#21153;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;</title><link>http://arxiv.org/abs/2309.15169</link><description>&lt;p&gt;
&#25581;&#31034;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#26469;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#31354;&#38388;&#25513;&#34109;&#21644;&#26102;&#38388;&#25513;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#28041;&#21450;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#24320;&#21457;&#33021;&#22815;&#26126;&#30830;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#31354;&#38388;-&#26102;&#38388;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;STMAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;MTS&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#25552;&#39640;&#31354;&#38388;-&#26102;&#38388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;STMAE&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#32534;&#30721;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#25513;&#34109;&#31574;&#30053;&#22788;&#29702;&#37096;&#20998;&#21487;&#35265;&#30340;MTS&#25968;&#25454;&#65292;&#21253;&#25324;&#22522;&#20110;&#20559;&#32622;&#38543;&#26426;&#28216;&#36208;&#30340;&#31354;&#38388;&#25513;&#34109;&#21644;&#22522;&#20110;&#34917;&#19969;&#30340;&#26102;&#38388;&#25513;&#34109;&#12290;&#38543;&#21518;&#65292;&#35299;&#30721;&#22120;&#26088;&#22312;&#37325;&#26500;&#20004;&#20010;&#25513;&#34109;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;RGB-D&#22270;&#20687;&#26144;&#23556;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#21363;&#21487;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15164</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#36827;&#34892;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
3D Reconstruction with Generalizable Neural Fields using Scene Priors. (arXiv:2309.15164v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15164
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#20808;&#39564;&#30340;&#21487;&#25512;&#24191;&#31070;&#32463;&#22330;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#20010;RGB-D&#22270;&#20687;&#26144;&#23556;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#21363;&#21487;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#22330;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#22330;&#26223;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;&#22330;&#26223;&#37117;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#25928;&#29575;&#20302;&#19979;&#65292;&#19988;&#22312;&#26377;&#38480;&#35270;&#35282;&#19979;&#26080;&#27861;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#32780;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#35270;&#22270;&#31435;&#20307;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20854;&#22810;&#35270;&#22270;&#35774;&#32622;&#20351;&#24471;&#23427;&#22312;&#25193;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#19981;&#22826;&#28789;&#27963;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35757;&#32451;&#21487;&#25512;&#24191;&#30340;&#34701;&#20837;&#22330;&#26223;&#20808;&#39564;&#30340;&#31070;&#32463;&#22330;&#65288;NFPs&#65289;&#26041;&#27861;&#12290;NFP&#32593;&#32476;&#23558;&#20219;&#20309;&#21333;&#35270;&#35282;&#30340;RGB-D&#22270;&#20687;&#26144;&#23556;&#25104;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#36752;&#23556;&#20540;&#12290;&#36890;&#36807;&#22312;&#20307;&#31215;&#31354;&#38388;&#20013;&#21512;&#24182;&#21333;&#24103;&#65292;&#21487;&#20197;&#37325;&#24314;&#23436;&#25972;&#30340;&#22330;&#26223;&#65292;&#26080;&#38656;&#34701;&#21512;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12290;&#22330;&#26223;&#20808;&#39564;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20351;&#29992;&#36739;&#23569;&#35270;&#35282;&#37325;&#24314;&#26032;&#22330;&#26223;&#12290;NFP&#19981;&#20165;&#23637;&#31034;&#20102;SOTA&#22330;&#26223;&#37325;&#24314;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
High-fidelity 3D scene reconstruction has been substantially advanced by recent progress in neural fields. However, most existing methods train a separate network from scratch for each individual scene. This is not scalable, inefficient, and unable to yield good results given limited views. While learning-based multi-view stereo methods alleviate this issue to some extent, their multi-view setting makes it less flexible to scale up and to broad applications. Instead, we introduce training generalizable Neural Fields incorporating scene Priors (NFPs). The NFP network maps any single-view RGB-D image into signed distance and radiance values. A complete scene can be reconstructed by merging individual frames in the volumetric space WITHOUT a fusion module, which provides better flexibility. The scene priors can be trained on large-scale datasets, allowing for fast adaptation to the reconstruction of a new scene with fewer views. NFP not only demonstrates SOTA scene reconstruction performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#21644;&#23454;&#29616;&#33021;&#37327;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.15140</link><description>&lt;p&gt;
AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on AI Algorithms for Energy Management in E-Mobility Services. (arXiv:2309.15140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#31639;&#27861;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#21644;&#23454;&#29616;&#33021;&#37327;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#20986;&#34892;&#20316;&#20026;&#35299;&#20915;&#20132;&#36890;&#37096;&#38376;&#32039;&#36843;&#30340;&#29615;&#22659;&#21644;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#30340;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#23835;&#36215;&#12290;&#21270;&#30707;&#29123;&#26009;&#30340;&#28040;&#32791;&#12289;&#19981;&#26029;&#19978;&#21319;&#30340;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#20197;&#21450;&#23545;&#25239;&#27668;&#20505;&#21464;&#21270;&#30340;&#36843;&#20999;&#38656;&#27714;&#20984;&#26174;&#20102;&#21521;&#30005;&#21160;&#27773;&#36710;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22312;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#20013;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25361;&#25112;&#28041;&#21450;&#20851;&#38190;&#22240;&#32032;&#22914;&#32493;&#33322;&#28966;&#34385;&#12289;&#20805;&#30005;&#36895;&#29575;&#20248;&#21270;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#23384;&#20648;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25991;&#29486;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21644;&#23454;&#29616;&#30005;&#21160;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#24403;&#21069;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-mobility, or electric mobility, has emerged as a pivotal solution to address pressing environmental and sustainability concerns in the transportation sector. The depletion of fossil fuels, escalating greenhouse gas emissions, and the imperative to combat climate change underscore the significance of transitioning to electric vehicles (EVs). This paper seeks to explore the potential of artificial intelligence (AI) in addressing various challenges related to effective energy management in e-mobility systems (EMS). These challenges encompass critical factors such as range anxiety, charge rate optimization, and the longevity of energy storage in EVs. By analyzing existing literature, we delve into the role that AI can play in tackling these challenges and enabling efficient energy management in EMS. Our objectives are twofold: to provide an overview of the current state-of-the-art in this research domain and propose effective avenues for future investigations. Through this analysis, we a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PINF&#65292;&#36825;&#26159;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#21644;&#25193;&#25955;&#26469;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15139</link><description>&lt;p&gt;
PINF&#65306;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
PINF: Continuous Normalizing Flows for Physics-Constrained Deep Learning. (arXiv:2309.15139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PINF&#65292;&#36825;&#26159;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#21644;&#25193;&#25955;&#26469;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23494;&#24230;&#30340;&#26631;&#20934;&#21270;&#32422;&#26463;&#23545;&#20110;&#35299;&#20915;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26631;&#20934;&#21270;&#27969;&#26159;&#19968;&#31181;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#21464;&#37327;&#21464;&#25442;&#20844;&#24335;&#30830;&#20445;&#27010;&#29575;&#23494;&#24230;&#23432;&#24658;&#65292;&#24182;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#30340;&#26032;&#39062;&#25193;&#23637;&#8212;&#8212;&#29289;&#29702;&#32422;&#26463;&#26631;&#20934;&#21270;&#27969;&#65288;PINF&#65289;&#65292;&#36890;&#36807;&#29305;&#24449;&#20540;&#27861;&#21017;&#32467;&#21512;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#26080;&#32593;&#26684;&#21644;&#26080;&#22240;&#26524;&#24615;&#30340;&#39640;&#25928;&#35299;&#20915;&#39640;&#32500;&#26102;&#21464;&#21644;&#31283;&#24577;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The normalization constraint on probability density poses a significant challenge for solving the Fokker-Planck equation. Normalizing Flow, an invertible generative model leverages the change of variables formula to ensure probability density conservation and enable the learning of complex data distributions. In this paper, we introduce Physics-Informed Normalizing Flows (PINF), a novel extension of continuous normalizing flows, incorporating diffusion through the method of characteristics. Our method, which is mesh-free and causality-free, can efficiently solve high dimensional time-dependent and steady-state Fokker-Planck equations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#22312;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;adapt autoregressive networks&#21644;normalizing flows&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#24403;&#21069;&#30340;copula-based&#32479;&#35745;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#27861;&#22269;TSO RTE&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15137</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Methods for Producing Forecast Trajectories in Power Systems. (arXiv:2309.15137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#22312;&#21457;&#30005;&#31995;&#32479;&#39044;&#27979;&#36712;&#36857;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;adapt autoregressive networks&#21644;normalizing flows&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#24403;&#21069;&#30340;copula-based&#32479;&#35745;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#27861;&#22269;TSO RTE&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#21147;&#28151;&#21512;&#20013;&#30340;&#25193;&#24352;&#65292;&#30005;&#32593;&#30340;&#21464;&#21270;&#24615;&#23558;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#21152;&#24378;&#31995;&#32479;&#20197;&#20445;&#35777;&#20854;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#36755;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;&#24517;&#39035;&#36827;&#34892;&#20998;&#26512;&#65292;&#27169;&#25311;&#26410;&#26469;&#21457;&#30005;&#31995;&#32479;&#30340;&#36816;&#34892;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#34987;&#29992;&#20316;&#20915;&#31574;&#36807;&#31243;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#29983;&#25104;&#33021;&#28304;&#20135;&#37327;&#21644;&#36127;&#33655;&#39044;&#27979;&#36712;&#36857;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#22238;&#24402;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#27604;&#24403;&#21069;&#22522;&#20110;copula&#30340;&#32479;&#35745;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#27861;&#22269;&#36755;&#30005;&#31995;&#32479;&#36816;&#33829;&#21830;RTE&#30340;&#39118;&#21147;&#39044;&#27979;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the expansion of renewables in the electricity mix, power grid variability will increase, hence a need to robustify the system to guarantee its security. Therefore, Transport System Operators (TSOs) must conduct analyses to simulate the future functioning of power systems. Then, these simulations are used as inputs in decision-making processes. In this context, we investigate using deep learning models to generate energy production and load forecast trajectories. To capture the spatiotemporal correlations in these multivariate time series, we adapt autoregressive networks and normalizing flows, demonstrating their effectiveness against the current copula-based statistical approach. We conduct extensive experiments on the French TSO RTE wind forecast data and compare the different models with \textit{ad hoc} evaluation metrics for time series generation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15135</link><description>&lt;p&gt;
&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#36866;&#29992;&#20110;&#20808;&#21069;&#25910;&#38598;&#35270;&#22270;&#24182;&#25552;&#21462;&#19968;&#33268;&#21644;&#20114;&#34917;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#35270;&#22270;&#25353;&#39034;&#24207;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#35270;&#22270;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#25552;&#21462;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24694;&#24847;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.15133</link><description>&lt;p&gt;
&#20174;&#36164;&#20135;&#27969;&#21040;&#29366;&#24577;&#12289;&#34892;&#21160;&#21644;&#24847;&#22270;&#30340;&#21457;&#29616;&#65306;&#21152;&#23494;&#36135;&#24065;&#20013;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Asset Flow to Status, Action and Intention Discovery: Early Malice Detection in Cryptocurrency. (arXiv:2309.15133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#21644;&#25552;&#21462;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24694;&#24847;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#20027;&#20307;&#30340;&#20266;&#21311;&#21517;&#24615;&#36136;&#65292;&#21152;&#23494;&#36135;&#24065;&#24448;&#24448;&#27604;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#26356;&#23481;&#26131;&#21463;&#21040;&#38750;&#27861;&#27963;&#21160;&#30340;&#24433;&#21709;&#12290;&#29702;&#24819;&#30340;&#26816;&#27979;&#27169;&#22411;&#24212;&#35813;&#28385;&#36275;&#26089;&#26399;&#26816;&#27979;&#12289;&#33391;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#20110;&#21508;&#31181;&#38750;&#27861;&#27963;&#21160;&#30340;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#32780;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#21482;&#36866;&#29992;&#20110;&#29305;&#23450;&#38750;&#27861;&#31867;&#22411;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#29305;&#24065; (BTC) &#30340;&#26089;&#26399;&#24694;&#24847;&#26816;&#27979;&#30340;&#24847;&#22270;&#30417;&#25511;&#31995;&#32479;&#65292;&#20854;&#20013;&#26576;&#20010;&#22320;&#22336;&#30340;&#38142;&#19978;&#35760;&#24405;&#25968;&#25454;&#27604;&#20854;&#20182;&#21152;&#23494;&#36135;&#24065;&#24179;&#21488;&#26356;&#31232;&#32570;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34917;&#20805;&#65288;DT-SC&#65289;&#26469;&#23450;&#20041;&#36164;&#20135;&#36716;&#31227;&#36335;&#24452;&#65292;&#20026;&#19981;&#21516;&#30340;&#24694;&#24847;&#31867;&#22411;&#26500;&#24314;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#29366;&#24577;/&#34892;&#21160;&#25552;&#26696;&#27169;&#22359;&#65288;S/A-PM&#65289;&#33719;&#21462;&#21487;&#33021;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#65292;&#36827;&#19968;&#27493;&#21457;&#29616;&#24694;&#24847;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of (I) early detection, (II) good interpretability, and (III) versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention-Monitor for early malice detection in Bitcoin (BTC), where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision-Tree based feature Selection and Complement (DT-SC) to build different feature sets for different malice types. Then, the Status/Action Proposal Module (S/A-PM) an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15129</link><description>&lt;p&gt;
&#29992;CogEval&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. (arXiv:2309.15129v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;CogEval&#21327;&#35758;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#35813;&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#22768;&#31216;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26032;&#20852;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#26696;&#20363;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#38598;&#30340;&#27745;&#26579;&#65292;&#25110;&#32773;&#32570;&#20047;&#28041;&#21450;&#22810;&#20010;&#20219;&#21153;&#12289;&#25511;&#21046;&#26465;&#20214;&#12289;&#22810;&#27425;&#36845;&#20195;&#21644;&#32479;&#35745;&#40065;&#26834;&#24615;&#27979;&#35797;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#37325;&#22823;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CogEval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#35748;&#30693;&#31185;&#23398;&#21551;&#21457;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;CogEval&#21327;&#35758;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;CogEval&#21327;&#35758;&#23545;&#20843;&#20010;LLMs&#65288;OpenAI GPT-4&#12289;GPT-3.5-turbo-175B&#12289;davinci-003-175B&#12289;Google Bard&#12289;Cohere-xlarge-52.4B&#12289;Anthropic Claude-1-52B&#12289;LLaMA-13B&#21644;Alpaca-7B&#65289;&#30340;&#35748;&#30693;&#22320;&#22270;&#21644;&#35268;&#21010;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#25552;&#31034;&#22522;&#20110;&#20154;&#31867;&#23454;&#39564;&#65292;&#26082;&#20855;&#26377;&#35780;&#20272;&#35268;&#21010;&#30340;&#24050;&#24314;&#31435;&#26500;&#36896;&#25928;&#24230;&#65292;&#21448;&#19981;&#23384;&#22312;&#20110;LLM&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#19968;&#20123;
&lt;/p&gt;
&lt;p&gt;
Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14877</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainable Sustainability for AI in the Arts. (arXiv:2309.14877v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22312;&#33402;&#26415;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#29992;&#20110;&#36890;&#30693;&#20174;&#19994;&#32773;&#26377;&#20851;AI&#30340;&#29615;&#22659;&#24433;&#21709;&#65288;&#20197;&#21450;&#20854;&#20182;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#65289;&#30340;&#24037;&#20855;&#36866;&#29992;&#20110;&#20854;&#20182;&#32972;&#26223;&#29615;&#22659;&#65292;&#32780;&#38750;&#21019;&#24847;&#23454;&#36341;&#32972;&#26223;&#65292;&#36825;&#20351;&#24471;&#33402;&#26415;&#23478;&#21644;&#21019;&#24847;&#20174;&#19994;&#32773;&#26080;&#27861;&#33719;&#21462;AI&#24037;&#20855;&#21644;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#25551;&#36848;&#20102;&#20004;&#20010;&#26088;&#22312;&#20026;AI&#33402;&#26415;&#24320;&#21457;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#21453;&#24605;&#31995;&#32479;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#21644;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#33402;&#26415;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI is becoming increasingly popular in artistic practices, but the tools for informing practitioners about the environmental impact (and other sustainability implications) of AI are adapted for other contexts than creative practices -- making the tools and sustainability implications of AI not accessible for artists and creative practitioners. In this position paper, I describe two empirical studies that aim to develop environmental sustainability reflection systems for AI Arts, and discuss and introduce Explainable Sustainability in for AI Arts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.14425</link><description>&lt;p&gt;
&#33258;&#24674;&#22797;&#25552;&#31034;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;&#33258;&#24674;&#22797;&#30340;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery. (arXiv:2309.14425v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#29615;&#22659;&#30340;&#21464;&#21270;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#33258;&#24674;&#22797;&#26426;&#21046;&#35299;&#20915;&#20449;&#24687;&#19981;&#36275;&#12289;&#35745;&#21010;&#29983;&#25104;&#38169;&#35823;&#21644;&#25191;&#34892;&#22833;&#36133;&#31561;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20219;&#21153;&#30340;&#25104;&#21151;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#65288;GPSR&#65289;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#39640;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#31995;&#32479;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#26412;&#25991;&#39318;&#20808;&#22522;&#20110;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#39030;&#23618;GPSR&#31995;&#32479;&#65292;&#29992;&#20110;&#20840;&#29699;&#31454;&#36187;&#65288;RoboCup@Home 2023&#65289;&#12290;&#35813;&#31995;&#32479;&#26082;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#21464;&#21270;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#27599;&#20010;&#27169;&#22411;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#26512;&#25152;&#24320;&#21457;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;GPSR&#24212;&#29992;&#35774;&#32622;&#20013;&#23384;&#22312;&#19977;&#31181;&#22833;&#36133;&#31867;&#22411;&#65306;&#20449;&#24687;&#19981;&#36275;&#12289;&#38169;&#35823;&#30340;&#35745;&#21010;&#29983;&#25104;&#21644;&#35745;&#21010;&#25191;&#34892;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24674;&#22797;&#25552;&#31034;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#25506;&#32034;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#24182;&#20462;&#25913;&#20854;&#25552;&#31034;&#26469;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#20855;&#26377;&#33258;&#24674;&#22797;&#26426;&#21046;&#30340;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#22833;&#36133;&#26696;&#20363;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#20379;&#34917;&#20805;&#30340;&#35270;&#39057;&#21487;&#22312;https://sites.google.com/view/srgpsr&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.14398</link><description>&lt;p&gt;
&#30475;&#35265;&#21644;&#21548;&#21040;&#27809;&#34987;&#35828;&#30340;&#35805;&#65306;&#21487;&#35299;&#37322;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#21160;&#26426;&#24615;&#35775;&#35848;&#23458;&#25143;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion. (arXiv:2309.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#22120;&#65292;&#22312;&#21160;&#26426;&#24615;&#35775;&#35848;&#20013;&#20934;&#30830;&#21306;&#20998;&#20102;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#19977;&#31181;&#31867;&#21035;&#12290;&#35813;&#20998;&#31867;&#22120;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#23545;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#25214;&#21040;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#24615;&#35775;&#35848;&#65288;MI&#65289;&#26159;&#19968;&#31181;&#24378;&#35843;&#21512;&#20316;&#24182;&#40723;&#21169;&#34892;&#20026;&#25913;&#21464;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;MI&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#21033;&#29992;MISC&#20195;&#30721;&#23558;&#23458;&#25143;&#35805;&#35821;&#20998;&#31867;&#20026;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#25110;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#12290;MI&#23545;&#35805;&#20013;&#21464;&#21270;&#35805;&#35821;&#30340;&#27604;&#20363;&#19982;&#27835;&#30103;&#32467;&#26524;&#21576;&#27491;&#30456;&#20851;&#65292;&#22240;&#27492;&#20934;&#30830;&#20998;&#31867;&#23458;&#25143;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#12289;&#22768;&#35843;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#29305;&#24449;&#20934;&#30830;&#21306;&#20998;&#19977;&#20010;MISC&#31867;&#21035;&#65288;&#21464;&#21270;&#35805;&#35821;&#12289;&#25345;&#32493;&#35805;&#35821;&#21644;&#36319;&#38543;/&#20013;&#31435;&#35805;&#35821;&#65289;&#12290;&#20026;&#20102;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;AnnoMI&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#34920;&#29616;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20915;&#31574;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#27169;&#24577;&#65292;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#24819;&#35937;&#26426;&#21046;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#20449;&#24687;&#22312;&#19981;&#21516;&#29366;&#24577;&#38388;&#24191;&#25773;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#29366;&#24577;&#38598;&#20013;&#20256;&#36755;&#65292;&#36825;&#31181;&#26426;&#21046;&#20419;&#36827;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.14243</link><description>&lt;p&gt;
&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#65306;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#26032;&#22411;&#24819;&#35937;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation. (arXiv:2309.14243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#30340;&#24819;&#35937;&#26426;&#21046;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#20449;&#24687;&#22312;&#19981;&#21516;&#29366;&#24577;&#38388;&#24191;&#25773;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#29366;&#24577;&#38598;&#20013;&#20256;&#36755;&#65292;&#36825;&#31181;&#26426;&#21046;&#20419;&#36827;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#38754;&#20020;&#25968;&#25454;&#25928;&#29575;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#26356;&#26032;&#26234;&#33021;&#20307;&#30340;&#35780;&#35770;&#23478;&#26102;&#24448;&#24448;&#20165;&#20381;&#36182;&#20110;&#21516;&#19968;&#38598;&#20013;&#30340;&#29366;&#24577;&#36716;&#25442;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25928;&#29575;&#20302;&#21644;&#35757;&#32451;&#26102;&#38388;&#28040;&#32791;&#20122;&#20248;&#12290;&#21463;&#21040;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24819;&#35937;&#26426;&#21046;&#65288;IM&#65289;&#8221;&#30340;&#26032;&#22411;&#32593;&#26684;&#20449;&#24687;&#20256;&#25773;&#26426;&#21046;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IM&#20351;&#24471;&#30001;&#21333;&#20010;&#26679;&#26412;&#29983;&#25104;&#30340;&#20449;&#24687;&#33021;&#22815;&#26377;&#25928;&#22320;&#24191;&#25773;&#21040;&#19981;&#21516;&#30340;&#38598;&#20013;&#29366;&#24577;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21516;&#19968;&#38598;&#20013;&#20256;&#36755;&#12290;&#36825;&#31181;&#33021;&#21147;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#29366;&#24577;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#20419;&#36827;&#20102;&#23545;&#26377;&#38480;&#26679;&#26412;&#20449;&#24687;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#23558;IM&#25193;&#23637;&#20026;&#19968;&#31181;&#21487;&#20197;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent's Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the 'Imagination Mechanism (IM)', designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model's comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.13781</link><description>&lt;p&gt;
ICU &#37325;&#26032;&#20837;&#38498;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#21307;&#38498;&#29615;&#22659;&#65292;&#21307;&#29983;&#30340;&#20915;&#31574;&#23545;&#24739;&#32773;&#30340;&#29983;&#21629;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#24517;&#39035;&#36981;&#24490;&#19968;&#26465;&#20840;&#38754;&#30340;&#25252;&#29702;&#36335;&#24452;&#26469;&#20943;&#23569;&#24182;&#21457;&#30151;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#12289;&#31454;&#20105;&#24615;&#21644;&#38750;&#35745;&#21010;&#24615;&#30340;&#22240;&#32032;&#22686;&#21152;&#20102;&#32479;&#19968;&#23454;&#26045;&#25252;&#29702;&#36335;&#24452;&#30340;&#22256;&#38590;&#12290;&#20877;&#20837;&#38498;&#26159;&#35813;&#36335;&#24452;&#30340;&#22256;&#38590;&#20043;&#19968;&#65292;&#21363;&#24739;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#20877;&#27425;&#20837;&#20303;ICU&#65292;&#23548;&#33268;&#39640;&#27515;&#20129;&#29575;&#21644;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24739;&#32773;&#30340;&#21307;&#30103;&#20449;&#24687;&#26469;&#39044;&#27979;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#20877;&#20837;&#38498;&#26102;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#23545;&#20877;&#20837;&#38498;&#39044;&#27979;&#36827;&#34892;&#36866;&#24403;&#30340;&#35780;&#20272;&#12289;&#25551;&#36848;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#65288;&#21363;&#21253;&#21547;166,355&#21517;&#24739;&#32773;&#30340;eICU&#38431;&#21015;&#65292;200,859&#21517;...&#65289;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
&lt;/p&gt;</description></item><item><title>ALLURE&#26159;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#24182;&#32435;&#20837;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;LLM&#23545;&#25991;&#26412;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13701</link><description>&lt;p&gt;
ALLURE: &#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning. (arXiv:2309.13701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13701
&lt;/p&gt;
&lt;p&gt;
ALLURE&#26159;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#35780;&#20272;&#30340;&#23457;&#35745;&#21644;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#24182;&#32435;&#20837;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;LLM&#23545;&#25991;&#26412;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35780;&#20998;&#35770;&#25991;&#21040;&#24635;&#32467;&#21307;&#30103;&#25991;&#20214;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35780;&#20272;&#30001;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#29992;&#24615;&#65292;LLM&#23384;&#22312;&#30528;&#26126;&#26174;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#38656;&#35201;&#23545;&#20854;&#25991;&#26412;&#35780;&#20272;&#33021;&#21147;&#36827;&#34892;&#24443;&#24213;&#23457;&#35745;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ALLURE&#65292;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#38169;&#35823;&#12290;ALLURE&#28041;&#21450;&#23558;LLM&#29983;&#25104;&#30340;&#35780;&#20272;&#19982;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36845;&#20195;&#22320;&#23558;&#37325;&#22823;&#20559;&#24046;&#30340;&#23454;&#20363;&#32435;&#20837;&#35780;&#20272;&#22120;&#20013;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25552;&#39640;&#21644;&#25913;&#36827;LLM&#23545;&#25991;&#26412;&#30340;&#40065;&#26834;&#35780;&#20272;&#12290;&#36890;&#36807;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#35780;&#20272;&#22120;LLM&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#23545;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#39044;&#35745;ALLURE&#23558;&#22312;&#19982;&#25991;&#26412;&#25968;&#25454;&#35780;&#20272;&#30456;&#20851;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#21307;&#23398;&#27010;&#25324;&#31561;&#65292;&#20026;LLM&#30340;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#32479;&#19968;&#30340;&#12289;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#65292;&#20174;&#32780;&#25581;&#31034;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.13550</link><description>&lt;p&gt;
&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#20934;&#30830;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#38598;&#20013;&#27880;&#24847;&#21147;&#65306;&#19968;&#20010;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290; &#65288;arXiv:2309.13550v2 [cs.CV]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and Interpretable AI System. (arXiv:2309.13550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#32479;&#19968;&#30340;&#12289;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#65292;&#20174;&#32780;&#25581;&#31034;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#35786;&#26029;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#20165;&#20851;&#27880;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#27880;&#35270;&#20301;&#32622;&#65292;&#36890;&#24120;&#36890;&#36807;&#26816;&#27979;&#12289;&#20998;&#21106;&#25110;&#20998;&#31867;&#31561;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27169;&#22411;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#32479;&#19968;&#30340;&#21487;&#25511;&#35299;&#37322;&#24615;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;CXR&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#27880;&#35270;&#20301;&#32622;&#12289;&#20182;&#20204;&#22312;&#29305;&#23450;&#21306;&#22495;&#30340;&#27880;&#24847;&#26102;&#38271;&#20197;&#21450;&#20182;&#20204;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#36890;&#36807;&#25429;&#25417;&#25918;&#23556;&#31185;&#21307;&#29983;&#27880;&#35270;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27934;&#23519;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#19982;&#24403;&#21069;&#20381;&#36182;&#20110;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#23481;&#26131;&#20174;&#25972;&#20010;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#38169;&#35823;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#23631;&#34109;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of chest X-ray (CXR) diagnosis, existing works often focus solely on determining where a radiologist looks, typically through tasks such as detection, segmentation, or classification. However, these approaches are often designed as black-box models, lacking interpretability. In this paper, we introduce a novel and unified controllable interpretable pipeline for decoding the intense focus of radiologists in CXR diagnosis. Our approach addresses three key questions: where a radiologist looks, how long they focus on specific areas, and what findings they diagnose. By capturing the intensity of the radiologist's gaze, we provide a unified solution that offers insights into the cognitive process underlying radiological interpretation. Unlike current methods that rely on black-box machine learning models, which can be prone to extracting erroneous information from the entire input image during the diagnosis process, we tackle this issue by effectively masking out irrelevant info
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12627</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65306;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#21644;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
A Quantum Computing-based System for Portfolio Optimization using Future Asset Values and Automatic Reduction of the Investment Universe. (arXiv:2309.12627v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;Q4FuturePOP&#65292;&#23427;&#21019;&#26032;&#22320;&#21033;&#29992;&#26410;&#26469;&#36164;&#20135;&#20215;&#20540;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#23454;&#39564;&#35752;&#35770;&#20102;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#37329;&#34701;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#35813;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#25216;&#26415;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Q4FuturePOP&#30340;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20197;&#19979;&#21019;&#26032;&#35299;&#20915;&#20102;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;i&#65289;&#35813;&#24037;&#20855;&#38024;&#23545;&#26410;&#26469;&#36164;&#20135;&#39044;&#27979;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#65307;ii&#65289;Q4FuturePOP&#21253;&#25324;&#19968;&#20010;&#26234;&#33021;&#20943;&#23569;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#33258;&#21160;&#20943;&#23569;&#25237;&#36164;&#33539;&#22260;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;Q4FuturePOP&#30340;&#21407;&#22411;&#29256;&#26412;&#30340;&#19981;&#21516;&#27169;&#22359;&#30340;&#21021;&#27493;&#24615;&#33021;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the problems in quantitative finance that has received the most attention is the portfolio optimization problem. Regarding its solving, this problem has been approached using different techniques, with those related to quantum computing being especially prolific in recent years. In this study, we present a system called Quantum Computing-based System for Portfolio Optimization with Future Asset Values and Automatic Universe Reduction (Q4FuturePOP), which deals with the Portfolio Optimization Problem considering the following innovations: i) the developed tool is modeled for working with future prediction of assets, instead of historical values; and ii) Q4FuturePOP includes an automatic universe reduction module, which is conceived to intelligently reduce the complexity of the problem. We also introduce a brief discussion about the preliminary performance of the different modules that compose the prototypical version of Q4FuturePOP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.11166</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#35789;&#32423;&#25200;&#21160;&#30495;&#30340;&#20855;&#26377;&#40065;&#26834;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#27169;&#21644;&#33021;&#21147;&#19978;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#38500;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36991;&#20813;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#28608;&#28872;&#21453;&#39304;&#22806;&#65292;&#30830;&#20445;LLM&#30340;&#36131;&#20219;&#24615;&#36824;&#38656;&#35201;&#20851;&#27880;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#20855;&#26377;&#39044;&#23450;&#20041;&#30417;&#30563;&#26631;&#31614;&#30340;&#20256;&#32479;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36825;&#19982;&#24403;&#20195;LLMs&#30340;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#29702;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#26469;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21512;&#29702;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;TREvaL&#65289;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;TREval&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22256;&#24785;&#24230;&#26469;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#27745;&#26579;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#21017;&#21463;&#27745;&#26579;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2309.10677</link><description>&lt;p&gt;
&#36890;&#36807;&#22256;&#24785;&#24230;&#20272;&#35745;&#27745;&#26579;&#65306;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#35760;&#24518;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation. (arXiv:2309.10677v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22256;&#24785;&#24230;&#26469;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#27745;&#26579;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#21017;&#21463;&#27745;&#26579;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#25968;&#25454;&#27745;&#26579;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#35821;&#26009;&#24211;&#32463;&#24120;&#26080;&#24847;&#20013;&#21253;&#21547;&#22522;&#20934;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#20998;&#26512;&#24050;&#25104;&#20026;&#21487;&#38752;&#27169;&#22411;&#35780;&#20272;&#19981;&#21487;&#36991;&#20813;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27745;&#26579;&#20998;&#26512;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#23545;&#20110;&#26368;&#26032;&#27169;&#22411;&#26469;&#35828;&#26159;&#20445;&#23494;&#30340;&#12290;&#36825;&#38459;&#27490;&#20102;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#23457;&#35745;&#21644;&#20934;&#30830;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22312;&#19981;&#35775;&#38382;&#23436;&#25972;&#35757;&#32451;&#38598;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#27745;&#26579;&#65292;&#21363;&#29992;&#22256;&#24785;&#24230;&#26469;&#34913;&#37327;&#27745;&#26579;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#21463;&#27426;&#36814;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#25688;&#35201;&#22522;&#20934;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#35760;&#24518;&#21270;&#65292;&#32780;&#22810;&#39033;&#36873;&#25321;&#20284;&#20046;&#27809;&#26377;&#37027;&#20040;&#21463;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination in model evaluation is getting increasingly prevalent as the massive training corpora of large language models often unintentionally include benchmark samples. Therefore, contamination analysis has became an inevitable part of reliable model evaluation. However, existing method of contamination analysis requires the access of the entire training data which is often confidential for recent models. This prevent the community to rigorously audit these models and conduct accurate assessment of their capability. In this paper, we propose a novel method to quantify contamination without the access of the full training set, that measure the extent of contamination with perplexity. Our analysis provides evidence of significant memorisation of recent foundation models in popular reading comprehension, summarisation benchmarks, while multiple choice appears less contaminated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;&#65292;&#35813;&#24211;&#21487;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#24179;&#21488;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#30340;&#26412;&#20307;&#65292;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#24182;&#25903;&#25345;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#21644;&#35821;&#20041;&#36164;&#28304;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.10288</link><description>&lt;p&gt;
AstroPortal: &#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
AstroPortal: An ontology repository concept for astronomy, astronautics and other space topics. (arXiv:2309.10288v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#27010;&#24565;&#65292;&#35813;&#24211;&#21487;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#24179;&#21488;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#30340;&#26412;&#20307;&#65292;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#24182;&#25903;&#25345;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#21644;&#35821;&#20041;&#36164;&#28304;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#22825;&#25991;&#23398;&#12289;&#33322;&#22825;&#23398;&#21644;&#20854;&#20182;&#31354;&#38388;&#30456;&#20851;&#20027;&#39064;&#30340;&#26412;&#20307;&#24211;&#12290;&#35813;&#24211;&#21487;&#31216;&#20026;AstroPortal&#65288;&#25110;SpacePortal&#65289;&#12289;AstroHub&#65288;&#25110;SpaceHub&#65289;&#31561;&#12290;&#21019;&#24314;&#35813;&#24211;&#23558;&#36866;&#29992;&#20110;&#23398;&#26415;&#12289;&#30740;&#31350;&#21644;&#20854;&#20182;&#25968;&#25454;&#23494;&#38598;&#22411;&#39046;&#22495;&#12290;&#23427;&#23545;&#20110;&#22826;&#31354;&#31185;&#23398;&#65288;&#21253;&#25324;&#22825;&#25991;&#23398;&#65289;&#12289;&#22320;&#29699;&#31185;&#23398;&#21644;&#33322;&#22825;&#23398;&#65288;&#33322;&#22825;&#39134;&#34892;&#65289;&#31561;&#25968;&#25454;&#23494;&#38598;&#22411;&#23398;&#31185;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#35813;&#24211;&#24212;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#25628;&#32034;&#12289;&#23457;&#26680;&#21644;&#21019;&#24314;&#19982;&#22825;&#25991;&#30456;&#20851;&#20027;&#39064;&#30340;&#26412;&#20307;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#30740;&#31350;&#26102;&#38388;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#26469;&#30740;&#31350;&#21644;&#27604;&#36739;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#25110;&#35821;&#20041;&#36164;&#28304;&#12290;&#30001;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#30446;&#21069;&#19981;&#23384;&#22312;&#21487;&#29992;&#30340;&#24211;&#65292;&#26412;&#25991;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a repository for ontologies of astronomy, astronautics, and other space-related topics. It may be called AstroPortal (or SpacePortal), AstroHub (or SpaceHub), etc. The creation of this repository will be applicable to academic, research and other data-intensive sectors. It is relevant for space sciences (including astronomy), Earth science, and astronautics (spaceflight), among other data-intensive disciplines. The repository should provide a centralized platform to search, review and create ontologies for astro-related topics. It thereby can decrease research time, while also providing a user-friendly means to study and compare knowledge organization systems or semantic resources of the target domains. With no apparent repository available on the target domain, this paper also expresses a novel concept.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.09404</link><description>&lt;p&gt;
&#29992;&#24320;&#25918;&#25968;&#25454;&#39537;&#21160;&#30340;&#22242;&#38431;&#25512;&#33616;&#20419;&#36827;&#30740;&#31350;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#38431;&#24314;&#35774;&#21644;&#20419;&#36827;&#21512;&#20316;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#21830;&#19994;&#27963;&#21160;&#12290;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;TeamingForFunding&#38382;&#39064;&#65292;&#30740;&#31350;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;&#21521;&#36164;&#21161;&#26426;&#26500;&#30003;&#35831;&#26102;&#65292;&#24076;&#26395;&#33021;&#22815;&#25214;&#21040;&#21512;&#20316;&#26426;&#20250;&#20197;&#22238;&#24212;&#21518;&#32773;&#30340;&#39033;&#30446;&#30003;&#35831;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#37117;&#33021;&#22815;&#36798;&#21040;&#26426;&#20250;&#35201;&#27714;&#30340;&#26368;&#39640;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#26159;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#21462;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#39033;&#30446;&#30003;&#35831;&#65288;&#38656;&#27714;&#65289;&#21644;&#30740;&#31350;&#20154;&#21592;&#31616;&#20171;&#65288;&#20379;&#32473;&#65289;&#20013;&#30340;&#25216;&#33021;&#28508;&#21147;&#65292;&#20351;&#29992;&#20998;&#31867;&#27861;&#23545;&#20854;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#38656;&#27714;&#21644;&#20379;&#32473;&#12290;&#25105;&#20204;&#21019;&#24314;&#22242;&#38431;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#30340;&#24230;&#37327;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#8230;
&lt;/p&gt;
&lt;p&gt;
Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08247</link><description>&lt;p&gt;
&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#20960;&#20309;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Autoencoders. (arXiv:2309.08247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#30340;&#20960;&#20309;&#26041;&#38754;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#34987;&#30456;&#23545;&#36739;&#23569;&#22320;&#35748;&#35782;&#21040;&#12290;&#32473;&#23450;&#19968;&#32452;&#20960;&#20046;&#20301;&#20110;&#26576;&#20010;&#36739;&#20302;&#32500;&#24230;&#27969;&#24418;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#28857;&#65292;&#33258;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;&#27969;&#24418;&#21644;&#20854;&#22352;&#26631;&#22270;&#12290;&#36825;&#31181;&#20960;&#20309;&#35282;&#24230;&#33258;&#28982;&#24341;&#21457;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#8220;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23545;&#24212;&#20110;&#21333;&#19968;&#30340;&#27969;&#24418;&#21527;&#65311;&#8221;&#25110;&#32773;&#8220;&#21482;&#26377;&#19968;&#20010;&#22352;&#26631;&#22270;&#21487;&#20197;&#34920;&#31034;&#27969;&#24418;&#21527;&#65311;&#8221;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#22238;&#31572;&#26159;&#21542;&#23450;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#26377;&#22810;&#20010;&#35299;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#20855;&#26377;&#20005;&#37325;&#30072;&#21464;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#38169;&#35823;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#36817;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the geometric aspect of the autoencoder framework, which, despite its importance, has been relatively less recognized. Given a set of high-dimensional data points that approximately lie on some lower-dimensional manifold, an autoencoder learns the \textit{manifold} and its \textit{coordinate chart}, simultaneously. This geometric perspective naturally raises inquiries like "Does a finite set of data points correspond to a single manifold?" or "Is there only one coordinate chart that can represent the manifold?". The responses to these questions are negative, implying that there are multiple solution autoencoders given a dataset. Consequently, they sometimes produce incorrect manifolds with severely distorted latent space representations. In this paper, we introduce recent geometric approaches that address these issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.06726</link><description>&lt;p&gt;
&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;BART&#24494;&#35843;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Generation by BART Finetuning with Splitting and Shuffling. (arXiv:2309.06726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#36890;&#36807;&#25286;&#20998;&#21644;&#37325;&#25490;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#26159;&#19968;&#39033;&#35782;&#21035;&#26368;&#20339;&#20195;&#34920;&#32473;&#23450;&#25991;&#26412;&#20027;&#39064;&#25110;&#20027;&#39064;&#30340;&#30701;&#35821;&#38598;&#30340;&#20219;&#21153;&#12290;&#20851;&#38190;&#30701;&#35821;&#20998;&#20026;&#20986;&#29616;&#21644;&#19981;&#22312;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#12290;&#26368;&#36817;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#19978;&#26174;&#31034;&#20986;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25214;&#21040;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#30340;&#38590;&#24230;&#65292;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#27169;&#22411;(Keyphrase-Focused BART)&#65292;&#21033;&#29992;&#20102;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20986;&#29616;&#21644;&#19981;&#20986;&#29616;&#20851;&#38190;&#30701;&#35821;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#20010;&#29420;&#31435;BART&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20851;&#38190;&#30701;&#35821;&#30340;&#37325;&#25490;&#21644;&#20505;&#36873;&#20851;&#38190;&#30701;&#35821;&#25490;&#24207;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23545;&#20110;&#19981;&#20986;&#29616;&#30340;&#20851;&#38190;&#30701;&#35821;&#65292;&#22312;&#20116;&#20010;&#20851;&#38190;&#30701;&#35821;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#20851;&#27880;&#20851;&#38190;&#30701;&#35821;&#30340;BART&#22312;F1@5&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation is a task of identifying a set of phrases that best repre-sent the main topics or themes of a given text. Keyphrases are dividend int pre-sent and absent keyphrases. Recent approaches utilizing sequence-to-sequence models show effectiveness on absent keyphrase generation. However, the per-formance is still limited due to the hardness of finding absent keyphrases. In this paper, we propose Keyphrase-Focused BART, which exploits the differ-ences between present and absent keyphrase generations, and performs fine-tuning of two separate BART models for present and absent keyphrases. We further show effective approaches of shuffling keyphrases and candidate keyphrase ranking. For absent keyphrases, our Keyphrase-Focused BART achieved new state-of-the-art score on F1@5 in two out of five keyphrase gen-eration benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10842</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27807;&#36890;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GC&#26234;&#33021;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#30340;&#35282;&#33394;&#12290;&#20511;&#21161;&#22522;&#20110;&#34892;&#21160;&#30340;&#28436;&#31034;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#30446;&#26631;&#23454;&#29616;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#8212;&#8212;&#25945;&#32946;&#23398;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#34701;&#20837;&#65292;&#25552;&#21319;&#20102;&#26234;&#33021;&#20307;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22810;&#27169;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#35782;&#21035;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#31215;&#26497;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#28389;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#26816;&#27979;&#65292;&#20294;&#26576;&#20123;&#24694;&#24847;&#28389;&#29992;&#38656;&#35201;&#36319;&#36394;&#23545;&#25163;&#29992;&#25143;&#20197;&#36827;&#34892;&#21453;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20301;&#27700;&#21360;&#36890;&#36807;&#39068;&#33394;&#32534;&#30721;&#8221;&#65288;COLOR&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#12290;&#21033;&#29992;&#38646;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#20248;&#21183;&#65288;Kirchenbauer&#31561;&#65292;2023a&#65289;&#65292;COLOR&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#65288;&#32422;500&#20010;&#26631;&#35760;&#65289;&#20013;&#25104;&#21151;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#25928;&#22320;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#36827;&#34892;&#21453;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03572</link><description>&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#23454;&#29616;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Range-Restricted Interpolation through Clausal Tableaux. (arXiv:2306.03572v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03572
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#23454;&#29616;&#21487;&#34892;&#30340;&#33539;&#22260;&#38480;&#21046;&#25554;&#20540;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#38454;&#36923;&#36753;&#30340;Clausal Tableaux&#35777;&#26126;&#31995;&#32479;&#65292;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#20256;&#36882;&#21464;&#21270;&#30340;&#33539;&#22260;&#38480;&#21046;&#21644;Horn&#24615;&#36136;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#23558;&#35777;&#26126;&#32467;&#26500;&#30340;&#25805;&#20316;&#19982;&#39640;&#24230;&#20248;&#21270;&#30340;&#19968;&#38454;&#35777;&#26126;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#21487;&#34892;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#20027;&#35201;&#24212;&#29992;&#20110;&#26597;&#35810;&#21512;&#25104;&#21644;&#25554;&#20540;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how variations of range-restriction and also the Horn property can be passed from inputs to outputs of Craig interpolation in first-order logic. The proof system is clausal tableaux, which stems from first-order ATP. Our results are induced by a restriction of the clausal tableau structure, which can be achieved in general by a proof transformation, also if the source proof is by resolution/paramodulation. Primarily addressed applications are query synthesis and reformulation with interpolation. Our methodical approach combines operations on proof structures with the immediate perspective of feasible implementation through incorporating highly optimized first-order provers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VQ-MPT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37319;&#26679;&#23383;&#20856;&#21644;Transformer&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26356;&#39640;&#25928;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#27492;&#26041;&#27861;&#23558;&#35268;&#21010;&#31354;&#38388;&#20998;&#21106;&#20026;&#31163;&#25955;&#38598;&#21512;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#36873;&#25321;&#37319;&#26679;&#21306;&#22495;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.00851</link><description>&lt;p&gt;
&#23398;&#20064;&#37319;&#26679;&#23383;&#20856;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#65288;&#20351;&#29992;Transformer&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning Sampling Dictionaries for Efficient and Generalizable Robot Motion Planning with Transformers. (arXiv:2306.00851v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VQ-MPT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37319;&#26679;&#23383;&#20856;&#21644;Transformer&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26356;&#39640;&#25928;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#27492;&#26041;&#27861;&#23558;&#35268;&#21010;&#31354;&#38388;&#20998;&#21106;&#20026;&#31163;&#25955;&#38598;&#21512;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#36873;&#25321;&#37319;&#26679;&#21306;&#22495;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#26159;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#65292;&#27604;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#22806;&#31185;&#25163;&#26415;&#26426;&#22120;&#20154;&#21644;&#24037;&#19994;&#25163;&#33218;&#12290;&#29616;&#26377;&#30340;&#35268;&#21010;&#26041;&#27861;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#26368;&#36817;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35268;&#21010;&#22120;&#22312;&#21152;&#36895;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#36866;&#29992;&#20110;&#20998;&#24067;&#22806;&#29615;&#22659;&#26041;&#38754;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30690;&#37327;&#37327;&#21270;&#36816;&#21160;&#35268;&#21010;Transformer&#65288;VQ-MPT&#65289;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#38190;&#26222;&#36866;&#24615;&#21644;&#25193;&#23637;&#24615;&#32570;&#28857;&#12290;VQ-MPT&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#30690;&#37327;&#37327;&#21270;&#8212;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#37319;&#26679;&#20998;&#24067;&#23398;&#20064;&#34920;&#31034;&#35268;&#21010;&#31354;&#38388;&#65292;&#31532;&#20108;&#38454;&#27573;&#26159;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#23398;&#20064;&#30340;&#37319;&#26679;&#20998;&#24067;&#38598;&#20013;&#36873;&#25321;&#26469;&#26500;&#24314;SMP&#30340;&#37319;&#26679;&#21306;&#22495;&#12290;&#36890;&#36807;&#23558;&#22823;&#35268;&#21010;&#31354;&#38388;&#20998;&#21106;&#20026;&#31163;&#25955;&#38598;&#21512;&#24182;&#36873;&#25321;&#24615;&#22320;&#36873;&#25321;&#37319;&#26679;&#21306;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion planning is integral to robotics applications such as autonomous driving, surgical robots, and industrial manipulators. Existing planning methods lack scalability to higher-dimensional spaces, while recent learning based planners have shown promise in accelerating sampling-based motion planners (SMP) but lack generalizability to out-of-distribution environments. To address this, we present a novel approach, Vector Quantized-Motion Planning Transformers (VQ-MPT) that overcomes the key generalization and scaling drawbacks of previous learning-based methods. VQ-MPT consists of two stages. Stage 1 is a Vector Quantized-Variational AutoEncoder model that learns to represent the planning space using a finite number of sampling distributions, and stage 2 is an Auto-Regressive model that constructs a sampling region for SMPs by selecting from the learned sampling distribution sets. By splitting large planning spaces into discrete sets and selectively choosing the sampling regions, our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.12333</link><description>&lt;p&gt;
GRACE++&#65306;&#36890;&#36807;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
GRACE++: Loss-Resilient Real-Time Video through Neural Codecs. (arXiv:2305.12333v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12333
&lt;/p&gt;
&lt;p&gt;
GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#35270;&#39057;&#36890;&#20449;&#20013;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#65292;&#37325;&#26032;&#20256;&#36755;&#20002;&#22833;&#30340;&#25968;&#25454;&#21253;&#22312;&#39640;&#24310;&#36831;&#32593;&#32476;&#19979;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#27809;&#26377;&#37325;&#20256;&#30340;&#20002;&#21253;&#24773;&#20917;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#31574;&#30053;--&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#21069;&#21521;&#24046;&#38169;&#32416;&#27491;&#65288;FEC&#65289;&#21644;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#38169;&#35823;&#38544;&#34255;&#12290;&#21069;&#32773;&#22312;&#20256;&#36755;&#20043;&#21069;&#29992;&#20887;&#20313;&#32534;&#30721;&#25968;&#25454;&#65292;&#20294;&#25552;&#21069;&#30830;&#23450;&#26368;&#20339;&#20887;&#20313;&#32423;&#21035;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21518;&#32773;&#20174;&#37096;&#20998;&#25910;&#21040;&#30340;&#24103;&#20013;&#37325;&#24314;&#35270;&#39057;&#65292;&#20294;&#23558;&#24103;&#21010;&#20998;&#20026;&#29420;&#31435;&#32534;&#30721;&#30340;&#20998;&#21306;&#20250;&#38477;&#20302;&#21387;&#32553;&#25928;&#29575;&#65292;&#24182;&#19988;&#20002;&#22833;&#30340;&#20449;&#24687;&#22312;&#27809;&#26377;&#36866;&#24212;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#26377;&#25928;&#22320;&#34987;&#35299;&#30721;&#22120;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE++&#30340;&#25239;&#20002;&#21253;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#30340;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder.  We present a loss-resilient real-time video system called GRACE++, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE++'s enhanced loss resilience is its joint training of the neural encoder an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.11625</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#32858;&#21512;&#21644;&#24726;&#35770;&#24615;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
Meaningful Causal Aggregation and Paradoxical Confounding. (arXiv:2304.11625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11625
&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#24615;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#21512;&#21464;&#37327;&#20013;&#65292;&#24178;&#39044;&#30340;&#24433;&#21709;&#36890;&#24120;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#23439;&#35266;&#24178;&#39044;&#30340;&#19981;&#21516;&#24494;&#35266;&#23454;&#29616;&#21487;&#33021;&#20250;&#23548;&#33268;&#19979;&#28216;&#23439;&#35266;&#21464;&#37327;&#30340;&#19981;&#21516;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#32858;&#21512;&#21464;&#37327;&#65292;&#22240;&#26524;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20351;&#24471;&#21407;&#26412;&#19981;&#28151;&#28102;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#28151;&#28102;&#65292;&#24182;&#19988;&#21453;&#20043;&#20134;&#28982;&#65292;&#36825;&#19968;&#28857;&#21462;&#20915;&#20110;&#30456;&#24212;&#30340;&#24494;&#35266;&#23454;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#22312;&#32858;&#21512;&#22240;&#26524;&#31995;&#32479;&#27809;&#26377;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25165;&#21487;&#20197;&#23454;&#38469;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#38656;&#35201;&#25509;&#21463;&#19968;&#28857;&#65292;&#23601;&#26159;&#23439;&#35266;&#22240;&#26524;&#20851;&#31995;&#36890;&#24120;&#21482;&#19982;&#24494;&#35266;&#29366;&#24577;&#30456;&#20851;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#23439;&#35266;&#24178;&#39044;&#30340;&#20998;&#24067;&#19982;&#35266;&#27979;&#20998;&#24067;&#20013;&#24494;&#35266;&#29366;&#24577;&#30340;&#20998;&#24067;&#30456;&#21516;&#26102;&#65292;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#35752;&#35770;&#20102;&#27492;&#35266;&#23519;&#30340;&#27010;&#25324;&#12290;
&lt;/p&gt;
&lt;p&gt;
In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective micro-realization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that cause-effect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution and also discuss generalizations of this observation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#26469;&#35299;&#20915;&#35745;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#36171;&#33021;LLMs&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11477</link><description>&lt;p&gt;
LLM+P&#65306;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
LLM+P: Empowering Large Language Models with Optimal Planning Proficiency. (arXiv:2304.11477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#26469;&#35299;&#20915;&#35745;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#36171;&#33021;LLMs&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65306;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#25552;&#20379;&#23545;&#35768;&#22810;&#26085;&#24120;&#29983;&#27963;&#20013;&#24120;&#35265;&#38382;&#39064;&#30340;&#21512;&#29702;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;LLM&#19981;&#33021;&#21487;&#38752;&#22320;&#35299;&#20915;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#20856;&#35268;&#21010;&#22120;&#19968;&#26086;&#20197;&#26684;&#24335;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#25628;&#32034;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#27491;&#30830;&#25110;&#29978;&#33267;&#26368;&#20248;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;LLM&#30340;&#26694;&#26550;&#12290;LLM+P&#25509;&#25910;&#19968;&#20010;&#35745;&#21010;&#38382;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#28982;&#21518;&#20197;&#33258;&#28982;&#35821;&#35328;&#36820;&#22238;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#27491;&#30830;&#65288;&#25110;&#26368;&#20248;&#65289;&#35745;&#21010;&#12290;LLM+P&#39318;&#20808;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#24555;&#36895;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#21518;&#23558;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#32763;&#35793;&#22238;&#33258;&#28982;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15343</link><description>&lt;p&gt;
Sigmoid Loss&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sigmoid Loss for Language Image Pre-Training. (arXiv:2303.15343v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20854;&#35757;&#32451;&#20986;&#26469;&#30340;&#27169;&#22411;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#23545;Sigmoid&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#19982;&#26631;&#20934;&#30340;&#20855;&#26377;softmax&#24402;&#19968;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;Sigmoid&#25439;&#22833;&#21482;&#25805;&#20316;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#19981;&#38656;&#35201;&#20840;&#23616;&#26597;&#30475;&#37197;&#23545;&#30456;&#20284;&#24615;&#20197;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;Sigmoid&#25439;&#22833;&#21516;&#26102;&#20351;&#25209;&#37327;&#22823;&#23567;&#36827;&#19968;&#27493;&#22686;&#21152;&#65292;&#24182;&#21487;&#22312;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#20165;&#20351;&#29992;&#22235;&#20010;TPUv4&#33455;&#29255;&#65292;&#25105;&#20204;&#23601;&#33021;&#22312;4k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;Base CLIP&#27169;&#22411;&#21644;&#22312;20k&#25209;&#37327;&#22823;&#23567;&#19979;&#35757;&#32451;&#20986;&#19968;&#20010;&#22823;&#35268;&#27169;LiT&#27169;&#22411;&#65292;&#21518;&#32773;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#20102;84.5%&#30340;ImageNet&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#23558;&#25209;&#37327;&#22823;&#23567;&#19982;&#25439;&#22833;&#20989;&#25968;&#20998;&#31163;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#31034;&#20363;&#19982;&#23545;&#20043;&#38388;&#12289;&#36127;-&#27491;&#20363;&#27604;&#29575;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25209;&#37327;&#22823;&#23567;&#25512;&#21040;&#26497;&#38480;&#65292;&#39640;&#36798;&#19968;&#30334;&#19975;&#65292;&#21457;&#29616;&#25193;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#22909;&#22788;&#24456;&#24555;&#23601;&#20250;&#20943;&#24369;&#65292;32k&#25209;&#37327;&#22823;&#23567;&#24050;&#32463;&#36275;&#22815;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#22815;&#28608;&#21457;&#36827;&#19968;&#27493;&#25506;&#32034;&#22914;&#20309;&#25552;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and
&lt;/p&gt;</description></item><item><title>Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.13873</link><description>&lt;p&gt;
Fantasia3D: &#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13873
&lt;/p&gt;
&lt;p&gt;
Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#25552;&#20379;&#65292;&#33258;&#21160;3D&#20869;&#23481;&#30340;&#21019;&#24314;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#24418;&#25104;&#20102;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26032;&#20852;&#35805;&#39064;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38544;&#24335;&#22330;&#26223;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#23558;&#20960;&#20309;&#21644;&#22806;&#35266;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23545;&#20110;&#24674;&#22797;&#26356;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#23454;&#29616;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#28210;&#26579;&#26159;&#27425;&#20248;&#30340;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#36164;&#20135;&#26041;&#38754;&#19981;&#22815;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fantasia3D&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#12290;Fantasia3D&#30340;&#20851;&#38190;&#22312;&#20110;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#20998;&#31163;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#23545;&#20110;&#20960;&#20309;&#23398;&#20064;&#65292;&#25105;&#20204;&#20381;&#38752;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#24314;&#35758;&#23558;&#20174;&#34920;&#31034;&#20013;&#25552;&#21462;&#30340;&#34920;&#38754;&#27861;&#32447;&#32534;&#30721;&#20316;&#20026;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#23545;&#20110;&#22806;&#35266;&#24314;&#27169;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#21487;&#21464;&#21452;&#21521;&#21453;&#23556;&#29575;&#20998;&#24067;&#20989;&#25968;&#65288;SVBRDF&#65289;&#26469;&#20998;&#31163;&#26448;&#26009;&#21644;&#20809;&#29031;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#20869;&#23481;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.08690</link><description>&lt;p&gt;
&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#29992;&#20110;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning. (arXiv:2303.08690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#30340;&#26032;&#22411;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#21487;&#20197;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#30456;&#20851;&#37096;&#20998;&#24555;&#36895;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#20013;&#29992;&#20110;&#30830;&#23450;&#25152;&#30740;&#31350;&#30340;&#23545;&#35937;&#65288;&#26080;&#35770;&#26159;&#21870;&#40831;&#21160;&#29289;&#36824;&#26159;&#20154;&#31867;&#65289;&#26159;&#21542;&#34920;&#29616;&#20986;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23398;&#20064;&#30340;&#20851;&#38190;&#34892;&#20026;&#29305;&#24449;&#20043;&#19968;&#26159;&#23545;&#29615;&#22659;&#20013;&#23616;&#37096;&#21464;&#21270;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26041;&#27861;&#36739;&#38590;&#36866;&#24212;&#36825;&#31181;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#25918;&#32531;&#20914;&#21306;&#65292;&#24102;&#26377;&#23616;&#37096;&#36951;&#24536;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#36951;&#24536;&#36807;&#26102;&#30340;&#32463;&#39564;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20445;&#30041;&#26087;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23548;&#33322;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#21152;&#24555;&#20102;&#23398;&#20064;&#36895;&#24230;&#24182;&#25913;&#21892;&#20102;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key behavioral characteristics used in neuroscience to determine whether the subject of study -- be it a rodent or a human -- exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#36890;&#36807;&#26032;&#22411;&#30340;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20844;&#24335;&#35757;&#32451;&#25345;&#32493;&#36816;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#30340;&#36755;&#20837;&#20107;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.05214</link><description>&lt;p&gt;
&#39535;&#26381;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20197;&#23398;&#20064;&#39034;&#24207;&#12289;&#20302;&#24310;&#36831;&#12289;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;
&lt;/p&gt;
&lt;p&gt;
Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow. (arXiv:2303.05214v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#36890;&#36807;&#26032;&#22411;&#30340;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#20844;&#24335;&#35757;&#32451;&#25345;&#32493;&#36816;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#30340;&#36755;&#20837;&#20107;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20026;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20107;&#20214;&#25968;&#25454;&#30340;&#29420;&#29305;&#24615;&#36136;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#24103;&#30340;&#25991;&#29486;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#20817;&#29616;&#36825;&#20123;&#25215;&#35834;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#39034;&#24207;&#20272;&#35745;&#22522;&#20110;&#20107;&#20214;&#30340;&#20809;&#27969;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#25512;&#29702;&#39057;&#29575;&#12290;&#26680;&#24515;&#26159;&#19968;&#20010;&#25345;&#32493;&#36816;&#34892;&#30340;&#24102;&#29366;&#24577;&#31070;&#32463;&#27169;&#22411;&#65292;&#20351;&#29992;&#23545;&#27604;&#24230;&#26368;&#22823;&#21270;&#30340;&#26032;&#22411;&#20844;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#23545;&#36755;&#20837;&#20107;&#20214;&#20013;&#30340;&#38750;&#32447;&#24615;&#21644;&#21464;&#21270;&#32479;&#35745;&#20445;&#25345;&#31283;&#20581;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#31934;&#24230;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras have recently gained significant traction since they open up new avenues for low-latency and low-power solutions to complex computer vision problems. To unlock these solutions, it is necessary to develop algorithms that can leverage the unique nature of event data. However, the current state-of-the-art is still highly influenced by the frame-based literature, and usually fails to deliver on these promises. In this work, we take this into consideration and propose a novel self-supervised learning pipeline for the sequential estimation of event-based optical flow that allows for the scaling of the models to high inference frequencies. At its core, we have a continuously-running stateful neural model that is trained using a novel formulation of contrast maximization that makes it robust to nonlinearities and varying statistics in the input events. Results across multiple datasets confirm the effectiveness of our method, which establishes a new state of the art in terms of ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21333;&#30340;&#23376;&#25216;&#33021;&#25511;&#21046;&#22120;&#24341;&#23548;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25163;&#37096;&#28789;&#24039;&#25805;&#20316;&#25216;&#33021;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20351;&#29992;&#25506;&#32034;&#24615;&#37325;&#32622;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#23398;&#20064;&#38590;&#20197;&#25506;&#32034;&#30340;&#25163;&#25351;&#27493;&#24577;&#25163;&#37096;&#25805;&#20316;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03533</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#23376;&#25216;&#33021;&#25511;&#21046;&#22120;&#25351;&#23548;&#25506;&#32034;&#65292;&#23454;&#29616;&#25163;&#37096;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Dexterous In-hand Manipulation by Guiding Exploration with Simple Sub-skill Controllers. (arXiv:2303.03533v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21333;&#30340;&#23376;&#25216;&#33021;&#25511;&#21046;&#22120;&#24341;&#23548;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25163;&#37096;&#28789;&#24039;&#25805;&#20316;&#25216;&#33021;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20351;&#29992;&#25506;&#32034;&#24615;&#37325;&#32622;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#23398;&#20064;&#38590;&#20197;&#25506;&#32034;&#30340;&#25163;&#25351;&#27493;&#24577;&#25163;&#37096;&#25805;&#20316;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#25552;&#39640;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#25163;&#37096;&#28789;&#24039;&#25805;&#20316;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23398;&#20064;&#36825;&#20123;&#25216;&#33021;&#65292;&#20294;&#20854;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#36739;&#20302;&#65292;&#36825;&#26159;&#22240;&#20026;&#36825;&#20123;&#25216;&#33021;&#26159;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#30340;&#65292;&#27809;&#26377;&#21463;&#30410;&#20110;&#20219;&#20309;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#30693;&#35782;&#21487;&#29992;&#30340;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#23398;&#20064;&#25163;&#37096;&#28789;&#24039;&#25805;&#20316;&#25216;&#33021;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#23376;&#25216;&#33021;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#36981;&#24490;&#36825;&#20123;&#25511;&#21046;&#22120;&#30340;&#21160;&#20316;&#65292;&#36890;&#36807;&#19968;&#20010;&#26694;&#26550;&#23558;&#25506;&#32034;&#24341;&#23548;&#21040;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#32780;&#23637;&#31034;&#20102;&#25552;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20351;&#29992;&#25506;&#32034;&#24615;&#37325;&#32622;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#20102;&#23398;&#20064;&#38590;&#20197;&#25506;&#32034;&#30340;&#25163;&#25351;&#27493;&#24577;&#25163;&#37096;&#25805;&#20316;&#25216;&#33021;&#12290;&#35270;&#39057;&#32467;&#26524;&#21487;&#20197;&#22312;https://roamlab.github.io/vge&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reinforcement learning has led to dexterous manipulation skills of increasing complexity. Nonetheless, learning these skills in simulation still exhibits poor sample-efficiency which stems from the fact these skills are learned from scratch without the benefit of any domain expertise. In this work, we aim to improve the sample efficiency of learning dexterous in-hand manipulation skills using controllers available via domain knowledge. To this end, we design simple sub-skill controllers and demonstrate improved sample efficiency using a framework that guides exploration toward relevant state space by following actions from these controllers. We are the first to demonstrate learning hard-to-explore finger-gaiting in-hand manipulation skills without the use of an exploratory reset distribution. Video results can be found at https://roamlab.github.io/vge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#22312;&#32447;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21608;&#22260;&#30340;&#24322;&#24120;&#24773;&#20917;&#20986;&#29616;&#26102;&#36805;&#36895;&#21709;&#24212;&#65292;&#21033;&#29992;&#20202;&#34920;&#30424;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#27169;&#22359;&#26469;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#31616;&#21333;&#30452;&#35266;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21033;&#29992;RGB&#24103;&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.10719</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#22312;&#32447;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Memory-augmented Online Video Anomaly Detection. (arXiv:2302.10719v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#22312;&#32447;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21608;&#22260;&#30340;&#24322;&#24120;&#24773;&#20917;&#20986;&#29616;&#26102;&#36805;&#36895;&#21709;&#24212;&#65292;&#21033;&#29992;&#20202;&#34920;&#30424;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#27169;&#22359;&#26469;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#31616;&#21333;&#30452;&#35266;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21033;&#29992;RGB&#24103;&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26469;&#35828;&#65292;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32447;&#24037;&#20316;&#30340;&#31995;&#32479;&#65292;&#20197;&#31435;&#21363;&#21709;&#24212;&#22260;&#32469;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20986;&#29616;&#30340;&#24322;&#24120;&#24773;&#20917;&#65292;&#20165;&#21033;&#29992;&#30001;&#20202;&#34920;&#30424;&#25668;&#20687;&#22836;&#25429;&#33719;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21517;&#20026;MOVAD&#65292;&#20381;&#36182;&#20110;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#30701;&#26399;&#35760;&#24518;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#25805;&#20316;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#30001;&#35270;&#39057;Swing Transformer (VST) &#23454;&#29616;&#65292;&#20197;&#21450;&#23884;&#20837;&#22312;&#20998;&#31867;&#22120;&#20869;&#37096;&#30340;&#38271;&#26399;&#35760;&#24518;&#27169;&#22359;&#65292;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#32593;&#32476;&#32771;&#34385;&#36828;&#31243;&#36807;&#21435;&#30340;&#20449;&#24687;&#21644;&#25805;&#20316;&#32972;&#26223;&#12290;MOVAD&#30340;&#20248;&#21183;&#19981;&#20165;&#22312;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36824;&#22312;&#20110;&#20854;&#31616;&#21333;&#30452;&#35266;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20165;&#20351;&#29992;RGB&#24103;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#20551;&#35774;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20351;&#20854;&#26131;&#20110;&#23454;&#26045;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;...
&lt;/p&gt;
&lt;p&gt;
The ability to understand the surrounding scene is of paramount importance for Autonomous Vehicles (AVs). This paper presents a system capable to work in an online fashion, giving an immediate response to the arise of anomalies surrounding the AV, exploiting only the videos captured by a dash-mounted camera. Our architecture, called MOVAD, relies on two main modules: a Short-Term Memory Module to extract information related to the ongoing action, implemented by a Video Swin Transformer (VST), and a Long-Term Memory Module injected inside the classifier that considers also remote past information and action context thanks to the use of a Long-Short Term Memory (LSTM) network. The strengths of MOVAD are not only linked to its excellent performance, but also to its straightforward and modular architecture, trained in a end-to-end fashion with only RGB frames with as less assumptions as possible, which makes it easy to implement and play with. We evaluated the performance of our method on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;NoC&#65289;&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#23398;&#20064;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20943;&#23569;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#40784;&#32423;&#21035;&#21487;&#25511;&#30340;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;</title><link>http://arxiv.org/abs/2212.13563</link><description>&lt;p&gt;
&#20174;&#32593;&#32476;&#29228;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#24863;&#30693;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Noise-aware Learning from Web-crawled Image-Text Data for Image Captioning. (arXiv:2212.13563v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;&#65288;NoC&#65289;&#65292;&#23427;&#21033;&#29992;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#23398;&#20064;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20943;&#23569;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#40784;&#32423;&#21035;&#21487;&#25511;&#30340;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#30452;&#35266;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#65292;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20016;&#23500;&#30340;&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#32423;&#21035;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#22266;&#26377;&#30340;&#22122;&#22768;&#65288;&#20363;&#22914;&#19981;&#23545;&#40784;&#30340;&#23545;&#65289;&#20351;&#24471;&#23398;&#20064;&#19968;&#20010;&#31934;&#30830;&#30340;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#36807;&#28388;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#21435;&#38500;&#22122;&#22768;&#25968;&#25454;&#65292;&#20294;&#23427;&#20250;&#23548;&#33268;&#21487;&#23398;&#20064;&#30693;&#35782;&#30340;&#20943;&#23569;&#65292;&#24182;&#26377;&#26102;&#24102;&#26469;&#25968;&#25454;&#19981;&#36275;&#30340;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#23383;&#24149;&#29983;&#25104;&#65288;NoC&#65289;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20174;&#25972;&#20010;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#36739;&#23569;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#26159;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#23545;&#40784;&#32423;&#21035;&#21487;&#25511;&#30340;&#23383;&#24149;&#29983;&#25104;&#22120;&#23454;&#29616;&#30340;&#65292;&#35813;&#29983;&#25104;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#23545;&#40784;&#32423;&#21035;&#20316;&#20026;&#25511;&#21046;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#12290;&#23545;&#40784;&#32423;&#21035;&#26465;&#20214;&#30340;&#35757;&#32451;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning is one of the straightforward tasks that can take advantage of large-scale web-crawled data which provides rich knowledge about the visual world for a captioning model. However, since web-crawled data contains image-text pairs that are aligned at different levels, the inherent noises (e.g., misaligned pairs) make it difficult to learn a precise captioning model. While the filtering strategy can effectively remove noisy data, it leads to a decrease in learnable knowledge and sometimes brings about a new problem of data deficiency. To take the best of both worlds, we propose a Noise-aware Captioning (NoC) framework, which learns rich knowledge from the whole web-crawled data while being less affected by the noises. This is achieved by the proposed alignment-level-controllable captioner, which is learned using alignment levels of the image-text pairs as a control signal during training. The alignment-level-conditioned training allows the model to generate high-quality cap
&lt;/p&gt;</description></item><item><title>UnICLAM&#26159;&#19968;&#31181;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2212.10729</link><description>&lt;p&gt;
UnICLAM&#65306;&#23545;&#25239;&#24615;&#23631;&#34109;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering. (arXiv:2212.10729v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10729
&lt;/p&gt;
&lt;p&gt;
UnICLAM&#26159;&#19968;&#31181;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Medical-VQA&#65289;&#26088;&#22312;&#22238;&#31572;&#26377;&#20851;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#20020;&#24202;&#38382;&#39064;&#65292;&#20026;&#21307;&#29983;&#25552;&#20379;&#20915;&#31574;&#36873;&#39033;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Medical-VQA&#27169;&#22411;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#32441;&#29702;&#32534;&#30721;&#22120;&#20998;&#21035;&#25918;&#32622;&#22312;&#21452;&#29420;&#31435;&#31354;&#38388;&#20013;&#26469;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#38388;&#25509;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnICLAM&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#23631;&#34109;&#23454;&#29616;&#32479;&#19968;&#21644;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#23398;&#20064;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#27969;&#39044;&#35757;&#32451;&#32467;&#26500;&#65292;&#37319;&#29992;&#36880;&#28176;&#36719;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;&#25216;&#26415;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#23398;&#20064;&#20102;&#19968;&#20010;&#32422;&#26463;&#65292;&#20351;&#24471;&#35270;&#35273;&#21644;&#32441;&#29702;&#32534;&#30721;&#22120;&#22312;&#21516;&#19968;&#31354;&#38388;&#20013;&#25509;&#36817;&#65292;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#36880;&#28176;&#25918;&#26494;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25226;&#25569;&#32479;&#19968;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#24615;&#23631;&#34109;&#25968;&#25454;&#22686;&#24378;&#25299;&#23637;&#21040;&#23545;&#27604;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Visual Question Answering (Medical-VQA) aims to to answer clinical questions regarding radiology images, assisting doctors with decision-making options. Nevertheless, current Medical-VQA models learn cross-modal representations through residing vision and texture encoders in dual separate spaces, which lead to indirect semantic alignment. In this paper, we propose UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive Representation Learning with Adversarial Masking. Specifically, to learn an aligned image-text representation, we first establish a unified dual-stream pre-training structure with the gradually soft-parameter sharing strategy. Technically, the proposed strategy learns a constraint for the vision and texture encoders to be close in a same space, which is gradually loosened as the higher number of layers. Moreover, for grasping the unified semantic representation, we extend the adversarial masking data augmentation to the contrastive representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#26469;&#25351;&#23548;&#26080;&#20154;&#26426;&#36873;&#25321;&#19979;&#19968;&#20010;&#35270;&#28857;&#65292;&#20197;&#25910;&#38598;&#23613;&#21487;&#33021;&#22810;&#26410;&#20998;&#31867;&#30446;&#26631;&#30340;&#35777;&#25454;&#65292;&#24182;&#32771;&#34385;&#30446;&#26631;&#30340;&#36816;&#21160;&#12289;&#26041;&#21521;&#21644;&#36974;&#25377;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#36824;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.03068</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#20027;&#21160;&#23545;&#31227;&#21160;&#30446;&#26631;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Active Classification of Moving Targets with Learned Control Policies. (arXiv:2212.03068v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#26469;&#25351;&#23548;&#26080;&#20154;&#26426;&#36873;&#25321;&#19979;&#19968;&#20010;&#35270;&#28857;&#65292;&#20197;&#25910;&#38598;&#23613;&#21487;&#33021;&#22810;&#26410;&#20998;&#31867;&#30446;&#26631;&#30340;&#35777;&#25454;&#65292;&#24182;&#32771;&#34385;&#30446;&#26631;&#30340;&#36816;&#21160;&#12289;&#26041;&#21521;&#21644;&#36974;&#25377;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#36824;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#19968;&#26550;&#26080;&#20154;&#26426;&#24517;&#39035;&#25910;&#38598;&#35821;&#20041;&#20449;&#24687;&#20197;&#23545;&#22810;&#20010;&#31227;&#21160;&#30446;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#29305;&#21035;&#35299;&#20915;&#20102;&#20351;&#29992;&#8220;&#40657;&#30418;&#8221;&#20998;&#31867;&#22120;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65289;&#25552;&#21462;&#20449;&#24687;&#26102;&#30340;&#25511;&#21046;&#36755;&#20837;&#35745;&#31639;&#25361;&#25112;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#32570;&#20047;&#35270;&#28857;&#19982;&#20854;&#20851;&#32852;&#36755;&#20986;&#20043;&#38388;&#30340;&#20998;&#26512;&#20851;&#31995;&#65292;&#23548;&#33268;&#26080;&#27861;&#22312;&#20449;&#24687;&#25910;&#38598;&#26041;&#26696;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36755;&#20986;&#19979;&#19968;&#20010;&#35270;&#28857;&#65292;&#20197;&#20415;&#26080;&#20154;&#26426;&#33021;&#22815;&#26377;&#21033;&#22320;&#33719;&#21462;&#23613;&#21487;&#33021;&#22810;&#26410;&#20998;&#31867;&#30446;&#26631;&#30340;&#35777;&#25454;&#65292;&#24182;&#25512;&#29702;&#20854;&#36816;&#21160;&#12289;&#26041;&#21521;&#21644;&#36974;&#25377;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#32423;MPC&#25511;&#21046;&#22120;&#23558;&#26080;&#20154;&#26426;&#31227;&#21160;&#21040;&#25152;&#38656;&#35270;&#28857;&#65292;&#32771;&#34385;&#21040;&#20854;&#23454;&#38469;&#21160;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20250;&#20998;&#31867;&#20934;&#30830;&#29575;&#24471;&#21040;&#25552;&#39640;&#65292;&#36824;&#33021;&#25552;&#39640;&#30446;&#26631;&#20998;&#31867;&#25152;&#38656;&#30340;&#35270;&#28857;&#25968;&#37327;&#65292;&#24182;&#19988;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem where a drone has to collect semantic information to classify multiple moving targets. In particular, we address the challenge of computing control inputs that move the drone to informative viewpoints, position and orientation, when the information is extracted using a "black-box" classifier, e.g., a deep learning neural network. These algorithms typically lack of analytical relationships between the viewpoints and their associated outputs, preventing their use in information-gathering schemes. To fill this gap, we propose a novel attention-based architecture, trained via Reinforcement Learning (RL), that outputs the next viewpoint for the drone favoring the acquisition of evidence from as many unclassified targets as possible while reasoning about their movement, orientation, and occlusions. Then, we use a low-level MPC controller to move the drone to the desired viewpoint taking into account its actual dynamics. We show that our approach not onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2212.02469</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#27169;&#22411;&#20808;&#39564;&#30340;&#19968;&#27425;&#24615;&#38544;&#24335;&#21160;&#30011;&#21270;&#22836;&#20687;&#21046;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELICIT&#65292;&#19968;&#31181;&#20174;&#21333;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#31867;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21019;&#24314;&#20154;&#31867;&#22836;&#20687;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#38656;&#35201;&#31264;&#23494;&#36755;&#20837;&#20449;&#21495;&#65288;&#22914;&#35270;&#39057;&#25110;&#22810;&#35270;&#35282;&#22270;&#20687;&#65289;&#65292;&#35201;&#20040;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#29305;&#23450;3D&#20154;&#20307;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#20808;&#39564;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#31232;&#30095;&#35270;&#35282;&#36755;&#20837;&#36827;&#34892;&#37325;&#24314;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#22312;&#20165;&#26377;&#19968;&#24352;&#22270;&#20687;&#26102;&#26080;&#27861;&#23454;&#29616;&#36924;&#30495;&#37325;&#24314;&#12290;&#20026;&#20102;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#36924;&#30495;&#21487;&#21160;3D&#20154;&#20307;&#30340;&#21019;&#24314;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELICIT&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#19968;&#24352;&#22270;&#29255;&#23398;&#20064;&#20154;&#20307;&#29305;&#23450;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26032;&#26041;&#27861;&#12290;&#21463;&#21040;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#36523;&#20307;&#20960;&#20309;&#24418;&#29366;&#24182;&#20174;&#19968;&#24352;&#22270;&#29255;&#20013;&#24819;&#35937;&#36896;&#22411;&#23436;&#25972;&#30340;&#34915;&#26588;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#22312;ELICIT&#20013;&#21033;&#29992;&#20102;&#20004;&#20010;&#20808;&#39564;&#65306;3D&#20960;&#20309;&#20808;&#39564;&#21644;&#35270;&#35273;&#35821;&#20041;&#20808;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ELICIT&#21033;&#29992;&#19968;&#20010;&#33945;&#30382;&#39030;&#28857;&#27169;&#26495;&#27169;&#22411;&#65288;&#21363;SMPL&#65289;&#30340;3D&#36523;&#20307;&#24418;&#29366;&#20960;&#20309;&#20808;&#39564;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;CLIP&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#20102;&#35270;&#35273;&#26381;&#35013;&#35821;&#20041;&#20808;&#39564;&#12290;&#36825;&#20004;&#20010;&#20808;&#39564;&#22343;&#29992;&#20110;&#20174;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#36924;&#30495;&#30340;&#21487;&#21160;3D&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used 
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12814</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65306;&#27010;&#24565;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12814
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#21463;VFL&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;VFL&#30340;&#27010;&#24565;&#21644;&#31639;&#27861;&#65292;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;VFL&#35774;&#32622;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#35814;&#23613;&#20998;&#31867;&#65292;&#24182;&#23545;&#27599;&#20010;&#21327;&#35758;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;VFLow&#65292;&#23427;&#32771;&#34385;&#20102;VFL&#38382;&#39064;&#22312;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#38544;&#31169;&#20197;&#21450;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;VFL&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
&lt;/p&gt;</description></item><item><title>GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14440</link><description>&lt;p&gt;
GeONet&#65306;&#19968;&#31181;&#23398;&#20064;Wasserstein&#27979;&#22320;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14440
&lt;/p&gt;
&lt;p&gt;
GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;(OT)&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20960;&#20309;&#19978;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#21644;&#27979;&#22320;&#30340;&#26041;&#27861;&#38656;&#35201;&#32593;&#26684;&#20381;&#36182;&#30340;&#22495;&#31163;&#25955;&#21270;&#65292;&#21516;&#26102;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GeONet&#65292;&#19968;&#31181;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#20102;&#23558;&#36755;&#20837;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#26144;&#23556;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#22312;&#33073;&#26426;&#35757;&#32451;&#38454;&#27573;&#65292;GeONet&#36890;&#36807;&#32806;&#21512;&#30340;PDE&#31995;&#32479;&#34920;&#24449;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#26368;&#20248;&#26465;&#20214;&#23398;&#20064;&#20102;OT&#38382;&#39064;&#30340;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#12290;&#21518;&#32493;&#30340;&#25512;&#29702;&#38454;&#27573;&#26159;&#30636;&#26102;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GeONet&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STAX&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#26102;&#23454;&#26102;&#25506;&#32034;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2111.01919</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#20013;&#21457;&#29616;&#21644;&#21033;&#29992;&#31232;&#30095;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Discovering and Exploiting Sparse Rewards in a Learned Behavior Space. (arXiv:2111.01919v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01919
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;STAX&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#26102;&#23454;&#26102;&#25506;&#32034;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23398;&#20064;&#20195;&#29702;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#34892;&#20026;&#36136;&#37327;&#30340;&#21453;&#39304;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#26159;&#19987;&#27880;&#20110;&#25506;&#32034;&#65292;&#24076;&#26395;&#33021;&#21457;&#29616;&#19968;&#20010;&#22870;&#21169;&#20449;&#21495;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23398;&#20064;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#65288;1&#65289;&#25506;&#32034;&#21487;&#33021;&#30340;&#20195;&#29702;&#34892;&#20026;&#21644;&#65288;2&#65289;&#21033;&#29992;&#21487;&#33021;&#21457;&#29616;&#30340;&#20219;&#20309;&#22870;&#21169;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#38656;&#35201;&#23450;&#20041;&#19968;&#20010;&#34892;&#20026;&#31354;&#38388;&#65292;&#23558;&#20195;&#29702;&#19982;&#20854;&#22312;&#21487;&#20197;&#25506;&#32034;&#30340;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#30340;&#34892;&#20026;&#30456;&#20851;&#32852;&#12290;&#38656;&#35201;&#23450;&#20041;&#36825;&#20010;&#31354;&#38388;&#26159;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;STAX&#65292;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#23454;&#26102;&#23398;&#20064;&#34892;&#20026;&#31354;&#38388;&#24182;&#22312;&#26377;&#25928;&#22320;&#20248;&#21270;&#20219;&#20309;&#21457;&#29616;&#30340;&#22870;&#21169;&#30340;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#23558;&#34892;&#20026;&#31354;&#38388;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#19982;&#22870;&#21169;&#30340;&#21033;&#29992;&#20998;&#24320;&#65292;&#20197;&#26367;&#20195;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Efficient exploration algorithms have been proposed that require to define a behavior space, that associates to an agent its resulting behavior in a space that is known to be worth exploring. The need to define this space is a limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while efficiently optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alterna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MimicNorm&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#24182;&#20445;&#25345;&#20854;&#26680;&#24515;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#26469;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290;MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21487;&#19982;BN&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2010.09278</link><description>&lt;p&gt;
MimicNorm: &#26435;&#37325;&#22343;&#20540;&#21644;&#26368;&#21518;&#19968;&#23618;&#25209;&#24402;&#19968;&#21270;&#23618;&#27169;&#20223;&#25209;&#24402;&#19968;&#21270;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.09278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MimicNorm&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#24182;&#20445;&#25345;&#20854;&#26680;&#24515;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#26469;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290;MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21487;&#19982;BN&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#23618;&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#25928;&#26524;&#19978;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;BN&#38656;&#35201;&#39069;&#22806;&#30340;&#20869;&#23384;&#21644;&#28014;&#28857;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#22312;&#24494;&#23567;&#25209;&#27425;&#19978;&#65292;BN&#20250;&#21464;&#24471;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#25209;&#27425;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21270;BN&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;BN&#23618;&#30340;&#20004;&#20010;&#22522;&#26412;&#24433;&#21709;&#65292;&#21363;&#25968;&#25454;&#21435;&#30456;&#20851;&#24615;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;MimicNorm&#65292;&#26469;&#25913;&#21892;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#12290; MimicNorm&#20165;&#21253;&#21547;&#20004;&#20010;&#36731;&#37327;&#32423;&#25805;&#20316;&#65292;&#21253;&#25324;&#20462;&#25913;&#30340;&#26435;&#37325;&#22343;&#20540;&#25805;&#20316;&#65288;&#20174;&#26435;&#37325;&#21442;&#25968;&#24352;&#37327;&#20013;&#20943;&#21435;&#22343;&#20540;&#20540;&#65289;&#21644;&#25439;&#22833;&#20989;&#25968;&#65288;&#26368;&#21518;&#30340;BN&#23618;&#65289;&#20043;&#21069;&#30340;&#19968;&#20010;BN&#23618;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26435;&#37325;&#22343;&#20540;&#25805;&#20316;&#21487;&#20197;&#30333;&#21270;&#28608;&#27963;&#65292;&#20351;&#32593;&#32476;&#36716;&#21270;&#20026;&#31867;&#20284;BN&#23618;&#30340;&#28151;&#27788;&#29366;&#24577;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#25910;&#25947;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. T
&lt;/p&gt;</description></item></channel></rss>