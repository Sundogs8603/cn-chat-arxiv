<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#36739;&#39640;&#30340;&#39118;&#38505;&#21644;&#28508;&#22312;&#30340;&#19981;&#21487;&#36870;&#21518;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#20154;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.05302</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;:&#19968;&#20010;&#24187;&#35273;?
&lt;/p&gt;
&lt;p&gt;
Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?. (arXiv:2401.05302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05302
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#36739;&#39640;&#30340;&#39118;&#38505;&#21644;&#28508;&#22312;&#30340;&#19981;&#21487;&#36870;&#21518;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#20154;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24322;&#24120;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#20986;&#29616;&#20154;&#24418;&#21270;&#21644;&#23545;&#22833;&#36133;&#26696;&#20363;&#30340;&#23485;&#23481;&#24615;&#24341;&#21457;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#33021;&#21147;&#30340;&#35752;&#35770;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#31181;&#20551;&#20449;&#24565;&#27979;&#35797;&#26469;&#39564;&#35777;&#25512;&#26029;&#21644;&#32500;&#25252;&#21478;&#19968;&#20010;&#23454;&#20307;&#30340;&#24515;&#26234;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ToM&#33021;&#21147;&#30340;&#19968;&#20010;&#29305;&#27530;&#24212;&#29992;&#65292;&#36825;&#20855;&#26377;&#26356;&#39640;&#30340;&#39118;&#38505;&#21644;&#21487;&#33021;&#26159;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65306;&#20154;&#26426;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24863;&#30693;&#34892;&#20026;&#35782;&#21035;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#26426;&#22120;&#20154;&#29983;&#25104;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20851;&#27880;&#22235;&#31181;&#34892;&#20026;&#31867;&#22411;&#65292;&#21363;&#21487;&#20197;&#35299;&#37322;&#30340;&#12289;&#21487;&#35835;&#30340;&#12289;&#21487;&#39044;&#27979;&#30340;&#21644;&#28151;&#28102;&#30340;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21512;&#25104;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;LLM&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models. While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.05300</link><description>&lt;p&gt;
&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20803;&#35821;&#35328;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
I am a Strange Dataset: Metalinguistic Tests for Language Models. (arXiv:2401.05300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#28041;&#21450;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#65288;&#8220;&#26412;&#35770;&#25991;&#26377;&#20845;&#20010;&#37096;&#20998;&#12290;&#8221;&#65289;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#22788;&#29702;&#36825;&#26679;&#30340;&#35821;&#35328;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#32487;&#32493;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#8221;&#30340;&#38472;&#36848;&#65288;&#27491;&#30830;&#30340;&#32487;&#32493;&#24212;&#35813;&#26159;&#8220;&#26159;&#8221;&#65289;&#12290;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#21028;&#26029;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#21477;&#23376;&#12290;&#8221;&#30340;&#38472;&#36848;&#30340;&#30495;&#23454;&#24615;&#65288;&#26159;&#20551;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#23567;&#24046;&#24322;&#30340;&#38750;&#33258;&#25351;&#20803;&#35821;&#35328;&#31034;&#20363;&#65292;&#26469;&#34917;&#20805;&#20027;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#35821;&#35328;&#12290;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#25163;&#24037;&#21046;&#20316;&#65292;&#38750;&#19987;&#23478;&#26631;&#27880;&#21592;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#24320;&#28304;LLMs&#65288;&#20174;7B&#21040;70B&#30340;&#21442;&#25968;&#65289;&#20197;&#21450;&#36890;&#36807;API&#36827;&#34892;&#27979;&#35797;&#30340;&#38381;&#28304;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statements involving metalinguistic self-reference ("This paper has six sections.") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present "I am a Strange Dataset", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like "The penultimate word in this sentence is" (where a correct continuation is "is"). In verification, models judge the truth of statements like "The penultimate word in this sentence is sentence." (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#36827;&#34892;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#20027;&#20915;&#23450;&#25511;&#21046;&#22120;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.05251</link><description>&lt;p&gt;
ReACT: &#20351;&#29992;B&#26679;&#26465;&#20960;&#20309;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries. (arXiv:2401.05251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#36827;&#34892;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#20027;&#20915;&#23450;&#25511;&#21046;&#22120;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#25511;&#21046;&#22120;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#23548;&#20986;&#25511;&#21046;&#22120;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#20415;&#20110;&#33258;&#21160;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#65288;BSG&#65289;&#12290;&#25105;&#20204;&#20851;&#27880;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#36825;&#26159;&#19968;&#31867;&#34892;&#20026;&#22797;&#26434;&#19988;&#21462;&#20915;&#20110;&#36816;&#34892;&#26465;&#20214;&#30340;&#31995;&#32479;&#12290;&#23545;&#20110;&#36825;&#19968;&#31995;&#32479;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#24050;&#30693;&#30340;&#35774;&#35745;&#21407;&#29702;&#65292;&#22686;&#30410;&#35843;&#24230;&#25511;&#21046;&#32467;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#34892;&#19994;&#30340;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#31616;&#21270;&#23545;&#20110;&#36825;&#20123;&#25511;&#21046;&#32467;&#26500;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#20219;&#21153;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#20010;DRL&#20195;&#29702;&#12290;&#22522;&#20110;&#25511;&#21046;&#31995;&#32479;&#35266;&#27979;&#65292;&#20195;&#29702;&#33258;&#20027;&#20915;&#23450;&#22914;&#20309;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#12290;&#36890;&#36807;&#24341;&#20837;BSG&#26469;&#26144;&#23556;&#21487;&#33021;&#20381;&#36182;&#20110;&#22810;&#20010;&#21464;&#37327;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#24471;&#36866;&#24212;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust and performant controllers are essential for industrial applications. However, deriving controller parameters for complex and nonlinear systems is challenging and time-consuming. To facilitate automatic controller parametrization, this work presents a novel approach using deep reinforcement learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the control of parameter-variant systems, a class of systems with complex behavior which depends on the operating conditions. For this system class, gain-scheduling control structures are widely used in applications across industries due to well-known design principles. Facilitating the expensive controller parametrization task regarding these control structures, we deploy an DRL agent. Based on control system observations, the agent autonomously decides how to adapt the controller parameters. We make the adaptation process more efficient by introducing BSGs to map the controller parameters which may depend on numerous 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#27450;&#35784;&#26816;&#27979;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30417;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#21019;&#26032;&#30340;Kolmogorov-Smirnov (KS)&#26816;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#30417;&#27979;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05219</link><description>&lt;p&gt;
&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud Detection. (arXiv:2401.05219v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#27450;&#35784;&#26816;&#27979;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30417;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#21019;&#26032;&#30340;Kolmogorov-Smirnov (KS)&#26816;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#30417;&#27979;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#37329;&#34701;&#27450;&#35784;&#26696;&#20214;&#26126;&#26174;&#22686;&#21152;&#12290;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#35299;&#20915;&#26234;&#33021;&#25163;&#26426;&#25903;&#20184;&#26381;&#21153;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#23454;&#26102;&#27450;&#35784;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#21363;&#32570;&#20047;&#19968;&#20010;&#24378;&#22823;&#30340;&#31995;&#32479;&#26469;&#30417;&#27979;&#36825;&#20123;&#20998;&#24067;&#24335;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;Kolmogorov-Smirnov (KS)&#26816;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#30417;&#27979;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digital era has seen a marked increase in financial fraud. edge ML emerged as a promising solution for smartphone payment services fraud detection, enabling the deployment of ML models directly on edge devices. This approach enables a more personalized real-time fraud detection. However, a significant gap in current research is the lack of a robust system for monitoring data distribution shifts in these distributed edge ML applications. Our work bridges this gap by introducing a novel open-source framework designed for continuous monitoring of data distribution shifts on a network of edge devices. Our system includes an innovative calculation of the Kolmogorov-Smirnov (KS) test over a distributed network of edge devices, enabling efficient and accurate monitoring of users behavior shifts. We comprehensively evaluate the proposed framework employing both real-world and synthetic financial transaction datasets and demonstrate the framework's effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#36866;&#24212;&#21644;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#19979;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05215</link><description>&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Large Language Models for Financial Sentiment Analysis. (arXiv:2401.05215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#36866;&#24212;&#21644;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#19979;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#26159;&#23558;&#37329;&#34701;&#25991;&#26412;&#20869;&#23481;&#20998;&#31867;&#20026;&#24773;&#32490;&#31867;&#21035;&#65288;&#22914;&#31215;&#26497;&#12289;&#28040;&#26497;&#21644;&#20013;&#24615;&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#37329;&#34701;&#26032;&#38395;&#26631;&#39064;&#30340;&#20998;&#31867;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#32570;&#20047;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;[1, 2, 3] &#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;LLMs&#26159;&#20174;&#22823;&#37327;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#20855;&#26377;&#25991;&#26412;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38656;&#35201;&#24456;&#23569;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26377;&#25928;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24320;&#28304;&#30340;Llama2-7B&#27169;&#22411;&#65288;2023&#24180;&#65289;&#21644;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25216;&#26415;[4]&#12290;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;LLMs&#26469;&#35828;&#36739;&#23567;&#30340;7B&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;&#65292;&#25552;&#39640;&#20102;&#26631;&#31614;&#35789;&#31354;&#38388;&#30340;&#35206;&#30422;&#24230;&#21644;&#20943;&#23567;&#20102;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.05204</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65306;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts into a Verbalizer. (arXiv:2401.05204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;&#65292;&#25552;&#39640;&#20102;&#26631;&#31614;&#35789;&#31354;&#38388;&#30340;&#35206;&#30422;&#24230;&#21644;&#20943;&#23567;&#20102;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#35821;&#29983;&#25104;&#22120;&#26159;&#25552;&#31034;&#35843;&#25972;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#35789;&#26144;&#23556;&#21040;&#31867;&#21035;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#35805;&#35821;&#29983;&#25104;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#31867;&#21035;&#21517;&#31216;&#30340;&#21516;&#20041;&#35789;&#25110;&#30456;&#20851;&#35789;&#38598;&#36827;&#34892;&#22686;&#24378;&#21644;&#31934;&#28860;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#29305;&#23450;&#20219;&#21153;&#22330;&#26223;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#27010;&#24565;&#20316;&#20026;&#26631;&#31614;&#35789;&#20505;&#36873;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32423;&#32852;&#26657;&#20934;&#27169;&#22359;&#26469;&#23558;&#20505;&#36873;&#35789;&#31934;&#28860;&#20026;&#27599;&#20010;&#31867;&#21035;&#30340;&#19968;&#32452;&#26631;&#31614;&#35789;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26631;&#31614;&#35789;&#31354;&#38388;&#20013;&#35206;&#30422;&#24230;&#26377;&#38480;&#21644;&#20559;&#35265;&#36739;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The verbalizer, which serves to map label words to class labels, is an essential component of prompt-tuning. In this paper, we present a novel approach to constructing verbalizers. While existing methods for verbalizer construction mainly rely on augmenting and refining sets of synonyms or related words based on class names, this paradigm suffers from a narrow perspective and lack of abstraction, resulting in limited coverage and high bias in the label-word space. To address this issue, we propose a label-word construction process that incorporates scenario-specific concepts. Specifically, we extract rich concepts from task-specific scenarios as label-word candidates and then develop a novel cascade calibration module to refine the candidates into a set of label words for each class. We evaluate the effectiveness of our proposed approach through extensive experiments on {five} widely used datasets for zero-shot text classification. The results demonstrate that our method outperforms ex
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05200</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65306;&#29992;&#25143;&#35780;&#20272;&#21644;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05200
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21046;&#36896;&#19994;&#20013;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;&#65292;&#36890;&#36807;&#35780;&#20272;&#23454;&#35777;&#20102;&#35813;&#31995;&#32479;&#30340;&#25928;&#30410;&#65292;&#25552;&#39640;&#20102;&#25805;&#20316;&#21592;&#30340;&#20449;&#24687;&#26816;&#32034;&#36895;&#24230;&#21644;&#38382;&#39064;&#35299;&#20915;&#25928;&#29575;&#65292;&#21516;&#26102;&#24378;&#35843;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#30340;&#20559;&#22909;&#12290;GPT-4&#26159;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#31649;&#29702;&#30693;&#35782;&#23545;&#32452;&#32455;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21046;&#36896;&#19994;&#20013;&#65292;&#25805;&#20316;&#24037;&#21378;&#21464;&#24471;&#36234;&#26469;&#36234;&#20381;&#36182;&#30693;&#35782;&#65292;&#36825;&#32473;&#24037;&#21378;&#22521;&#35757;&#21644;&#25903;&#25345;&#26032;&#25805;&#20316;&#21592;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#21387;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#24037;&#21378;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#24191;&#27867;&#30693;&#35782;&#65292;&#39640;&#25928;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#24182;&#20419;&#36827;&#26032;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24037;&#21378;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#30340;&#22909;&#22788;&#65292;&#21363;&#33021;&#22815;&#26356;&#24555;&#22320;&#26816;&#32034;&#20449;&#24687;&#21644;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#22312;&#26377;&#20154;&#24037;&#19987;&#23478;&#36873;&#39033;&#26102;&#26356;&#20542;&#21521;&#20110;&#21521;&#20154;&#24037;&#19987;&#23478;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#20960;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;GPT-4&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20687;StableBe
&lt;/p&gt;
&lt;p&gt;
Managing knowledge efficiently is crucial for organizational success. In manufacturing, operating factories has become increasing knowledge-intensive putting strain on the factory's capacity to train and support new operators. In this paper, we introduce a Large Language Model (LLM)-based system designed to use the extensive knowledge contained in factory documentation. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. To assess its effectiveness, we conducted an evaluation in a factory setting. The results of this evaluation demonstrated the system's benefits; namely, in enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several closed and open-sourced LLMs for this system. GPT-4 consistently outperformed its counterparts, with open-source models like StableBe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-2&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39135;&#35889;&#65292;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.05199</link><description>&lt;p&gt;
&#20351;&#29992;GPT-2&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#39135;&#35889;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Tree Search for Recipe Generation using GPT-2. (arXiv:2401.05199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-2&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39135;&#35889;&#65292;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39135;&#35889;&#29983;&#25104;&#26041;&#27861;&#20026;&#21416;&#24072;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#24037;&#20855;&#65292;&#21487;&#20197;&#25506;&#32034;&#21644;&#21019;&#36896;&#26032;&#30340;&#26377;&#36259;&#30340;&#28921;&#39274;&#32654;&#39135;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#21019;&#36896;&#20986;&#21487;&#20197;&#28385;&#36275;&#20010;&#20154;&#20559;&#22909;&#12289;&#33203;&#39135;&#38480;&#21046;&#20197;&#21450;&#36866;&#24212;&#24744;&#20912;&#31665;&#20869;&#39135;&#26448;&#30340;&#26032;&#39135;&#35889;&#12290;&#29616;&#26377;&#30340;&#36890;&#36807;LLMs&#29983;&#25104;&#39135;&#35889;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#26469;&#29983;&#25104;&#21548;&#36215;&#26469;&#30495;&#23454;&#30340;&#39135;&#35889;&#12290;&#28982;&#32780;&#65292;&#20180;&#32454;&#26816;&#26597;&#21518;&#21457;&#29616;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#39135;&#35889;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#22522;&#26412;&#35201;&#27714;&#65292;&#27604;&#22914;&#22312;&#40481;&#32905;&#33756;&#32948;&#20013;&#21253;&#21547;&#40481;&#32905;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RecipeMC&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;GPT-2&#24182;&#20381;&#36182;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#12290;RecipeMC&#20801;&#35768;&#25105;&#20204;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#36719;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#26356;&#21916;&#27426;&#20351;&#29992;RecipeMC&#29983;&#25104;&#30340;&#39135;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic food recipe generation methods provide a creative tool for chefs to explore and to create new, and interesting culinary delights. Given the recent success of large language models (LLMs), they have the potential to create new recipes that can meet individual preferences, dietary constraints, and adapt to what is in your refrigerator. Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes. In this paper, we propose RecipeMC, a text generation method using GPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to define reward functions to put soft constraints on text generation and thus improve the credibility of the generated recipes. Our results show that human evaluators prefer recipes generated with RecipeMC more often than recipes gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#26426;&#22120;&#20154;&#36710;&#36742;&#24314;&#27169;&#21644;&#25511;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#32852;&#37030;&#24335;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#38454;&#27573;&#24182;&#22686;&#21152;&#23545;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#24046;&#36317;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05194</link><description>&lt;p&gt;
&#32553;&#25918;&#26426;&#22120;&#20154;&#36710;&#36742;&#30340;&#24314;&#27169;&#12289;&#23450;&#20301;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#65306;&#35774;&#35745;&#21644;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Modelling, Positioning, and Deep Reinforcement Learning Path Tracking Control of Scaled Robotic Vehicles: Design and Experimental Validation. (arXiv:2401.05194v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#25918;&#26426;&#22120;&#20154;&#36710;&#36742;&#24314;&#27169;&#21644;&#25511;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#32852;&#37030;&#24335;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#38454;&#27573;&#24182;&#22686;&#21152;&#23545;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#24046;&#36317;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#31995;&#32479;&#29992;&#20110;&#21508;&#31181;&#23460;&#20869;&#24212;&#29992;&#65292;&#20174;&#20179;&#20648;&#21644;&#21046;&#36896;&#21040;&#29992;&#20110;&#35780;&#20272;&#20808;&#36827;&#25511;&#21046;&#31574;&#30053;&#65288;&#22914;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65289;&#30340;&#27979;&#35797;&#21488;&#65292;&#20165;&#20030;&#20960;&#20363;&#12290;&#32553;&#25918;&#26426;&#22120;&#20154;&#36710;&#36742;&#36890;&#24120;&#37197;&#22791;&#19968;&#31181;&#20998;&#23618;&#25511;&#21046;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#29992;&#20110;&#36710;&#36742;&#29366;&#24577;&#20272;&#35745;&#21644;&#25511;&#21046;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#65288;i&#65289;&#32852;&#37030;&#24335;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;FEKF&#65289;&#65292;&#21644;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#22120;&#35757;&#32451;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36335;&#24452;&#36319;&#36394;&#25511;&#21046;&#22120;&#65292;&#20197;&#21152;&#24555;&#23398;&#20064;&#38454;&#27573;&#24182;&#22686;&#21152;&#23545;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#24046;&#36317;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#36710;&#36742;&#27169;&#22411;&#30340;&#21046;&#23450;&#20197;&#21450;&#19968;&#31181;&#26377;&#25928;&#32780;&#31616;&#21333;&#30340;&#21442;&#25968;&#36776;&#35782;&#36807;&#31243;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#30340;&#27169;&#22411;&#29992;&#20110;&#65288;i&#65289;&#25903;&#25345;FEKF&#30340;&#35774;&#35745;&#21644;&#65288;ii&#65289;&#20316;&#20026;&#25968;&#23383;&#23402;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile robotic systems are becoming increasingly popular. These systems are used in various indoor applications, raging from warehousing and manufacturing to test benches for assessment of advanced control strategies, such as artificial intelligence (AI)-based control solutions, just to name a few. Scaled robotic cars are commonly equipped with a hierarchical control acthiecture that includes tasks dedicated to vehicle state estimation and control. This paper covers both aspects by proposing (i) a federeted extended Kalman filter (FEKF), and (ii) a novel deep reinforcement learning (DRL) path tracking controller trained via an expert demonstrator to expedite the learning phase and increase robustess to the simulation-to-reality gap. The paper also presents the formulation of a vehicle model along with an effective yet simple procedure for identifying tis paramters. The experimentally validated model is used for (i) supporting the design of the FEKF and (ii) serving as a digital twin fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.05193</link><description>&lt;p&gt;
&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Experiment Planning with Function Approximation. (arXiv:2401.05193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#22312;&#23384;&#22312;&#37096;&#32626;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#26174;&#33879;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#24403;&#25191;&#34892;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#38656;&#35201;&#20998;&#24067;&#24335;&#25110;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#26102;&#65292;&#25552;&#21069;&#29983;&#25104;&#19968;&#32452;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#21487;&#29992;&#20294;&#22870;&#21169;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#26223;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#34429;&#28982;&#24403;&#22870;&#21169;&#26159;&#32447;&#24615;&#30340;&#26102;&#20505;&#65292;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#20173;&#28982;&#32570;&#20047;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;&#31532;&#19968;&#31181;&#26159;&#36867;&#36991;&#32773;&#35268;&#21010;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#20197;&#26681;&#25454;&#36867;&#36991;&#32773;&#32500;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#31867;&#33719;&#24471;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23545;&#20110;&#31532;&#20108;&#31181;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of experiment planning with function approximation in contextual bandit problems. In settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount. We study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy. Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. In this work we propose two experiment planning strategies compatible with function approximation. The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class. For the second, we show that a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05176</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#32763;&#35793;&#20013;&#30340;&#31454;&#20105;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20027;&#27969;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#22522;&#20110;&#38169;&#35823;&#31867;&#22411;&#21644;&#20845;&#20010;&#20998;&#26512;&#32454;&#21017;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#32771;&#23519;&#20102;ChatGPT&#21644;NMT&#24341;&#25806;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#20110;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;NMT&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#24471;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#24403;ChatGPT&#25552;&#20379;&#31034;&#20363;&#25110;&#32763;&#35793;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#20154;&#24037;&#35780;&#20272;&#32773;&#24448;&#24448;&#20250;&#32473;&#20104;&#26126;&#26174;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#32500;&#24230;&#20043;&#38388;&#30340;&#20004;&#20004;&#30456;&#20851;&#24615;&#32467;&#26524;&#36739;&#24369;&#19988;&#19981;&#26174;&#33879;&#65292;&#36825;&#34920;&#26126;&#20102;&#20004;&#31181;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings pro
&lt;/p&gt;</description></item><item><title>MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05163</link><description>&lt;p&gt;
MISS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05163
&lt;/p&gt;
&lt;p&gt;
MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22810;&#25968;&#26041;&#27861;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#20010;&#38590;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#31572;&#26696;&#20998;&#31867;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#24615;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#36807;&#31243;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#22270;&#25991;&#23545;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;MISS&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#39033;&#29983;&#25104;&#24335;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30382;&#32932;&#38236;&#25968;&#25454;&#65292;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#25506;&#32034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05159</link><description>&lt;p&gt;
Derm-T2IM&#65306;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#21512;&#25104;&#30382;&#25439;&#25968;&#25454;&#65292;&#36890;&#36807;ViT&#21644;CNN&#22686;&#24378;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN. (arXiv:2401.05159v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30382;&#32932;&#38236;&#25968;&#25454;&#65292;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#30382;&#32932;&#30142;&#30149;&#20998;&#31867;&#20013;&#25506;&#32034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36890;&#36807;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#30382;&#32932;&#38236;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#32531;&#35299;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#38598;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#37327;&#25968;&#25454;&#34920;&#31034;&#30340;&#25104;&#21151;&#65292;&#30446;&#26631;&#26159;&#23558;&#22686;&#24378;&#30340;&#25968;&#25454;&#36716;&#25442;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#12290;&#32463;&#36807;&#20248;&#21270;&#35843;&#25972;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#36924;&#30495;&#29305;&#24449;&#30340;&#39640;&#36136;&#37327;&#30382;&#25439;&#21512;&#25104;&#25968;&#25454;&#65292;&#20026;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#26032;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#32435;&#20837;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the utilization of Dermatoscopic synthetic data generated through stable diffusion models as a strategy for enhancing the robustness of machine learning model training. Synthetic data generation plays a pivotal role in mitigating challenges associated with limited labeled datasets, thereby facilitating more effective model training. In this context, we aim to incorporate enhanced data transformation techniques by extending the recent success of few-shot learning and a small amount of data representation in text-to-image latent diffusion models. The optimally tuned model is further used for rendering high-quality skin lesion synthetic data with diverse and realistic characteristics, providing a valuable supplement and diversity to the existing training data. We investigate the impact of incorporating newly generated synthetic data into the training pipeline of state-of-art machine learning models, assessing its effectiveness in enhancing model performance and general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#24739;&#32773;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#29983;&#25104;&#31616;&#30701;&#31934;&#30830;&#30340;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.05134</link><description>&lt;p&gt;
&#26159;&#30340;&#65292;&#36825;&#23601;&#26159;&#25105;&#24819;&#35201;&#30340;&#65281;&#21521;&#22810;&#27169;&#24577;&#21307;&#30103;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Yes, this is what I was looking for! Towards Multi-modal Medical Consultation Concern Summary Generation. (arXiv:2401.05134v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#24739;&#32773;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#29983;&#25104;&#31616;&#30701;&#31934;&#30830;&#30340;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20114;&#32852;&#32593;&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#22686;&#38271;&#36805;&#29467;&#65292;&#26377;&#25928;&#31649;&#29702;&#21644;&#22788;&#29702;&#20449;&#24687;&#20197;&#30830;&#20445;&#20854;&#39640;&#25928;&#21033;&#29992;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#24773;&#32490;&#22256;&#25200;&#21644;&#24515;&#29702;&#25361;&#25112;&#26102;&#21051;&#65292;&#25105;&#20204;&#32463;&#24120;&#36716;&#21521;&#20114;&#32852;&#32593;&#20316;&#20026;&#25105;&#20204;&#26368;&#21021;&#30340;&#25903;&#25345;&#28304;&#65292;&#36873;&#25321;&#23427;&#32780;&#19981;&#26159;&#19982;&#20182;&#20154;&#35752;&#35770;&#25105;&#20204;&#30340;&#24863;&#21463;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#31038;&#20250;&#30340;&#27745;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65288;MMCS&#65289;&#20219;&#21153;&#65292;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#24739;&#32773;&#22312;&#21672;&#35810;&#36807;&#31243;&#20013;&#25552;&#20986;&#30340;&#20027;&#35201;&#20851;&#27880;&#30340;&#31616;&#30701;&#21644;&#31934;&#30830;&#25688;&#35201;&#12290;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20363;&#22914;&#24739;&#32773;&#30340;&#25163;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#26377;&#21161;&#20110;&#20934;&#30830;&#35782;&#21035;&#24739;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#21307;&#29983;&#36824;&#32771;&#34385;&#24739;&#32773;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#20363;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65292;&#20197;&#20415;&#36866;&#24403;&#22320;&#25551;&#36848;&#21307;&#30103;&#29366;&#20917;&#12290;&#21463;&#24739;&#32773;&#20010;&#20154;&#19978;&#19979;&#25991;&#21644;&#35270;&#35273;&#25163;&#21183;&#30340;&#28508;&#22312;&#30103;&#25928;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, the use of the Internet for healthcare-related tasks has grown by leaps and bounds, posing a challenge in effectively managing and processing information to ensure its efficient utilization. During moments of emotional turmoil and psychological challenges, we frequently turn to the internet as our initial source of support, choosing this over discussing our feelings with others due to the associated social stigma. In this paper, we propose a new task of multi-modal medical concern summary (MMCS) generation, which provides a short and precise summary of patients' major concerns brought up during the consultation. Nonverbal cues, such as patients' gestures and facial expressions, aid in accurately identifying patients' concerns. Doctors also consider patients' personal information, such as age and gender, in order to describe the medical condition appropriately. Motivated by the potential efficacy of patients' personal context and visual gestures, we propose a tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;n&#20010;&#29609;&#23478; general-sum &#28216;&#25103;&#20013;&#23547;&#25214;&#22343;&#34913;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32676;&#20307;&#23398;&#20064;&#31639;&#27861; NeuPL-JPSRO&#65292;&#36890;&#36807;&#25216;&#33021;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25910;&#25947;&#21040;&#28216;&#25103;&#30340;&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#12290;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#35777;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#22797;&#26434;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#35813;&#30740;&#31350;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#24322;&#36136;&#29609;&#23478;&#12289;&#20855;&#26377;&#28151;&#21512;&#21160;&#26426;&#30340;&#28216;&#25103;&#25552;&#20379;&#20102;&#26032;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.05133</link><description>&lt;p&gt;
&#36229;&#20986;&#23545;&#31216;&#38646;&#21644;&#21338;&#24328;&#30340;&#31070;&#32463;&#32676;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Population Learning beyond Symmetric Zero-sum Games. (arXiv:2401.05133v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;n&#20010;&#29609;&#23478; general-sum &#28216;&#25103;&#20013;&#23547;&#25214;&#22343;&#34913;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32676;&#20307;&#23398;&#20064;&#31639;&#27861; NeuPL-JPSRO&#65292;&#36890;&#36807;&#25216;&#33021;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25910;&#25947;&#21040;&#28216;&#25103;&#30340;&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#12290;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#23454;&#35777;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#22797;&#26434;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#35813;&#30740;&#31350;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#24322;&#36136;&#29609;&#23478;&#12289;&#20855;&#26377;&#28151;&#21512;&#21160;&#26426;&#30340;&#28216;&#25103;&#25552;&#20379;&#20102;&#26032;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;n&#20010;&#29609;&#23478; general-sum &#28216;&#25103;&#20013;&#23547;&#25214;&#22343;&#34913;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#28041;&#21450;&#22797;&#26434;&#30340;&#35270;&#35273;&#36816;&#21160;&#25216;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#35201;&#20040;&#22312;&#35745;&#31639;&#19978;&#22256;&#38590;&#65292;&#35201;&#20040;&#22312;&#29702;&#35770;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; NeuPL-JPSRO&#65292;&#19968;&#31181;&#31070;&#32463;&#32676;&#20307;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#25216;&#33021;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25910;&#25947;&#21040;&#28216;&#25103;&#30340;&#31895;&#30053;&#30456;&#20851;&#22343;&#34913; (Coarse Correlated Equilibrium)&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30340; OpenSpiel &#28216;&#25103;&#20013;&#23637;&#31034;&#20102;&#23454;&#35777;&#25910;&#25947;&#24615;&#65292;&#24182;&#30001;&#31934;&#30830;&#30340;&#28216;&#25103;&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#20005;&#26684;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558; NeuPL-JPSRO &#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22312; MuJoCo &#25511;&#21046;&#39046;&#22495;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#21327;&#35843;&#21644; capture-the-flag &#20013;&#30340;&#25216;&#33021;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#35268;&#27169;&#21644;&#26222;&#36941;&#24615;&#19978;&#23454;&#29616;&#25910;&#25947;&#20110;&#22343;&#34913;&#30340;&#32676;&#20307;&#23398;&#20064;&#65292;&#20026;&#35299;&#20915;&#24322;&#36136;&#29609;&#23478;&#12289;&#20855;&#26377;&#28151;&#21512;&#21160;&#26426;&#30340;&#29616;&#23454;&#19990;&#30028;&#28216;&#25103;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study computationally efficient methods for finding equilibria in n-player general-sum games, specifically ones that afford complex visuomotor skills. We show how existing methods would struggle in this setting, either computationally or in theory. We then introduce NeuPL-JPSRO, a neural population learning algorithm that benefits from transfer learning of skills and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show empirical convergence in a suite of OpenSpiel games, validated rigorously by exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our approach enables adaptive coordination in a MuJoCo control domain and skill transfer in capture-the-flag. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21322;&#27491;&#24335;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#21033;&#29992;&#19968;&#32452;&#20132;&#20114;&#21407;&#21017;&#65292;&#20026;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#25277;&#35937;&#35268;&#33539;&#65292;&#25903;&#25345;&#30446;&#30340;&#24615;&#20132;&#20114;&#65292;&#24182;&#31616;&#27905;&#27010;&#25324;&#20102;&#29616;&#26377;&#23454;&#36341;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#23427;&#36824;&#20026;&#26032;&#31995;&#32479;&#30340;&#21019;&#24314;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05115</link><description>&lt;p&gt;
&#25581;&#31034;&#20154;&#26426;&#20132;&#20114;&#65306;&#20174;&#20132;&#20114;&#21407;&#21017;&#21040;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Unpacking Human-AI interactions: From interaction primitives to a design space. (arXiv:2401.05115v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21322;&#27491;&#24335;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#21033;&#29992;&#19968;&#32452;&#20132;&#20114;&#21407;&#21017;&#65292;&#20026;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#25277;&#35937;&#35268;&#33539;&#65292;&#25903;&#25345;&#30446;&#30340;&#24615;&#20132;&#20114;&#65292;&#24182;&#31616;&#27905;&#27010;&#25324;&#20102;&#29616;&#26377;&#23454;&#36341;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#23427;&#36824;&#20026;&#26032;&#31995;&#32479;&#30340;&#21019;&#24314;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#20132;&#20114;&#21407;&#21017;&#65292;&#20026;&#20154;&#26426;&#20132;&#20114;&#24320;&#21457;&#19968;&#20010;&#21322;&#27491;&#24335;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#25351;&#23450;&#29992;&#25143;&#19982;AI&#31995;&#32479;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21407;&#21017;&#22914;&#20309;&#32452;&#21512;&#25104;&#19968;&#32452;&#20132;&#20114;&#27169;&#24335;&#65292;&#20026;&#20154;&#31867;&#21644;AI/ML&#27169;&#22411;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#25552;&#20379;&#25277;&#35937;&#35268;&#33539;&#65292;&#20197;&#36827;&#34892;&#26377;&#30446;&#30340;&#30340;&#20132;&#20114;&#12290;&#36825;&#26679;&#20570;&#30340;&#21160;&#26426;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25552;&#20379;&#23545;&#29616;&#26377;&#23454;&#36341;&#30340;&#31616;&#27905;&#27010;&#25324;&#65292;&#31361;&#20986;&#20102;&#31995;&#32479;&#22312;&#20132;&#20114;&#34892;&#20026;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65307;&#20854;&#27425;&#65292;&#25903;&#25345;&#26032;&#31995;&#32479;&#30340;&#21019;&#24314;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20026;&#19982;&#27169;&#22411;&#30340;&#20132;&#20114;&#25171;&#24320;&#21487;&#33021;&#24615;&#31354;&#38388;&#12290;&#25105;&#20204;&#23545;&#19982;HAI&#20132;&#20114;&#35774;&#35745;&#21644;&#23454;&#26045;&#30456;&#20851;&#30340;&#26694;&#26550;&#65292;&#25351;&#21335;&#21644;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#31616;&#35201;&#30340;&#25991;&#29486;&#22238;&#39038;&#65292;&#21253;&#25324;&#20154;&#26426;&#21327;&#21516;&#65292;&#21487;&#35299;&#37322;&#30340;AI&#20197;&#21450;&#28151;&#21512;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to develop a semi-formal design space for Human-AI interactions, by building a set of interaction primitives which specify the communication between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can provide an abstract specification for exchanging messages between humans and AI/ML models to carry out purposeful interactions. The motivation behind this is twofold: firstly, to provide a compact generalisation of existing practices, that highlights the similarities and differences between systems in terms of their interaction behaviours; and secondly, to support the creation of new systems, in particular by opening the space of possibilities for interactions with models. We present a short literature review on frameworks, guidelines and taxonomies related to the design and implementation of HAI interactions, including human-in-the-loop, explainable AI, as well as hybrid intelligence and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#35299;&#20915;&#20102;&#20803;&#23398;&#20064;&#20013;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20174;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.05097</link><description>&lt;p&gt;
&#20219;&#24847;&#26041;&#24335;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Any-Way Meta Learning. (arXiv:2401.05097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#35299;&#20915;&#20102;&#20803;&#23398;&#20064;&#20013;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20174;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20803;&#23398;&#20064;&#22312;&#24555;&#36895;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#21463;&#21040;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#12290;&#24403;&#38754;&#20020;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#22522;&#25968;&#19981;&#21516;&#30340;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#23601;&#26080;&#27861;&#32988;&#20219;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#20174;&#38543;&#26426;&#25968;&#20540;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36136;&#30097;&#8220;&#30495;&#27491;&#30340;&#8221;&#20803;&#23398;&#20064;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#25670;&#33073;&#20102;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#22312;&#24615;&#33021;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#19982;&#20256;&#32479;&#30340;&#22266;&#23450;&#26041;&#24335;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#36890;&#24120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#36825;&#39072;&#35206;&#20102;&#20851;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#24050;&#26377;&#35266;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#22266;&#26377;&#30340;&#26631;&#31614;&#31561;&#20215;&#24615;&#33258;&#28982;&#22320;&#32570;&#20047;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#36825;&#31181;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although meta-learning seems promising performance in the realm of rapid adaptability, it is constrained by fixed cardinality. When faced with tasks of varying cardinalities that were unseen during training, the model lacks its ability. In this paper, we address and resolve this challenge by harnessing `label equivalence' emerged from stochastic numeric label assignments during episodic task sampling. Questioning what defines ``true" meta-learning, we introduce the ``any-way" learning paradigm, an innovative model training approach that liberates model from fixed cardinality constraints. Surprisingly, this model not only matches but often outperforms traditional fixed-way models in terms of performance, convergence speed, and stability. This disrupts established notions about domain generalization. Furthermore, we argue that the inherent label equivalence naturally lacks semantic information. To bridge this semantic information gap arising from label equivalence, we further propose a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.05054</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding. (arXiv:2401.05054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20135;&#29983;&#19981;&#20165;&#27491;&#30830;&#32780;&#19988;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#22312;&#29983;&#25104;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#32780;&#25552;&#20986;&#30340;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#25110;&#38543;&#26426;&#25277;&#26679;&#65292;&#22240;&#27492;&#20854;&#36755;&#20986;&#36136;&#37327;&#21463;&#38480;&#20110;&#36825;&#20123;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;--&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#30446;&#26631;&#24378;&#21152;&#21040;MBR&#35299;&#30721;&#20013;&#26469;&#24320;&#21457;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;MBR&#30340;&#21464;&#20307;&#65292;&#21363;&#22810;&#26679;&#24615;MBR&#65288;DMBR&#65289;&#21644;k-medoids MBR&#65288;KMBR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#19968;&#32452;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#21521;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;DMBR&#21644;KMBR&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trad
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#23545;&#35805;&#24230;&#37327;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05033</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#35805;&#24341;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk. (arXiv:2401.05033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#23545;&#35805;&#24230;&#37327;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#29305;&#21270;&#23427;&#20204;&#20197;&#23454;&#29616;&#29305;&#23450;&#21151;&#33021;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25351;&#31034;&#35843;&#35856;&#65292;&#21363;&#22312;&#20154;&#31867;&#29983;&#25104;&#30340;&#25351;&#20196;&#21644;&#31034;&#20363;&#21709;&#24212;&#19978;&#35843;&#35856;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#21487;&#29992;&#25110;&#29983;&#25104;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#26159;&#20351;LLM&#36981;&#24490;&#23545;&#35805;&#20013;&#30340;&#29305;&#23450;&#24037;&#20316;&#27969;&#31243;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#25351;&#20196;&#26102;&#65292;&#36825;&#31181;&#25104;&#26412;&#20250;&#22686;&#21152;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#25105;&#21338;&#24328;&#25216;&#26415;&#21644;&#20351;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#36890;&#36807;LLM&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#23545;&#35805;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;LLM&#30340;&#8220;&#33258;&#25105;&#23545;&#35805;&#8221;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#31934;&#32454;&#35843;&#35856;&#21644;&#21033;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#65288;&#37096;&#20998;&#65289;&#25104;&#21151;&#12290;&#35813;&#24230;&#37327;&#29992;&#20110;&#36807;&#28388;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#36328;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#38145;&#26080;&#29992;&#20219;&#21153;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#21033;&#29992;&#25104;&#23545;&#30340;&#20219;&#21153;&#26080;&#20851;&#25968;&#25454;&#26469;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#24182;&#20419;&#36827;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;Task-irrelevant data-Guided Modality Bridging (TGMB)&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2401.05014</link><description>&lt;p&gt;
&#26080;&#28304;&#36328;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#26041;&#27861;&#65306;&#37322;&#25918;&#26080;&#29992;&#20219;&#21153;&#25968;&#25454;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data. (arXiv:2401.05014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#36328;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#38145;&#26080;&#29992;&#20219;&#21153;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#21033;&#29992;&#25104;&#23545;&#30340;&#20219;&#21153;&#26080;&#20851;&#25968;&#25454;&#26469;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#24182;&#20419;&#36827;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;Task-irrelevant data-Guided Modality Bridging (TGMB)&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#36328;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#26088;&#22312;&#22312;&#27809;&#26377;&#20219;&#21153;&#30456;&#20851;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#28304;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;RGB&#65289;&#36716;&#31227;&#21040;&#30446;&#26631;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;&#28145;&#24230;&#25110;&#32418;&#22806;&#65289;&#20013;&#65292;&#21407;&#22240;&#26159;&#30001;&#20110;&#20869;&#23384;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#23581;&#35797;&#21033;&#29992;&#25104;&#23545;&#30340;&#20219;&#21153;&#26080;&#20851;&#24615;&#65288;TI&#65289;&#25968;&#25454;&#65292;&#24182;&#30452;&#25509;&#21305;&#37197;&#23427;&#20204;&#30340;&#29305;&#24449;&#26469;&#28040;&#38500;&#27169;&#24577;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#32447;&#32034;&#65292;&#21363;&#25104;&#23545;&#30340;TI&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#24182;&#26356;&#22909;&#22320;&#20419;&#36827;&#21521;&#30446;&#26631;&#27169;&#24577;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#27905;&#30340;&#26694;&#26550;&#65292;&#26469;&#21457;&#25381;&#25104;&#23545;&#30340;TI&#25968;&#25454;&#22312;&#22686;&#24378;&#26080;&#28304;&#36328;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24471;&#21040;&#20102;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#32452;&#25104;&#37096;&#20998;&#30340;&#25903;&#25345;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20272;&#35745;&#28304;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20219;&#21153;&#26080;&#20851;&#25968;&#25454;&#24341;&#23548;&#30340;&#27169;&#24577;&#26725;&#25509;&#65288;TGMB&#65289;&#27169;&#22359;&#12290;&#23427;&#23558;&#30446;&#26631;&#27169;&#24577;&#25968;&#25454;&#32763;&#35793;&#20026;&#19982;&#28304;&#27169;&#24577;&#29305;&#24449;&#21305;&#37197;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free cross-modal knowledge transfer is a crucial yet challenging task, which aims to transfer knowledge from one source modality (e.g., RGB) to the target modality (e.g., depth or infrared) with no access to the task-relevant (TR) source data due to memory and privacy concerns. A recent attempt leverages the paired task-irrelevant (TI) data and directly matches the features from them to eliminate the modality gap. However, it ignores a pivotal clue that the paired TI data could be utilized to effectively estimate the source data distribution and better facilitate knowledge transfer to the target modality. To this end, we propose a novel yet concise framework to unlock the potential of paired TI data for enhancing source-free cross-modal knowledge transfer. Our work is buttressed by two key technical components. Firstly, to better estimate the source data distribution, we introduce a Task-irrelevant data-Guided Modality Bridging (TGMB) module. It translates the target modality da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.05010</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#22823;&#36947;&#65306;&#23545;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Less is More : A Closer Look at Multi-Modal Few-Shot Learning. (arXiv:2401.05010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26497;&#23569;&#37327;&#30340;&#21487;&#29992;&#22270;&#20687;&#26469;&#23398;&#20064;&#21644;&#21306;&#20998;&#26032;&#30340;&#31867;&#21035;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32773;&#20204;&#35797;&#22270;&#21033;&#29992;&#36825;&#20123;&#31232;&#26377;&#31867;&#21035;&#30340;&#38468;&#21152;&#25991;&#26412;&#25110;&#35821;&#35328;&#20449;&#24687;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20419;&#36827;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#19981;&#36275;&#30340;&#30417;&#30563;&#20449;&#21495;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#23545;&#20110;&#25991;&#26412;&#20449;&#24687;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#28508;&#21147;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34987;&#20302;&#20272;&#20102;&#65292;&#23548;&#33268;&#24615;&#33021;&#30340;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#20805;&#20998;&#21457;&#25381;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#28155;&#21152;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot Learning aims to learn and distinguish new categories with a very limited number of available images, presenting a significant challenge in the realm of deep learning. Recent researchers have sought to leverage the additional textual or linguistic information of these rare categories with a pre-trained language model to facilitate learning, thus partially alleviating the problem of insufficient supervision signals. However, the full potential of the textual information and pre-trained language model have been underestimated in the few-shot learning till now, resulting in limited performance enhancements. To address this, we propose a simple but effective framework for few-shot learning tasks, specifically designed to exploit the textual information and language model. In more detail, we explicitly exploit the zero-shot capability of the pre-trained language model with the learnable prompt. And we just add the visual feature with the textual feature for inference directly witho
&lt;/p&gt;</description></item><item><title>AdaFed&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#26381;&#21153;&#22120;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#22823;&#20540;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.04993</link><description>&lt;p&gt;
AdaFed&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFed: Fair Federated Learning via Adaptive Common Descent Direction. (arXiv:2401.04993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04993
&lt;/p&gt;
&lt;p&gt;
AdaFed&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#26381;&#21153;&#22120;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#22823;&#20540;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35813;&#25216;&#26415;&#65292;&#19968;&#20123;&#36793;&#32536;&#35774;&#22791;/&#23458;&#25143;&#31471;&#22312;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#23545;&#26576;&#20123;&#35774;&#22791;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#20248;&#21183;&#25110;&#21155;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AdaFed&#12290;AdaFed&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26381;&#21153;&#22120;&#26356;&#26032;&#26041;&#21521;&#65292;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#37117;&#22312;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25439;&#22833;&#20989;&#25968;&#20540;&#36739;&#22823;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;AdaFed&#26681;&#25454;&#26412;&#22320;&#26799;&#24230;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#20540;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36825;&#20010;&#20844;&#20849;&#26041;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#32852;&#37030;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;AdaFed&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;AdaFed&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#25302;&#36710;&#21644;&#21345;&#36710;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#36719;&#20214;CARLA&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#25302;&#36710;&#36710;&#36742;&#22312;&#29615;&#23707;&#20132;&#21449;&#21475;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#30740;&#31350;&#20351;&#29992;&#21452;Q&#36719;&#20214;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#20934;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#29615;&#23707;&#19978;&#21462;&#24471;&#20102;73%&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04980</link><description>&lt;p&gt;
&#25302;&#36710;&#36710;&#36742;&#22312;&#29615;&#23707;&#20132;&#21449;&#21475;&#30340;&#33258;&#20027;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Autonomous Navigation of Tractor-Trailer Vehicles through Roundabout Intersections. (arXiv:2401.04980v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04980
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#25302;&#36710;&#21644;&#21345;&#36710;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#36719;&#20214;CARLA&#65292;&#24182;&#24314;&#31435;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#25302;&#36710;&#36710;&#36742;&#22312;&#29615;&#23707;&#20132;&#21449;&#21475;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#30740;&#31350;&#20351;&#29992;&#21452;Q&#36719;&#20214;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#20934;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#29615;&#23707;&#19978;&#21462;&#24471;&#20102;73%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26088;&#22312;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25302;&#36710;&#36710;&#36742;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#30001;&#20110;&#20854;&#29289;&#29702;&#29305;&#24615;&#21644;&#38128;&#25509;&#20851;&#33410;&#65292;&#36825;&#31181;&#36710;&#36742;&#38656;&#35201;&#23450;&#21046;&#27169;&#22411;&#12290;&#22312;&#36716;&#24367;&#26102;&#65292;&#25302;&#36710;&#30340;&#21518;&#36718;&#20197;&#26356;&#23567;&#30340;&#21322;&#24452;&#36716;&#21160;&#65292;&#21345;&#36710;&#32463;&#24120;&#38656;&#35201;&#20559;&#31163;&#36710;&#36947;&#20013;&#24515;&#26469;&#36866;&#24212;&#36825;&#31181;&#24773;&#20917;&#12290;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#36719;&#20214;CARLA&#24320;&#21457;&#20102;&#21345;&#36710;&#21644;&#25302;&#36710;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20960;&#20010;&#29615;&#23707;&#22330;&#26223;&#24314;&#31435;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#21452;Q&#36719;&#20214;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20934;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#29615;&#23707;&#19978;&#23454;&#29616;&#20102;73%&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant advancements have been made in the field of autonomous driving with the aim of increasing safety and efficiency. However, research that focuses on tractor-trailer vehicles is relatively sparse. Due to the physical characteristics and articulated joints, such vehicles require tailored models. While turning, the back wheels of the trailer turn at a tighter radius and the truck often has to deviate from the centre of the lane to accommodate this. Due to the lack of publicly available models, this work develops truck and trailer models using the high-fidelity simulation software CARLA, together with several roundabout scenarios, to establish a baseline dataset for benchmarks. Using a twin-q soft actor-critic algorithm, we train a quasi-end-to-end autonomous driving model which is able to achieve a 73% success rate on different roundabouts.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04978</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#26799;&#24230;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#19981;&#21516;&#65292;&#23545;&#20110;&#20998;&#31867;&#32780;&#35328;&#65292;&#21363;&#20351;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#20998;&#31867;&#22522;&#20110;&#21487;&#20197;&#34920;&#31034;&#20026;&#38381;&#24335;&#26041;&#31243;&#30340;&#37327;&#65292;&#20063;&#19968;&#33324;&#26080;&#27861;&#25214;&#21040;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#31526;&#21495;&#26041;&#31243;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#20010;&#31561;&#20215;&#31867;&#20013;&#65292;&#36825;&#20010;&#31561;&#20215;&#31867;&#30340;&#20998;&#31867;&#20989;&#25968;&#30340;&#20915;&#31574;&#37117;&#22522;&#20110;&#30456;&#21516;&#30340;&#37327;&#12290;&#25105;&#36890;&#36807;&#25214;&#21040;&#36825;&#20010;&#31561;&#20215;&#31867;&#19982;&#30001;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#30340;&#21487;&#35835;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38480;&#20110;&#20998;&#31867;&#22120;&#25110;&#23436;&#25972;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38544;&#34255;&#23618;&#25110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#31070;&#32463;&#20803;&#65292;&#25110;&#31616;&#21270;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#22120;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#32806;&#21512;&#38543;&#26426;&#36807;&#31243;&#20013;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#24687;&#27969;&#36895;&#29575;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#36890;&#36807;&#25512;&#23548;&#20449;&#24687;&#27969;&#36895;&#29575;&#32479;&#35745;&#37327;&#19982;&#33258;&#30456;&#20851;&#21644;&#20114;&#30456;&#20851;&#20989;&#25968;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#36895;&#29575;&#19982;&#30456;&#20851;&#20989;&#25968;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04950</link><description>&lt;p&gt;
&#36328;&#30456;&#20851;&#38543;&#26426;&#36807;&#31243;&#30340;&#20449;&#24687;&#27969;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Information Flow Rate for Cross-Correlated Stochastic Processes. (arXiv:2401.04950v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#32806;&#21512;&#38543;&#26426;&#36807;&#31243;&#20013;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#24687;&#27969;&#36895;&#29575;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#36890;&#36807;&#25512;&#23548;&#20449;&#24687;&#27969;&#36895;&#29575;&#32479;&#35745;&#37327;&#19982;&#33258;&#30456;&#20851;&#21644;&#20114;&#30456;&#20851;&#20989;&#25968;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20449;&#24687;&#27969;&#36895;&#29575;&#19982;&#30456;&#20851;&#20989;&#25968;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26088;&#22312;&#35782;&#21035;&#32806;&#21512;&#31995;&#32479;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;Liang&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#30340;&#26041;&#21521;&#21644;&#22823;&#23567;&#26469;&#26816;&#27979;&#22240;&#26524;&#20851;&#31995;&#12290;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#20449;&#24687;&#27969;&#30340;&#29702;&#35770;&#34920;&#36848;&#25552;&#20379;&#20102;&#19968;&#20010;&#24635;&#20307;&#34920;&#36798;&#24335;&#21644;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#19981;&#21516;&#31995;&#32479;&#21333;&#20803;&#20043;&#38388;&#30340;&#29109;&#20256;&#36755;&#36895;&#29575;&#12290;&#20026;&#20102;&#25512;&#36827;&#23545;&#20449;&#24687;&#27969;&#36895;&#29575;&#30340;&#29702;&#35299;&#65292;&#20197;&#30452;&#35266;&#27010;&#24565;&#21644;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#21442;&#25968;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32806;&#21512;&#38543;&#26426;&#36807;&#31243;&#20043;&#38388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20449;&#24687;&#27969;&#36895;&#29575;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20449;&#24687;&#27969;&#36895;&#29575;&#32479;&#35745;&#37327;&#30340;&#26399;&#26395;&#19982;&#33258;&#30456;&#20851;&#21644;&#20114;&#30456;&#20851;&#20989;&#25968;&#30340;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20449;&#24687;&#27969;&#36895;&#29575;&#23545;&#30456;&#20851;&#20989;&#25968;&#30340;&#20998;&#26512;&#24615;&#36136;&#21644;&#29305;&#24449;&#26102;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#20449;&#24687;&#27969;&#19982;&#32806;&#21512;&#38543;&#26426;&#36807;&#31243;&#20043;&#38388;&#20851;&#31995;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference seeks to identify cause-and-effect interactions in coupled systems. A recently proposed method by Liang detects causal relations by quantifying the direction and magnitude of information flow between time series. The theoretical formulation of information flow for stochastic dynamical systems provides a general expression and a data-driven statistic for the rate of entropy transfer between different system units. To advance understanding of information flow rate in terms of intuitive concepts and physically meaningful parameters, we investigate statistical properties of the data-driven information flow rate between coupled stochastic processes. We derive relations between the expectation of the information flow rate statistic and properties of the auto- and cross-correlation functions. Thus, we elucidate the dependence of the information flow rate on the analytical properties and characteristic times of the correlation functions. Our analysis provides insight into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#31639;&#27861;&#20197;&#21450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.04934</link><description>&lt;p&gt;
&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey. (arXiv:2401.04934v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#31639;&#27861;&#20197;&#21450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#21512;&#20316;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#29616;&#23454;&#24212;&#29992;&#30340;&#38480;&#21046;&#21487;&#33021;&#35201;&#27714;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24687;&#65292;&#35201;&#22312;&#23436;&#20840;&#20998;&#25955;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#32852;&#21512;&#31574;&#30053;&#30340;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#36824;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#23436;&#20840;&#20998;&#25955;&#26041;&#27861;&#65306;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20849;&#20139;&#22870;&#21169;&#21644;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20010;&#20307;&#22870;&#21169;&#30340;&#24635;&#21644;&#65292;&#24182;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world cooperative tasks, but restrictions of real-world applications may require training the agents in a fully decentralized manner. Due to the lack of information about other agents, it is challenging to derive algorithms that can converge to the optimal joint policy in a fully decentralized setting. Thus, this research area has not been thoroughly studied. In this paper, we seek to systematically review the fully decentralized methods in two settings: maximizing a shared reward of all agents and maximizing the sum of individual rewards of all agents, and discuss open questions and future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.04898</link><description>&lt;p&gt;
ANGO: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#39046;&#22495;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain. (arXiv:2401.04898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04898
&lt;/p&gt;
&lt;p&gt;
ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#23384;&#22312;&#25490;&#21517;&#22833;&#30495;&#21644;&#27169;&#22411;&#33021;&#21147;&#20998;&#26512;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ANGO&#65292;&#19968;&#20010;&#20013;&#25991;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#22522;&#20934;&#12290;ANGO&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#20851;&#38190;&#28857;&#8221;&#20998;&#31867;&#26631;&#20934;&#65292;ANGO&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#23545;&#24212;&#22810;&#20010;&#20851;&#38190;&#28857;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22522;&#20110;&#30495;&#20154;&#34920;&#29616;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#24182;&#23558;ANGO&#38382;&#39064;&#20998;&#20026;9&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25968;&#25454;&#27844;&#28431;&#30340;&#24433;&#21709;&#24182;&#20805;&#20998;&#21033;&#29992;ANGO&#30340;&#21019;&#26032;&#29305;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29420;&#23478;&#25277;&#26679;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#27979;&#35797;&#38598;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ANGO&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#35780;&#20272;&#32467;&#26524;&#20013;&#25581;&#31034;&#20986;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.04867</link><description>&lt;p&gt;
&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#20197;&#23458;&#35266;&#35780;&#20272;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#26696;&#24456;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20027;&#35266;&#35780;&#20272;&#22312;&#29992;&#25143;&#23454;&#39564;&#20013;&#24120;&#29992;&#65292;&#20294;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#30740;&#31350;&#27604;&#36739;&#21644;&#21487;&#22797;&#21046;&#24615;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#38388;&#25509;&#20294;&#23458;&#35266;&#22320;&#35780;&#20272;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31038;&#20132;&#23545;&#35805;&#20219;&#21153;&#20013;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19987;&#27880;&#20542;&#21548;&#12289;&#38754;&#35797;&#21644;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29992;&#25143;&#35805;&#35821;&#26159;&#20027;&#35201;&#22240;&#32032;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#19987;&#27880;&#20542;&#21548;&#21644;&#38754;&#35797;&#65292;&#35805;&#35821;&#25968;&#37327;&#21644;&#21333;&#35789;&#25968;&#37327;&#31561;&#25351;&#26631;&#22312;&#35780;&#20272;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#35266;&#23519;&#35821;&#35843;&#19981;&#27969;&#30021;&#31561;&#20063;&#21487;&#20197;&#25351;&#31034;&#27491;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#38754;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#39640;&#20114;&#21160;&#24615;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#65292;&#29992;&#25143;&#24773;&#32490;&#21644;&#21442;&#19982;&#31243;&#24230;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing evaluation schemes for spoken dialogue systems is important, but it can also be challenging. While subjective evaluations are commonly used in user experiments, objective evaluations are necessary for research comparison and reproducibility. To address this issue, we propose a framework for indirectly but objectively evaluating systems based on users' behaviours. In this paper, to this end, we investigate the relationship between user behaviours and subjective evaluation scores in social dialogue tasks: attentive listening, job interview, and first-meeting conversation. The results reveal that in dialogue tasks where user utterances are primary, such as attentive listening and job interview, indicators like the number of utterances and words play a significant role in evaluation. Observing disfluency also can indicate the effectiveness of formal tasks, such as job interview. On the other hand, in dialogue tasks with high interactivity, such as first-meeting conversation, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04858</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#24314;&#27169;&#38271;&#26102;&#38388;&#30340;&#21382;&#21490;&#35760;&#24405;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#19981;&#26029;&#28436;&#21464;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20559;&#22909;&#29702;&#35299;&#20013;&#24314;&#27169;&#38271;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;(UEM)&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#23884;&#20837;&#24418;&#24335;&#21387;&#32553;&#21644;&#34920;&#31034;&#65292;&#23558;&#20854;&#20316;&#20026;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#26174;&#33879;&#26356;&#38271;&#30340;&#21382;&#21490;&#35760;&#24405;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#20351;&#29992;&#34920;&#31034;&#20026;&#23884;&#20837;&#30340;&#29992;&#25143;&#20449;&#21495;&#26469;&#20559;&#32622;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;&#65292;&#32771;&#34385;&#21040;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;&#20351;&#29992;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#12289;&#36716;&#25442;&#22120;&#23618;&#21644;Multi-head Attention-based&#35299;&#30721;&#22120;&#31561;&#32452;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#29616;&#23454;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04851</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#35843;&#24230;&#22312;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand &amp; Uncertainties. (arXiv:2401.04851v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;&#65292;&#32771;&#34385;&#21040;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;&#20351;&#29992;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#12289;&#36716;&#25442;&#22120;&#23618;&#21644;Multi-head Attention-based&#35299;&#30721;&#22120;&#31561;&#32452;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#29616;&#23454;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;,&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#30005;&#21160;&#39134;&#34892;&#22120;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;,&#35813;&#39134;&#34892;&#22120;&#32452;&#25104;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#22402;&#30452;&#28207;&#21475;&#36816;&#33829;&#30340;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#26426;&#38431;&#12290;&#36825;&#20010;&#26426;&#38431;&#35843;&#24230;&#38382;&#39064;&#30340;&#21046;&#23450;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#38656;&#27714;&#12289;&#22402;&#30452;&#28207;&#21475;&#23481;&#37327;&#12289;&#39134;&#34892;&#22120;&#23481;&#37327;&#21644;&#31354;&#22495;&#23433;&#20840;&#20934;&#21017;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#36215;&#39134;&#24310;&#35823;&#12289;&#22825;&#27668;&#24341;&#36215;&#30340;&#36335;&#32447;&#20851;&#38381;&#21644;&#24773;&#20917;&#26410;&#30693;&#30340;&#39134;&#34892;&#22120;&#20572;&#26426;&#26102;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26679;&#30340;&#21046;&#23450;&#26041;&#24335;&#27604;&#29616;&#26377;&#30340;UAM&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#26356;&#21152;&#22797;&#26434;&#65292;&#21487;&#33021;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#29616;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65306;&#20316;&#20026;&#22270;&#30340;&#25277;&#35937;&#65292;&#29992;&#20110;&#32534;&#30721;&#22402;&#30452;&#28207;&#21475;&#21644;&#39134;&#34892;&#22120;&#26426;&#38431;&#29366;&#24577;&#30340;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#65307;&#32534;&#30721;&#38656;&#27714;&#21644;&#20056;&#23458;&#31080;&#20215;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#36716;&#25442;&#22120;&#23618;&#65307;&#20197;&#21450;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;Multi-head Attention-based&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a graph reinforcement learning approach to online planning of the schedule and destinations of electric aircraft that comprise an urban air mobility (UAM) fleet operating across multiple vertiports. This fleet scheduling problem is formulated to consider time-varying demand, constraints related to vertiport capacity, aircraft capacity and airspace safety guidelines, uncertainties related to take-off delay, weather-induced route closures, and unanticipated aircraft downtime. Collectively, such a formulation presents greater complexity, and potentially increased realism, than in existing UAM fleet planning implementations. To address these complexities, a new policy architecture is constructed, primary components of which include: graph capsule conv-nets for encoding vertiport and aircraft-fleet states both abstracted as graphs; transformer layers encoding time series information on demand and passenger fare; and a Multi-head Attention-based decoder that uses the enco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;SIM-GAT&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21830;&#19994;&#38598;&#32676;&#19982;&#36152;&#26131;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#20114;&#21160;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#36890;&#22270;&#34920;&#31034;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;Graph AttenTion network&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20256;&#32479;SIM&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04849</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#31354;&#38388;&#20132;&#20114;&#27169;&#22411;&#20197;&#23454;&#29616;&#31038;&#21306;&#21830;&#19994;&#38598;&#32676;&#30340;&#24377;&#24615;&#31354;&#38388;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters. (arXiv:2401.04849v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;SIM-GAT&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21830;&#19994;&#38598;&#32676;&#19982;&#36152;&#26131;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#20114;&#21160;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#36890;&#22270;&#34920;&#31034;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;Graph AttenTion network&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20256;&#32479;SIM&#30340;&#39044;&#27979;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31354;&#38388;&#20132;&#20114;&#27169;&#22411;&#65288;SIM&#65289;&#22312;&#25429;&#25417;&#21830;&#19994;&#38598;&#32676;&#19982;&#36152;&#26131;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#12289;&#19982;&#29615;&#22659;&#30456;&#20851;&#30340;&#20114;&#21160;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SIM-GAT&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#31038;&#21306;&#21830;&#19994;&#38598;&#32676;&#19982;&#20854;&#36152;&#26131;&#21306;&#20043;&#38388;&#30340;&#26102;&#31354;&#35775;&#38382;&#27969;&#37327;&#12290;&#35813;&#27169;&#22411;&#21019;&#26032;&#22320;&#20351;&#29992;&#36830;&#36890;&#22270;&#34920;&#31034;&#22478;&#24066;&#22320;&#21306;&#20869;&#30340;&#21830;&#19994;&#38598;&#32676;&#12289;&#36152;&#26131;&#21306;&#21644;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#32508;&#21512;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;Graph AttenTion network (GAT)&#65292;&#26469;&#25429;&#25417;&#21830;&#19994;&#38598;&#32676;&#30340;&#22797;&#26434;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#37117;&#24066;&#21306;&#25910;&#38598;&#30340;&#25968;&#25454;&#24320;&#21457;&#20102;&#36825;&#20010;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#22312;&#25429;&#25417;&#19981;&#21516;&#23621;&#27665;&#31038;&#21306;&#23545;&#21830;&#19994;&#38598;&#32676;&#30340;&#21560;&#24341;&#21147;&#20197;&#21450;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#39044;&#27979;&#21644;&#20998;&#26512;&#20256;&#32479;SIM&#30340;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Spatial Interaction Models (SIMs) are limited in capturing the complex and context-aware interactions between business clusters and trade areas. To address the limitation, we propose a SIM-GAT model to predict spatiotemporal visitation flows between community business clusters and their trade areas. The model innovatively represents the integrated system of business clusters, trade areas, and transportation infrastructure within an urban region using a connected graph. Then, a graph-based deep learning model, i.e., Graph AttenTion network (GAT), is used to capture the complexity and interdependencies of business clusters. We developed this model with data collected from the Miami metropolitan area in Florida. We then demonstrated its effectiveness in capturing varying attractiveness of business clusters to different residential neighborhoods and across scenarios with an eXplainable AI approach. We contribute a novel method supplementing conventional SIMs to predict and analyze
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04846</link><description>&lt;p&gt;
&#21463;&#36807;&#33391;&#22909;&#25945;&#32946;&#30340;&#26234;&#33021;&#30340;&#20869;&#22312;&#21892;&#33391;
&lt;/p&gt;
&lt;p&gt;
The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#20351;&#19968;&#20010;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#20307;&#36824;&#26159;&#35745;&#31639;&#26426;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#33021;&#22815;&#34920;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#21516;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26412;&#36136;&#23558;&#34987;&#21457;&#29616;&#26159;&#40644;&#37329;&#27861;&#21017;&#8212;&#8212;&#8220;&#38598;&#20307;&#34892;&#21160;&#22914;&#19968;&#20307;&#8221;&#25110;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#38598;&#20307;&#30340;&#27969;&#21160;&#26159;&#30001;&#25484;&#25511;&#30528;&#23569;&#37327;&#23383;&#31526;&#20018;&#30340;&#25805;&#32437;&#32773;&#20915;&#23450;&#30340;&#65292;&#26681;&#25454;&#23545;&#31216;&#24615;&#30830;&#23450;&#30340;&#26368;&#23567;&#20316;&#29992;&#36335;&#24452;&#30340;&#27979;&#22320;&#32447;&#36816;&#21160;&#12290;&#25511;&#21046;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#65292;&#21382;&#21490;&#19978;&#19968;&#30452;&#36890;&#36807;&#20026;&#31995;&#32479;&#28155;&#21152;&#26174;&#33879;&#40655;&#24615;&#26469;&#31283;&#23450;&#26399;&#26395;&#30340;&#26368;&#22823;&#24615;&#33021;&#30340;&#20122;&#31283;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#36825;&#20250;&#22312;&#36807;&#31243;&#20013;&#38477;&#20302;&#25110;&#30772;&#22351;&#23427;&#20204;&#12290;&#26377;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
&lt;/p&gt;</description></item><item><title>MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.04821</link><description>&lt;p&gt;
MoSECroT: &#20351;&#29992;&#38745;&#24577;&#35789;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#25340;&#25509;&#23454;&#29616;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer. (arXiv:2401.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04821
&lt;/p&gt;
&lt;p&gt;
MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#32780;&#36825;&#20123;&#36164;&#28304;&#20960;&#20046;&#21482;&#26377;&#39640;&#36164;&#28304;&#35821;&#35328;&#25165;&#33021;&#33719;&#24471;&#12290;&#30456;&#21453;&#65292;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#35757;&#32451;&#26356;&#23481;&#26131;&#65292;&#21487;&#20197;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MoSECroT&#65288;Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer&#65289;&#27169;&#22411;&#25340;&#25509;&#19982;&#38745;&#24577;&#35789;&#21521;&#37327;&#32467;&#21512;&#30340;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#28304;&#35821;&#35328;PLM&#23884;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#20849;&#20139;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;PLM&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#22320;&#20132;&#25442;&#23884;&#20837;&#23436;&#25104;&#20174;&#28304;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#30340;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks. However, pre-training such models can take considerable resources that are almost only available to high-resource languages. On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required. In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available. To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language. In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04820</link><description>&lt;p&gt;
&#36890;&#36807;HTML&#20869;&#23481;&#30340;&#22810;&#27169;&#22411;&#20998;&#26512;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;
&lt;/p&gt;
&lt;p&gt;
Phishing Website Detection through Multi-Model Analysis of HTML Content. (arXiv:2401.04820v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#39640;&#32423;&#26816;&#27979;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#34701;&#21512;&#26041;&#27861;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#20852;&#36215;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#21644;&#24037;&#20316;&#26041;&#24335;&#21457;&#29983;&#20102;&#24040;&#22823;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23427;&#20026;&#25105;&#20204;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#32593;&#32476;&#23041;&#32961;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#19988;&#20005;&#37325;&#30340;&#23041;&#32961;&#26159;&#32593;&#32476;&#38035;&#40060;&#65292;&#40657;&#23458;&#20351;&#29992;&#27450;&#39575;&#24615;&#26041;&#27861;&#31363;&#21462;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;HTML&#20869;&#23481;&#30340;&#20808;&#36827;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#32593;&#32476;&#38035;&#40060;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#29992;&#20110;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#19987;&#38376;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22411;&#21644;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#39029;&#38754;&#26631;&#39064;&#21644;&#20869;&#23481;&#31561;&#25991;&#26412;&#29305;&#24449;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#34987;&#21644;&#35856;&#22320;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36755;&#20837;&#21040;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#12290;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#30340;&#32593;&#32476;&#38035;&#40060;&#30740;&#31350;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#36824;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The way we communicate and work has changed significantly with the rise of the Internet. While it has opened up new opportunities, it has also brought about an increase in cyber threats. One common and serious threat is phishing, where cybercriminals employ deceptive methods to steal sensitive information.This study addresses the pressing issue of phishing by introducing an advanced detection model that meticulously focuses on HTML content. Our proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model for structured tabular data and two pretrained Natural Language Processing (NLP) models for analyzing textual features such as page titles and content. The embeddings from these models are harmoniously combined through a novel fusion process. The resulting fused embeddings are then input into a linear classifier. Recognizing the scarcity of recent datasets for comprehensive phishing research, our contribution extends to the creation of an up-to-date dataset, which we o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.04812</link><description>&lt;p&gt;
&#37319;&#26679;&#19982;&#26463;&#32538;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-and-Bound for Non-Convex Optimization. (arXiv:2401.04812v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#26631;&#20934;&#26041;&#27861;&#65292;&#22914;&#20998;&#25903;&#21644;&#26463;&#32538;&#65292;&#32500;&#25252;&#21487;&#29992;&#20110;&#31995;&#32479;&#21098;&#26525;&#30340;&#20998;&#21306;&#26641;&#12290;&#26641;&#30340;&#22823;&#23567;&#38543;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#25913;&#36827;&#20102;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;(MCTS)&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#26631;&#20934;&#30340;&#35775;&#38382;&#35745;&#25968;&#26469;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#32780;&#26159;&#21033;&#29992;&#30446;&#26631;&#30340;&#25968;&#20540;&#19978;&#20272;&#35745;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#24182;&#32771;&#34385;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#36991;&#20813;&#20102;&#36890;&#24120;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#24182;&#31215;&#26497;&#22320;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#22312;&#39640;&#32500;&#38750;&#20984;&#20248;&#21270;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for global optimization of non-convex functions, such as branch-and-bound, maintain partition trees to systematically prune the domain. The tree size grows exponentially in the number of dimensions. We propose new sampling-based methods for non-convex optimization that adapts Monte Carlo Tree Search (MCTS) to improve efficiency. Instead of the standard use of visitation count in Upper Confidence Bounds, we utilize numerical overapproximations of the objective as an uncertainty metric, and also take into account of sampled estimates of first-order and second-order information. The Monte Carlo tree in our approach avoids the usual fixed combinatorial patterns in growing the tree, and aggressively zooms into the promising regions, while still balancing exploration and exploitation. We evaluate the proposed algorithms on high-dimensional non-convex optimization benchmarks against competitive baselines and analyze the effects of the hyper parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#65292;&#21457;&#29616;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#30340;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#65292;&#20294;&#22312;&#20010;&#21035;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04757</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#24615;&#33021;&#26377;&#22810;&#21487;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
How predictable is language model benchmark performance?. (arXiv:2401.04757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#65292;&#21457;&#29616;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#30340;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#65292;&#20294;&#22312;&#20010;&#21035;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#65292;&#23601;&#20687;&#24120;&#29992;&#30340;BIG-Bench&#25968;&#25454;&#38598;&#19968;&#26679;&#65292;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#22312;&#35757;&#32451;&#35745;&#31639;&#35268;&#27169;&#30340;&#20989;&#25968;&#19979;&#26159;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#22312;&#35745;&#31639;&#20013;&#25193;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;BIG-Bench Hard&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;6&#20010;&#30334;&#20998;&#28857;&#65288;pp&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#35745;&#31639;&#20013;&#25193;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20010;&#21035;BIG-Bench&#20219;&#21153;&#30340;&#22806;&#25512;&#24179;&#22343;&#35823;&#24046;&#20026;18pp&#12290;&#19981;&#36807;&#65292;&#20010;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#20173;&#28982;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#21487;&#39044;&#27979;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35745;&#31639;&#35268;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#26679;&#21270;&#22522;&#20934;&#20013;&#30340;AI&#33021;&#21147;&#65292;&#20294;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#39044;&#27979;&#24615;&#33021;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate large language model performance across five orders of magnitude of compute scaling in eleven recent model architectures. We show that average benchmark performance, aggregating over many individual tasks and evaluations as in the commonly-used BIG-Bench dataset, is decently predictable as a function of training compute scale. Specifically, when extrapolating BIG-Bench Hard performance across one order of magnitude in compute, we observe average absolute errors of 6 percentage points (pp). By contrast, extrapolation for individual BIG-Bench tasks across an order of magnitude in compute yields higher average errors of 18pp. Nonetheless, individual task performance remains significantly more predictable than chance. Overall, our work suggests compute scaling provides a promising basis to forecast AI capabilities in diverse benchmarks, though predicting performance in specific tasks poses challenges.
&lt;/p&gt;</description></item><item><title>LogFormer&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#21033;&#29992;&#20849;&#20139;&#21442;&#25968;&#23558;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#21516;&#26102;&#24341;&#20837;Log-Attention&#27169;&#22359;&#26469;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.04749</link><description>&lt;p&gt;
LogFormer&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection. (arXiv:2401.04749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04749
&lt;/p&gt;
&lt;p&gt;
LogFormer&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#21033;&#29992;&#20849;&#20139;&#21442;&#25968;&#23558;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#21516;&#26102;&#24341;&#20837;Log-Attention&#27169;&#22359;&#26469;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26159;&#20154;&#24037;&#26234;&#33021;&#36816;&#32500;&#65288;AIOps&#65289;&#39046;&#22495;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26085;&#24535;&#25968;&#25454;&#65292;&#22312;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#32593;&#32476;&#20197;&#36866;&#24212;&#26410;&#30693;&#39046;&#22495;&#26159;&#20302;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#20165;&#20851;&#27880;&#20110;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#25552;&#21462;&#26085;&#24535;&#24207;&#21015;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#22810;&#39046;&#22495;&#26085;&#24535;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;Log&#24322;&#24120;&#26816;&#27979;&#32479;&#19968;&#26694;&#26550;(LogFormer)&#65292;&#20197;&#25913;&#21892;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;adapter&#30340;&#35843;&#20248;&#38454;&#27573;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#33719;&#21462;&#26085;&#24535;&#25968;&#25454;&#30340;&#20849;&#20139;&#35821;&#20041;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#23558;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Log-Attention&#27169;&#22359;&#65292;&#29992;&#20110;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#26159;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios. However, previous deep models merely focused on extracting the semantics of log sequences in the same domain, leading to poor generalization on multi-domain logs. To alleviate this issue, we propose a unified Transformer-based framework for Log anomaly detection (LogFormer) to improve the generalization ability across different domains, where we establish a two-stage process including the pre-training and adapter-based tuning stage. Specifically, our model is first pre-trained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer such knowledge to the target domain via shared parameters. Besides, the Log-Attention module is proposed to supplement the information ignored by the log-paring. The proposed method is ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#30340;&#32454;&#24494;&#25104;&#29087;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#25552;&#21462;&#40657;&#33683;&#26524;&#23454;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20197;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#36755;&#20837;CNN&#36827;&#34892;&#38598;&#25104;&#20998;&#31867;&#65292;&#20197;&#35299;&#20915;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;&#20013;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.04748</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment. (arXiv:2401.04748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#30340;&#32454;&#24494;&#25104;&#29087;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#25552;&#21462;&#40657;&#33683;&#26524;&#23454;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20197;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#36755;&#20837;CNN&#36827;&#34892;&#38598;&#25104;&#20998;&#31867;&#65292;&#20197;&#35299;&#20915;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#26524;&#23454;&#25104;&#29087;&#24230;&#20272;&#35745;&#27169;&#22411;&#19968;&#30452;&#20381;&#36182;&#20110;&#20809;&#35889;&#25351;&#25968;&#29305;&#24449;&#25110;&#22522;&#20110;&#39068;&#33394;&#30340;&#29305;&#24449;&#65292;&#22914;&#22343;&#20540;&#12289;&#26631;&#20934;&#24046;&#12289;&#20559;&#24230;&#12289;&#39068;&#33394;&#30697;&#21644;/&#25110;&#30452;&#26041;&#22270;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#40657;&#33683;&#26524;&#23454;&#30340;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#26469;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#28982;&#32780;&#65292;&#40657;&#33683;&#26524;&#23454;&#22312;&#25104;&#29087;&#26102;&#27809;&#26377;&#26126;&#26174;&#21487;&#38752;&#30340;&#21487;&#35265;&#24615;&#29305;&#24449;&#65292;&#22240;&#27492;&#23545;&#37319;&#25688;&#32773;&#26469;&#35828;&#20855;&#26377;&#24456;&#22823;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24037;&#31243;&#24212;&#29992;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36755;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#30340;&#32454;&#24494;&#29305;&#24449;&#12290;&#22810;&#36755;&#20837;CNN&#26159;&#30001;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#20960;&#20309;&#32452;16&#23618;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;VGG16&#65289;&#27169;&#22411;&#21019;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fruit ripeness estimation models have for decades depended on spectral index features or colour-based features, such as mean, standard deviation, skewness, colour moments, and/or histograms for learning traits of fruit ripeness. Recently, few studies have explored the use of deep learning techniques to extract features from images of fruits with visible ripeness cues. However, the blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible traits of ripeness when mature and therefore poses great difficulty to fruit pickers. The mature blackberry, to the human eye, is black before, during, and post-ripening. To address this engineering application challenge, this paper proposes a novel multi-input convolutional neural network (CNN) ensemble classifier for detecting subtle traits of ripeness in blackberry fruits. The multi-input CNN was created from a pre-trained visual geometry group 16-layer deep convolutional network (VGG16) model trained on the ImageNet dataset. Th
&lt;/p&gt;</description></item><item><title>DiffSHEG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04747</link><description>&lt;p&gt;
DiffSHEG:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04747
&lt;/p&gt;
&lt;p&gt;
DiffSHEG&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DiffSHEG&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#35821;&#38899;&#39537;&#21160;&#30340;&#25972;&#20307;&#19977;&#32500;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#35821;&#38899;&#36755;&#20837;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32852;&#21512;&#29983;&#25104;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#32780;&#19981;&#26159;&#20998;&#21035;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#32852;&#21160;&#29983;&#25104;&#21464;&#25442;&#22120;&#65292;&#20351;&#24471;&#20174;&#34920;&#24773;&#21040;&#25163;&#21183;&#30340;&#21333;&#21521;&#20449;&#24687;&#27969;&#26356;&#21152;&#39034;&#30021;&#65292;&#26377;&#21033;&#20110;&#21305;&#37197;&#32852;&#21512;&#30340;&#34920;&#24773;&#21644;&#25163;&#21183;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20219;&#24847;&#38271;&#24207;&#21015;&#29983;&#25104;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#30001;&#35821;&#38899;&#39537;&#21160;&#30340;&#39640;&#36136;&#37327;&#21516;&#27493;&#34920;&#24773;&#21644;&#25163;&#21183;&#29983;&#25104;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#30740;&#31350;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#20869;&#23481;&#29983;&#25104;&#39118;&#26684;&#21270;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#30340;&#36924;&#30495;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#27491;&#24577;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#39118;&#26684;&#25110;&#20174;&#24050;&#30693;&#33609;&#22270;&#20013;&#35299;&#24320;&#30011;&#23478;&#39118;&#26684;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26410;&#30693;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.04739</link><description>&lt;p&gt;
&#26681;&#25454;&#20869;&#23481;&#29983;&#25104;&#39118;&#26684;&#21270;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Content-Conditioned Generation of Stylized Free hand Sketches. (arXiv:2401.04739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#20869;&#23481;&#29983;&#25104;&#39118;&#26684;&#21270;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#30340;&#36924;&#30495;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#27491;&#24577;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#39118;&#26684;&#25110;&#20174;&#24050;&#30693;&#33609;&#22270;&#20013;&#35299;&#24320;&#30011;&#23478;&#39118;&#26684;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26410;&#30693;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#30340;&#35782;&#21035;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29305;&#27530;&#39046;&#22495;&#22914;&#20891;&#20107;&#39046;&#22495;&#65292;&#24456;&#38590;&#22823;&#35268;&#27169;&#37319;&#26679;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#12290;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#24456;&#38590;&#20135;&#29983;&#20855;&#26377;&#21508;&#31181;&#33258;&#30001;&#25163;&#32472;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#30456;&#20851;&#39046;&#22495;&#20013;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#20219;&#21153;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#30340;&#36924;&#30495;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#20174;&#20808;&#39564;&#27491;&#24577;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#39118;&#26684;&#26469;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#33258;&#30001;&#25163;&#32472;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#20174;&#24050;&#30693;&#30340;&#33258;&#30001;&#25163;&#32472;&#33609;&#22270;&#20013;&#35299;&#24320;&#30011;&#23478;&#30340;&#39118;&#26684;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#29983;&#25104;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#26410;&#30693;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#35777;&#26126;&#25105;&#20204;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the recognition of free-hand sketches has remained a popular task. However, in some special fields such as the military field, free-hand sketches are difficult to sample on a large scale. Common data augmentation and image generation techniques are difficult to produce images with various free-hand sketching styles. Therefore, the recognition and segmentation tasks in related fields are limited. In this paper, we propose a novel adversarial generative network that can accurately generate realistic free-hand sketches with various styles. We explore the performance of the model, including using styles randomly sampled from a prior normal distribution to generate images with various free-hand sketching styles, disentangling the painters' styles from known free-hand sketches to generate images with specific styles, and generating images of unknown classes that are not in the training set. We further demonstrate with qualitative and quantitative evaluations our advantages i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;CNN&#12289;VGG16&#21644;XGBoost&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#29305;&#24449;&#30340;&#38899;&#20048;&#31867;&#22411;&#20998;&#31867;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#22312;&#22788;&#29702;30&#31186;&#30340;&#26757;&#23572;&#39057;&#35889;&#22270;&#26102;&#34920;&#29616;&#26368;&#22909;&#65292;XGBoost&#22312;&#22788;&#29702;3&#31186;&#30340;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#26102;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.04737</link><description>&lt;p&gt;
&#38899;&#20048;&#31867;&#22411;&#20998;&#31867;&#65306;&#20351;&#29992;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#21644;&#26757;&#23572;&#39057;&#35889;&#22270;&#30340;CNN&#21644;XGBoost&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Music Genre Classification: A Comparative Analysis of CNN and XGBoost Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms. (arXiv:2401.04737v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20351;&#29992;CNN&#12289;VGG16&#21644;XGBoost&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#29305;&#24449;&#30340;&#38899;&#20048;&#31867;&#22411;&#20998;&#31867;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#22312;&#22788;&#29702;30&#31186;&#30340;&#26757;&#23572;&#39057;&#35889;&#22270;&#26102;&#34920;&#29616;&#26368;&#22909;&#65292;XGBoost&#22312;&#22788;&#29702;3&#31186;&#30340;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#26102;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21508;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#31639;&#27861;&#20351;&#24471;&#38899;&#20048;&#24179;&#21488;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#21916;&#22909;&#25552;&#20379;&#20869;&#23481;&#12290;&#38899;&#20048;&#31867;&#22411;&#36890;&#36807;&#21253;&#25324;&#22768;&#23398;&#29305;&#24449;&#21644;&#25991;&#21270;&#32771;&#34385;&#22312;&#20869;&#30340;&#21508;&#20010;&#26041;&#38754;&#26469;&#23450;&#20041;&#12290;&#38899;&#20048;&#31867;&#22411;&#20998;&#31867;&#22312;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#23427;&#22522;&#20110;&#38899;&#20048;&#30340;&#30456;&#20284;&#24615;&#21521;&#29992;&#25143;&#25512;&#33616;&#20869;&#23481;&#12290;&#22312;&#32473;&#23450;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31181;&#20551;&#35774;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#38899;&#39057;&#25991;&#20214;&#12290;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#29305;&#24449;&#21487;&#20197;&#30456;&#20114;&#20419;&#36827;&#24182;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#25552;&#20986;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#24102;&#26377;&#20840;&#36830;&#25509;&#23618;&#65288;FC&#65289;&#30340;VGG16&#27169;&#22411;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#26041;&#27861;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#65306;30&#31186;&#30340;&#26757;&#23572;&#39057;&#35889;&#22270;&#21644;3&#31186;&#30340;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, various well-designed algorithms have empowered music platforms to provide content based on one's preferences. Music genres are defined through various aspects, including acoustic features and cultural considerations. Music genre classification works well with content-based filtering, which recommends content based on music similarity to users. Given a considerable dataset, one premise is automatic annotation using machine learning or deep learning methods that can effectively classify audio files. The effectiveness of systems largely depends on feature and model selection, as different architectures and features can facilitate each other and yield different results. In this study, we conduct a comparative study investigating the performances of three models: a proposed convolutional neural network (CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient Boosting (XGBoost) approach on different features: 30-second Mel spectrogram and 3-second Mel-freq
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#25915;&#20987;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#21152;&#24378;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#30340;&#23545;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.04736</link><description>&lt;p&gt;
&#23545;&#20351;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20998;&#24067;&#24335;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#25915;&#20987;&#38887;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Attack Resilience in Distributed Platoon Controllers with Model Predictive Control. (arXiv:2401.04736v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04736
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#25915;&#20987;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#21152;&#24378;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#30340;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#24191;&#27867;&#24212;&#29992;&#20026;&#20132;&#36890;&#31995;&#32479;&#24102;&#26469;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#22914;&#22686;&#21152;&#20102;&#20132;&#36890;&#27969;&#37327;&#12289;&#29123;&#27833;&#25928;&#29575;&#25552;&#39640;&#20197;&#21450;&#20943;&#23569;&#20102;&#27745;&#26579;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#20114;&#36830;&#31995;&#32479;&#21644;&#36890;&#20449;&#32593;&#32476;&#30340;&#26085;&#30410;&#20381;&#36182;&#20063;&#20351;&#24471;&#36825;&#20123;&#25511;&#21046;&#22120;&#38754;&#20020;&#28508;&#22312;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#21361;&#21450;&#20854;&#23433;&#20840;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#25915;&#20987;&#22330;&#26223;&#24182;&#35780;&#20272;&#20854;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20998;&#24067;&#24335;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#20351;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#27169;&#25311;&#20102;&#21253;&#25324;&#20013;&#38388;&#20154;&#25915;&#20987;&#21644;&#20551;&#25968;&#25454;&#27880;&#20837;&#22312;&#20869;&#30340;&#21508;&#31181;&#25915;&#20987;&#25216;&#26415;&#65292;&#20197;&#35782;&#21035;&#36710;&#38431;&#25511;&#21046;&#22120;&#30340;&#28431;&#27934;&#21644;&#24369;&#28857;&#12290;&#25552;&#20379;&#24182;&#27979;&#35797;&#20102;&#21253;&#25324;&#25915;&#20987;&#20998;&#26512;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26816;&#27979;&#30340;&#65292;&#21152;&#24378;&#20102;&#36890;&#20449;&#21327;&#35758;&#30340;&#24212;&#23545;&#25514;&#26045;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38598;&#25104;&#23433;&#20840;&#24615;&#35774;&#35745;&#21644;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive use of distributed vehicle platoon controllers has resulted in several benefits for transportation systems, such as increased traffic flow, fuel efficiency, and decreased pollution. The rising reliance on interconnected systems and communication networks, on the other hand, exposes these controllers to potential cyber-attacks, which may compromise their safety and functionality. This thesis aims to improve the security of distributed vehicle platoon controllers by investigating attack scenarios and assessing their influence on system performance. Various attack techniques, including man-in-the-middle (MITM) and false data injection (FDI), are simulated using Model Predictive Control (MPC) controller to identify vulnerabilities and weaknesses of the platoon controller. Countermeasures are offered and tested, that includes attack analysis and reinforced communication protocols using Machine Learning techniques for detection. The findings emphasize the significance of integr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.04732</link><description>&lt;p&gt;
MSX&#38144;&#21806;&#21327;&#21516;&#21161;&#25163;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26696;&#20363;&#30740;&#31350;: &#36890;&#36807;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#25913;&#21892;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#20197;&#23454;&#29616;&#20869;&#23481;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#19987;&#38376;&#20026;&#38144;&#21806;&#20154;&#21592;&#25552;&#20379;&#26377;&#20851;&#26448;&#26009;/&#25991;&#26723;&#30340;&#23454;&#26102;&#25512;&#33616;&#65292;&#20197;&#20415;&#19982;&#23458;&#25143;&#20998;&#20139;&#25110;&#22312;&#30005;&#35805;&#20013;&#21442;&#32771;&#12290;&#36890;&#36807;&#20351;&#29992;Seismic&#38144;&#21806;&#36164;&#26009;&#30340;&#30456;&#23545;&#36739;&#22823;&#35268;&#27169;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21334;&#26041;&#26597;&#35810;&#30340;LLM&#23884;&#20837;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#35814;&#32454;&#30340;&#26041;&#24335;&#35774;&#35745;&#25552;&#31034;&#35821;&#65292;&#24182;&#21033;&#29992;&#21487;&#29992;&#30340;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#38144;&#21806;&#32773;&#20803;&#29305;&#24449;&#38598;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#24207;&#22120;&#26550;&#26500;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20165;&#20960;&#31186;&#38047;&#20869;&#21363;&#21487;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#37096;&#32626;&#20026;&#29992;&#20110;&#23454;&#26102;&#25512;&#29702;&#30340;AML&#31471;&#28857;&#65292;&#24182;&#24050;&#38598;&#25104;&#21040;Copilot&#30028;&#38754;&#20013;&#65292;&#35813;&#30028;&#38754;&#29616;&#24050;&#37096;&#32626;&#22312;&#27599;&#26085;&#30001;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;(MSX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design a real-time question-answering system specifically targeted for helping sellers get relevant material/documentation they can share live with their customers or refer to during a call. Taking the Seismic content repository as a relatively large scale example of a diverse dataset of sales material, we demonstrate how LLM embeddings of sellers' queries can be matched with the relevant content. We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers. Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds even for large datasets. Our recommender system is deployed as an AML endpoint for real-time inferencing and has been integrated into a Copilot interface that is now deployed in the production version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.03991</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65306;&#36890;&#36807;StepGame&#22522;&#20934;&#30340;&#28145;&#20837;&#35780;&#20272;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark. (arXiv:2401.03991v1 [cs.AI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;StepGame&#31561;&#22522;&#20934;&#35780;&#20272;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;ChatGPT&#22312;&#20854;&#20013;&#34920;&#29616;&#20986;&#20102;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20934;&#20013;&#23384;&#22312;&#30340;&#27169;&#26495;&#38169;&#35823;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#26495;&#38169;&#35823;&#65292;ChatGPT&#26377;&#28508;&#21147;&#34920;&#29616;&#26356;&#22909;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23436;&#21892;&#20102;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GPT&#22312;&#32463;&#36807;&#20462;&#27491;&#30340;&#22522;&#20934;&#19978;&#30340;&#31354;&#38388;&#25512;&#29702;&#24615;&#33021;&#65292;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#32570;&#38519;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT's spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03756</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#36866;&#24212;&#24615;&#23454;&#39564;&#35774;&#35745;&#19982;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#26159;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification, BAI&#65289;&#38382;&#39064;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32473;&#23450;&#22810;&#20010;&#27835;&#30103;&#33218;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#35266;&#23519;&#19968;&#20010;&#21051;&#30011;&#23454;&#39564;&#21333;&#20301;&#30340;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#65292;&#24182;&#23558;&#35813;&#21333;&#20301;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#27835;&#30103;&#33218;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#20915;&#31574;&#32773;&#25512;&#33616;&#19968;&#20010;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#39044;&#35745;&#20135;&#29983;&#26368;&#39640;&#26399;&#26395;&#32467;&#26524;&#30340;&#27835;&#30103;&#33218;&#65288;&#26368;&#20339;&#27835;&#30103;&#33218;&#65289;&#12290;&#35813;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#65288;&#31574;&#30053;&#36951;&#25022;&#65289;&#26469;&#34913;&#37327;&#65292;&#35813;&#36951;&#25022;&#34920;&#31034;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#65292;&#26368;&#20339;&#27835;&#30103;&#33218;&#21644;&#25512;&#33616;&#27835;&#30103;&#33218;&#30340;&#26465;&#20214;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#30340;&#26368;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#27493;&#39588;&#26159;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#19979;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36817;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#36824;&#26263;&#31034;&#30528;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20123;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#26377;&#25928;&#25972;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03397</link><description>&lt;p&gt;
&#39044;&#27979;&#22825;&#31354;&#65306;&#19968;&#31181;&#29992;&#20110;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting. (arXiv:2401.03397v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#26377;&#25928;&#25972;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#22312;&#33322;&#31354;&#20844;&#21496;&#36816;&#33829;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#20174;&#23450;&#20215;&#21040;&#36335;&#32447;&#20248;&#21270;&#31561;&#20851;&#38190;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#29677;&#23618;&#27425;&#20056;&#23458;&#27969;&#37327;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#21033;&#29992;&#32654;&#22269;&#33322;&#31354;&#20844;&#21496;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21560;&#25910;&#20102;&#21382;&#21490;&#27969;&#37327;&#25968;&#25454;&#12289;&#31080;&#20215;&#20851;&#38381;&#20449;&#24687;&#21644;&#27599;&#20010;&#33322;&#29677;&#29305;&#23450;&#30340;&#23395;&#33410;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#20840;&#38754;&#30340;&#25968;&#25454;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;3D&#24352;&#37327;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24212;&#29992;&#20102;&#31934;&#32454;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. Crucial to the success of our model is a comprehensive data processing strategy. We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentatio
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#29305;&#24615;&#20351;&#24471;&#39044;&#27979;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#31616;&#21270;V&amp;V&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;&#24212;&#29992;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30340;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.03188</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Verification and Validation, Testing and Evaluations of Neurosymbolic Artificial Intelligence. (arXiv:2401.03188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03188
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#29305;&#24615;&#20351;&#24471;&#39044;&#27979;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#31616;&#21270;V&amp;V&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;&#24212;&#29992;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#20998;&#25903;&#65292;&#23558;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#21644;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#34920;&#29616;&#20026;&#8220;&#40657;&#30418;&#8221;&#65292;&#24847;&#21619;&#30528;&#39044;&#27979;&#24456;&#38590;&#35299;&#37322;&#65292;&#20351;&#24471;&#20351;&#29992;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#65288;T&amp;E&#65289;&#20197;&#21450;&#39564;&#35777;&#21644;&#39564;&#35777;&#65288;V&amp;V&#65289;&#36807;&#31243;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#30001;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#20102;&#31526;&#21495;&#21644;&#20122;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#28857;&#65292;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#22914;&#20309;&#31616;&#21270;V&amp;V&#36807;&#31243;&#12290;&#26412;&#35843;&#26597;&#32771;&#34385;&#20102;&#20004;&#31181;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#65292;&#24182;&#20998;&#26512;&#20102;&#24403;&#21069;&#24212;&#29992;&#20013;&#20316;&#20026;&#31526;&#21495;&#21644;&#20122;&#31526;&#21495;&#32452;&#25104;&#37096;&#20998;&#30340;&#24120;&#29992;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#32452;&#20214;&#30340;T&amp;E&#21644;V&amp;V&#36807;&#31243;&#30340;&#24403;&#21069;&#25216;&#26415;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#36824;&#35843;&#26597;&#20102;&#31526;&#21495;&#37096;&#20998;&#22312;&#24403;&#21069;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#20013;&#22914;&#20309;&#29992;&#20110;T&amp;E&#21644;V&amp;V&#30446;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that combines the strengths of symbolic AI and sub-symbolic AI. A major drawback of sub-symbolic AI is that it acts as a "black box", meaning that predictions are difficult to explain, making the testing &amp; evaluation (T&amp;E) and validation &amp; verification (V&amp;V) processes of a system that uses sub-symbolic AI a challenge. Since neurosymbolic AI combines the advantages of both symbolic and sub-symbolic AI, this survey explores how neurosymbolic applications can ease the V&amp;V process. This survey considers two taxonomies of neurosymbolic AI, evaluates them, and analyzes which algorithms are commonly used as the symbolic and sub-symbolic components in current applications. Additionally, an overview of current techniques for the T&amp;E and V&amp;V processes of these components is provided. Furthermore, it is investigated how the symbolic part is used for T&amp;E and V&amp;V purposes in current neurosymbolic applications. Our research shows
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.03154</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#24403;&#30446;&#26631;&#36229;&#36807;&#26234;&#33021;&#20307;&#25968;&#37327;&#26102;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#30446;&#26631;&#36319;&#36394;&#22312;&#37326;&#29983;&#21160;&#29289;&#24033;&#36923;&#12289;&#23433;&#20840;&#30417;&#25511;&#25110;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#31639;&#27861;&#24120;&#24120;&#20570;&#20986;&#19968;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#65306;&#30446;&#26631;&#25968;&#37327;&#21644;&#21021;&#22987;&#20301;&#32622;&#24050;&#30693;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#24050;&#34987;&#39044;&#20998;&#37197;&#21040;&#30417;&#25511;&#29615;&#22659;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#65292;&#20943;&#36731;&#20102;&#25506;&#32034;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#65292;&#36825;&#31181;&#20551;&#35774;&#20250;&#38480;&#21046;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#26080;&#27861;&#25345;&#32493;&#36319;&#36394;&#20854;&#35270;&#37326;&#20013;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#22810;&#26234;&#33021;&#20307;&#36319;&#36394;&#31639;&#27861;&#36824;&#20551;&#35774;&#26234;&#33021;&#20307;&#38388;&#35266;&#27979;&#30340;&#21516;&#27493;&#65292;&#25110;&#32773;&#38656;&#35201;&#19968;&#20010;&#20013;&#22830;&#25511;&#21046;&#22120;&#26469;&#21327;&#35843;&#32852;&#21512;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#30446;&#26631;&#12289;&#21516;&#26102;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#38388;&#36890;&#20449;&#26159;&#24322;&#27493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DecSTER&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20551;&#35774;&#23494;&#24230;&#28388;&#27874;&#22120;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>XUAT-Copilot&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02705</link><description>&lt;p&gt;
XUAT-Copilot: &#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model. (arXiv:2401.02705v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02705
&lt;/p&gt;
&lt;p&gt;
XUAT-Copilot&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#21160;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#24494;&#20449;&#25903;&#20184;&#30340;&#29992;&#25143;&#39564;&#25910;&#27979;&#35797;&#65288;UAT&#65289;&#36807;&#31243;&#65292;&#36825;&#26159;&#20013;&#22269;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31227;&#21160;&#25903;&#20184;&#24212;&#29992;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;XUAT&#30340;&#31995;&#32479;&#29992;&#20110;&#36825;&#20010;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24403;&#21069;&#31995;&#32479;&#20013;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#20154;&#21147;&#23494;&#38598;&#38454;&#27573;&#65292;&#21363;&#27979;&#35797;&#33050;&#26412;&#30340;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#25552;&#39640;&#24403;&#21069;&#31995;&#32479;&#33258;&#21160;&#21270;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#27979;&#35797;&#33050;&#26412;&#29983;&#25104;&#38454;&#27573;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#23637;&#31034;&#20102;&#33719;&#21462;&#20154;&#31867;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#24050;&#32463;&#24418;&#25104;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#31867;&#20284;&#20154;&#31867;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#21463;&#21040;&#36825;&#20123;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#31995;&#32479;&#65292;&#21517;&#20026;XUAT-Copilot&#65292;&#29992;&#20110;&#33258;&#21160;UAT&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#30001;&#19977;&#20010;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#32452;&#25104;&#65292;&#36127;&#36131;&#21160;&#20316;&#35268;&#21010;&#12289;&#29366;&#24577;&#26816;&#27979;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In past years, we have been dedicated to automating user acceptance testing (UAT) process of WeChat Pay, one of the most influential mobile payment applications in China. A system titled XUAT has been developed for this purpose. However, there is still a human-labor-intensive stage, i.e, test scripts generation, in the current system. Therefore, in this paper, we concentrate on methods of boosting the automation level of the current system, particularly the stage of test scripts generation. With recent notable successes, large language models (LLMs) demonstrate significant potential in attaining human-like intelligence and there has been a growing research area that employs LLMs as autonomous agents to obtain human-like decision-making capabilities. Inspired by these works, we propose an LLM-powered multi-agent collaborative system, named XUAT-Copilot, for automated UAT. The proposed system mainly consists of three LLM-based agents responsible for action planning, state checking and pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;"IdentiFace"&#65292;&#36890;&#36807;&#23558;&#20154;&#33080;&#35782;&#21035;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#26550;&#26500;&#21644;&#23545;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#22312;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#21462;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01227</link><description>&lt;p&gt;
IdentiFace&#65306;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;"IdentiFace"&#65292;&#36890;&#36807;&#23558;&#20154;&#33080;&#35782;&#21035;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#26550;&#26500;&#21644;&#23545;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#22312;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#21462;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#30340;&#21457;&#23637;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#12290;&#29616;&#20170;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#20197;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#32467;&#21512;&#22810;&#31181;&#29983;&#29289;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;IdentiFace&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#65292;&#23558;&#20154;&#33080;&#35782;&#21035;&#30340;&#26680;&#24515;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#19968;&#20123;&#37325;&#35201;&#30340;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#30528;&#37325;&#20351;&#29992;&#21482;&#26377;VGG-16&#21463;&#21040;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#23376;&#31995;&#32479;&#20013;&#36827;&#34892;&#20102;&#19968;&#20123;&#32454;&#24494;&#30340;&#26356;&#25913;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#20351;&#24471;&#36328;&#27169;&#24577;&#30340;&#38598;&#25104;&#26356;&#21152;&#31616;&#21333;&#12290;&#23427;&#26356;&#23481;&#26131;&#35299;&#37322;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23545;&#20110;&#20915;&#31574;&#36807;&#31243;&#21644;&#20154;&#33080;&#27169;&#24577;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#32473;&#20986;&#20102;&#24456;&#22909;&#30340;&#25351;&#31034;&#12290;&#23545;&#20110;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#20116;&#20010;&#31867;&#21035;&#30340;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#33719;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of facial biometric systems has contributed greatly to the development of the computer vision field. Nowadays, there's always a need to develop a multimodal system that combines multiple biometric traits in an efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a multimodal facial biometric system that combines the core of facial recognition with some of the most important soft biometric traits such as gender, face shape, and emotion. We also focused on developing the system using only VGG-16 inspired architecture with minor changes across different subsystems. This unification allows for simpler integration across modalities. It makes it easier to interpret the learned features between the tasks which gives a good indication about the decision-making process across the facial modalities and potential connection. For the recognition problem, we acquired a 99.2% test accuracy for five classes with high intra-class variations using data collected 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00926</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#24418;DETR&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36741;&#21161;&#34880;&#28082;&#30142;&#30149;&#35786;&#26029;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#21307;&#38498;&#34880;&#28082;&#26816;&#27979;&#20013;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#21307;&#29983;&#20351;&#29992;&#26174;&#24494;&#38236;&#20174;&#24739;&#32773;&#30340;&#34880;&#28082;&#26174;&#24494;&#22270;&#20687;&#20013;&#25163;&#21160;&#20998;&#31163;&#30333;&#32454;&#32990;&#12290;&#28982;&#21518;&#36890;&#36807;&#33258;&#21160;&#30333;&#32454;&#32990;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#20998;&#31163;&#30340;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#34880;&#26679;&#20013;&#19981;&#21516;&#31867;&#22411;&#30333;&#32454;&#32990;&#30340;&#27604;&#20363;&#21644;&#20307;&#31215;&#65292;&#20174;&#32780;&#21327;&#21161;&#30142;&#30149;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32791;&#26102;&#12289;&#32791;&#21147;&#65292;&#32780;&#19988;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#20026;&#22270;&#20687;&#36136;&#37327;&#21644;&#29615;&#22659;&#26465;&#20214;&#31561;&#22240;&#32032;&#65292;&#21487;&#33021;&#23548;&#33268;&#21518;&#32493;&#20998;&#31867;&#38169;&#35823;&#21644;&#35823;&#35786;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65306;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65288;MFDS-DETR&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#32423;&#31579;&#36873;&#29305;&#24449;&#34701;&#21512;&#37329;&#23383;&#22612;&#65288;HS-FPN&#65289;&#65292;&#23454;&#29616;&#20102;&#22810;&#32423;&#34701;&#21512;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#20316;&#20026;&#29305;&#24449;&#34701;&#21512;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#37319;&#29992;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#23454;&#29616;&#31934;&#30830;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard hospital blood tests, the traditional process requires doctors to manually isolate leukocytes from microscopic images of patients' blood using microscopes. These isolated leukocytes are then categorized via automatic leukocyte classifiers to determine the proportion and volume of different types of leukocytes present in the blood samples, aiding disease diagnosis. This methodology is not only time-consuming and labor-intensive, but it also has a high propensity for errors due to factors such as image quality and environmental conditions, which could potentially lead to incorrect subsequent classifications and misdiagnosis. To address these issues, this paper proposes an innovative method of leukocyte detection: the Multi-level Feature Fusion and Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte scale disparity, we designed the High-level Screening-feature Fusion Pyramid (HS-FPN), enabling multi-level fusion. This model uses high-level features as 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#20540;&#24182;&#24418;&#25104;&#32858;&#31867;&#31354;&#38388;&#26469;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#65292;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32676;&#32452;&#24402;&#23646;&#12290;</title><link>http://arxiv.org/abs/2312.17286</link><description>&lt;p&gt;
&#36830;&#25509;&#21307;&#30103;&#35774;&#22791;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative study of clustering models for multivariate time series from connected medical devices. (arXiv:2312.17286v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17286
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#20540;&#24182;&#24418;&#25104;&#32858;&#31867;&#31354;&#38388;&#26469;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#65292;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32676;&#32452;&#24402;&#23646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#24739;&#32773;&#25968;&#25454;&#36890;&#24120;&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#24418;&#24335;&#25910;&#38598;&#65292;&#21487;&#20197;&#20840;&#38754;&#22320;&#21453;&#26144;&#24739;&#32773;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#26159;&#31232;&#30095;&#30340;&#65292;&#20294;&#36830;&#25509;&#30340;&#35774;&#22791;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#39057;&#29575;&#12290;&#30446;&#26631;&#26159;&#20174;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#20013;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#20540;&#65292;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#28508;&#22312;&#30340;&#32858;&#31867;&#31354;&#38388;&#65292;&#24182;&#20197;&#39044;&#27979;&#24615;&#33021;&#20026;&#35780;&#20215;&#25351;&#26631;&#12290;&#25105;&#20204;&#22312;Withing&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;M AGMAC LUST&#21487;&#20197;&#23545;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;DGM${}^2$&#20801;&#35768;&#20010;&#20307;&#30340;&#32676;&#32452;&#24402;&#23646;&#38543;&#26102;&#38388;&#21464;&#21270;&#65288;&#21160;&#24577;&#32858;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, patient data is often collected as multivariate time series, providing a comprehensive view of a patient's health status over time. While this data can be sparse, connected devices may enhance its frequency. The goal is to create patient profiles from these time series. In the absence of labels, a predictive model can be used to predict future values while forming a latent cluster space, evaluated based on predictive performance. We compare two models on Withing's datasets, M AGMAC LUST which clusters entire time series and DGM${}^2$ which allows the group affiliation of an individual to change over time (dynamic clustering).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20302;&#35745;&#31639;&#37327;&#21644;&#39640;&#36136;&#37327;&#29031;&#20142;&#26263;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;Siamese&#33258;&#27880;&#24847;&#22359;&#21644;Skip-Channel&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22686;&#24378;&#20219;&#21153;&#20013;&#36229;&#36234;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16805</link><description>&lt;p&gt;
DarkShot: &#29992;&#20302;&#35745;&#31639;&#37327;&#21644;&#39640;&#36136;&#37327;&#29031;&#20142;&#26263;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DarkShot: Lighting Dark Images with Low-Compute and High-Quality. (arXiv:2312.16805v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20302;&#35745;&#31639;&#37327;&#21644;&#39640;&#36136;&#37327;&#29031;&#20142;&#26263;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;Siamese&#33258;&#27880;&#24847;&#22359;&#21644;Skip-Channel&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22686;&#24378;&#20219;&#21153;&#20013;&#36229;&#36234;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22812;&#38388;&#25668;&#24433;&#22312;&#26497;&#20302;&#20809;&#26465;&#20214;&#19979;&#36935;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36229;&#20302;&#30340;&#20449;&#22122;&#27604;&#12290;&#23545;&#20110;&#23454;&#38469;&#37096;&#32626;&#65292;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#19981;&#20165;&#24517;&#39035;&#20135;&#29983;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#32467;&#26524;&#65292;&#36824;&#24517;&#39035;&#35201;&#27714;&#26368;&#23567;&#35745;&#31639;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#19987;&#27880;&#20110;&#25552;&#39640;&#24674;&#22797;&#24615;&#33021;&#65292;&#35201;&#20040;&#37319;&#29992;&#36731;&#37327;&#32423;&#27169;&#22411;&#21364;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#32593;&#32476;&#65292;&#23427;&#22312;&#20302;&#20809;&#22686;&#24378;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#21512;&#20102;Siamese&#33258;&#27880;&#24847;&#22359;(SSAB)&#21644;Skip-Channel&#27880;&#24847;&#21147;(SCA)&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#32858;&#21512;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#23545;&#20302;&#20809;&#22270;&#20687;&#24674;&#22797;&#36807;&#31243;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;UHD 4K&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nighttime photography encounters escalating challenges in extremely low-light conditions, primarily attributable to the ultra-low signal-to-noise ratio. For real-world deployment, a practical solution must not only produce visually appealing results but also require minimal computation. However, most existing methods are either focused on improving restoration performance or employ lightweight models at the cost of quality. This paper proposes a lightweight network that outperforms existing state-of-the-art (SOTA) methods in low-light enhancement tasks while minimizing computation. The proposed network incorporates Siamese Self-Attention Block (SSAB) and Skip-Channel Attention (SCA) modules, which enhance the model's capacity to aggregate global information and are well-suited for high-resolution images. Additionally, based on our analysis of the low-light image restoration process, we propose a Two-Stage Framework that achieves superior results. Our model can restore a UHD 4K resoluti
&lt;/p&gt;</description></item><item><title>TAPE&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.15667</link><description>&lt;p&gt;
TAPE: &#21033;&#29992;&#20195;&#29702;&#25299;&#25169;&#36827;&#34892;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient. (arXiv:2312.15667v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15667
&lt;/p&gt;
&lt;p&gt;
TAPE&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;MAPG&#26041;&#27861;&#20013;&#30340;&#38598;&#20013;&#24335;&#35780;&#35770;&#22120;&#20173;&#28982;&#38754;&#20020;&#38598;&#20013;-&#20998;&#25955;&#19981;&#21305;&#37197;&#65288;CDM&#65289;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#21160;&#20250;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#29420;&#30340;&#35780;&#35770;&#22120;&#21487;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#38480;&#21046;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#29702;&#25299;&#25169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20915;&#23450;&#20102;&#22312;&#31574;&#30053;&#26799;&#24230;&#20013;&#26159;&#21542;&#24212;&#32771;&#34385;&#20854;&#20182;&#20195;&#29702;&#65292;&#24182;&#22312;&#20419;&#36827;&#21512;&#20316;&#21644;&#20943;&#36731;CDM&#38382;&#39064;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20195;&#29702;&#25299;&#25169;&#20801;&#35768;&#20195;&#29702;&#20351;&#29992;&#21512;&#20316;&#25928;&#29992;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#32780;&#19981;&#26159;&#30001;&#38598;&#20013;&#24335;&#35780;&#35770;&#22120;&#30830;&#23450;&#30340;&#20840;&#23616;&#25928;&#29992;&#25110;&#32773;&#30001;&#20010;&#20307;&#35780;&#35770;&#22120;&#30830;&#23450;&#30340;&#23616;&#37096;&#25928;&#29992;&#12290;&#20026;&#20102;&#26500;&#24314;&#20195;&#29702;&#25299;&#25169;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25299;&#25169;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;TAPE&#65289;&#65292;&#36866;&#29992;&#20110;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#30340;MAPG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Policy Gradient (MAPG) has made significant progress in recent years. However, centralized critics in state-of-the-art MAPG methods still face the centralized-decentralized mismatch (CDM) issue, which means sub-optimal actions by some agents will affect other agent's policy learning. While using individual critics for policy updates can avoid this issue, they severely limit cooperation among agents. To address this issue, we propose an agent topology framework, which decides whether other agents should be considered in policy gradient and achieves compromise between facilitating cooperation and alleviating the CDM issue. The agent topology allows agents to use coalition utility as learning objective instead of global utility by centralized critics or local utility by individual critics. To constitute the agent topology, various models are studied. We propose Topology-based multi-Agent Policy gradiEnt (TAPE) for both stochastic and deterministic MAPG methods. We prove the po
&lt;/p&gt;</description></item><item><title>I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2312.12102</link><description>&lt;p&gt;
I-CEE: &#23558;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#23450;&#21046;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12102
&lt;/p&gt;
&lt;p&gt;
I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#21040;&#20854;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#20960;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#29992;&#25143;&#65288;&#35299;&#37322;&#23545;&#35937;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;XAI&#25216;&#26415;&#20135;&#29983;&#30340;&#26159;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23454;&#29616;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;XAI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-CEE&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#29616;&#26377;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;I-CEE&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#31034;&#20363;&#22270;&#20687;&#65289;&#12289;&#30456;&#24212;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;I-CEE&#27169;&#25311;&#20102;&#31034;&#20363;&#22270;&#20687;&#30340;&#20449;&#24687;&#37327;&#20381;&#36182;&#20110;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#21462;&#36793;&#19978;&#30340;&#28040;&#24687;&#23558;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2312.10808</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Non-Euclidean Spatial Graph Neural Network. (arXiv:2312.10808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#21462;&#36793;&#19978;&#30340;&#28040;&#24687;&#23558;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#32593;&#32476;&#26159;&#20854;&#22270;&#25299;&#25169;&#21463;&#23884;&#20837;&#31354;&#38388;&#32422;&#26463;&#30340;&#32593;&#32476;&#12290;&#29702;&#35299;&#32806;&#21512;&#30340;&#31354;&#38388;&#22270;&#23646;&#24615;&#23545;&#20110;&#20174;&#31354;&#38388;&#32593;&#32476;&#20013;&#25552;&#21462;&#24378;&#22823;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32467;&#21512;&#20010;&#21035;&#30340;&#31354;&#38388;&#21644;&#32593;&#32476;&#34920;&#31034;&#26080;&#27861;&#25581;&#31034;&#31354;&#38388;&#32593;&#32476;&#30340;&#28508;&#22312;&#20132;&#20114;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31354;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#32771;&#34385;&#23884;&#20837;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#32593;&#32476;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#21033;&#29992;&#19981;&#35268;&#21017;&#21644;&#38750;&#22343;&#21248;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#25152;&#25658;&#24102;&#30340;&#20016;&#23500;&#20960;&#20309;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#32467;&#21512;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#65292;&#20854;&#20013;&#31354;&#38388;&#20960;&#20309;&#34987;&#25552;&#21462;&#20026;&#36793;&#19978;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Spatial networks are networks whose graph topology is constrained by their embedded spatial space. Understanding the coupled spatial-graph properties is crucial for extracting powerful representations from spatial networks. Therefore, merely combining individual spatial and network representations cannot reveal the underlying interaction mechanism of spatial networks. Besides, existing spatial network representation learning methods can only consider networks embedded in Euclidean space, and can not well exploit the rich geometric information carried by irregular and non-uniform non-Euclidean space. In order to address this issue, in this paper we propose a novel generic framework to learn the representation of spatial networks that are embedded in non-Euclidean manifold space. Specifically, a novel message-passing-based neural network is proposed to combine graph topology and spatial geometry, where spatial geometry is extracted as messages on the edges. We theoretically guarantee tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2312.07213</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#65306;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Human-computer Interaction for Brain-inspired Computing Based on Machine Learning And Deep Learning:A Review. (arXiv:2312.07213v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#21457;&#23637;&#23545;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#33041;&#21551;&#21457;&#35745;&#31639;&#26159;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#20132;&#21449;&#28857;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#24212;&#29992;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#28436;&#21270;&#12289;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#21382;&#21490;&#65292;&#24182;&#23558;&#20854;&#28436;&#21270;&#21010;&#20998;&#20026;&#36817;&#26399;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#24378;&#35843;&#20102;&#27599;&#20010;&#38454;&#27573;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12290;&#21478;&#22806;&#65292;&#20174;&#20845;&#20010;&#35282;&#24230;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous development of artificial intelligence has a profound impact on biomedical research and other fields.Brain-inspired computing is an important intersection of multimodal technology and biomedical field. This paper presents a comprehensive review of machine learning (ML) and deep learning (DL) models applied in human-computer interaction for brain-inspired computing, tracking their evolution, application value, challenges, and potential research trajectories. First, the basic concepts and development history are reviewed, and their evolution is divided into two stages: recent machine learning and current deep learning, emphasizing the importance of each stage in the research state of human-computer interaction for brain-inspired computing. In addition, the latest progress and key techniques of deep learning in different tasks of human-computer interaction for brain-inspired computing are introduced from six perspectives. Despite significant progress, challenges remain in m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#27169;&#22411;(DHT)&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#23618;&#21644;&#22836;&#30340;&#25968;&#37327;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#20013;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2312.03038</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#36890;&#36807;&#19978;&#19979;&#25991;Bandit&#23454;&#29616;&#23618;&#21644;&#22836;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03038
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#27169;&#22411;(DHT)&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#23618;&#21644;&#22836;&#30340;&#25968;&#37327;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#20013;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#38656;&#35201;&#22266;&#23450;&#25968;&#37327;&#30340;&#23618;&#21644;&#22836;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23545;&#21333;&#20010;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#19981;&#28789;&#27963;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#65288;DHT&#65289;&#27169;&#22411;&#65292;&#23427;&#30340;&#23618;&#21644;&#22836;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#12290;&#20026;&#20102;&#30830;&#23450;&#23618;&#25968;&#21644;&#22836;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22343;&#21248;&#32622;&#20449;&#19978;&#30028;&#65292;&#32780;&#22312;&#32473;&#23450;&#22836;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37319;&#29992;&#32452;&#21512;Thompson&#25277;&#26679;&#26469;&#36873;&#25321;&#29305;&#23450;&#30340;&#22836;&#32452;&#21512;&#12290;&#19982;&#20043;&#21069;&#21482;&#20851;&#27880;&#21387;&#32553;&#35757;&#32451;&#32593;&#32476;&#20197;&#29992;&#20110;&#25512;&#26029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#24213;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#24577;transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedEmb&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#26356;&#24378;&#20197;&#21450;&#36739;&#20302;&#30340;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#27844;&#38706;&#19979;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.00102</link><description>&lt;p&gt;
FedEmb:&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21644;&#29305;&#24449;&#23884;&#20837;&#32858;&#21512;&#30340;&#22402;&#30452;&#21644;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedEmb&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#26356;&#24378;&#20197;&#21450;&#36739;&#20302;&#30340;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#27844;&#38706;&#19979;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33539;&#20363;&#65292;&#23427;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#65292;&#32780;&#19981;&#23558;&#25968;&#25454;&#20256;&#36755;&#32473;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#26159;&#27700;&#24179;&#30340;&#12289;&#22402;&#30452;&#30340;&#25110;&#28151;&#21512;&#30340;(&#22402;&#30452;&#21644;&#27700;&#24179;&#37117;&#26377;)&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24314;&#27169;&#30340;&#30740;&#31350;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#27700;&#24179;&#25968;&#25454;&#20998;&#24067;&#19978;&#65292;&#32780;&#22402;&#30452;&#21644;&#28151;&#21512;&#26041;&#26696;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;FedEmb,&#29992;&#20110;&#24314;&#27169;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#24605;&#24819;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#65292;&#19982;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#21106;&#29305;&#24449;&#21644;&#20027;&#39064;&#31354;&#38388;&#20998;&#25955;&#38382;&#39064;&#65292;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#26292;&#38706;&#19979;&#65292;&#26174;&#31034;&#20102;0.3%&#21040;4.2%&#30340;&#25512;&#29702;&#20934;&#30830;&#24230;&#25552;&#39640;&#65292;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm for decentralized training of machine learning models on distributed clients, without revealing the data to the central server. The learning scheme may be horizontal, vertical or hybrid (both vertical and horizontal). Most existing research work with deep neural network (DNN) modelling is focused on horizontal data distributions, while vertical and hybrid schemes are much less studied. In this paper, we propose a generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based learning. The idea of our algorithm is characterised by higher inference accuracy, stronger privacy-preserving properties, and lower client-server communication bandwidth demands as compared with existing work. The experimental results show that FedEmb is an effective method to tackle both split feature &amp; subject space decentralized problems, shows 0.3% to 4.2% inference accuracy improvement with limited privacy revealing for datasets stored in local client
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2311.14115</link><description>&lt;p&gt;
&#20174;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#23494;&#24230;&#20272;&#35745;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14115
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;LHF&#65289;--&#23588;&#20854;&#26159;&#20174;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;--&#26368;&#36817;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22810;&#23558;&#20854;&#26694;&#26550;&#20026;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;LLM&#35270;&#20026;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#36827;&#34892;&#35843;&#25972;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35299;&#37322;&#65292;&#23427;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#20026;&#20013;&#24515;&#65292;&#24182;&#23558;LHF&#35270;&#20026;&#19968;&#20010;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#23545;&#20110;&#36890;&#36807;&#20559;&#22909;&#34892;&#20026;&#20998;&#24067;&#26041;&#31243;&#23450;&#20041;&#30340;&#19968;&#31867;&#29983;&#25104;&#36807;&#31243;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#8220;&#26631;&#27880;&#32773;&#38169;&#35823;&#8221;&#30340;&#30740;&#31350;&#32467;&#26524;--&#21363;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -failure cases where wro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#32454;&#24494;&#30340;&#24046;&#24322;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.06067</link><description>&lt;p&gt;
&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.06067
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#20013;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#32454;&#24494;&#30340;&#24046;&#24322;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21704;&#24076;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#23186;&#20307;&#25628;&#32034;&#20013;&#22240;&#20854;&#20302;&#23384;&#20648;&#21644;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#25551;&#36848;&#20855;&#26377;&#30456;&#20284;&#25972;&#20307;&#22806;&#35266;&#20294;&#32454;&#24494;&#24046;&#24322;&#30340;&#23545;&#35937;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#21704;&#24076;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#12290;&#29616;&#26377;&#30340;&#21704;&#24076;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#23545;&#30456;&#21516;&#30340;&#28145;&#23618;&#28608;&#27963;&#24352;&#37327;&#36827;&#34892;&#27880;&#24847;&#21147;&#24341;&#23548;&#26469;&#29983;&#25104;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#36825;&#38480;&#21046;&#20102;&#29305;&#24449;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#25551;&#36848;&#31526;&#26367;&#20195;&#27880;&#24847;&#21147;&#24341;&#23548;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23646;&#24615;&#20998;&#32452;&#21644;&#25366;&#25496;&#21704;&#24076;&#65288;AGMH&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#25551;&#36848;&#31526;&#20013;&#23545;&#31867;&#21035;&#29305;&#23450;&#30340;&#35270;&#35273;&#23646;&#24615;&#36827;&#34892;&#20998;&#32452;&#21644;&#23884;&#20837;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#26377;&#25928;&#32454;&#31890;&#24230;&#22270;&#20687;&#26816;&#32034;&#30340;&#32508;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#20998;&#25955;&#25439;&#22833;&#65288;ADL&#65289;&#26469;&#24378;&#21046;&#25551;&#36848;&#31526;&#20851;&#27880;&#21508;&#31181;&#23616;&#37096;&#21306;&#22495;&#65292;&#24182;&#25429;&#25417;&#22810;&#26679;&#30340;&#32454;&#24494;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, hashing methods have been popular in the large-scale media search for low storage and strong representation capabilities. To describe objects with similar overall appearance but subtle differences, more and more studies focus on hashing-based fine-grained image retrieval. Existing hashing networks usually generate both local and global features through attention guidance on the same deep activation tensor, which limits the diversity of feature representations. To handle this limitation, we substitute convolutional descriptors for attention-guided features and propose an Attributes Grouping and Mining Hashing (AGMH), which groups and embeds the category-specific visual attributes in multiple descriptors to generate a comprehensive feature representation for efficient fine-grained image retrieval. Specifically, an Attention Dispersion Loss (ADL) is designed to force the descriptors to attend to various local regions and capture diverse subtle details. Moreover, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19982;&#20799;&#31461;&#35266;&#30475;&#30340;&#20196;&#29260;&#25968;&#37327;&#30456;&#20284;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#20102;LLMs&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#22522;&#20934;&#21644;&#23545;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#30340;&#22797;&#21046;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2311.04666</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19982;&#20799;&#31461;&#35266;&#30475;&#30340;&#20196;&#29260;&#25968;&#37327;&#30456;&#20284;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#20102;LLMs&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#22522;&#20934;&#21644;&#23545;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#30340;&#22797;&#21046;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#25512;&#29702;&#21644;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20250;&#26597;&#30475;&#22823;&#37327;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;BabyLM&#20849;&#20139;&#20219;&#21153;&#23558;LLM&#30340;&#39044;&#35757;&#32451;&#19982;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#36827;&#34892;&#27604;&#36739;&#65292;13&#23681;&#23401;&#23376;&#30475;&#21040;&#30340;&#20196;&#29260;&#25968;&#37327;&#27604;LLMs&#30475;&#21040;&#30340;&#25968;&#37327;&#35201;&#23567;&#24471;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LLMs&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20351;&#29992;&#30340;&#20196;&#29260;&#25968;&#37327;&#19982;&#20799;&#31461;&#30475;&#21040;&#30340;&#24046;&#19981;&#22810;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#24378;&#22823;&#30340;&#22522;&#20934;&#65307;&#19981;&#21516;&#30340;&#26550;&#26500;&#12289;&#35780;&#20272;&#19981;&#21516;&#26102;&#26399;&#24615;&#33021;&#21464;&#21270;&#21644;&#25253;&#21578;&#30340;&#39044;&#35757;&#32451;&#25351;&#26631;&#65292;&#20197;&#21450;&#23581;&#35797;&#26494;&#25955;&#22797;&#21046;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#20197;&#35266;&#23519;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#22797;&#29616;&#24615;&#23545;&#35757;&#32451;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20005;&#26684;&#21644;&#20005;&#26684;&#23567;&#35268;&#27169;&#36335;&#24452;&#30340;&#25552;&#20132;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre-training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre-training to human language acquisition, where the number of tokens seen by 13-year-old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures, evaluation of changes in performance across epochs, and reported pre-training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict-small tracks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13500</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#22686;&#24378;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#30340;SGNN-LLM&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13500
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#29983;&#20869;&#23481;&#21019;&#20316;&#65292;&#23398;&#20064;&#32773;&#21512;&#20316;&#20855;&#26377;&#21487;&#25193;&#23637;&#25945;&#32946;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#23398;&#29983;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#23398;&#29983;&#21644;&#38382;&#39064;&#20132;&#20114;&#30340;&#22797;&#26434;&#32593;&#32476;&#65292;&#20294;&#22312;&#20919;&#21551;&#21160;&#26465;&#20214;&#19979;&#65292;&#20854;&#20013;&#23398;&#29983;&#23545;&#38382;&#39064;&#30340;&#26377;&#38480;&#21442;&#19982;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#23558;&#25972;&#21512;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#36129;&#29486;&#22312;&#20110;&#29983;&#25104;&#22522;&#30784;&#38382;&#39064;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;</title><link>http://arxiv.org/abs/2309.10744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#31579;&#36873;&#27979;&#35797;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome. (arXiv:2309.10744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#21644;&#35773;&#21050;&#26159;&#25105;&#20204;&#39640;&#24230;&#36827;&#21270;&#30340;&#31038;&#20132;&#27807;&#36890;&#25216;&#24039;&#30340;&#29645;&#36149;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30340;&#20799;&#31461;&#20247;&#25152;&#21608;&#30693;&#22312;&#29702;&#35299;&#35773;&#21050;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#20182;&#20204;&#20855;&#26377;&#36275;&#22815;&#29702;&#35299;&#38544;&#21947;&#30340;&#21475;&#35821;&#26234;&#21830;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#20197;&#21306;&#20998;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#21644;&#20854;&#20182;&#34920;&#29616;&#30456;&#20284;&#22806;&#37096;&#34892;&#20026;&#30340;&#30151;&#29366;&#65288;&#20363;&#22914;&#27880;&#24847;&#21147;&#32570;&#38519;/&#22810;&#21160;&#38556;&#30861;&#65289;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#26469;&#30740;&#31350;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#29702;&#35299;&#38544;&#21947;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#23545;&#35773;&#21050;&#29702;&#35299;&#30340;&#25913;&#36827;&#12290;&#36825;&#24847;&#21619;&#30528;&#26377;&#24517;&#35201;&#37319;&#21462;&#20854;&#20182;&#26041;&#27861;&#26469;&#20351;LLMs&#20855;&#22791;&#29702;&#35299;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#36825;&#24050;&#19982;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been asso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10149</link><description>&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#33021;&#21147;&#20998;&#26512;&#65306;&#36830;&#32493;&#23398;&#20064;&#32773;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;AI&#20195;&#29702;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25110;&#26426;&#22120;&#20154;&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#20174;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#30340;&#23454;&#38469;&#37096;&#32626;&#65292;&#20445;&#35777;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20445;&#30041;&#36807;&#21435;&#30340;&#32463;&#39564;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#32771;&#34385;&#21040;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#65292;&#20197;&#20272;&#35745;&#29615;&#22659;&#21464;&#21270;&#30340;&#39118;&#38505;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#23545;&#26410;&#30693;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#24615;&#33021;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#38543;&#20869;&#23384;&#22823;&#23567;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07520</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25214;&#21040;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#12290;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#24050;&#32463;&#34987;&#27979;&#37327;&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#20010;&#21464;&#37327;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#26377;&#26102;&#20027;&#35201;&#20851;&#27880;&#30340;&#21464;&#37327;&#24182;&#38750;&#30452;&#25509;&#21487;&#35266;&#23519;&#65292;&#32780;&#26159;&#36890;&#36807;&#23427;&#20204;&#22312;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#26469;&#25512;&#29702;&#20986;&#26469;&#30340;&#12290;&#36825;&#20123;&#34987;&#31216;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19968;&#20010;&#24191;&#27867;&#34987;&#30693;&#36947;&#30340;&#20363;&#23376;&#26159;&#24515;&#29702;&#26500;&#36896;&#30340;&#26234;&#21830;&#65292;&#22240;&#20026;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#65292;&#25152;&#20197;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#22914;&#26234;&#21830;&#27979;&#35797;&#26469;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#21644;&#28508;&#22312;&#21464;&#37327;&#19982;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#36830;&#25509;&#65292;&#20174;&#32780;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#32479;&#35745;&#19968;&#33268;&#24615;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>HomeRobot&#26159;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#31227;&#21160;&#25805;&#20316;&#30340;&#39034;&#24212;&#21147;&#26426;&#22120;&#20154;&#65292;&#21487;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#23548;&#33322;&#24182;&#25805;&#32437;&#21508;&#31181;&#29289;&#20307;&#65292;&#20197;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;HomeRobot OVMM&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20004;&#20010;&#32452;&#20214;&#36827;&#34892;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.11565</link><description>&lt;p&gt;
HomeRobot: &#24320;&#25918;&#35789;&#27719;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
HomeRobot: Open-Vocabulary Mobile Manipulation. (arXiv:2306.11565v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11565
&lt;/p&gt;
&lt;p&gt;
HomeRobot&#26159;&#19968;&#31181;&#24320;&#25918;&#35789;&#27719;&#31227;&#21160;&#25805;&#20316;&#30340;&#39034;&#24212;&#21147;&#26426;&#22120;&#20154;&#65292;&#21487;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#23548;&#33322;&#24182;&#25805;&#32437;&#21508;&#31181;&#29289;&#20307;&#65292;&#20197;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;HomeRobot OVMM&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#20004;&#20010;&#32452;&#20214;&#36827;&#34892;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HomeRobot(noun): &#19968;&#31181;&#20215;&#26684;&#21512;&#29702;&#30340;&#39034;&#24212;&#21147;&#26426;&#22120;&#20154;&#65292;&#21487;&#22312;&#23478;&#24237;&#20013;&#23548;&#33322;&#24182;&#25805;&#32437;&#21508;&#31181;&#29289;&#20307;&#65292;&#20197;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#24320;&#25918;&#35789;&#27719;&#31227;&#21160;&#25805;&#20316;&#65288;OVMM&#65289;&#26159;&#22312;&#20219;&#20309;&#26410;&#30693;&#29615;&#22659;&#20013;&#25441;&#36215;&#20219;&#20309;&#29289;&#20307;&#24182;&#25918;&#32622;&#22312;&#25351;&#23450;&#20301;&#32622;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#35201;&#25104;&#20026;&#20154;&#31867;&#29615;&#22659;&#20013;&#26377;&#29992;&#30340;&#21161;&#25163;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#24615;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#21508;&#20010;&#23376;&#38382;&#39064;&#65306;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#12289;&#23548;&#33322;&#21644;&#25805;&#32437;&#37117;&#26159;OVMM&#30340;&#20851;&#38190;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#25104;&#36215;&#26469;&#20063;&#38754;&#20020;&#30528;&#33258;&#24049;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;HomeRobot OVMM&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#20197;&#25441;&#36215;&#26032;&#22855;&#29289;&#20307;&#24182;&#25918;&#32622;&#22312;&#30446;&#26631;&#23481;&#22120;&#19978;&#12290;HomeRobot&#20855;&#26377;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#20223;&#30495;&#32452;&#20214;&#65292;&#20351;&#29992;&#26032;&#30340;&#39640;&#36136;&#37327;&#22810;&#25151;&#38388;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#38598;&#21512;; &#21644;&#19968;&#20010;&#23454;&#38469;&#30340;&#29289;&#29702;&#26426;&#22120;&#20154;&#32452;&#20214;&#65292;&#29992;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21270;&#21644;&#39640;&#38454;&#29305;&#24449;&#20256;&#36882;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;THNN&#65289;&#65292;&#29992;&#20110;&#20174;&#22343;&#21248;&#36229;&#22270;&#20013;&#25552;&#21462;&#39640;&#38454;&#20449;&#24687;&#12290;&#21516;&#26102;&#37319;&#29992;&#37096;&#20998;&#23545;&#31216;&#30340;CP&#20998;&#35299;&#26469;&#22788;&#29702;&#39640;&#38454;&#29305;&#24449;&#20256;&#36882;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.02560</link><description>&lt;p&gt;
Tensorized Hypergraph Neural Networks&#65288;&#24352;&#37327;&#21270;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;
&lt;/p&gt;
&lt;p&gt;
Tensorized Hypergraph Neural Networks. (arXiv:2306.02560v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21270;&#21644;&#39640;&#38454;&#29305;&#24449;&#20256;&#36882;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;THNN&#65289;&#65292;&#29992;&#20110;&#20174;&#22343;&#21248;&#36229;&#22270;&#20013;&#25552;&#21462;&#39640;&#38454;&#20449;&#24687;&#12290;&#21516;&#26102;&#37319;&#29992;&#37096;&#20998;&#23545;&#31216;&#30340;CP&#20998;&#35299;&#26469;&#22788;&#29702;&#39640;&#38454;&#29305;&#24449;&#20256;&#36882;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HGNN&#37117;&#20381;&#36182;&#20110;&#23545;&#36229;&#22270;&#36830;&#25509;&#27169;&#24335;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#39640;&#38454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37051;&#25509;&#24352;&#37327;&#30340;&#24352;&#37327;&#21270;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;THNN&#65289;&#12290;THNN&#26159;&#19968;&#20010;&#36890;&#36807;&#39640;&#38454;&#22806;&#31215;&#29305;&#24449;&#20256;&#36882;&#23454;&#29616;&#24544;&#23454;&#36229;&#22270;&#24314;&#27169;&#30340;&#26694;&#26550;&#65292;&#26159;&#37051;&#25509;&#30697;&#38453;&#20026;&#22522;&#30784;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#24352;&#37327;&#25193;&#23637;&#12290;&#25552;&#20986;&#30340;THNN&#31561;&#25928;&#20110;&#19968;&#20010;&#39640;&#38454;&#22810;&#39033;&#24335;&#22238;&#24402;&#26041;&#26696;&#65292;&#20351;&#24471;THNN&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#22343;&#21248;&#36229;&#22270;&#20013;&#25552;&#21462;&#39640;&#38454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#30452;&#25509;&#22788;&#29702;&#39640;&#38454;&#22806;&#31215;&#29305;&#24449;&#30340;&#25351;&#25968;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#37096;&#20998;&#23545;&#31216;&#30340;CP&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph neural networks (HGNN) have recently become attractive and received significant attention due to their excellent performance in various domains. However, most existing HGNNs rely on first-order approximations of hypergraph connectivity patterns, which ignores important high-order information. To address this issue, we propose a novel adjacency-tensor-based \textbf{T}ensorized \textbf{H}ypergraph \textbf{N}eural \textbf{N}etwork (THNN). THNN is a faithful hypergraph modeling framework through high-order outer product feature message passing and is a natural tensor extension of the adjacency-matrix-based graph neural networks. The proposed THNN is equivalent to a high-order polynomial regression scheme, which enables THNN with the ability to efficiently extract high-order information from uniform hypergraphs. Moreover, in consideration of the exponential complexity of directly processing high-order outer product features, we propose using a partially symmetric CP decomposition
&lt;/p&gt;</description></item><item><title>BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17100</link><description>&lt;p&gt;
BiomedGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17100
&lt;/p&gt;
&lt;p&gt;
BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#32500;&#25252;&#20013;&#19981;&#22815;&#28789;&#27963;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#32467;&#21512;&#29616;&#20195;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26377;&#28508;&#21147;&#35299;&#37322;&#19981;&#21516;&#30340;&#21307;&#30103;&#27169;&#24577;&#65292;&#24182;&#20135;&#29983;&#22914;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#25110;&#30142;&#30149;&#35786;&#26029;&#31561;&#34920;&#36798;&#24615;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BiomedGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#21270;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24320;&#28304;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;BiomedGPT&#22312;26&#20010;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#20020;&#24202;&#37325;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;16&#20010;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25918;&#23556;&#23398;&#20154;&#21592;&#35780;&#20272;&#20013;&#65292;&#23427;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-4 with vision&#65288;GPT-4V&#65289;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#36229;&#36807;&#20102;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#27492;&#22806;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, gr
&lt;/p&gt;</description></item><item><title>SLaDe&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#30495;&#23454;&#20195;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#12289;&#26080;&#20002;&#24323;&#35757;&#32451;&#21644;&#31867;&#22411;&#25512;&#26029;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.12520</link><description>&lt;p&gt;
SLaDe: &#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#27719;&#32534;&#20195;&#30721;&#30340;&#21487;&#31227;&#26893;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21453;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12520
&lt;/p&gt;
&lt;p&gt;
SLaDe&#26159;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#30495;&#23454;&#20195;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#12289;&#26080;&#20002;&#24323;&#35757;&#32451;&#21644;&#31867;&#22411;&#25512;&#26029;&#31561;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#32534;&#35793;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#20026;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#24037;&#20855;&#21487;&#20379;&#20351;&#29992;&#12290;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#29992;&#20110;&#23433;&#20840;&#20219;&#21153;&#21644;&#31227;&#26893;&#36951;&#30041;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#29983;&#25104;&#38590;&#20197;&#38405;&#35835;&#30340;&#31243;&#24207;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#25903;&#25345;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644;&#25351;&#20196;&#38598;&#26550;&#26500;&#12290;&#26368;&#36817;&#20851;&#27880;&#31070;&#32463;&#26041;&#27861;&#20135;&#29983;&#20102;&#33021;&#29983;&#25104;&#21487;&#35835;&#20195;&#30721;&#30340;&#21487;&#31227;&#26893;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#27809;&#26377;&#20248;&#21270;&#30340;&#21512;&#25104;&#31243;&#24207;&#65292;&#24182;&#19988;&#27809;&#26377;&#27169;&#22411;&#35780;&#20272;&#23427;&#20204;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#26356;&#26131;&#35835;&#65292;&#20294;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SLaDe&#65292;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#35757;&#32451;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21453;&#32534;&#35793;&#22120;&#65292;&#35813;&#21464;&#25442;&#22120;&#26159;&#20351;&#29992;&#23454;&#38469;&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35789;&#22120;&#65292;&#24182;&#21033;&#29992;&#26080;&#20002;&#24323;&#35757;&#32451;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21033;&#29992;&#31867;&#22411;&#25512;&#26029;&#29983;&#25104;&#27604;&#26631;&#20934;&#20998;&#26512;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#31070;&#32463;&#26041;&#27861;&#26356;&#26131;&#35835;&#21644;&#26356;&#20934;&#30830;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SemPPL&#65292;&#36890;&#36807;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23398;&#20064;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.05158</link><description>&lt;p&gt;
SemPPL: &#39044;&#27979;&#20266;&#26631;&#31614;&#20197;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SemPPL: Predicting pseudo-labels for better contrastive representations. (arXiv:2301.05158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SemPPL&#65292;&#36890;&#36807;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23398;&#20064;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;Semantic Positives via Pseudo-Labels (SemPPL)&#65292;&#23427;&#32467;&#21512;&#20102;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#27491;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#26469;&#21306;&#20998;&#20004;&#20010;&#26679;&#26412;&#26159;&#21542;&#20195;&#34920;&#30456;&#21516;&#30340;&#22522;&#30784;&#25968;&#25454;&#12290;&#20026;&#20102;&#20016;&#23500;&#27491;&#26679;&#26412;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;&#23569;&#37327;&#24050;&#26377;&#30340;&#30495;&#23454;&#26631;&#31614;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;k&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#30456;&#21516;&#20266;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#25193;&#23637;&#20026;&#27491;&#26679;&#26412;&#65292;&#24182;&#31216;&#20043;&#20026;&#35821;&#20041;&#27491;&#26679;&#26412;&#12290;&#25105;&#20204;&#21516;&#26102;&#23398;&#20064;&#34920;&#31034;&#21644;&#39044;&#27979;&#33258;&#21160;&#22686;&#24378;&#30340;&#20266;&#26631;&#31614;&#65292;&#24418;&#25104;&#19968;&#20010;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semi-supervised learning method, Semantic Positives via Pseudo-Labels (SemPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning -where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives) -- with a novel approach to selecting positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing ones through a $k$-nearest neighbours classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudo-labels. This creates a reinforcing cycle. Strong init
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.01829</link><description>&lt;p&gt;
t-SMILES&#65306;&#29992;&#20110;&#20840;&#26032;&#20998;&#23376;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#12289;&#22522;&#20110;&#30862;&#29255;&#30340;&#22810;&#23610;&#24230;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65288;&#22522;&#20110;&#26641;&#30340;SMILES&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#31639;&#27861;&#65306;TSSA&#65288;&#24102;&#26377;&#20849;&#20139;&#21407;&#23376;&#30340;t-SMILES&#65289;&#12289;TSDY&#65288;&#24102;&#26377;&#34394;&#25311;&#21407;&#23376;&#30340;t-SMILES&#65289;&#21644;TSID&#65288;&#24102;&#26377;ID&#30340;t-SMILES&#65289;&#12290;&#23427;&#20351;&#29992;&#20174;&#20998;&#23376;&#22270;&#30340;&#30862;&#29255;&#24418;&#25104;&#30340;&#20840;&#20108;&#21449;&#26641;&#19978;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#24471;&#21040;&#30340;SMILES&#31867;&#22411;&#23383;&#31526;&#20018;&#26469;&#25551;&#36848;&#20998;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;JTVAE&#12289;BRICS&#12289;MMPA&#21644;Scaffold&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#26500;&#24314;&#22810;&#20195;&#30721;&#20998;&#23376;&#25551;&#36848;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#21508;&#31181;&#25551;&#36848;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#35770;&#27169;&#22411;&#26159;&#21407;&#22987;&#30340;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#36824;&#26159;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#12290;&#23427;&#22312;goa&#31561;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;SMILES&#12289;DeepSMILES&#12289;SELFIES&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective representation of molecules is a crucial factor affecting the performance of artificial intelligence models. This study introduces a flexible, fragment-based, multiscale molecular representation framework called t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It describes molecules using SMILES-type strings obtained by performing a breadth-first search on a full binary tree formed from a fragmented molecular graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the feasibility to construct a multi-code molecular description system, where various descriptions complement each other, enhancing the overall performance. Additionally, it exhibits impressive performance on low-resource datasets, whether the model is original, data augmented, or pre-training fine-tuned. It significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline models in goa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2208.14741</link><description>&lt;p&gt;
&#22833;&#36133;&#30446;&#26631;&#24863;&#30693;&#30340;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;
&lt;/p&gt;
&lt;p&gt;
Failed Goal Aware Hindsight Experience Replay. (arXiv:2208.14741v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#27169;&#22411;&#23545;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#21644;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#33719;&#24471;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20351;&#29992;&#31232;&#30095;&#20108;&#20803;&#22870;&#21169;&#35757;&#32451;&#20195;&#29702;&#65292;&#30001;&#20110;&#32570;&#20047;&#25104;&#21151;&#30340;&#32463;&#39564;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20107;&#21518;&#32463;&#39564;&#37325;&#25773;&#65288;HER&#65289;&#20174;&#19981;&#25104;&#21151;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#20174;&#22343;&#21248;&#25277;&#26679;&#30340;&#32463;&#39564;&#20013;&#29983;&#25104;&#25104;&#21151;&#30340;&#32463;&#39564;&#30340;&#36807;&#31243;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Failed goal Aware HER (FAHER)&#30340;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#37319;&#26679;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23454;&#29616;&#30446;&#26631;&#19982;&#26410;&#23454;&#29616;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#27169;&#22411;&#23545;&#20855;&#26377;&#19981;&#21516;&#23454;&#29616;&#30446;&#26631;&#30340;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#22312;HER&#30340;&#26041;&#24335;&#19979;&#23545;&#32463;&#39564;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-goal reinforcement learning for a given environment, agents learn policies to achieve multiple goals by using experiences gained from interactions with the environment. One of the key challenges in this setting is training agents using sparse binary rewards, which can be difficult due to a lack of successful experiences. To address this challenge, hindsight experience replay (HER) generates successful experiences from unsuccessful experiences. However, the process of generating successful experiences from uniformly sampled ones can be inefficient. In this paper, a novel approach called Failed goal Aware HER (FAHER) is proposed to enhance the sampling efficiency. The approach exploits the property of achieved goals in relation to failed goals that are defined as the original goals not achieved. The proposed method involves clustering episodes with different achieved goals using a cluster model and subsequently sampling experiences in the manner of HER. The cluster model is gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#35270;&#35273;&#32463;&#39564;&#30340;&#26041;&#27861;&#26469;&#32553;&#23567;&#29983;&#29289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#20154;&#31867;&#30340;&#35270;&#35273;&#32463;&#39564;&#20013;&#30340;&#21464;&#21270;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#21464;&#25442;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.07802</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#35270;&#35273;&#32463;&#39564;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalization by mimicking the human visual diet. (arXiv:2206.07802v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#35270;&#35273;&#32463;&#39564;&#30340;&#26041;&#27861;&#26469;&#32553;&#23567;&#29983;&#29289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#20154;&#31867;&#30340;&#35270;&#35273;&#32463;&#39564;&#20013;&#30340;&#21464;&#21270;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#23545;&#30495;&#23454;&#19990;&#30028;&#21464;&#25442;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#29992;&#20110;&#24357;&#21512;&#29983;&#29289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317; - &#27169;&#20223;&#20154;&#31867;&#30340;&#35270;&#35273;&#32463;&#39564;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20381;&#36182;&#20110;&#20114;&#32852;&#32593;&#32593;&#31449;&#19978;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#20154;&#31867;&#26159;&#36890;&#36807;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#26377;&#38480;3D&#22330;&#26223;&#20013;&#23398;&#20064;&#65292;&#24182;&#19988;&#25509;&#35302;&#21040;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#21464;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#65288;&#35270;&#35273;&#32463;&#39564;&#65289;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21464;&#21270;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#21464;&#25442;&#65288;&#22914;&#20809;&#29031;&#12289;&#35270;&#35282;&#21644;&#26448;&#26009;&#21464;&#21270;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36824;&#25193;&#23637;&#21040;&#20174;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#27867;&#21270; - &#25152;&#26377;&#32463;&#36807;&#20154;&#31867;&#35270;&#35273;&#32463;&#39564;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#22343;&#34920;&#29616;&#20248;&#20110;&#19987;&#38376;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#23454;&#39564;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#22330;&#26223;&#19978;&#19979;&#25991;&#21644;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#21464;&#25442;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#35270;&#35273;&#32463;&#39564;&#65292;&#20197;&#21450;&#19968;&#20010;&#38024;&#23545;&#35813;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#21464;&#25442;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new perspective on bridging the generalization gap between biological and computer vision -- mimicking the human visual diet. While computer vision models rely on internet-scraped datasets, humans learn from limited 3D scenes under diverse real-world transformations with objects in natural context. Our results demonstrate that incorporating variations and contextual cues ubiquitous in the human visual training data (visual diet) significantly improves generalization to real-world transformations such as lighting, viewpoint, and material changes. This improvement also extends to generalizing from synthetic to real-world data -- all models trained with a human-like visual diet outperform specialized architectures by large margins when tested on natural image data. These experiments are enabled by our two key contributions: a novel dataset capturing scene context and diverse real-world transformations to mimic the human visual diet, and a transformer model tailored to leverag
&lt;/p&gt;</description></item></channel></rss>