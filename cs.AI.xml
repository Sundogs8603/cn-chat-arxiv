<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.04232</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#24773;&#24863;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Agents Predict Emotion?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#35768;&#22810;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#20849;&#24773;&#29702;&#35299;&#21644;&#24773;&#32490;&#29366;&#24577;&#23578;&#26410;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;LLM&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24773;&#22659;&#20013;&#29702;&#35299;&#26032;&#32463;&#39564;&#65292;&#26681;&#25454;&#24773;&#32490;&#35780;&#20272;&#29702;&#35770;&#65292;&#36825;&#23545;&#20110;&#24773;&#32490;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#26234;&#33021;&#20307;&#23558;&#26032;&#32463;&#39564;&#24863;&#30693;&#20026;&#26102;&#38388;&#24207;&#21015;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#24863;&#30693;&#27599;&#20010;&#26032;&#36755;&#20837;&#21518;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#36807;&#21435;&#30456;&#20851;&#35760;&#24518;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#8220;&#35268;&#33539;&#8221;&#65292;&#24182;&#23558;&#26032;&#32463;&#39564;&#19982;&#27492;&#35268;&#33539;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#26234;&#33021;&#20307;&#22914;&#20309;&#22312;&#24773;&#22659;&#20013;&#23545;&#26032;&#32463;&#39564;&#20570;&#20986;&#21453;&#24212;&#12290;&#20351;&#29992;&#24773;&#24863;&#27979;&#35797;PANAS&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#27979;&#35797;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#22312;&#26032;&#32463;&#39564;&#21518;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#40060;&#31867;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#21644;&#31070;&#32463;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#32676;&#20307;&#36867;&#36920;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#39062;&#30340;&#21463;&#40060;&#31867;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#21644;&#31070;&#32463;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26234;&#33021;&#32676;&#20307;&#36867;&#36920;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04228
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#40060;&#31867;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#21644;&#31070;&#32463;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#32676;&#20307;&#36867;&#36920;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40060;&#32676;&#36890;&#36807;&#31616;&#21333;&#30340;&#20010;&#20307;&#20114;&#21160;&#23454;&#29616;&#39640;&#25928;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#21487;&#20197;&#38598;&#20307;&#36801;&#24473;&#21644;&#21160;&#24577;&#36867;&#31163;&#25429;&#39135;&#32773;&#12290;&#40060;&#32676;&#30340;&#34892;&#20026;&#36890;&#24120;&#21487;&#20197;&#28608;&#21457;&#35774;&#35745;&#32676;&#20307;&#26426;&#22120;&#20154;&#25511;&#21046;&#26550;&#26500;&#30340;&#28789;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40060;&#31867;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#23454;&#29616;&#32676;&#20307;&#36867;&#31163;&#30340;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;(BINN)&#36890;&#36807;&#21560;&#24341;&#21147;&#21644;&#26021;&#21147;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#26080;&#30896;&#25758;&#30340;&#36867;&#36920;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#20026;&#24212;&#23545;&#21160;&#24577;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#33258;&#36866;&#24212;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#32676;&#20307;&#26426;&#22120;&#20154;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#31867;&#20284;&#20110;&#40060;&#31867;&#30340;&#36867;&#36920;&#26426;&#21160;&#65292;&#27169;&#25311;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32676;&#20307;&#26426;&#22120;&#20154;&#33021;&#22815;&#38598;&#20307;&#36828;&#31163;&#23041;&#32961;&#12290;&#22810;&#20010;&#23545;&#27604;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04210</link><description>&lt;p&gt;
&#8220;&#20219;&#21153;&#25104;&#21151;&#8221;&#36828;&#36828;&#19981;&#22815;&#65306;&#25506;&#31350;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#20197;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
"Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25277;&#26679;&#26377;&#24847;&#20041;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#24456;&#26377;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#24403;&#27169;&#22411;&#19982;&#22806;&#37096;&#39564;&#35777;&#32773;&#32467;&#21512;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#21453;&#39304;&#36880;&#27493;&#25110;&#36880;&#28176;&#24471;&#20986;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#23427;&#20204;&#30340;&#23436;&#20840;&#33021;&#21147;&#26356;&#22909;&#22320;&#34987;&#21033;&#29992;&#12290;&#22312;&#20855;&#36523;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#65292;&#39564;&#35777;&#36890;&#24120;&#20165;&#28041;&#21450;&#35780;&#20272;&#25351;&#20196;&#20013;&#25351;&#23450;&#30340;&#30446;&#26631;&#26465;&#20214;&#26159;&#21542;&#24050;&#28385;&#36275;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#20195;&#29702;&#32773;&#26080;&#32541;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#26356;&#24191;&#27867;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#65292;&#36229;&#36234;&#20165;&#20219;&#21153;&#25104;&#21151;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#35880;&#24910;&#22320;&#25235;&#20303;&#38754;&#21253;&#65292;&#20197;&#36991;&#20813;&#26126;&#26174;&#30340;&#21464;&#24418;&#65289;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26080;&#38480;&#33539;&#22260;&#65292;&#26500;&#24314;&#31867;&#20284;&#20110;&#29992;&#20110;&#26174;&#24335;&#30693;&#35782;&#20219;&#21153;&#65288;&#22914;&#22260;&#26827;&#21644;&#23450;&#29702;&#35777;&#26126;&#65289;&#30340;&#33050;&#26412;&#21270;&#39564;&#35777;&#22120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27809;&#26377;&#21487;&#38752;&#30340;&#39564;&#35777;&#32773;&#21487;&#29992;&#26102;&#65292;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#20195;&#29702;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#22312;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#21457;&#23637;&#20026;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;&#24613;&#24615;&#32958;&#25439;&#20260;&#12290;&#20869;&#22806;&#37096;&#39564;&#35777;&#21644;&#20122;&#32452;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04209</link><description>&lt;p&gt;
&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#24613;&#24615;&#32958;&#25439;&#20260;&#39044;&#27979;&#65306;&#22238;&#39038;&#24615;&#22806;&#37096;&#21644;&#20869;&#37096;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04209
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#22312;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#21457;&#23637;&#20026;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;&#24613;&#24615;&#32958;&#25439;&#20260;&#12290;&#20869;&#22806;&#37096;&#39564;&#35777;&#21644;&#20122;&#32452;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#65292;&#21363;&#32958;&#25490;&#27844;&#21151;&#33021;&#19979;&#38477;&#65292;&#22312;&#20303;&#38498;&#24739;&#32773;&#20013;&#21457;&#29983;&#30340;&#27604;&#20363;&#39640;&#36798;18%&#12290;AKI&#30340;&#36827;&#23637;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36870;&#30340;&#32958;&#33039;&#25439;&#20260;&#12290;&#26041;&#27861;&#65306;&#26412;&#22238;&#39038;&#24615;&#38431;&#21015;&#30740;&#31350;&#21253;&#25324;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#30103;&#20013;&#24515;&#65288;UPMC&#65289;&#30340;&#38750;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#25509;&#21463;&#27835;&#30103;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;n=46,815&#65289;&#65292;&#20197;&#21450;&#22312;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#65288;UFH&#65289;&#25509;&#21463;&#27835;&#30103;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;n=127,202&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#36827;&#23637;&#33267;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;AKI&#12290;&#25105;&#20204;&#20998;&#21035;&#20026;&#21508;&#20010;&#21307;&#38498;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65288;&#22312;UFH&#19978;&#35757;&#32451;&#30340;UFH&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;UPMC&#19978;&#35757;&#32451;&#30340;UPMC&#27169;&#22411;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#21307;&#38498;&#30340;&#21457;&#23637;&#38431;&#21015;&#24739;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#27169;&#22411;&#65288;UFH-UPMC&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#21307;&#38498;&#20869;&#22806;&#37096;&#39564;&#35777;&#20102;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#20122;&#32452;&#20998;&#26512;&#12290;&#32467;&#26524;&#65306;UFH&#21644;UPMC&#30340;&#24739;&#32773;&#20013;&#65292;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;AKI&#20998;&#21035;&#21457;&#29983;&#22312;3%&#65288;n=3,257&#65289;&#21644;8%&#65288;n=2,296&#65289;&#30340;&#24739;&#32773;&#20013;&#12290;roc&#31215;&#20998;&#29575;
&lt;/p&gt;
&lt;p&gt;
Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the rec
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#30340;&#20960;&#20309;&#33021;&#21147;&#26469;&#33258;&#20110;&#24515;&#29702;&#34920;&#24449;&#20013;&#30340;&#31163;&#25955;&#31526;&#21495;&#32467;&#26500;&#65292;&#32780;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22797;&#26434;&#24615;&#12289;&#35268;&#21017;&#24615;&#21644;&#37096;&#20214;&#19982;&#20851;&#31995;&#30340;&#24863;&#30693;&#25935;&#24863;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25277;&#35937;&#20960;&#20309;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04203</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#31867;&#20154;&#20960;&#20309;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Human-Like Geometric Abstraction in Large Pre-trained Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04203
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#20960;&#20309;&#33021;&#21147;&#26469;&#33258;&#20110;&#24515;&#29702;&#34920;&#24449;&#20013;&#30340;&#31163;&#25955;&#31526;&#21495;&#32467;&#26500;&#65292;&#32780;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22797;&#26434;&#24615;&#12289;&#35268;&#21017;&#24615;&#21644;&#37096;&#20214;&#19982;&#20851;&#31995;&#30340;&#24863;&#30693;&#25935;&#24863;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25277;&#35937;&#20960;&#20309;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#21644;&#25805;&#20316;&#25277;&#35937;&#32467;&#26500;&#26041;&#38754;&#25317;&#26377;&#38750;&#20961;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#20960;&#20309;&#39046;&#22495;&#34920;&#29616;&#23588;&#20026;&#26126;&#26174;&#12290;&#26368;&#36817;&#30340;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#65292;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#20154;&#31867;&#30340;&#20960;&#20309;&#33021;&#21147;&#26469;&#33258;&#20110;&#24515;&#29702;&#34920;&#24449;&#20013;&#30340;&#31163;&#25955;&#31526;&#21495;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#24403;&#26631;&#20934;&#26550;&#26500;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#19978;&#25193;&#22823;&#21518;&#65292;&#31070;&#32463;&#32593;&#32476;&#24320;&#22987;&#23637;&#29616;&#20986;&#26356;&#20855;&#20154;&#31867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#35748;&#30693;&#31185;&#23398;&#20013;&#20851;&#20110;&#20960;&#20309;&#35270;&#35273;&#22788;&#29702;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20309;&#35270;&#35273;&#22788;&#29702;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20559;&#35265;&#65306;&#23545;&#22797;&#26434;&#24615;&#12289;&#35268;&#21017;&#24615;&#21644;&#37096;&#20214;&#19982;&#20851;&#31995;&#30340;&#24863;&#30693;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#25991;&#29486;&#20013;&#25506;&#27979;&#36825;&#20123;&#20559;&#35265;&#30340;&#20154;&#31867;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;AI&#20013;&#20351;&#29992;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#31867;&#20284;&#20154;&#31867;&#30340;&#25277;&#35937;&#20960;&#20309;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.
&lt;/p&gt;</description></item><item><title>COPS&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30701;&#20449;&#35784;&#39575;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;&#35774;&#22791;&#20869;&#31649;&#36947;&#65292;&#36890;&#36807;&#26234;&#33021;&#35782;&#21035;&#27450;&#35784;&#20449;&#24687;&#21644;URL&#29305;&#24449;&#65292;&#24182;&#23454;&#26102;&#35686;&#31034;&#29992;&#25143;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#65292;COPS&#22312;&#30701;&#20449;&#35784;&#39575;&#21644;URL&#38035;&#40060;&#26816;&#27979;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;98.15%&#21644;99.5%&#12290;</title><link>https://arxiv.org/abs/2402.04173</link><description>&lt;p&gt;
COPS&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30701;&#20449;&#35784;&#39575;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;&#35774;&#22791;&#20869;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
COPS: A Compact On-device Pipeline for real-time Smishing detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04173
&lt;/p&gt;
&lt;p&gt;
COPS&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#30701;&#20449;&#35784;&#39575;&#26816;&#27979;&#30340;&#32039;&#20945;&#22411;&#35774;&#22791;&#20869;&#31649;&#36947;&#65292;&#36890;&#36807;&#26234;&#33021;&#35782;&#21035;&#27450;&#35784;&#20449;&#24687;&#21644;URL&#29305;&#24449;&#65292;&#24182;&#23454;&#26102;&#35686;&#31034;&#29992;&#25143;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#65292;COPS&#22312;&#30701;&#20449;&#35784;&#39575;&#21644;URL&#38035;&#40060;&#26816;&#27979;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;98.15%&#21644;99.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20960;&#20046;&#21487;&#20197;&#23436;&#25104;&#25152;&#26377;&#20107;&#24773;&#65292;&#20174;&#27807;&#36890;&#21040;&#22312;&#32447;&#36141;&#29289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#37327;&#30340;&#22686;&#21152;&#65292;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#30340;&#32593;&#32476;&#29359;&#32618;&#20063;&#22312;&#28608;&#22686;&#12290;&#29305;&#21035;&#26159;&#30701;&#20449;&#35784;&#39575;&#25915;&#20987;&#65292;&#22312;&#36817;&#24180;&#26469;&#35266;&#23519;&#21040;&#26174;&#33879;&#22686;&#38271;&#12290;&#36825;&#20010;&#38382;&#39064;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#29359;&#32618;&#20998;&#23376;&#27599;&#22825;&#21019;&#24314;&#26032;&#30340;&#27450;&#39575;&#24615;&#32593;&#31449;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#32593;&#31449;&#30340;&#24179;&#22343;&#29983;&#21629;&#21608;&#26399;&#19981;&#21040;15&#23567;&#26102;&#12290;&#36825;&#20351;&#24471;&#32500;&#25252;&#19968;&#20010;&#24694;&#24847;URL&#25968;&#25454;&#24211;&#30340;&#26631;&#20934;&#20570;&#27861;&#21464;&#24471;&#26080;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#22791;&#20869;&#31649;&#36947;&#65306;COPS&#65292;&#23427;&#33021;&#22815;&#26234;&#33021;&#22320;&#35782;&#21035;&#27450;&#35784;&#24615;&#20449;&#24687;&#21644;URL&#30340;&#29305;&#24449;&#65292;&#23454;&#26102;&#35686;&#31034;&#29992;&#25143;&#12290;COPS&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#31649;&#36947;&#65292;&#20854;&#20013;&#30340;&#26816;&#27979;&#27169;&#22359;&#22522;&#20110;&#22823;&#23567;&#20026;3.46MB&#30340;&#35299;&#32544;&#32467;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#30701;&#20449;&#35784;&#39575;&#21644;URL&#38035;&#40060;&#26816;&#27979;&#65292;&#24182;&#22312;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;98.15&#65285;&#21644;99.5&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;&#37117;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive ra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CodeCompose&#65292;&#19968;&#27454;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#21019;&#20316;&#24037;&#20855;&#65292;&#23558;&#21333;&#34892;&#24314;&#35758;&#25193;&#23637;&#20026;&#22810;&#34892;&#24314;&#35758;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#22810;&#20010;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22810;&#34892;&#24314;&#35758;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#23545;&#24320;&#21457;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04141</link><description>&lt;p&gt;
&#22810;&#34892;AI&#36741;&#21161;&#20195;&#30721;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multi-line AI-assisted Code Authoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;CodeCompose&#65292;&#19968;&#27454;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#21019;&#20316;&#24037;&#20855;&#65292;&#23558;&#21333;&#34892;&#24314;&#35758;&#25193;&#23637;&#20026;&#22810;&#34892;&#24314;&#35758;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#22810;&#20010;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#22810;&#34892;&#24314;&#35758;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#23545;&#24320;&#21457;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CodeCompose&#26159;&#19968;&#27454;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#21019;&#20316;&#24037;&#20855;&#65292;&#20026;Meta&#30340;&#25968;&#19975;&#21517;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20869;&#32852;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22914;&#20309;&#23558;&#35813;&#20135;&#21697;&#20174;&#26174;&#31034;&#21333;&#34892;&#24314;&#35758;&#25193;&#23637;&#21040;&#22810;&#34892;&#24314;&#35758;&#12290;&#36825;&#19968;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#25913;&#36827;&#36825;&#20123;&#24314;&#35758;&#23545;&#24320;&#21457;&#20154;&#21592;&#21487;&#29992;&#24615;&#30340;&#22810;&#20010;&#29420;&#29305;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#34892;&#24314;&#35758;&#21487;&#33021;&#20135;&#29983;&#30340;&#8220;&#21050;&#32819;&#8221;&#25928;&#26524;&#65292;&#22240;&#20026;LLM&#30340;&#24314;&#35758;&#20250;&#19981;&#26029;&#31227;&#21160;&#24320;&#21457;&#20154;&#21592;&#24050;&#26377;&#30340;&#20195;&#30721;&#65292;&#20174;&#32780;&#38477;&#20302;&#29983;&#20135;&#21147;&#21644;&#28385;&#24847;&#24230;&#12290;&#20854;&#27425;&#65292;&#29983;&#25104;&#22810;&#34892;&#24314;&#35758;&#38656;&#35201;&#26356;&#38271;&#30340;&#26102;&#38388;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#39033;&#21019;&#26032;&#25237;&#36164;&#65292;&#20197;&#20943;&#23569;&#29992;&#25143;&#24863;&#30693;&#30340;&#24310;&#36831;&#12290;&#36825;&#20123;&#27169;&#22411;&#25176;&#31649;&#20248;&#21270;&#23558;&#22810;&#34892;&#24314;&#35758;&#30340;&#24310;&#36831;&#26102;&#38388;&#21152;&#24555;&#20102;2.5&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25968;&#19975;&#21517;&#24037;&#31243;&#24072;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#22810;&#34892;&#24314;&#35758;&#23545;&#24320;&#21457;&#20154;&#21592;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggesti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04140</link><description>&lt;p&gt;
&#25512;&#36827;&#27861;&#24459;&#25512;&#29702;&#65306;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#22788;&#29702;&#20840;&#29699;&#27861;&#29702;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20559;&#35265;&#30340;&#21322;&#33258;&#21160;&#21270;&#20210;&#35009;&#27969;&#31243;&#65288;SAAPs&#65289;
&lt;/p&gt;
&lt;p&gt;
Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;&#21253;&#25324;&#32654;&#22269;&#12289;&#33521;&#22269;&#12289;&#21346;&#26106;&#36798;&#12289;&#29790;&#20856;&#21644;&#39321;&#28207;&#22312;&#20869;&#30340;&#20116;&#20010;&#22269;&#23478;&#30340;&#27861;&#38498;&#21028;&#20915;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#27861;&#24459;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20132;&#21449;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35782;&#21035;&#20154;&#31867;&#20559;&#35265;&#21644;&#20419;&#36827;&#27861;&#38498;&#21028;&#20915;&#30340;&#22810;&#26041;&#35770;&#35777;&#30340;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#35282;&#33394;&#65292;&#20174;&#32780;&#30830;&#20445;&#27861;&#24459;&#22312;&#21508;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#21644;&#36328;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#21644;&#26032;&#24341;&#20837;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#26694;&#26550;&#65292;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#20197;ALMs&#20026;&#22522;&#30784;&#30340;&#22522;&#20110;Grounded Theory&#30340;&#27861;&#23398;&#23454;&#36341;&#30740;&#31350;&#35774;&#35745;&#12290;SHIRLEY&#26159;&#22522;&#20110;OpenAI&#30340;GPT&#25216;&#26415;&#26500;&#24314;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20027;&#35201;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;&#27861;&#24459;&#20915;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#33258;&#21160;&#20998;&#37197;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04108</link><description>&lt;p&gt;
&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#23618;&#24310;&#36831;&#24402;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#33258;&#21160;&#20998;&#37197;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#25351;&#20196;&#35201;&#27714;&#23545;&#21015;&#36710;&#24310;&#36831;&#36827;&#34892;&#31995;&#32479;&#24615;&#36319;&#36394;&#12290;&#22312;&#29790;&#20856;&#65292;&#29790;&#20856;&#20132;&#36890;&#31649;&#29702;&#23616;&#27880;&#20876;&#24182;&#20998;&#37197;&#36866;&#24403;&#30340;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#26159;&#25163;&#21160;&#20998;&#37197;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#36827;&#34892;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#20998;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#12290;&#20351;&#29992;TF-IDF&#36716;&#25442;&#25991;&#26412;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#19982;&#38543;&#26426;&#22343;&#21248;&#20998;&#31867;&#22120;&#20197;&#21450;&#29790;&#20856;&#20132;&#36890;&#31649;&#29702;&#23616;&#30340;&#20998;&#31867;&#24615;&#33021;&#36827;&#34892;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20998;&#23618;&#21644;&#25153;&#24179;&#20004;&#31181;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#22343;&#21248;&#20998;&#31867;&#22120;&#65292;&#20294;&#34920;&#29616;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;RFM&#26694;&#26550;&#21644;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.04103</link><description>&lt;p&gt;
&#33521;&#22269;&#38646;&#21806;&#24066;&#22330;&#30340;&#23458;&#25143;&#20998;&#32676;&#31639;&#27861;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;RFM&#26694;&#26550;&#21644;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#36141;&#20080;&#30340;&#24847;&#35782;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#32447;&#38646;&#21806;&#24179;&#21488;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#23458;&#25143;&#36141;&#20080;&#34892;&#20026;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#38656;&#27714;&#20063;&#22240;&#27492;&#20135;&#29983;&#12290;&#38646;&#21806;&#20844;&#21496;&#38754;&#20020;&#30528;&#22788;&#29702;&#22823;&#37327;&#23458;&#25143;&#36141;&#20080;&#30340;&#21387;&#21147;&#65292;&#36825;&#38656;&#35201;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#23458;&#25143;&#20998;&#32676;&#26041;&#27861;&#12290;&#23458;&#25143;&#20998;&#32676;&#26159;&#19968;&#31181;&#24066;&#22330;&#20998;&#26512;&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#20013;&#24515;&#26381;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#30408;&#21033;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#25913;&#21892;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#26469;&#33258;UCI&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38646;&#21806;&#25968;&#25454;&#38598;&#21253;&#21547;541,909&#20010;&#23458;&#25143;&#35760;&#24405;&#21644;8&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;RFM&#65288;&#26368;&#36817;&#24615;&#12289;&#39057;&#29575;&#21644;&#36135;&#24065;&#65289;&#26694;&#26550;&#26469;&#37327;&#21270;&#23458;&#25143;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering alg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#20998;&#26512;&#36827;&#34892;&#20998;&#26512;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#21487;&#25191;&#34892;&#25991;&#20214;&#20998;&#25104;&#19981;&#21516;&#37096;&#20998;&#65292;&#24182;&#23558;&#27599;&#20010;&#37096;&#20998;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#22788;&#29702;&#65292;&#26368;&#32456;&#24471;&#21040;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20998;&#25968;&#65292;&#24182;&#25552;&#20379;&#23545;&#27599;&#20010;&#37096;&#20998;&#37325;&#35201;&#24615;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.04102</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Use of Multi-CNNs for Section Analysis in Static Malware Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#38745;&#24577;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#20998;&#26512;&#36827;&#34892;&#20998;&#26512;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#21487;&#25191;&#34892;&#25991;&#20214;&#20998;&#25104;&#19981;&#21516;&#37096;&#20998;&#65292;&#24182;&#23558;&#27599;&#20010;&#37096;&#20998;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#22788;&#29702;&#65292;&#26368;&#32456;&#24471;&#21040;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20998;&#25968;&#65292;&#24182;&#25552;&#20379;&#23545;&#27599;&#20010;&#37096;&#20998;&#37325;&#35201;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30740;&#31350;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#26816;&#27979;&#29575;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20102;&#35299;&#31639;&#27861;&#30340;&#32467;&#26524;&#25110;&#33719;&#21462;&#26356;&#22810;&#20449;&#24687;&#20063;&#24456;&#37325;&#35201;&#65292;&#27604;&#22914;&#23545;&#20110;&#20998;&#26512;&#21592;&#26469;&#35828;&#65292;&#22312;&#25991;&#20214;&#20013;&#35201;&#35843;&#26597;&#30340;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;&#21487;&#31227;&#26893;&#21487;&#25191;&#34892;&#25991;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#20214;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#37096;&#20998;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37096;&#20998;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#20197;&#20415;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19987;&#38376;&#22788;&#29702;&#27599;&#20010;&#34987;&#35782;&#21035;&#30340;&#37096;&#20998;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;CNN&#36820;&#22238;&#30340;&#25152;&#26377;&#36825;&#20123;&#20998;&#25968;&#35745;&#31639;&#26368;&#32456;&#26816;&#27979;&#20998;&#25968;&#65292;&#20351;&#29992;&#27169;&#22411;&#26469;&#25913;&#36827;&#25105;&#20204;&#23545;&#27599;&#20010;&#37096;&#20998;&#22312;&#26368;&#32456;&#20998;&#25968;&#20013;&#30340;&#37325;&#35201;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section. Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04088</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Use of a Large Language Model for Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#30427;&#34892;&#20026;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#32593;&#32476;&#27450;&#20940;&#28192;&#36947;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#27450;&#20940;&#26159;&#24403;&#20170;&#32593;&#32476;&#19990;&#30028;&#20013;&#26368;&#26222;&#36941;&#30340;&#29616;&#35937;&#65292;&#23545;&#20844;&#27665;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#36825;&#23601;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#31995;&#32479;&#65292;&#20197;&#38459;&#27490;&#22312;&#32447;&#35770;&#22363;&#12289;&#21338;&#23458;&#21644;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#27450;&#20940;&#20869;&#23481;&#65292;&#20197;&#31649;&#29702;&#20854;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#19981;&#31283;&#23450;&#12290;&#36817;&#24180;&#26469;&#65292;&#20687;BERT&#21644;RoBERTa&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#22312;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#29616;&#26377;&#30740;&#31350;&#65288;Formspring&#21644;Twitter&#65289;&#20013;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#35757;&#32451;&#26041;&#27861;&#30340;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#31216;&#20026;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#29305;&#24449;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04087</link><description>&lt;p&gt;
&#19968;&#20010;&#38590;&#20197;&#36229;&#36234;&#30340;&#22522;&#32447;&#29992;&#20110;&#26080;&#38656;&#35757;&#32451;&#30340;CLIP&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#35757;&#32451;&#26041;&#27861;&#30340;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#31216;&#20026;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#29305;&#24449;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22240;&#20854;&#26174;&#33879;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#65292;&#20197;&#25552;&#39640;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#12290;&#36890;&#24120;&#65292;GDA&#20551;&#35774;&#27599;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#37117;&#36981;&#24490;&#20855;&#26377;&#30456;&#21516;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#31867;&#21035;&#30340;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#26469;&#34920;&#31034;&#20998;&#31867;&#22120;&#65292;&#36825;&#21487;&#20197;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#24471;&#21040;&#12290;&#20026;&#20102;&#25972;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;CLIP&#20013;&#30340;&#21407;&#22987;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#38598;&#25104;&#12290;&#22312;17&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas
&lt;/p&gt;</description></item><item><title>&#25151;&#20215;&#39044;&#27979;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;XGBoost&#65292;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#32467;&#26524;&#34920;&#26126;XGBoost&#26159;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;&#65306;XGBoost
&lt;/p&gt;
&lt;p&gt;
An Optimal House Price Prediction Algorithm: XGBoost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04082
&lt;/p&gt;
&lt;p&gt;
&#25151;&#20215;&#39044;&#27979;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;XGBoost&#65292;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#32467;&#26524;&#34920;&#26126;XGBoost&#26159;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#25151;&#20215;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#25151;&#20135;&#20215;&#20540;&#19981;&#20165;&#20165;&#30001;&#20854;&#29289;&#29702;&#23646;&#24615;&#20915;&#23450;&#65292;&#32780;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#20854;&#21608;&#22260;&#31038;&#21306;&#30340;&#24433;&#21709;&#12290;&#22312;&#24179;&#34913;&#39044;&#31639;&#38480;&#21046;&#30340;&#21516;&#26102;&#28385;&#36275;&#20010;&#20154;&#22810;&#26679;&#21270;&#30340;&#20303;&#25151;&#38656;&#27714;&#26159;&#25151;&#22320;&#20135;&#24320;&#21457;&#21830;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#25151;&#20215;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#37319;&#29992;&#20102;&#33021;&#22815;&#34920;&#36798;&#29420;&#31435;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#32654;&#22269;&#29233;&#33655;&#21326;&#24030;&#22467;&#22982;&#26031;&#24066;&#30340;&#20303;&#25151;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#12289;XGBoost&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#22312;&#25151;&#20215;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#25151;&#23627;&#25104;&#26412;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#26174;&#31034;XGBoost&#26159;&#26368;&#20339;&#30340;&#25151;&#20215;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04081</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#25913;&#36827;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Generalization of Weight Space Networks via Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#65288;DWS&#65289;&#20013;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#22312;2D&#21644;3D&#31070;&#32463;&#22330;&#65288;INRs&#65292;NeRFs&#65289;&#20197;&#21450;&#23545;&#20854;&#20182;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#20102;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;DWS&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#34429;&#28982;&#19968;&#20010;&#32473;&#23450;&#30340;&#23545;&#35937;&#21487;&#20197;&#34987;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#37197;&#32622;&#25152;&#34920;&#31034;&#65292;&#20294;&#20856;&#22411;&#30340;INR&#35757;&#32451;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#34920;&#31034;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;INR&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#25193;&#20805;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26435;&#37325;&#31354;&#38388;&#30340;MixUp&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#21319;&#31867;&#20284;&#20110;&#25317;&#26377;&#22810;&#36798;10&#20493;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#22810;&#31867;&#36947;&#36335;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#22810;&#20010;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#22359;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#20840;&#23616;&#21270;&#30340;&#36947;&#36335;&#32570;&#38519;&#24418;&#24577;&#21644;&#22270;&#20687;&#39068;&#33394;&#28145;&#24230;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#21644;&#19982;&#20043;&#21069;&#26041;&#27861;&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04064</link><description>&lt;p&gt;
&#22810;&#31867;&#36947;&#36335;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#65306;&#22522;&#20110;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#33258;&#20027;&#36947;&#36335;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04064
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#22810;&#31867;&#36947;&#36335;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#22810;&#20010;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#22359;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#20840;&#23616;&#21270;&#30340;&#36947;&#36335;&#32570;&#38519;&#24418;&#24577;&#21644;&#22270;&#20687;&#39068;&#33394;&#28145;&#24230;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#21644;&#19982;&#20043;&#21069;&#26041;&#27861;&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#36335;&#38754;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#23545;&#20110;&#24320;&#21457;&#33258;&#20027;&#36947;&#36335;&#20462;&#22797;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36947;&#36335;&#36335;&#38754;&#22270;&#20687;&#30340;&#32441;&#29702;&#31616;&#21333;&#24615;&#12289;&#32570;&#38519;&#20960;&#20309;&#24418;&#29366;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#31867;&#21035;&#20043;&#38388;&#24418;&#24577;&#27169;&#31946;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36827;&#34892;&#22810;&#31867;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#30340;&#24320;&#21457;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#36947;&#36335;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#21547;&#22810;&#20010;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#21147;&#22359;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#36328;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#30340;&#20840;&#23616;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#20123;&#27880;&#24847;&#21147;&#22359;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#36947;&#36335;&#32570;&#38519;&#30340;&#24418;&#24577;&#20449;&#24687;&#65288;&#31354;&#38388;&#29305;&#24449;&#65289;&#21644;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#30340;&#26356;&#20840;&#23616;&#21270;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04059</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multivariate Time Series Imputation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#23384;&#22312;&#30340;&#32570;&#22833;&#20540;&#23548;&#33268;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#37096;&#20998;&#35266;&#27979;&#65292;&#30772;&#22351;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#23436;&#25972;&#24615;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#22312;&#25552;&#39640;&#25439;&#22351;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24378;&#35843;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#26469;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#37197;&#32622;&#65292;&#21253;&#25324;&#23450;&#26399;&#32500;&#25252;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#35770;&#25991;&#21015;&#34920;&#65292;&#21487;&#20197;&#22312;&#20197;&#19979;&#20301;&#32622;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be foun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04050</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#22312;&#23558;&#20854;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#25237;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#23613;&#31649;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#36890;&#24120;&#36873;&#25321;&#23558;&#20854;&#20316;&#20026;&#40657;&#30418;&#25552;&#20379;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#65288;CraFT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#40657;&#30418;VLMs fine-tuning&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;CraFT&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#25991;&#26412;&#25552;&#31034;&#30340;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#20197;&#27531;&#24046;&#26041;&#24335;&#22686;&#24378;&#36755;&#20986;&#39044;&#27979;&#30340;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20419;&#36827;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#19968;&#33268;&#20248;&#21270;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#35757;&#32451;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04049</link><description>&lt;p&gt;
&#35770;&#35821;&#26009;&#24211;&#27169;&#25311;&#36777;&#35770;&#20013;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Systematic Biases in LLM Simulations of Debates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20026;&#26500;&#24314;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;&#20064;&#22120;&#65292;&#27809;&#26377;&#30452;&#25509;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#20114;&#21160;&#20013;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20542;&#21521;&#23548;&#33268;&#20986;&#29616;&#34892;&#20026;&#27169;&#24335;&#65292;&#20284;&#20046;&#20559;&#31163;&#20102;&#20154;&#31867;&#20043;&#38388;&#24050;&#32463;&#30830;&#31435;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#21152;&#24378;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25805;&#32437;LLMs&#20869;&#37096;&#30340;&#20559;&#35265;&#65292;&#24182;&#35777;&#26126;&#20195;&#29702;&#38543;&#21518;&#19982;&#36825;&#20123;&#35843;&#25972;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04046</link><description>&lt;p&gt;
&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#32852;&#21512;&#25193;&#25955;&#65292;&#23454;&#29616;&#22270;&#24418;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#23398;&#31185;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#36793;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#20013;&#36793;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20351;&#24471;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#20294;&#32463;&#39564;&#35843;&#26597;&#26174;&#31034;&#23427;&#20204;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#24456;&#22909;&#22320;&#27169;&#25311;&#22270;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#35780;&#20998;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#24418;&#29983;&#25104;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#65306;(i) &#23558;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#32467;&#21512;&#22312;&#19968;&#20010;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#22522;&#20110;&#36825;&#20004;&#20010;&#22240;&#32032;&#29983;&#25104;&#26679;&#26412;&#65307;(ii) &#22312;&#22270;&#24418;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#30456;&#20114;&#20381;&#36182;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#23454;&#38469;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#36793;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are cr
&lt;/p&gt;</description></item><item><title>HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.04032</link><description>&lt;p&gt;
HEAM: &#20351;&#29992;&#22788;&#29702;-&#20869;&#23384;&#36827;&#34892;&#25955;&#21015;&#23884;&#20837;&#21152;&#36895;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HEAM : Hashed Embedding Acceleration using Processing-In-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04032
&lt;/p&gt;
&lt;p&gt;
HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#23884;&#20837;&#25805;&#20316;&#26102;&#38656;&#35201;&#22823;&#23481;&#37327;&#30340;&#20869;&#23384;&#21644;&#39640;&#24102;&#23485;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;DIMM-based&#36817;&#20869;&#23384;&#22788;&#29702;&#25216;&#26415;&#25110;&#24341;&#20837;3D&#22534;&#21472;DRAM&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#21644;&#25193;&#23637;&#20869;&#23384;&#24102;&#23485;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#26085;&#30410;&#25193;&#22823;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22823;&#23567;&#26102;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#25512;&#33616;&#27169;&#22411;&#24050;&#32463;&#22686;&#38271;&#21040;&#36229;&#36807;&#25968;&#21313;TB&#30340;&#22823;&#23567;&#65292;&#23548;&#33268;&#22312;&#20256;&#32479;&#21333;&#33410;&#28857;&#25512;&#26029;&#26381;&#21153;&#22120;&#19978;&#39640;&#25928;&#36816;&#34892;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26041;&#27861;&#26469;&#20943;&#23567;&#23884;&#20837;&#34920;&#23481;&#37327;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#20869;&#23384;&#35775;&#38382;&#22686;&#21152;&#25110;&#20869;&#23384;&#36164;&#28304;&#21033;&#29992;&#20302;&#25928;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;HEAM&#65292;&#19968;&#31181;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#36895;&#32452;&#21512;&#23884;&#20837;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is util
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04028</link><description>&lt;p&gt;
AlbNews&#65306;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#20027;&#39064;&#24314;&#27169;&#26631;&#39064;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNews: A Corpus of Headlines for Topic Modeling in Albanian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#31232;&#32570;&#65292;&#36825;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNews&#65292;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#20110;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;AlbNews&#26679;&#26412;&#35757;&#32451;&#30340;&#19968;&#20123;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#21021;&#22987;&#20998;&#31867;&#24471;&#20998;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#22522;&#26412;&#27169;&#22411;&#32988;&#36807;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
&lt;/p&gt;</description></item><item><title>LAST&#26041;&#27861;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#21442;&#25968;&#65292;&#23558;&#21487;&#35757;&#32451;&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#32806;&#65292;&#21033;&#29992;&#20302;&#31209;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#35757;&#32451;&#19968;&#20010;&#20391;&#21521;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04009</link><description>&lt;p&gt;
&#20302;&#31209;&#27880;&#24847;&#21147;&#20391;&#21521;&#35843;&#25972;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04009
&lt;/p&gt;
&lt;p&gt;
LAST&#26041;&#27861;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#21442;&#25968;&#65292;&#23558;&#21487;&#35757;&#32451;&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#32806;&#65292;&#21033;&#29992;&#20302;&#31209;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#35757;&#32451;&#19968;&#20010;&#20391;&#21521;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#24182;&#34892;&#21270;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#20026;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#20250;&#23548;&#33268;&#26174;&#23384;&#21344;&#29992;&#39640;&#21644;&#35757;&#32451;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20114;&#32544;&#32467;&#65292;&#24494;&#35843;&#36807;&#31243;&#20013;&#24517;&#39035;&#35745;&#31639;&#24182;&#23384;&#20648;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#20923;&#32467;&#21442;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20302;&#31209;&#27880;&#24847;&#21147;&#20391;&#21521;&#35843;&#25972;&#65288;LAST&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#20165;&#20923;&#32467;&#21442;&#25968;&#65292;&#32780;&#19988;&#20923;&#32467;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23558;&#21487;&#35757;&#32451;&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#32806;&#12290;LAST&#35757;&#32451;&#19968;&#20010;&#21482;&#30001;&#20302;&#31209;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#32452;&#25104;&#30340;&#20391;&#21521;&#32593;&#32476;&#12290;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35270;&#20026;&#20923;&#32467;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20391;&#21521;&#32593;&#32476;&#25509;&#21463;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#65292;&#24182;&#19987;&#27880;&#20110;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LAST&#33021;&#22815;&#22312;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#39640;&#24230;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#21644;&#21327;&#35843;&#25506;&#32034;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20869;&#22312;&#21160;&#26426;&#22870;&#21169;&#32852;&#21512;&#36712;&#36857;&#30340;&#26032;&#39062;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#21644;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03972</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#20869;&#22312;&#21160;&#26426;&#20197;&#20419;&#36827;&#21327;&#35843;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31232;&#30095;&#22870;&#21169;&#21644;&#21327;&#35843;&#25506;&#32034;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#20869;&#22312;&#21160;&#26426;&#22870;&#21169;&#32852;&#21512;&#36712;&#36857;&#30340;&#26032;&#39062;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#21644;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#38382;&#39064;&#24120;&#24120;&#38754;&#20020;&#31232;&#30095;&#22870;&#21169;&#30340;&#25361;&#25112;&#12290;&#24403;&#26234;&#33021;&#20307;&#20043;&#38388;&#38656;&#35201;&#21327;&#35843;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#30001;&#20110;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#32852;&#21512;&#34892;&#20026;&#65292;&#25214;&#21040;&#19968;&#20010;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#21487;&#20197;&#36890;&#36807;&#31215;&#26497;&#25506;&#32034;&#19981;&#21516;&#30340;&#32852;&#21512;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20849;&#21516;&#23637;&#31034;&#26032;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;JIM(&#32852;&#21512;&#20869;&#22312;&#21160;&#26426;)&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#65292;&#36981;&#24490;&#38598;&#20013;&#23398;&#20064;&#21644;&#20998;&#25955;&#25191;&#34892;&#30340;&#33539;&#24335;&#12290;JIM&#26681;&#25454;&#19968;&#20010;&#38024;&#23545;&#36830;&#32493;&#29615;&#22659;&#35774;&#35745;&#30340;&#38598;&#20013;&#27979;&#37327;&#30340;&#26032;&#39062;&#24615;&#22870;&#21169;&#32852;&#21512;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to rev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.03970</link><description>&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#65306;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tabular Data: Is Attention All You Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20248;&#21183;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24110;&#21161;&#30740;&#31350;&#21644;&#23454;&#36341;&#31038;&#21306;&#22312;&#26410;&#26469;&#30340;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#20013;&#20570;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoWA&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#24377;&#24615;&#21464;&#24418;&#23545;&#36755;&#20837;&#36827;&#34892;&#22686;&#24378;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36328;&#27169;&#22411;&#31181;&#26063;&#30340;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#25915;&#20987;&#19981;&#21516;&#27169;&#22411;&#31181;&#26063;&#31995;&#32479;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.03951</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#31181;&#26063;&#30340;&#25197;&#26354;&#38480;&#21046;&#25915;&#20987;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCoWA&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#36890;&#36807;&#24377;&#24615;&#21464;&#24418;&#23545;&#36755;&#20837;&#36827;&#34892;&#22686;&#24378;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36328;&#27169;&#22411;&#31181;&#26063;&#30340;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#25915;&#20987;&#19981;&#21516;&#27169;&#22411;&#31181;&#26063;&#31995;&#32479;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24377;&#24615;&#21464;&#24418;&#23545;&#36755;&#20837;&#26679;&#26412;&#36827;&#34892;&#22686;&#24378;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#25511;&#21046;&#31574;&#30053;&#38480;&#21046;&#25197;&#26354;&#21464;&#25442;&#30340;&#24378;&#24230;&#21644;&#26041;&#21521;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deformation-Constrained Warping Attack (DeCoWA)&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#36328;&#27169;&#22411;&#31181;&#26063;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#36716;&#31227;&#30340;&#26679;&#26412;&#31034;&#20363;&#22312;&#25915;&#20987;&#19981;&#21516;&#27169;&#22411;&#31181;&#26063;&#31995;&#32479;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22312;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#20013;&#35782;&#21035;&#23398;&#29983;&#30340;&#29305;&#24449;&#65292;&#24182;&#33258;&#21160;&#25512;&#26029;&#20986;&#23545;&#23398;&#29983;&#21644;&#25945;&#24072;&#26377;&#29992;&#30340;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.03948</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#20013;&#35782;&#21035;&#23398;&#29983;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Identifying Student Profiles Within Online Judge Systems Using Explainable Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22312;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#20013;&#35782;&#21035;&#23398;&#29983;&#30340;&#29305;&#24449;&#65292;&#24182;&#33258;&#21160;&#25512;&#26029;&#20986;&#23545;&#23398;&#29983;&#21644;&#25945;&#24072;&#26377;&#29992;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#36890;&#24120;&#22312;&#32534;&#31243;&#30456;&#20851;&#30340;&#35838;&#31243;&#20013;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#24182;&#23458;&#35266;&#22320;&#35780;&#20272;&#23398;&#29983;&#32534;&#20889;&#30340;&#20195;&#30721;&#12290;&#36890;&#24120;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#21482;&#32473;&#20986;&#19968;&#20010;&#20915;&#31574;&#65292;&#20027;&#35201;&#26159;&#21028;&#26029;&#25552;&#20132;&#26159;&#21542;&#25104;&#21151;&#23436;&#25104;&#20102;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#19981;&#22815;&#30340;&#65292;&#23398;&#29983;&#21644;&#25945;&#24072;&#37117;&#24076;&#26395;&#33719;&#24471;&#20851;&#20110;&#20219;&#21153;&#25972;&#20307;&#21457;&#23637;&#30340;&#39069;&#22806;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36827;&#19968;&#27493;&#21033;&#29992;&#22312;&#32447;&#35780;&#27979;&#31995;&#32479;&#25910;&#38598;&#30340;&#20449;&#24687;&#65292;&#33258;&#21160;&#25512;&#26029;&#20986;&#23545;&#23398;&#29983;&#21644;&#25945;&#24072;&#26377;&#29992;&#30340;&#21453;&#39304;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22810;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#21644;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#24314;&#27169;&#23398;&#29983;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Judge (OJ) systems are typically considered within programming-related courses as they yield fast and objective assessments of the code developed by the students. Such an evaluation generally provides a single decision based on a rubric, most commonly whether the submission successfully accomplished the assignment. Nevertheless, since in an educational context such information may be deemed insufficient, it would be beneficial for both the student and the instructor to receive additional feedback about the overall development of the task. This work aims to tackle this limitation by considering the further exploitation of the information gathered by the OJ and automatically inferring feedback for both the student and the instructor. More precisely, we consider the use of learning-based schemes -- particularly, multi-instance learning (MIL) and classical machine learning formulations -- to model student behavior. Besides, explainable artificial intelligence (XAI) is contemplated t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03927</link><description>&lt;p&gt;
&#27844;&#28431;&#12289;&#27450;&#39575;&#12289;&#37325;&#22797;&#65306;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;&#23436;&#20840;&#25110;&#37096;&#20998;&#23553;&#38381;&#28304;&#30340;&#12290;&#23545;&#20110;&#27169;&#22411;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#35775;&#38382;&#26435;&#38480;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21453;&#22797;&#23545;&#25968;&#25454;&#27745;&#26579;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20165;&#38480;&#20110;&#20010;&#21035;&#26696;&#20363;&#21644;&#35797;&#38169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#8220;&#38388;&#25509;&#8221;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#20351;&#29992;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#20998;&#26512;&#65292;&#36825;&#20123;&#26159;&#24403;&#20170;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#24182;&#32771;&#34385;&#20102;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#27169;&#22411;&#21457;&#24067;&#21518;&#19968;&#24180;&#20869;&#27844;&#38706;&#32473;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20027;&#35201;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03921</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Enhance Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23427;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#24378;&#35843;&#65292;&#29305;&#21035;&#26159;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#26377;&#25928;&#22320;&#24179;&#34913;&#21208;&#25506;&#21644;&#24320;&#21457;&#12290;&#23613;&#31649;&#22312;BO&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24179;&#34913;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24494;&#22937;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;LLAMBO&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19982;BO&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#20351;LLM&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#35780;&#20272;&#25552;&#20986;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#32467;&#21512;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#26469;&#22686;&#24378;&#22522;&#20110;&#27169;&#22411;&#30340;BO&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLAMBO&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#25193;&#23637;&#29616;&#23454;&#30340;&#26356;&#21253;&#23481;&#21644;&#21442;&#19982;&#65292;&#24182;&#26377;&#26395;&#25512;&#21160;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03907</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#65306;&#21253;&#23481;&#24615;&#12289;&#21442;&#19982;&#24230;&#21644;&#38544;&#31169;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#20351;&#29992;LLMs&#21487;&#20197;&#23454;&#29616;&#25193;&#23637;&#29616;&#23454;&#30340;&#26356;&#21253;&#23481;&#21644;&#21442;&#19982;&#65292;&#24182;&#26377;&#26395;&#25512;&#21160;&#20854;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#30828;&#20214;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#26368;&#26032;&#21457;&#23637;&#21487;&#33021;&#23548;&#33268;&#25193;&#23637;&#29616;&#23454;&#35774;&#22791;&#30340;&#26222;&#21450;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23884;&#20837;&#25193;&#23637;&#29616;&#23454;&#20013;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#23884;&#20837;&#34394;&#25311;&#35282;&#33394;&#25110;&#20316;&#20026;&#21465;&#20107;&#26041;&#24335;&#65292;&#26469;&#20419;&#36827;&#26356;&#21253;&#23481;&#30340;&#20307;&#39564;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#21253;&#23481;&#24615;&#23558;&#26377;&#21161;&#20110;&#25193;&#23637;&#29616;&#23454;&#30340;&#22810;&#26679;&#24615;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;LLMs&#30340;&#22810;&#21151;&#33021;&#23545;&#35805;&#33021;&#21147;&#23558;&#22686;&#21152;&#29992;&#25143;&#19982;&#25193;&#23637;&#29616;&#23454;&#29615;&#22659;&#30340;&#21442;&#19982;&#24230;&#65292;&#20174;&#32780;&#24110;&#21161;&#25193;&#23637;&#29616;&#23454;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combin
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03877</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#26465;&#21644;&#22278;&#22280;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20309;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#21644;&#31639;&#27861;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#25216;&#33021;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#26500;&#36896;&#24615;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20154;&#31867;&#25968;&#23398;&#25512;&#29702;&#21457;&#23637;&#20013;&#26368;&#22522;&#30784;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23613;&#31649;&#22312;&#31867;&#20284;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;LLMs&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#32463;&#24120;&#20250;&#38169;&#35823;&#22320;&#34920;&#31034;&#21644;&#33222;&#36896;&#23545;&#35937;&#21450;&#20854;&#25918;&#32622;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#36827;&#34892;&#20869;&#37096;&#23545;&#35805;&#26469;&#22686;&#24378;&#23427;&#20204;&#29616;&#26377;&#30340;&#25512;&#29702;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;LLMs&#22312;&#20960;&#20309;&#25512;&#29702;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03855</link><description>&lt;p&gt;
&#12298;&#23450;&#20301;&#35770;&#25991;&#65306;&#25506;&#32034;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Toward New Frameworks for Studying Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03855
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#23398;&#20064;&#30340;&#30830;&#20999;&#31639;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;MI&#30740;&#31350;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#37117;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#21644;&#31526;&#21495;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33021;&#21147;&#24182;&#19981;&#37027;&#20040;&#24494;&#19981;&#36275;&#36947;&#65292;&#36825;&#20026;&#30740;&#31350;&#32593;&#32476;&#20869;&#37096;&#30340;&#38544;&#34255;&#34920;&#31034;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#22238;&#39038;&#65292;&#23545;&#29305;&#24449;&#21644;&#34892;&#20026;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#30340;&#34920;&#31034;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#21644;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#21644;&#25506;&#32034;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30740;&#31350;&#34920;&#31034;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#30446;&#21069;MI&#20013;&#24314;&#31435;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#29702;&#35299;&#34920;&#31034;&#65292;&#22240;&#27492;&#25512;&#21160;&#30740;&#31350;&#30028;&#26397;&#30528;&#30740;&#31350;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03843</link><description>&lt;p&gt;
&#20809;&#23398;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method for optical steel rope non-destructive damage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#28023;&#25300;&#29615;&#22659;&#65288;&#31354;&#20013;&#21514;&#32034;&#36947;&#65289;&#20013;&#30340;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RGBD-UNet&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#38050;&#19997;&#32499;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#30340;CMA&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#21644;&#32467;&#21512;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;VovNetV3.5&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#38050;&#19997;&#32499;&#12290;&#23427;&#23558;VovNet&#26550;&#26500;&#19982;DBB&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32972;&#26223;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#21516;&#22330;&#26223;&#20013;&#38050;&#19997;&#32499;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#27492;&#31639;&#27861;&#30340;&#20256;&#24863;&#22120;&#35782;&#21035;&#24615;&#33021;&#65288;h&#65289;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
&lt;/p&gt;</description></item><item><title>&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03824</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
A call for embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03824
&lt;/p&gt;
&lt;p&gt;
&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#36827;&#34892;&#23545;&#27604;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36328;&#36234;&#20102;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#23545;&#20855;&#35937;&#27010;&#24565;&#30340;&#28436;&#21464;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#21306;&#21035;&#20110;&#38745;&#24577;&#23398;&#20064;&#30340;&#32463;&#20856;&#33539;&#24335;&#12290;&#36890;&#36807;&#25193;&#22823;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24378;&#35843;&#30693;&#35273;&#12289;&#34892;&#21160;&#12289;&#35760;&#24518;&#21644;&#23398;&#20064;&#20316;&#20026;&#20855;&#35937;&#20195;&#29702;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35832;&#22914;&#21046;&#23450;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#29702;&#35770;&#21644;&#21019;&#26032;&#20808;&#36827;&#30828;&#20214;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#20026;&#26410;&#26469;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. High
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.03807</link><description>&lt;p&gt;
SEABO: &#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SEABO: A Simple Search-Based Method for Offline Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03807
&lt;/p&gt;
&lt;p&gt;
SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#33021;&#22815;&#20174;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#28040;&#38500;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;RL&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#26377;&#22870;&#21169;&#26631;&#31614;&#30340;&#31163;&#32447;&#36716;&#25442;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26377;&#26102;&#26159;&#22256;&#38590;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#25110;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25226;&#37325;&#28857;&#25918;&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#35774;&#32622;&#19978;&#65292;&#26088;&#22312;&#22522;&#20110;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;IL&#26041;&#27861;&#65292;&#31216;&#20026;SEABO&#12290;SEABO&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#36739;&#22823;&#30340;&#22870;&#21169;&#20998;&#37197;&#32473;&#19982;&#19987;&#23478;&#28436;&#31034;&#20013;&#26368;&#25509;&#36817;&#30340;&#36716;&#25442;&#65292;&#21542;&#21017;&#20998;&#37197;&#36739;&#23567;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEABO&#33021;&#22815;&#36798;&#21040;&#19982;&#31163;&#32447;RL&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;ReLU^2&#65292;&#23427;&#22312;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.03804</link><description>&lt;p&gt;
ReLU^2&#33719;&#32988;&#65306;&#21457;&#29616;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03804
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;ReLU^2&#65292;&#23427;&#22312;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#31232;&#30095;&#35745;&#31639;&#20026;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#27880;&#37325;&#22522;&#20110;ReLU&#30340;LLM&#65292;&#21033;&#29992;&#28608;&#27963;&#20540;&#20013;&#30340;&#38646;&#65292;&#20294;&#25105;&#20204;&#23558;&#31232;&#30095;LLM&#30340;&#33539;&#22260;&#25193;&#22823;&#21040;&#38646;&#28608;&#27963;&#20540;&#20043;&#22806;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20803;&#36755;&#20986;&#24133;&#24230;&#21644;&#23450;&#21046;&#30340;&#24133;&#24230;&#38408;&#20540;&#26469;&#23450;&#20041;&#31070;&#32463;&#20803;&#28608;&#27963;&#65292;&#24182;&#35777;&#26126;&#38750;ReLU LLM&#20063;&#34920;&#29616;&#20986;&#31232;&#30095;&#28608;&#27963;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#39640;&#25928;&#30340;&#31232;&#30095;&#35745;&#31639;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#23519;LLM&#30340;&#31232;&#30095;&#24615;&#65306;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ReLU, SwiGLU, ReGLU &#21644; ReLU^2&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;ReLU^2&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#20854;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03796</link><description>&lt;p&gt;
&#20154;&#33080;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Face Detection: Present State and Research Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#20854;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22788;&#29702;&#21253;&#21547;&#20154;&#31867;&#22270;&#20687;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#37117;&#20351;&#29992;&#20154;&#33080;&#26816;&#27979;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#12290;&#23613;&#31649;&#23545;&#35813;&#20027;&#39064;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20154;&#33080;&#26816;&#27979;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20197;&#21450;&#20173;&#38656;&#35299;&#20915;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#20197;&#20316;&#20026;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30740;&#31350;&#39033;&#30446;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;Legendre&#22810;&#39033;&#24335;&#22522;&#30784;&#19978;&#26500;&#24314;MDP&#34920;&#31034;&#26469;&#35299;&#20915;$\nu-$&#24179;&#28369; MDPs&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.03792</link><description>&lt;p&gt;
&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26080;&#24724;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#28369;MDPs&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
No-Regret Reinforcement Learning in Smooth MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;Legendre&#22810;&#39033;&#24335;&#22522;&#30784;&#19978;&#26500;&#24314;MDP&#34920;&#31034;&#26469;&#35299;&#20915;$\nu-$&#24179;&#28369; MDPs&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#22914;&#20309;&#33719;&#24471;&#26080;&#24724;&#20445;&#35777;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38500;&#38750;&#26159;&#38750;&#24120;&#29305;&#23450;&#30340;&#24773;&#26223;&#65292;&#21542;&#21017;&#36825;&#20010;&#26222;&#36941;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#36827;&#34892;&#32467;&#26500;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#21363;$\nu-$&#24179;&#28369;&#24615;&#65292;&#35813;&#26041;&#27861;&#25512;&#24191;&#20102;&#30446;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#35774;&#32622;&#65288;&#22914;&#32447;&#24615;MDPs&#21644;Lipschitz MDPs&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22312;$\nu-$&#24179;&#28369; MDPs&#20013;&#36827;&#34892;&#24724;&#24680;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#22522;&#20110;&#36890;&#36807;&#22522;&#20110;Legendre&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#29305;&#24449;&#26144;&#23556;&#26500;&#24314;MDP&#34920;&#31034;&#30340;&#24605;&#24819;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861;\textsc{Legendre-Eleanor}&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#20294;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#31532;&#20108;&#20010;\textsc{Legendre-LSVI}&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;
&lt;/p&gt;
&lt;p&gt;
Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial time, although 
&lt;/p&gt;</description></item><item><title>AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03784</link><description>&lt;p&gt;
AirPhyNet: &#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03784
&lt;/p&gt;
&lt;p&gt;
AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#21644;&#24314;&#27169;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#29615;&#22659;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24110;&#21161;&#20010;&#20154;&#21644;&#24403;&#23616;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#31934;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#32570;&#20047;&#22362;&#23454;&#29289;&#29702;&#22522;&#30784;&#30340;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Physics guided Neural Network for Air Quality Prediction&#65288;AirPhyNet&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#20004;&#20010;&#25104;&#29087;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#25193;&#25955;&#21644;&#24179;&#27969;&#65289;&#23558;&#20854;&#34920;&#31034;&#20026;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#23558;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#25429;&#25417;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#19981;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03782</link><description>&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#36328;&#35821;&#35328;&#36801;&#31227;&#65306;&#36234;&#23569;&#36234;&#22909;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#19981;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36755;&#20837;&#23618;&#25554;&#20837;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#65292;&#25110;&#36719;&#25552;&#31034;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#21442;&#25968;&#65292;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;SPT&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#19982;&#20808;&#21069;&#20851;&#20110;SPT&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22362;&#25345;SPT&#30340;&#21407;&#22987;&#24847;&#22270;&#65292;&#21363;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#20445;&#25345;&#27169;&#22411;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#19981;&#20165;&#20943;&#23569;&#20102;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;SPT&#22266;&#26377;&#30340;&#21442;&#25968;&#25928;&#29575;&#33021;&#22815;&#25552;&#39640;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#19981;&#21516;&#22240;&#32032;&#65292;&#22914;&#38271;&#24230;&#25110;&#20854;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>MobileVLM V2&#26159;&#22312;MobileVLM&#22522;&#30784;&#19978;&#25913;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#35774;&#35745;&#12289;&#25913;&#36827;&#30340;&#35757;&#32451;&#26041;&#26696;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#22823;&#35268;&#27169;VLMs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03766</link><description>&lt;p&gt;
MobileVLM V2: &#26356;&#24555;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
MobileVLM V2: Faster and Stronger Baseline for Vision Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03766
&lt;/p&gt;
&lt;p&gt;
MobileVLM V2&#26159;&#22312;MobileVLM&#22522;&#30784;&#19978;&#25913;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#26550;&#26500;&#35774;&#35745;&#12289;&#25913;&#36827;&#30340;&#35757;&#32451;&#26041;&#26696;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#22823;&#35268;&#27169;VLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MobileVLM V2&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22312;MobileVLM&#22522;&#30784;&#19978;&#26174;&#33879;&#25913;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#26032;&#39062;&#30340;&#26550;&#26500;&#35774;&#35745;&#12289;&#20026;&#31227;&#21160;VLM&#37327;&#36523;&#23450;&#21046;&#30340;&#25913;&#36827;&#35757;&#32451;&#26041;&#26696;&#20197;&#21450;&#20016;&#23500;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#31574;&#21010;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;VLM&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MobileVLM V2 1.7B&#22312;&#26631;&#20934;VLM&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#35268;&#27169;&#20026;3B&#30340;&#26356;&#22823; VLMs &#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;3B&#27169;&#22411;&#22312;7B+&#33539;&#22260;&#20869;&#34920;&#29616;&#20248;&#20110;&#22823;&#37327;&#30340;VLMs&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22312;https://github.com/Meituan-AutoML/MobileVLM &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;QuantAgent&#65292;&#36890;&#36807;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20004;&#23618;&#24490;&#29615;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#36880;&#27493;&#36924;&#36817;&#20855;&#26377;&#21487;&#35777;&#26126;&#25928;&#29575;&#30340;&#26368;&#20339;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#21644;&#38598;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#24211;&#65292;&#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03755</link><description>&lt;p&gt;
QuantAgent&#65306;&#36890;&#36807;&#33258;&#25105;&#25552;&#21319;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20132;&#26131;&#20013;&#23547;&#27714;&#22307;&#26479;
&lt;/p&gt;
&lt;p&gt;
QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;QuantAgent&#65292;&#36890;&#36807;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20004;&#23618;&#24490;&#29615;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#36880;&#27493;&#36924;&#36817;&#20855;&#26377;&#21487;&#35777;&#26126;&#25928;&#29575;&#30340;&#26368;&#20339;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26500;&#24314;&#21644;&#38598;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#24211;&#65292;&#22312;&#37327;&#21270;&#25237;&#36164;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#20027;&#20195;&#29702;&#65292;&#22312;&#21046;&#23450;&#35745;&#21010;&#21644;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#26041;&#38754;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20195;&#29702;&#23450;&#21046;&#20026;&#37327;&#21270;&#25237;&#36164;&#31561;&#19987;&#19994;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#26680;&#24515;&#25361;&#25112;&#22312;&#20110;&#20026;&#20195;&#29702;&#30340;&#23398;&#20064;&#36807;&#31243;&#39640;&#25928;&#22320;&#26500;&#24314;&#21644;&#38598;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#23618;&#24490;&#29615;&#12290;&#22312;&#20869;&#23618;&#24490;&#29615;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#20449;&#24687;&#26469;&#25913;&#36827;&#20854;&#21709;&#24212;&#65292;&#32780;&#22312;&#22806;&#23618;&#24490;&#29615;&#20013;&#65292;&#36825;&#20123;&#21709;&#24212;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#33258;&#21160;&#22686;&#24378;&#30693;&#35782;&#24211;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#36880;&#27493;&#36924;&#36817;&#20855;&#26377;&#21487;&#35777;&#26126;&#25928;&#29575;&#30340;&#26368;&#20339;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;QuantAgent&#30340;&#33258;&#20027;&#20195;&#29702;&#26469;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#25366;&#25496;&#20132;&#26131;&#20449;&#21495;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;QuantAgent&#22312;&#21457;&#29616;&#20132;&#26131;&#20449;&#21495;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncoverin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#40784;&#22270;&#21644;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03750</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#65306;&#19968;&#31181;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#40784;&#22270;&#21644;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#31227;&#21160;&#24615;&#24314;&#27169;&#25104;&#20026;&#21033;&#29992;&#22823;&#37327;&#31227;&#21160;&#24615;&#25968;&#25454;&#21019;&#24314;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#31227;&#21160;&#24615;&#24314;&#27169;&#21487;&#20197;&#20174;&#31227;&#21160;&#24615;&#25968;&#25454;&#20013;&#25552;&#21462;&#22478;&#24066;&#20132;&#36890;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#26159;&#21508;&#31181;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#24212;&#29992;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#24615;&#39640;&#21644;&#25968;&#25454;&#37327;&#24040;&#22823;&#65292;&#31227;&#21160;&#24615;&#24314;&#27169;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#36890;&#36807;&#21019;&#24314;&#32593;&#32476;&#30340;&#34394;&#25311;&#34920;&#31034;&#26469;&#27169;&#25311;&#20854;&#34892;&#20026;&#65292;&#20026;&#25104;&#26412;&#26377;&#25928;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#31649;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#20102;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#25105;&#20204;&#26500;&#24314;&#23545;&#40784;&#22270;&#26469;&#23436;&#25104;&#26102;&#31354;&#30456;&#20851;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#65288;DACN&#65289;&#26469;&#23398;&#20064;&#31934;&#32454;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#65288;DTMP&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) fra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03732</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Outdated Fact Detection in Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22240;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36807;&#26102;&#20107;&#23454;&#30340;&#38382;&#39064;&#32473;KG&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#20854;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#20449;&#24687;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DEAN&#65288;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;KG&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#21306;&#20998;&#20107;&#23454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33258;&#24049;&#30340;&#29420;&#29305;&#20043;&#22788;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#65292;DEAN&#37319;&#29992;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#21040;&#33410;&#28857;&#65288;R2N&#65289;&#22270;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#35813;&#22270;&#30001;&#23454;&#20307;&#25968;&#37327;&#21152;&#26435;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03728</link><description>&lt;p&gt;
&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#33268;&#32852;&#21512;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Consistent Joint Decision-Making with Heterogeneous Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#30001;&#19981;&#21516;&#27169;&#22411;&#20570;&#20986;&#30340;&#20915;&#31574;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#20840;&#23616;&#24402;&#19968;&#21270;&#21644;&#21487;&#27604;&#36739;&#30340;&#20540;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20915;&#31574;&#30340;&#20808;&#39564;&#27010;&#29575;&#12289;&#32622;&#20449;&#24230;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03720</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#29992;&#20110;&#22270;&#24418;LLMs
&lt;/p&gt;
&lt;p&gt;
Similarity-based Neighbor Selection for Graph LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03720
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#30452;&#25509;&#22788;&#29702;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#24191;&#27867;&#24120;&#35782;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;TAGs&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#20379;&#20102;&#26497;&#22823;&#30340;&#24076;&#26395;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24050;&#32463;&#35299;&#20915;&#20102;&#36807;&#24230;&#21387;&#32553;&#12289;&#24322;&#36136;&#24615;&#21644;&#20449;&#24687;&#38598;&#25104;&#19981;&#24403;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#21463;&#21040;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#39640;&#32423;LLMs&#30340;&#20302;&#21033;&#29992;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#12290;&#20351;&#29992;SimCSE&#21644;&#39640;&#32423;&#37051;&#23621;&#36873;&#25321;&#25216;&#26415;&#65292;SNS&#26377;&#25928;&#25552;&#39640;&#20102;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#24182;&#20943;&#36731;&#20102;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#24615;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#19968;&#31181;&#24402;&#32435;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;SNS&#22312;&#20256;&#32479;GNN&#26041;&#27861;&#19978;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#31526;&#21512;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20998;&#21306;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaMAI&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20027;&#21160;&#35810;&#38382;&#30340;&#26041;&#24335;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#38169;&#35823;&#35299;&#35835;&#30340;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.03719</link><description>&lt;p&gt;
&#29992;&#20027;&#21160;&#35810;&#38382;&#36171;&#20104;&#35821;&#35328;&#27169;&#22411;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering Language Models with Active Inquiry for Deeper Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaMAI&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20027;&#21160;&#35810;&#38382;&#30340;&#26041;&#24335;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#38169;&#35823;&#35299;&#35835;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#38761;&#26032;&#20102;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#23545;&#29992;&#25143;&#26597;&#35810;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#20250;&#38169;&#35823;&#35299;&#35835;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23548;&#33268;&#19981;&#22826;&#26377;&#24110;&#21161;&#30340;&#22238;&#22797;&#12290;&#22312;&#33258;&#28982;&#20154;&#31867;&#20114;&#21160;&#20013;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#38382;&#39064;&#23547;&#27714;&#28548;&#28165;&#65292;&#20197;&#25581;&#31034;&#19981;&#26126;&#30830;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LaMAI&#65288;&#24102;&#26377;&#20027;&#21160;&#35810;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#26088;&#22312;&#36171;&#20104;LLMs&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#24615;&#12290;LaMAI&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#21160;&#24577;&#30340;&#21452;&#21521;&#23545;&#35805;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32553;&#23567;&#20102;&#19978;&#19979;&#25991;&#24046;&#36317;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#26356;&#21152;&#31526;&#21512;&#29992;&#25143;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;LaMAI&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;LLMs&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#30340;&#20934;&#30830;&#29575;&#20174;31.9%&#25552;&#39640;&#21040;50%
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50
&lt;/p&gt;</description></item><item><title>MMAUD&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21453;&#26080;&#20154;&#26426;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#24863;&#30693;&#36755;&#20837;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#24403;&#19979;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#20851;&#38190;&#32570;&#21475;&#65292;&#25104;&#20026;&#19968;&#39033;&#26080;&#20215;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.03706</link><description>&lt;p&gt;
MMAUD: &#20026;&#29616;&#20195;&#23567;&#22411;&#26080;&#20154;&#26426;&#23041;&#32961;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#21453;&#26080;&#20154;&#26426;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03706
&lt;/p&gt;
&lt;p&gt;
MMAUD&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21453;&#26080;&#20154;&#26426;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#24863;&#30693;&#36755;&#20837;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#24357;&#34917;&#20102;&#24403;&#19979;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#20851;&#38190;&#32570;&#21475;&#65292;&#25104;&#20026;&#19968;&#39033;&#26080;&#20215;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#23567;&#22411;&#26080;&#20154;&#26426;&#24102;&#26469;&#30340;&#19981;&#26029;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21487;&#33021;&#25658;&#24102;&#26377;&#23475;&#36733;&#33655;&#25110;&#29420;&#31435;&#36896;&#25104;&#25439;&#23475;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMAUD: &#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#21453;&#26080;&#20154;&#26426;&#25968;&#25454;&#38598;&#12290;MMAUD&#36890;&#36807;&#32858;&#28966;&#26080;&#20154;&#26426;&#26816;&#27979;&#12289;UAV&#31867;&#22411;&#20998;&#31867;&#21644;&#36712;&#36857;&#20272;&#35745;&#65292;&#24357;&#34917;&#20102;&#24403;&#19979;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#20013;&#30340;&#20851;&#38190;&#32570;&#21475;&#12290;MMAUD&#36890;&#36807;&#32467;&#21512;&#31435;&#20307;&#35270;&#35273;&#12289;&#21508;&#31181;&#28608;&#20809;&#38647;&#36798;&#12289;&#38647;&#36798;&#21644;&#22768;&#38899;&#38453;&#21015;&#31561;&#22810;&#31181;&#24863;&#30693;&#36755;&#20837;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;&#23427;&#25552;&#20379;&#20102;&#27604;&#20351;&#29992;&#28909;&#20687;&#21644;RGB&#20174;&#29305;&#23450;&#35270;&#35282;&#25429;&#25417;&#30340;&#25968;&#25454;&#38598;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#29420;&#29305;&#30340;&#31354;&#20013;&#30417;&#27979;&#12290;&#27492;&#22806;&#65292;MMAUD&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;Leica&#29983;&#25104;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#21487;&#20449;&#24230;&#65292;&#24182;&#33021;&#22815;&#33258;&#20449;&#22320;&#25913;&#36827;&#31639;&#27861;&#21644;&#27169;&#22411;&#65292;&#36825;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#20174;&#26410;&#20986;&#29616;&#36807;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#19981;&#20250;&#20844;&#24320;&#20854;&#25968;&#25454;&#38598;&#65292;&#20351;MMAUD&#25104;&#20026;&#19968;&#39033;&#26080;&#20215;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for de
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GenLens&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#23545;GenAI&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#24182;&#24320;&#21457;&#20102;GenLens&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;GenLens&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#26597;&#30475;&#21644;&#27880;&#37322;GenAI&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#33021;&#22815;&#23450;&#21046;&#38382;&#39064;&#26631;&#31614;&#21644;&#20998;&#31867;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;GenLens&#33021;&#22815;&#25552;&#39640;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03700</link><description>&lt;p&gt;
GenLens:&#19968;&#31181;&#23545;&#35270;&#35273;GenAI&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GenLens: A Systematic Evaluation of Visual GenAI Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GenLens&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#23545;GenAI&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#24182;&#24320;&#21457;&#20102;GenLens&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;GenLens&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#26597;&#30475;&#21644;&#27880;&#37322;GenAI&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#33021;&#22815;&#23450;&#21046;&#38382;&#39064;&#26631;&#31614;&#21644;&#20998;&#31867;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;GenLens&#33021;&#22815;&#25552;&#39640;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#29983;&#25104;&#24335;AI&#65288;GenAI&#65289;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#26377;&#25928;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#20445;&#20854;&#36136;&#37327;&#21644;&#20844;&#24179;&#24615;&#12290;&#29616;&#26377;&#24037;&#20855;&#20027;&#35201;&#20851;&#27880;&#25968;&#25454;&#38598;&#36136;&#37327;&#20445;&#35777;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#27169;&#22411;&#24320;&#21457;&#26399;&#38388;&#22312;GenAI&#36755;&#20986;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#24403;&#21069;&#30340;&#20570;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#24320;&#21457;&#20154;&#21592;&#30340;&#20027;&#35266;&#35270;&#35273;&#35780;&#20272;&#65292;&#21487;&#33021;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#19982;GenAI&#27169;&#22411;&#24320;&#21457;&#32773;&#36827;&#34892;&#24418;&#25104;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#20102;GenLens&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#22312;&#27169;&#22411;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#23545;GenAI&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#20998;&#26512;&#30028;&#38754;&#12290;GenLens&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#26597;&#30475;&#21644;&#27880;&#37322;&#22833;&#36133;&#26696;&#20363;&#65292;&#23450;&#21046;&#38382;&#39064;&#26631;&#31614;&#21644;&#20998;&#31867;&#65292;&#24182;&#20174;&#22810;&#20010;&#29992;&#25143;&#32858;&#21512;&#27880;&#37322;&#20197;&#22686;&#24378;&#21327;&#20316;&#12290;&#19982;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;GenLens&#33021;&#22815;&#25552;&#39640;&#35780;&#20272;GenAI&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness. Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens
&lt;/p&gt;</description></item><item><title>ServeFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;-&#24930;&#36895;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#12290;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#25910;&#38598;&#25968;&#25454;&#21253;&#30340;&#25968;&#37327;&#21644;&#24212;&#29992;&#20110;&#19981;&#21516;&#27969;&#37327;&#30340;&#27169;&#22411;&#65292;ServeFlow&#23454;&#29616;&#20102;&#26368;&#23567;&#24310;&#36831;&#12289;&#39640;&#26381;&#21153;&#29575;&#21644;&#39640;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ServeFlow&#33021;&#22815;&#22312;16ms&#20869;&#23545;76.3%&#30340;&#27969;&#37327;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#26159;&#20013;&#20301;&#25968;&#25512;&#29702;&#26102;&#38388;&#30340;40.5&#20493;&#21152;&#36895;&#65281;</title><link>https://arxiv.org/abs/2402.03694</link><description>&lt;p&gt;
ServeFlow&#65306;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#30340;&#24555;&#36895;-&#24930;&#36895;&#27169;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03694
&lt;/p&gt;
&lt;p&gt;
ServeFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;-&#24930;&#36895;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#12290;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#25910;&#38598;&#25968;&#25454;&#21253;&#30340;&#25968;&#37327;&#21644;&#24212;&#29992;&#20110;&#19981;&#21516;&#27969;&#37327;&#30340;&#27169;&#22411;&#65292;ServeFlow&#23454;&#29616;&#20102;&#26368;&#23567;&#24310;&#36831;&#12289;&#39640;&#26381;&#21153;&#29575;&#21644;&#39640;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ServeFlow&#33021;&#22815;&#22312;16ms&#20869;&#23545;76.3%&#30340;&#27969;&#37327;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#26159;&#20013;&#20301;&#25968;&#25512;&#29702;&#26102;&#38388;&#30340;40.5&#20493;&#21152;&#36895;&#65281;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#25972;&#21512;&#21644;&#27969;&#37327;&#30340;&#21152;&#23494;&#65292;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24102;&#23485;&#32593;&#32476;&#20013;&#65292;&#27969;&#37327;&#24448;&#24448;&#27604;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#29575;&#26356;&#24555;&#12290;&#32593;&#32476;&#27969;&#37327;&#30340;&#26102;&#38388;&#24615;&#36136;&#38480;&#21046;&#20102;&#22312;&#20854;&#20182;&#39640;&#27969;&#37327;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#31616;&#21333;&#25193;&#23637;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;ServeFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#32593;&#32476;&#27969;&#37327;&#20998;&#26512;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#25910;&#38598;&#25968;&#25454;&#21253;&#30340;&#25968;&#37327;&#21644;&#24212;&#29992;&#20110;&#19981;&#21516;&#27969;&#37327;&#30340;&#27169;&#22411;&#65292;&#26469;&#23454;&#29616;&#26368;&#23567;&#24310;&#36831;&#12289;&#39640;&#26381;&#21153;&#29575;&#21644;&#39640;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#30456;&#21516;&#30340;&#20219;&#21153;&#19978;&#65292;&#19981;&#21516;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#21487;&#20197;&#30456;&#24046;2.7&#20493;&#21040;136.3&#20493;&#65292;&#32780;&#20013;&#20301;&#25968;&#25968;&#25454;&#21253;&#31561;&#24453;&#26102;&#38388;&#36890;&#24120;&#27604;&#25512;&#29702;&#26102;&#38388;&#39640;6&#21040;8&#20010;&#25968;&#37327;&#32423;&#65281;ServeFlow&#33021;&#22815;&#22312;16ms&#20869;&#23545;76.3%&#30340;&#27969;&#37327;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#26159;&#20013;&#20301;&#25968;&#25512;&#29702;&#26102;&#38388;&#30340;40.5&#20493;&#21152;&#36895;&#65281;
&lt;/p&gt;
&lt;p&gt;
Network traffic analysis increasingly uses complex machine learning models as the internet consolidates and traffic gets more encrypted. However, over high-bandwidth networks, flows can easily arrive faster than model inference rates. The temporal nature of network flows limits simple scale-out approaches leveraged in other high-traffic machine learning applications. Accordingly, this paper presents ServeFlow, a solution for machine-learning model serving aimed at network traffic analysis tasks, which carefully selects the number of packets to collect and the models to apply for individual flows to achieve a balance between minimal latency, high service rate, and high accuracy. We identify that on the same task, inference time across models can differ by 2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of magnitude higher than the inference time! ServeFlow is able to make inferences on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median end-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#30340;&#35270;&#35282;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#34892;&#19994;&#21644;&#23454;&#36341;&#32773;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25351;&#23548;&#21644;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03688</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#30340;&#35270;&#35282;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#34892;&#19994;&#21644;&#23454;&#36341;&#32773;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25351;&#23548;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;Vertical Federated Learning&#65292;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#22810;&#20010;&#21442;&#19982;&#32773;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#26679;&#26412;&#38598;&#65292;&#20294;&#25345;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;VFL&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#38544;&#31169;&#23041;&#32961;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;VFL&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#26681;&#25454;&#29305;&#24449;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#22260;&#32469;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#23637;&#24320;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#38454;&#27573;&#36935;&#21040;&#30340;&#38544;&#31169;&#23041;&#32961;&#21450;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;&#36825;&#39033;&#35843;&#26597;&#26082;&#20026;&#30740;&#31350;&#30028;&#25552;&#20379;&#20102;&#36164;&#28304;&#65292;&#20063;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25351;&#23548;&#21644;&#21487;&#34892;&#30340;&#35265;&#35299;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>PPIretrieval&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;PPIs&#65292;&#24182;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.03675</link><description>&lt;p&gt;
&#20351;&#29992;PPIretrieval&#36827;&#34892;&#26377;&#25928;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Effective Protein-Protein Interaction Exploration with PPIretrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03675
&lt;/p&gt;
&lt;p&gt;
PPIretrieval&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;PPIs&#65292;&#24182;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;&#22312;&#35843;&#25511;&#35768;&#22810;&#32454;&#32990;&#21151;&#33021;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20449;&#21495;&#20256;&#23548;&#12289;&#36816;&#36755;&#21644;&#20813;&#30123;&#38450;&#24481;&#12290;&#38543;&#30528;&#22810;&#38142;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#32467;&#26500;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#65292;&#25361;&#25112;&#24050;&#32463;&#36716;&#21521;&#26377;&#25928;&#22320;&#23548;&#33322;&#24222;&#22823;&#30340;&#22797;&#26434;&#23431;&#23449;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;PPIs&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPIretrieval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25506;&#32034;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#29616;&#26377;&#30340;PPI&#25968;&#25454;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;&#30340;PPIs&#65292;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;&#24403;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#26597;&#35810;&#34507;&#30333;&#36136;&#21450;&#20854;&#30456;&#20851;&#30340;&#32467;&#21512;&#20301;&#28857;&#26102;&#65292;PPIretrieval&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#35782;&#21035;&#28508;&#22312;&#30340;&#32467;&#21512;&#20276;&#20387;&#21450;&#20854;&#30456;&#24212;&#30340;&#32467;&#21512;&#20301;&#28857;&#65292;&#20419;&#36827;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03667</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38388;&#25509;&#25512;&#29702;&#22120;&#65306;&#23545;&#33258;&#21160;&#25512;&#29702;&#30340;&#21453;&#35777;&#21644;&#30683;&#30462;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36981;&#24490;&#30452;&#25509;&#25512;&#29702;&#65288;DR&#65289;&#26694;&#26550;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#22240;&#27492;&#22312;&#35299;&#20915;&#24456;&#38590;&#36890;&#36807;DR&#35299;&#20915;&#30340;&#20247;&#22810;&#23454;&#38469;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38388;&#25509;&#25512;&#29702;&#65288;IR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#20107;&#23454;&#25512;&#29702;&#21644;&#25968;&#23398;&#35777;&#26126;&#31561;IR&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#35777;&#30340;&#36923;&#36753;&#31561;&#20215;&#24615;&#26469;&#22686;&#24378;LLMs&#30340;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#29702;&#35299;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#25552;&#31034;&#27169;&#26495;&#65292;&#35302;&#21457;LLMs&#36827;&#34892;&#22522;&#20110;&#30683;&#30462;&#35777;&#26126;&#30340;IR&#65292;&#20854;&#36923;&#36753;&#19978;&#31561;&#20215;&#20110;&#21407;&#22987;&#30340;DR&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;IR&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#20013;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;&#21407;&#21017;&#65292;&#21363;&#29992;&#20110;&#25512;&#29702;&#30340;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#39044;&#27979;&#24517;&#39035;&#19982;&#36755;&#20837;&#25968;&#25454;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#21305;&#37197;&#12290;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65292;&#24182;&#20026;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03663</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#31526;&#21495;&#23618;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Symbol Correctness in Deep Neural Networks Containing Symbolic Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#20013;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;&#21407;&#21017;&#65292;&#21363;&#29992;&#20110;&#25512;&#29702;&#30340;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#39044;&#27979;&#24517;&#39035;&#19982;&#36755;&#20837;&#25968;&#25454;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#21305;&#37197;&#12290;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65292;&#24182;&#20026;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#20197;&#24863;&#30693;&#21644;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#65292;&#23427;&#20204;&#38500;&#20102;&#20256;&#32479;&#30340;&#31070;&#32463;&#23618;&#20043;&#22806;&#65292;&#36824;&#21253;&#21547;&#31526;&#21495;&#23618;&#65306;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30001;&#31526;&#21495;&#27714;&#35299;&#22120;&#35780;&#20272;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#65288;&#20363;&#22914;&#65292;SAT&#20844;&#24335;&#65292;&#36923;&#36753;&#31243;&#24207;&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#30452;&#35266;&#12289;&#39640;&#23618;&#27425;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#25351;&#23548;NS-DNNs&#30340;&#35774;&#35745;&#21644;&#20998;&#26512;&#65306;&#31526;&#21495;&#27491;&#30830;&#24615;&#65292;&#21363;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#27491;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#65288;&#36890;&#24120;&#26410;&#30693;&#30340;&#65289;&#22522;&#26412;&#31526;&#21495;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65288;&#23613;&#31649;&#36890;&#24120;&#26080;&#27861;&#36827;&#34892;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31526;&#21495;&#27491;&#30830;&#24615;&#26694;&#26550;&#22312;&#31070;&#32463;&#31526;&#21495;&#36793;&#30028;&#22788;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22522;&#26412;&#26435;&#34913;&#26377;&#20102;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#21644;&#21487;&#29992;&#25968;&#25454;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#36827;&#34892;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;</title><link>https://arxiv.org/abs/2402.03661</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Transductive Reward Inference on Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#21644;&#21487;&#29992;&#25968;&#25454;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#36827;&#34892;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22870;&#21169;&#20449;&#24687;&#20256;&#25773;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#22870;&#21169;&#25512;&#26029;&#26159;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#20851;&#38190;&#65292;&#32780;&#30452;&#25509;&#30340;&#29615;&#22659;&#20132;&#20114;&#35201;&#20040;&#25104;&#26412;&#22826;&#39640;&#65292;&#35201;&#20040;&#26159;&#19981;&#36947;&#24503;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#21487;&#35775;&#38382;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#30340;&#19978;&#19979;&#25991;&#29305;&#24615;&#30340;&#22870;&#21169;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#26469;&#25512;&#26029;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#22870;&#21169;&#27880;&#37322;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#20854;&#20013;&#36793;&#26435;&#37325;&#21253;&#21547;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26500;&#24314;&#30340;&#22270;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03660</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#36328;&#20219;&#21153;&#32447;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#24050;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#27969;&#36235;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20174;&#20844;&#20849;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#65288;CTL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#25105;&#20204;&#32447;&#24615;&#25554;&#20540;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#26435;&#37325;&#25554;&#20540;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#22823;&#33268;&#31561;&#20110;&#27599;&#23618;&#20013;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#29305;&#24449;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#36825;&#26679;&#30340;&#36328;&#20219;&#21153;&#32447;&#24615;&#22312;&#21516;&#34892;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#27880;&#24847;&#21040;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25903;&#25345;&#20174;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24320;&#22987;&#30340;&#24494;&#35843;&#27169;&#22411;&#19968;&#33268;&#20986;&#29616;CTL&#12290;&#25105;&#20204;&#25512;&#27979;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26412;&#36136;&#19978;&#26159;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#12289;&#21442;&#25968;&#20849;&#20139;&#31561;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
&lt;/p&gt;</description></item><item><title>CAMBranch&#26159;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;MILP&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;MILP&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;CAMBranch&#33021;&#22815;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03647</link><description>&lt;p&gt;
CAMBranch: &#22522;&#20110;&#22686;&#24378;MILP&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20998;&#25903;
&lt;/p&gt;
&lt;p&gt;
CAMBranch: Contrastive Learning with Augmented MILPs for Branching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03647
&lt;/p&gt;
&lt;p&gt;
CAMBranch&#26159;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;MILP&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;MILP&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;CAMBranch&#33021;&#22815;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(B\&amp;B)&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#24378;&#20998;&#25903;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#24378;&#20998;&#25903;&#30340;&#26679;&#26412;&#65292;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMBranch: &#22522;&#20110;&#22686;&#24378;MILP&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26377;&#38480;&#25968;&#37327;&#30340;&#26469;&#33258;&#21407;&#22987;MILP&#30340;&#19987;&#23478;&#25968;&#25454;&#24212;&#29992;&#21464;&#37327;&#36716;&#31227;&#26469;&#29983;&#25104;&#22686;&#24378;MILP (AMILP)&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#12290;CAMBranch&#21033;&#29992;MILP&#21644;AMILP&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25429;&#25417;MILP&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&amp;B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#21152;&#36895;&#30340;&#36817;&#20284;&#26041;&#27861;torchmSAT&#26469;&#35299;&#20915;&#26368;&#22823;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#24494;&#20989;&#25968;&#21644;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;GPU&#21152;&#36895;&#35745;&#31639;&#26469;&#25552;&#39640;&#35299;&#20915;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#31181;MaxSAT&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03640</link><description>&lt;p&gt;
torchmSAT&#65306;&#19968;&#31181;GPU&#21152;&#36895;&#30340;&#26368;&#22823;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#21152;&#36895;&#30340;&#36817;&#20284;&#26041;&#27861;torchmSAT&#26469;&#35299;&#20915;&#26368;&#22823;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#20854;&#36890;&#36807;&#25512;&#23548;&#20986;&#21487;&#24494;&#20989;&#25968;&#21644;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;GPU&#21152;&#36895;&#35745;&#31639;&#26469;&#25552;&#39640;&#35299;&#20915;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#31181;MaxSAT&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#26512;&#31163;&#25955;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#19982;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#30340;&#37325;&#35270;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#27714;&#35299;&#24490;&#29615;&#20013;&#27880;&#20837;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#25628;&#32034;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#33021;&#22815;&#36817;&#20284;&#35299;&#20915;&#26368;&#22823;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;(MaxSAT)&#30340;&#21487;&#24494;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#24314;&#27169;&#25105;&#20204;&#30340;&#21487;&#24494;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36880;&#27493;&#27714;&#35299;MaxSAT&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38454;&#27573;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#23601;&#26159;&#35299;&#20915;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;GPU&#30340;&#35745;&#31639;&#33021;&#21147;&#21152;&#36895;&#36825;&#20123;&#35745;&#31639;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MaxSAT&#31034;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#31181;MaxSAT&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms. Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process. In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT). Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation. This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm. Additionally, we leverage the computational power of GPUs to accelerate these computations. Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT sol
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;IDE&#30340;&#26412;&#22320;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#25552;&#20379;&#30340;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#26469;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;&#65292;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03630</link><description>&lt;p&gt;
&#36890;&#36807;IDE&#27966;&#29983;&#30340;&#38745;&#24577;&#19978;&#19979;&#25991;&#30340;&#26412;&#22320;&#38598;&#25104;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03630
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;IDE&#30340;&#26412;&#22320;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#25552;&#20379;&#30340;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#26469;&#22686;&#24378;LLM-Based&#32534;&#30721;&#24037;&#20855;&#65292;&#35299;&#20915;&#20102;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#23436;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#22914;Copilot&#31561;&#20195;&#30721;&#21161;&#25163;&#26381;&#21153;&#30340;&#37325;&#35201;&#20316;&#29992;&#25152;&#35777;&#26126;&#12290;&#30446;&#21069;&#30340;LLM&#36890;&#36807;&#23545;&#25991;&#20214;&#19978;&#19979;&#25991;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#20010;&#28304;&#25991;&#20214;&#30340;&#20195;&#30721;&#23436;&#25104;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38656;&#35201;&#36328;&#25991;&#20214;&#20449;&#24687;&#30340;&#22823;&#22411;&#36719;&#20214;&#39033;&#30446;&#30340;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#65292;&#23545;&#23427;&#20204;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#30740;&#31350;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#25991;&#20214;&#19978;&#19979;&#25991;&#65292;&#20294;&#22240;LLM&#30340;&#31934;&#24230;&#20302;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;(IDE)&#21487;&#20197;&#20026;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#23436;&#25104;&#25552;&#20379;&#30452;&#25509;&#12289;&#20934;&#30830;&#21644;&#23454;&#26102;&#30340;&#36328;&#25991;&#20214;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IDECoder&#65292;&#19968;&#20010;&#23454;&#38469;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;IDE&#30340;&#26412;&#22320;&#38745;&#24577;&#19978;&#19979;&#25991;&#36827;&#34892;&#36328;&#19978;&#19979;&#25991;&#26500;&#24314;&#21644;&#33258;&#25105;&#23436;&#21892;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;IDECoder&#21033;&#29992;&#20016;&#23500;&#30340;&#36328;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.03627</link><description>&lt;p&gt;
&#36817;&#20284;&#30340;&#20013;&#24515;&#21270;softmax&#25439;&#22833;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partially Recentralization Softmax Loss for Vision-Language Models Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#31361;&#30772;&#65292;&#22810;&#27169;&#24577;&#25216;&#26415;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#32780;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#34429;&#28982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#25216;&#26415;&#65292;&#20294;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36824;&#27809;&#26377;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#26469;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#35780;&#20272;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#65292;&#23545;&#25239;&#24120;&#35265;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#24212;&#35813;&#25506;&#32034;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20043;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#27010;&#29575;&#30005;&#36335;&#20013;&#36793;&#38469;MAP&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#20272;&#35745;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#24182;&#23558;&#20854;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#33258;&#25105;&#30417;&#30563;&#21644;&#39640;&#25928;&#24615;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.03621</link><description>&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#20013;&#29992;&#20110;&#36793;&#38469;MAP&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Network Approximators for Marginal MAP in Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#27010;&#29575;&#30005;&#36335;&#20013;&#36793;&#38469;MAP&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#20272;&#35745;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#24182;&#23558;&#20854;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#33258;&#25105;&#30417;&#30563;&#21644;&#39640;&#25928;&#24615;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#22914;&#21644;&#31215;&#32593;&#32476;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#22810;&#21464;&#37327;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#31561;&#20854;&#20182;&#27010;&#29575;&#34920;&#31034;&#30456;&#27604;&#65292;PCs&#26356;&#21463;&#38738;&#30544;&#65292;&#22240;&#20026;PCs&#21487;&#20197;&#22312;&#32593;&#32476;&#22823;&#23567;&#32447;&#24615;&#25193;&#23637;&#30340;&#26102;&#38388;&#20869;&#35299;&#20915;&#36793;&#38469;&#25512;&#29702;&#65288;MAR&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#21644;&#36793;&#38469;MAP&#65288;MMAP&#65289;&#20219;&#21153;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#20173;&#28982;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#21463;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;&#22914;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;PCs&#20013;&#30340;(M)MAP&#25512;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#36817;&#20284;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65292;&#21363;&#33258;&#25105;&#30417;&#30563;&#21644;&#22312;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20043;&#21518;&#65292;&#23427;&#21482;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires 
&lt;/p&gt;</description></item><item><title>SELF-DISCOVER&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#33021;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#35745;&#31639;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03620</link><description>&lt;p&gt;
&#33258;&#25105;&#21457;&#29616;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;&#25512;&#29702;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Self-Discover: Large Language Models Self-Compose Reasoning Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03620
&lt;/p&gt;
&lt;p&gt;
SELF-DISCOVER&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#33021;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#35745;&#31639;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SELF-DISCOVER&#65292;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;LLM&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#20197;&#24212;&#23545;&#23545;&#20110;&#20856;&#22411;&#25552;&#31034;&#26041;&#27861;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#33258;&#25105;&#21457;&#29616;&#36807;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;LLM&#36873;&#25321;&#22810;&#20010;&#21407;&#23376;&#25512;&#29702;&#27169;&#22359;&#65292;&#22914;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#36880;&#27493;&#24605;&#32771;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;LLM&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36981;&#24490;&#30340;&#26126;&#30830;&#25512;&#29702;&#32467;&#26500;&#12290;SELF-DISCOVER&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22914;BigBench-Hard&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#25512;&#29702;&#21644;MATH&#19978;&#65292;&#30456;&#36739;&#20110;Chain of Thought (CoT)&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;32%&#12290;&#27492;&#22806;&#65292;SELF-DISCOVER&#22312;&#38656;&#35201;10-40&#20493;&#36739;&#23569;&#25512;&#29702;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;CoT-Self-Consistency&#31561;&#25512;&#29702;&#23494;&#38598;&#26041;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#65292;&#36229;&#36807;20%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#21457;&#29616;&#30340;&#25512;&#29702;&#32467;&#26500;&#22312;&#27169;&#22411;&#23478;&#26063;&#20043;&#38388;&#26159;&#26222;&#36941;&#36866;&#29992;&#30340;&#65306;&#20174;PaLM 2-L&#21040;GPT-4&#65292;&#20174;GPT-4&#21040;Llama2&#65292;&#24182;&#20998;&#20139;&#20102;
&lt;/p&gt;
&lt;p&gt;
We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-4&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03618</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03618
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-4&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20174;&#22024;&#26434;&#30340;&#24863;&#23448;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#19990;&#30028;&#25277;&#35937;&#12290;&#36830;&#32493;&#22797;&#21046;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#31867;&#20284;&#20110;&#30005;&#35805;&#28216;&#25103;&#30340;&#33539;&#24335;&#26469;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#26500;&#24314;&#19990;&#30028;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#35266;&#23519;&#19968;&#20010;&#21050;&#28608;&#24182;&#23558;&#20854;&#22797;&#21046;&#32473;&#19979;&#19968;&#20010;&#20154;&#24418;&#25104;&#19968;&#26465;&#22797;&#21046;&#38142;&#12290;&#36807;&#21435;&#30340;&#36830;&#32493;&#22797;&#21046;&#23454;&#39564;&#36890;&#24120;&#37319;&#29992;&#21333;&#19968;&#24863;&#35273;&#27169;&#24335;&#65292;&#20294;&#20154;&#31867;&#32463;&#24120;&#36890;&#36807;&#35821;&#35328;&#30456;&#20114;&#20256;&#36798;&#19990;&#30028;&#30340;&#25277;&#35937;&#12290;&#20026;&#20102;&#35843;&#26597;&#35821;&#35328;&#23545;&#25277;&#35937;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;&#25509;&#25910;&#35270;&#35273;&#21050;&#28608;&#30340;&#20154;&#20197;&#35821;&#35328;&#24418;&#24335;&#26469;&#22797;&#21046;&#23427;&#65292;&#24182;&#21453;&#20043;&#20134;&#28982;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;GPT-4&#36827;&#34892;&#20102;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#22797;&#21046;&#65292;&#24182;&#21457;&#29616;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#12290;&#36825;&#34920;&#26126;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#30340;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#26041;&#38754;&#20855;&#26377;&#36229;&#36234;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#39640;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.03616</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Hybrid Workplace Decision Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#30340;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#26041;&#38754;&#20855;&#26377;&#36229;&#36234;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#39640;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25191;&#34892;&#21508;&#31181;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#24182;&#20026;&#25552;&#35758;&#30340;&#25805;&#20316;&#25110;&#20915;&#31574;&#25552;&#20379;&#25991;&#26412;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#22312;&#28151;&#21512;&#24037;&#20316;&#26102;&#20195;&#65292;LLMs&#21487;&#20197;&#20026;&#35774;&#35745;&#28151;&#21512;&#24037;&#20316;&#35745;&#21010;&#30340;&#24037;&#20316;&#32773;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#12290;&#29305;&#21035;&#26159;&#23427;&#20204;&#21487;&#20197;&#20026;&#24179;&#34913;&#20247;&#22810;&#20915;&#31574;&#22240;&#32032;&#30340;&#24037;&#20316;&#32773;&#25552;&#20379;&#24314;&#35758;&#21644;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#24378;&#20182;&#20204;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#28151;&#21512;&#24037;&#20316;&#29615;&#22659;&#20013;&#24037;&#20316;&#21306;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#23545;&#20110;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#36229;&#36234;&#20102;&#25552;&#31034;&#20013;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;LLMs&#21487;&#20197;&#22312;&#24037;&#20316;&#21306;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#24037;&#20316;&#32773;&#22312;&#24037;&#20316;&#21306;&#36873;&#25321;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35780;&#20272;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24037;&#20316;&#32773;&#30340;&#20915;&#31574;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03610</link><description>&lt;p&gt;
RAP&#65306;&#20855;&#26377;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21487;&#20197;&#34987;&#37096;&#32626;&#20026;&#29992;&#20110;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;API&#38598;&#25104;&#31561;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20915;&#31574;&#24212;&#29992;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#25351;&#23548;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#65288;RAP&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21160;&#24577;&#22320;&#21033;&#29992;&#36807;&#21435;&#19982;&#24403;&#21069;&#24773;&#20917;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#32463;&#39564;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;RAP&#30340;&#29305;&#28857;&#22312;&#20110;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#23427;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;RAP&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;SOTA&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;RAP&#22312;&#25512;&#36827;LLM&#30340;&#21151;&#33021;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03607</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#33829;&#38144;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#65306;&#30693;&#35782;&#22522;&#30784;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#30340;&#26222;&#21450;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#32447;&#20307;&#39564;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#27169;&#22411;&#65288;LVM&#65289;&#20173;&#28982;&#21463;&#21040;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#20851;&#31995;&#30340;&#25972;&#20307;&#24847;&#20041;&#30340;&#38480;&#21046;&#12290;&#32570;&#20047;&#26126;&#30830;&#30340;&#24120;&#35782;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65289;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20165;&#36890;&#36807;&#25429;&#25417;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#39640;&#32423;&#27169;&#24335;&#26469;&#23398;&#20064;&#38544;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#26174;&#24335;&#30340;&#24120;&#35782;&#30693;&#35782;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#19982;&#22823;&#22411;&#30340;VLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#33829;&#38144;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#26102;&#38656;&#35201;&#32771;&#34385;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#35266;&#27979;RL&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03590</link><description>&lt;p&gt;
&#35780;&#20272;&#20998;&#24067;&#36716;&#21464;&#23545;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Distribution Shift on Reinforcement Learning Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03590
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#26102;&#38656;&#35201;&#32771;&#34385;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#35266;&#27979;RL&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#27491;&#22312;&#35299;&#20915;&#33258;&#36523;&#30340;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#29305;&#21035;&#26159;&#65292;&#24378;&#21270;&#23398;&#20064;(RL)&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#27604;&#36739;&#28857;&#20272;&#35745;&#21644;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#31034;&#25104;&#21151;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#22270;&#34920;&#21487;&#33021;&#20250;&#25513;&#30422;&#36807;&#25311;&#21512;&#25110;&#23545;&#23454;&#39564;&#35774;&#32622;&#30340;&#20381;&#36182;&#24615;&#12290;&#34429;&#28982;RL&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21487;&#38752;&#24615;&#25351;&#26631;&#20197;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27599;&#20010;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20294;&#36807;&#21435;&#30340;&#24037;&#20316;&#24314;&#35758;&#24182;&#19981;&#20551;&#35774;&#23384;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#35266;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#34913;&#37327;&#20102;&#22312;&#20998;&#24067;&#36716;&#21464;&#19979;RL&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#24037;&#20855;&#25903;&#25345;&#22312;&#20195;&#29702;&#22312;&#20854;&#29615;&#22659;&#20013;&#34892;&#21160;&#26102;&#32771;&#34385;&#24615;&#33021;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23588;&#20854;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20316;&#20026;&#35266;&#27979;RL&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RL&#21644;&#27169;&#25311;&#21160;&#21147;&#23398;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#20010;&#20165;&#22312;&#28304;&#22495;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20943;&#23569;&#20102;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.03588</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Continual Domain Adversarial Adaptation via Double-Head Discriminators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#20010;&#20165;&#22312;&#28304;&#22495;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20943;&#23569;&#20102;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#35774;&#32622;&#19979;&#30340;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23384;&#22312;&#23545;&#20043;&#21069;&#30340;&#28304;&#22495;&#25968;&#25454;&#30340;&#35775;&#38382;&#38480;&#21046;&#12290;&#23613;&#31649;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20165;&#20165;&#20351;&#29992;&#23569;&#37327;&#23384;&#20648;&#30340;&#28304;&#22495;&#25968;&#25454;&#65288;&#36825;&#26159;&#35760;&#24518;&#37325;&#25773;&#26041;&#27861;&#20013;&#30340;&#26631;&#20934;&#35774;&#32622;&#65289;&#26080;&#27861;&#26377;&#25928;&#23436;&#25104;&#23545;&#25239;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#20010;&#38480;&#21046;&#26469;&#33258;&#20110;&#20351;&#29992;&#23569;&#37327;&#28304;&#22495;&#26679;&#26412;&#23545;$\gH$-divergence&#36827;&#34892;&#32463;&#39564;&#20272;&#35745;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#22836;&#21028;&#21035;&#22120;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20165;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20174;&#28304;&#22495;&#19968;&#20391;&#20943;&#23569;&#20102;$\gH$-divergence&#30456;&#20851;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#12290;&#36827;&#19968;&#27493;&#22312;&#29616;&#26377;&#30340;&#39046;&#22495;&#36866;&#24212;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\%$
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03583</link><description>&lt;p&gt;
MQuinE:&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20013;&#8220;Z-&#24726;&#35770;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03583
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;KGE&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#27969;&#34892;&#30340;&#29616;&#26377;KGE&#27169;&#22411;&#23384;&#22312;&#34920;&#36798;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;Z-&#24726;&#35770;&#8221;&#12290;&#21463;&#21040;Z-&#24726;&#35770;&#30340;&#23384;&#22312;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KGE&#27169;&#22411;&#65292;&#31216;&#20026;MQuinE&#65292;&#22312;&#19981;&#21463;Z-&#24726;&#35770;&#30340;&#22256;&#25200;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;/&#38750;&#23545;&#31216;&#65292;&#36870;&#21521;&#65292;1-N/N-1/N-N&#21644;&#32452;&#21512;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#23545;&#23454;&#38469;&#30693;&#35782;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Z-&#24726;&#35770;&#30830;&#23454;&#38477;&#20302;&#20102;&#29616;&#26377;KGE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#36229;&#36807;20&#65285;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;MQuinE&#21487;&#20197;&#20943;&#36731;Z-&#24726;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20197;&#26126;&#26174;&#20248;&#21183;&#36229;&#36234;&#29616;&#26377;&#30340;KGE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25361;&#25112;&#19982;&#24320;&#25918;&#38382;&#39064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#12289;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12289;&#31649;&#29702;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25913;&#21892;&#20869;&#23384;&#31649;&#29702;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21306;&#22359;&#38142;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.03578</link><description>&lt;p&gt;
LLM&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;&#25361;&#25112;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LLM Multi-Agent Systems: Challenges and Open Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25361;&#25112;&#19982;&#24320;&#25918;&#38382;&#39064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#12289;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#12289;&#31649;&#29702;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25913;&#21892;&#20869;&#23384;&#31649;&#29702;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21306;&#22359;&#38142;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29616;&#26377;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#35782;&#21035;&#20986;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#22810;&#26679;&#33021;&#21147;&#21644;&#35282;&#33394;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#21327;&#20316;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20248;&#21270;&#20219;&#21153;&#20998;&#37197;&#12289;&#36890;&#36807;&#36845;&#20195;&#36777;&#35770;&#20419;&#36827;&#24378;&#22823;&#25512;&#29702;&#12289;&#31649;&#29702;&#22797;&#26434;&#21644;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#21450;&#22686;&#24378;&#20869;&#23384;&#31649;&#29702;&#20197;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20869;&#30340;&#22797;&#26434;&#20132;&#20114;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22312;&#21306;&#22359;&#38142;&#31995;&#32479;&#20013;&#24212;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#20197;&#21551;&#31034;&#20854;&#22312;&#30495;&#23454;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#21644;&#35757;&#32451;AI&#20195;&#29702;&#26469;&#27604;&#36739;&#21644;&#23545;&#27604;&#20154;&#31867;&#21644;AI&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#39640;&#32423;&#34892;&#20026;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.03575</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Toward Human-AI Alignment in Large-Scale Multi-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#22810;&#20154;&#28216;&#25103;&#20013;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#21644;&#35757;&#32451;AI&#20195;&#29702;&#26469;&#27604;&#36739;&#21644;&#23545;&#27604;&#20154;&#31867;&#21644;AI&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#39640;&#32423;&#34892;&#20026;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#21516;&#23545;&#20110;&#21019;&#24314;&#22686;&#24378;&#28216;&#25103;&#20307;&#39564;&#30340;&#21487;&#20449;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#31181;&#21327;&#21516;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#38598;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#39640;&#32423;&#34892;&#20026;&#20219;&#21153;&#32780;&#38750;&#20302;&#32423;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;Xbox&#30340;Bleeding Edge&#65288;10&#19975;+&#28216;&#25103;&#65289;&#30340;&#22823;&#37327;&#20154;&#31867;&#28216;&#25103;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#22797;&#26434;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20316;&#20026;&#34892;&#20026;&#27969;&#24418;&#30340;&#22522;&#30784;&#38598;&#21512;&#65292;&#25429;&#25417;&#21487;&#35299;&#37322;&#30340;&#36724;&#65306;&#25112;&#26007;-&#36867;&#36305;&#12289;&#25506;&#32034;-&#21033;&#29992;&#20197;&#21450;&#21333;&#20154;-&#22810;&#20154;&#26234;&#33021;&#20307;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#22240;&#26524;&#21464;&#25442;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#29609;Bleeding Edge&#65292;&#24182;&#27979;&#37327;&#20854;&#34892;&#20026;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#28216;&#25103;&#26144;&#23556;&#21040;&#25552;&#20986;&#30340;&#34892;&#20026;&#27969;&#24418;&#20013;&#36827;&#34892;&#27604;&#36739;&#21644;&#23545;&#27604;&#12290;&#36825;&#26679;&#21487;&#20197;&#35299;&#37322;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#29609;&#23478;&#22312;&#25112;&#26007;-&#36867;&#36305;&#26041;&#38754;&#34920;&#29616;&#21464;&#21270;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03561</link><description>&lt;p&gt;
VLN-Video: &#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#36827;&#34892;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03561
&lt;/p&gt;
&lt;p&gt;
VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#36924;&#30495;&#30340;&#19977;&#32500;&#23460;&#22806;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#29616;&#26377;&#30340;VLN&#26041;&#27861;&#22312;&#23548;&#33322;&#29615;&#22659;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VLN-Video&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#32654;&#22269;&#22810;&#20010;&#22478;&#24066;&#30340;&#34892;&#36710;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#23460;&#22806;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#23548;&#33322;&#25351;&#20196;&#21644;&#21160;&#20316;&#26469;&#25552;&#39640;&#23460;&#22806;VLN&#24615;&#33021;&#12290;VLN-Video&#32467;&#21512;&#20102;&#30452;&#35266;&#32463;&#20856;&#26041;&#27861;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#27169;&#26495;&#22635;&#20805;&#29983;&#25104;&#26377;&#23454;&#38469;&#22522;&#30784;&#30340;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22270;&#20687;&#26059;&#36716;&#30456;&#20284;&#24230;&#30340;&#23548;&#33322;&#21160;&#20316;&#39044;&#27979;&#22120;&#20174;&#34892;&#36710;&#35270;&#39057;&#20013;&#33719;&#21462;VLN&#39118;&#26684;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;VLN&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Touchdown&#25968;&#25454;&#38598;&#21644;&#30001;&#34892;&#36710;&#35270;&#39057;&#21019;&#24314;&#30340;&#35270;&#39057;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#22238;&#31572;&#38598;&#32534;&#31243;&#30340;&#32467;&#26500;&#21442;&#25968;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26680;&#20197;&#23454;&#29616;&#19982;&#39030;&#28857;&#35206;&#30422;&#22823;&#23567;&#23545;&#24212;&#30340;&#21333;&#25351;&#25968;&#26102;&#38388;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.03539</link><description>&lt;p&gt;
&#23545;&#22238;&#31572;&#38598;&#32534;&#31243;&#30340;&#32467;&#26500;&#38590;&#24230;&#36827;&#34892;&#30340;&#25193;&#23637;&#30740;&#31350;&#65306;&#32467;&#26500;&#33021;&#21542;&#26377;&#25928;&#38480;&#21046;&#26512;&#21462;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Extended Version of: On the Structural Hardness of Answer Set Programming: Can Structure Efficiently Confine the Power of Disjunctions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03539
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#22238;&#31572;&#38598;&#32534;&#31243;&#30340;&#32467;&#26500;&#21442;&#25968;&#30340;&#20998;&#31867;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#26680;&#20197;&#23454;&#29616;&#19982;&#39030;&#28857;&#35206;&#30422;&#22823;&#23567;&#23545;&#24212;&#30340;&#21333;&#25351;&#25968;&#26102;&#38388;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#38382;&#39064;&#24314;&#27169;&#21644;&#27714;&#35299;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#34920;&#31034;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22797;&#26434;&#24615;&#30340;&#30740;&#31350;&#23548;&#33268;&#20102;&#38590;&#24230;&#30340;&#29305;&#24449;&#21270;&#21644;&#30830;&#23450;&#20854;&#26469;&#28304;&#65292;&#20197;&#21450;&#31867;&#20284;&#20108;&#20998;&#27861;&#32467;&#26524;&#30340;&#32454;&#33410;&#24615;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#24050;&#30693;&#30340;&#21442;&#25968;&#26641;&#23485;&#19979;&#65292;&#21512;&#21462;&#35268;&#27169;&#30340;&#38750;&#21512;&#21462;&#31243;&#24207;&#22312;&#21512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20551;&#35774;&#19979;&#38656;&#35201;&#21452;&#25351;&#25968;&#26102;&#38388;&#36816;&#34892;&#65292;&#36825;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#36798;&#21040;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20851;&#20110;&#38750;&#21512;&#21462;ASP&#30340;&#32467;&#26500;&#21442;&#25968;&#30340;&#20998;&#31867;&#38382;&#39064;&#65288;&#20837;&#23556;&#22270;&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26680;&#26469;&#33719;&#24471;&#19982;&#39030;&#28857;&#35206;&#30422;&#22823;&#23567;&#23545;&#24212;&#30340;&#21333;&#25351;&#25968;&#26102;&#38388;&#36816;&#34892;&#65292;&#23613;&#31649;&#23376;&#38598;&#26368;&#23567;&#21270;&#22312;&#31243;&#24207;&#30340;&#32467;&#26500;&#20013;&#27809;&#26377;&#34987;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#39030;&#28857;&#35206;&#30422;&#22823;&#23567;&#21644;&#26641;&#23485;&#20043;&#38388;&#20005;&#26684;&#26356;&#22909;&#30340;&#32467;&#26500;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answer Set Programming (ASP) is a generic problem modeling and solving framework with a strong focus on knowledge representation and a rapid growth of industrial applications. So far, the study of complexity resulted in characterizing hardness and determining their sources, fine-grained insights in the form of dichotomy-style results, as well as detailed parameterized complexity landscapes. Unfortunately, for the well-known parameter treewidth disjunctive programs require double-exponential runtime under reasonable complexity assumptions. This quickly becomes out of reach. We deal with the classification of structural parameters for disjunctive ASP on the program's rule structure (incidence graph).   First, we provide a polynomial kernel to obtain single-exponential runtime in terms of vertex cover size, despite subset-minimization being not represented in the program's structure. Then we turn our attention to strictly better structural parameters between vertex cover size and treewidt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;Mantis Shrimp&#65292;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22810;&#35843;&#26597;&#20809;&#24230;&#32418;&#31227;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32043;&#22806;&#32447;&#12289;&#20809;&#23398;&#21644;&#32418;&#22806;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#35786;&#26029;&#25216;&#26415;&#30740;&#31350;&#20102;&#20854;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#36755;&#20837;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.03535</link><description>&lt;p&gt;
&#23545;&#34430;&#30340;&#21021;&#27493;&#25253;&#21578;&#65306;&#22810;&#35843;&#26597;&#35745;&#31639;&#26426;&#35270;&#35273;&#20809;&#24230;&#32418;&#31227;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;Mantis Shrimp&#65292;&#19968;&#20010;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22810;&#35843;&#26597;&#20809;&#24230;&#32418;&#31227;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#32043;&#22806;&#32447;&#12289;&#20809;&#23398;&#21644;&#32418;&#22806;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#35786;&#26029;&#25216;&#26415;&#30740;&#31350;&#20102;&#20854;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#36755;&#20837;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#12289;&#20844;&#20849;&#12289;&#22810;&#27169;&#24577;&#22825;&#25991;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20026;&#36328;&#31185;&#23398;&#19982;&#22825;&#25991;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20809;&#24230;&#32418;&#31227;&#20272;&#35745;&#26159;&#22825;&#25991;&#23398;&#30340;&#19968;&#20010;&#25104;&#29087;&#23376;&#39046;&#22495;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#20248;&#20110;&#22522;&#20110;&#30446;&#24405;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#22312;&#34701;&#21512;&#26469;&#33258;&#22810;&#20010;&#20202;&#22120;&#25110;&#20256;&#24863;&#22120;&#30340;&#22270;&#20687;&#26102;&#38754;&#20020;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#21019;&#24314;&#30340;Mantis Shrimp&#65292;&#19968;&#20010;&#29992;&#20110;&#20809;&#24230;&#32418;&#31227;&#20272;&#35745;&#30340;&#22810;&#35843;&#26597;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#23427;&#34701;&#21512;&#20102;&#32043;&#22806;&#32447;&#65288;GALEX&#65289;&#12289;&#20809;&#23398;&#65288;PanSTARRS&#65289;&#21644;&#32418;&#22806;&#65288;UnWISE&#65289;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#35786;&#26029;&#26469;&#34913;&#37327;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#36755;&#20837;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#20174;&#29289;&#29702;&#19978;&#22522;&#20110;&#30340;&#26143;&#31995;&#23646;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;CNN&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of large, public, multi-modal astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy. Photometric redshift estimation is a well-established subfield of astronomy. Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor. In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs. We reason about the behavior of the CNNs from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#24182;&#33021;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03525</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Picker Routing Problem in Warehousing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#24182;&#33021;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35746;&#21333;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#26159;&#20179;&#24211;&#36816;&#33829;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#23454;&#36341;&#20013;&#24120;&#24120;&#20351;&#29992;&#27425;&#20248;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#65292;&#21487;&#33021;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#21442;&#25968;&#19978;&#19982;&#29616;&#26377;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#28857;&#26159;&#21487;&#20197;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning. Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#20219;&#21153;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#20013;&#25552;&#39640;&#20102;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#21644;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03519</link><description>&lt;p&gt;
&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#35299;&#20915;&#36716;&#24405;&#27495;&#20041;&#65306;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#29992;&#20110;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#20219;&#21153;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#20013;&#25552;&#39640;&#20102;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#21644;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#21518;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20197;&#22686;&#24378;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#24182;&#20419;&#36827;&#21518;&#32493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35789;&#27719;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#20013;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#26410;&#26631;&#28857;&#30340;&#38472;&#36848;&#21477;&#21644;&#30097;&#38382;&#21477;&#20043;&#38388;&#24120;&#24120;&#23384;&#22312;&#27495;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#31995;&#32479;&#65292;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#36807;&#31243;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#20844;&#20849;&#21644;&#20869;&#37096;&#30340;&#35199;&#29677;&#29273;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#27604;&#36739;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ASR&#27169;&#22359;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20063;&#21463;&#30410;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03509</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#35780;&#20272;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#38646;&#26679;&#26412;&#65288;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65289;&#29983;&#25104;&#25688;&#35201;&#65292;&#32463;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#36825;&#20123;&#25688;&#35201;&#24448;&#24448;&#19982;&#25163;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25688;&#35201;&#30456;&#27604;&#65292;&#29978;&#33267;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#30740;&#31350;&#20960;&#20046;&#19987;&#27880;&#20110;&#35780;&#20272;&#26032;&#38395;&#25991;&#31456;&#30340;&#25688;&#35201;&#12290;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#22312;&#20854;&#20182;&#65288;&#21487;&#33021;&#26356;&#19987;&#19994;&#65289;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36328;&#19987;&#19994;&#39046;&#22495;&#20013;&#38646;&#26679;&#26412;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#65288;&#38500;&#20102;&#26631;&#20934;&#26032;&#38395;&#25688;&#35201;&#30340;&#21442;&#32771;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#20174;&#39046;&#22495;&#19987;&#23478;&#22788;&#33719;&#21462;&#27880;&#37322;&#65292;&#20197;&#35782;&#21035;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#31995;&#32479;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#26159;&#21542;&#20250;&#24433;&#21709;&#22312;&#35813;&#39046;&#22495;&#30340;&#25991;&#31456;&#30340;&#25688;&#35201;&#30340;&#25552;&#21462;&#21644;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03507</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#21644;&#25512;&#29702;&#65306;&#36808;&#21521;&#26426;&#22120;&#30340;&#24191;&#27867;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural networks for abstraction and reasoning: Towards broad generalization in machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03507
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#19968;&#30452;&#35797;&#22270;&#22797;&#21046;&#20154;&#31867;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#33021;&#21147;-&#21019;&#36896;&#20986;&#33021;&#22815;&#20174;&#19968;&#23567;&#32452;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#65292;&#22312;&#20154;&#20204;&#21457;&#29616;&#36825;&#24456;&#23481;&#26131;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#36229;&#36234;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#35299;&#20915;&#25277;&#35937;&#19982;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;ARC&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#31639;&#27861;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#26377;&#19977;&#20010;&#22269;&#38469;&#31454;&#36187;&#25552;&#20379;&#20102;10&#19975;&#32654;&#20803;&#30340;&#22870;&#37329;&#65292;&#26368;&#22909;&#30340;&#31639;&#27861;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#22823;&#22810;&#25968;ARC&#20219;&#21153;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25163;&#24037;&#35268;&#21017;&#65292;&#27809;&#26377;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#26159;&#21542;&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;DreamCoder&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#27714;&#35299;&#22120;&#24212;&#29992;&#21040;ARC&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction &amp; Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#20272;&#35745;&#12289;&#20462;&#22797;&#25513;&#27169;&#21019;&#24314;&#21644;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#29031;&#29255;&#20013;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#21644;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.03501</link><description>&lt;p&gt;
&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
An Inpainting-Infused Pipeline for Attire and Background Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#20272;&#35745;&#12289;&#20462;&#22797;&#25513;&#27169;&#21019;&#24314;&#21644;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#29031;&#29255;&#20013;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#21644;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#24341;&#21457;&#20102;&#19968;&#22330;&#21464;&#38761;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#20855;&#20307;&#25506;&#35752;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#26469;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21019;&#24314;&#20462;&#22797;&#25513;&#27169;&#12289;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#29983;&#25104;&#21644;&#26367;&#25442;&#32972;&#26223;&#65292;&#36890;&#36807;&#20462;&#22797;&#27969;&#31243;&#26367;&#25442;&#26381;&#35013;&#21644;&#24212;&#29992;&#23457;&#32654;&#20462;&#25913;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#20854;&#20135;&#29983;&#35270;&#35273;&#21560;&#24341;&#21147;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#20351;&#29992;&#25143;&#33021;&#22815;&#36755;&#20837;&#20010;&#20154;&#29031;&#29255;&#24182;&#23545;&#20854;&#36827;&#34892;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65288;CRLQAS&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#30340;&#19981;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03500</link><description>&lt;p&gt;
&#37327;&#23376;&#30828;&#20214;&#38169;&#35823;&#19979;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Curriculum reinforcement learning for quantum architecture search under hardware errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65288;CRLQAS&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#30340;&#19981;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#65292;&#25214;&#21040;&#19982;&#24403;&#21069;&#35774;&#22791;&#38480;&#21046;&#20860;&#23481;&#30340;&#26377;&#29992;&#30005;&#36335;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#36890;&#36807;&#22266;&#23450;&#30005;&#36335;&#26550;&#26500;&#21644;&#20248;&#21270;&#22806;&#37096;&#24490;&#29615;&#20013;&#30340;&#20010;&#21035;&#38376;&#21442;&#25968;&#26469;&#25552;&#20379;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;&#20248;&#21270;&#21487;&#33021;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#21021;&#22987;&#36873;&#25321;&#30340;&#30005;&#36335;&#26550;&#26500;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#35774;&#35745;&#26377;&#29992;&#30340;&#30005;&#36335;&#26550;&#26500;&#12290;&#22312;&#20165;&#38480;&#21442;&#25968;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#35266;&#23519;&#21040;&#22122;&#22768;&#25928;&#24212;&#20005;&#37325;&#24433;&#21709;&#20248;&#21270;&#22120;&#21644;&#26368;&#32456;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#26465;&#37325;&#35201;&#30340;&#30740;&#31350;&#32447;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#65292;&#21487;&#33021;&#21516;&#26679;&#37325;&#35201;&#30340;&#38382;&#39064;&#30446;&#21069;&#36824;&#19981;&#22826;&#29702;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;QAS&#65288;CRLQAS&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm desi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.03494</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#23383;&#65306;&#36890;&#36807;&#35821;&#38899;&#32447;&#32034;&#25913;&#21892;LLM&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#34920;&#26126;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#23545;&#35805;&#30340;&#27169;&#24577;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;LLM&#22312;&#22788;&#29702;&#25991;&#26412;&#26041;&#38754;&#22312;&#36825;&#20123;&#20154;&#26426;&#23545;&#35805;&#20013;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#31038;&#20132;&#23548;&#33322;&#31561;&#24773;&#22659;&#19979;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#21475;&#22836;&#25351;&#20196;&#30340;&#32454;&#24494;&#20043;&#22788;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#20854;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#25991;&#23383;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#38899;&#39057;&#22238;&#24212;&#30340;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#21475;&#22836;&#20132;&#27969;&#20013;&#19981;&#28041;&#21450;&#25991;&#23383;&#25514;&#36766;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#24847;&#20041;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36229;&#36234;&#25991;&#23383;&#8221;&#65307;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#38899;&#39057;&#36716;&#24405;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#30340;&#37096;&#20998;&#26469;&#25913;&#21892;LLM&#20915;&#31574;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#20391;&#37325;&#24773;&#24863;&#21644;&#26356;&#19982;&#20154;&#26426;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20020;&#24202;&#25968;&#25454;&#20013;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;XGBoost&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03486</link><description>&lt;p&gt;
&#20020;&#24202;&#29615;&#22659;&#20013;&#26089;&#26399;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Early prediction of onset of sepsis in Clinical Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20020;&#24202;&#25968;&#25454;&#20013;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;XGBoost&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#32445;&#32422;&#24067;&#26391;&#20811;&#26031;Montefiore&#21307;&#30103;&#20013;&#24515;&#30340;&#21435;&#26631;&#35782;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#12290;&#37319;&#29992;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;80%&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;107&#20010;&#29305;&#24449;&#65288;&#21253;&#25324;&#21407;&#22987;&#21644;&#34893;&#29983;&#29305;&#24449;&#65289;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#22312;&#21097;&#20313;&#30340;20%&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#23436;&#20840;&#26410;&#30693;&#30340;&#21069;&#30651;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#22312;&#20010;&#20307;&#24739;&#32773;&#27700;&#24179;&#19978;&#30340;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#21450;&#26102;&#24615;&#65292;&#20351;&#29992;&#20102;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#65292;&#36825;&#26159;&#33043;&#27602;&#30151;&#26816;&#27979;&#20013;&#24191;&#27867;&#35748;&#21487;&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#22914;PhysioNet Sepsis Challenge&#35770;&#25991;&#20013;&#25152;&#36848;&#12290;&#36824;&#35774;&#35745;&#20102;F1&#20540;&#12289;&#25935;&#24863;&#24615;&#12289;&#29305;&#24322;&#24615;&#21644;&#26631;&#24535;&#29575;&#31561;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#20026;0.494&#65292;&#22312;&#21069;&#30651;&#25968;&#25454;&#19978;&#30340;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#20026;0.378&#65288;&#38408;&#20540;&#20026;0.3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The
&lt;/p&gt;</description></item><item><title>SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;</title><link>https://arxiv.org/abs/2402.03483</link><description>&lt;p&gt;
SWAG: &#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;
&lt;/p&gt;
&lt;p&gt;
SWAG: Storytelling With Action Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03483
&lt;/p&gt;
&lt;p&gt;
SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38271;&#31687;&#25925;&#20107;&#29983;&#25104;&#36890;&#24120;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19968;&#27425;&#24615;&#21019;&#24314;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#36830;&#36143;&#20294;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;&#65288;SWAG&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65306;&#19968;&#20010;LLM&#29983;&#25104;&#25925;&#20107;&#20869;&#23481;&#65292;&#21478;&#19968;&#20010;&#36741;&#21161;LLM&#29992;&#20110;&#36873;&#25321;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#8220;&#34892;&#21160;&#8221;&#65292;&#20197;&#24341;&#23548;&#25925;&#20107;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SWAG&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;&#25925;&#20107;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#19988;&#25105;&#20204;&#21482;&#20351;&#29992;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36234;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#30740;&#20102;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#25903;&#25345;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03480</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#21457;&#29616;&#26381;&#21153;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65306;&#19968;&#39033;&#35843;&#30740;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#30740;&#20102;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#25903;&#25345;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27491;&#22312;&#25913;&#21464;&#30740;&#31350;&#24037;&#20316;&#65292;&#23454;&#29616;&#26032;&#25216;&#26415;&#65292;&#24182;&#26368;&#32456;&#24102;&#26469;&#26032;&#30340;&#21457;&#29616;&#12290;&#38543;&#30528;&#23545;&#26356;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#19968;&#20010;&#20806;&#32423;&#21442;&#25968;&#27169;&#22411;&#65288;TPM&#65289;&#30340;&#26102;&#20195;&#65292;&#21363;&#20855;&#26377;&#36229;&#36807;&#19968;&#19975;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#22914;&#21326;&#20026;&#30340;PanGu-$ \ Sigma $&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#38024;&#23545;&#31185;&#23398;&#30028;&#29305;&#23450;&#38656;&#27714;&#30340;TPM&#29992;&#25143;&#21644;&#25552;&#20379;&#32773;&#29983;&#24577;&#31995;&#32479;&#30340;&#24895;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20026;&#25552;&#20379;TPM&#26381;&#21153;&#30340;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#37325;&#22823;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#36719;&#20214;&#22534;&#26632;&#21644;&#25509;&#21475;&#35201;&#27714;&#65292;&#20197;&#25903;&#25345;&#30740;&#31350;&#32773;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries. As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community. We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery. Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.03471</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20960;&#20309;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Information of Large Language Model Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03471
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#34920;&#31034;&#29109;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#30340;&#29616;&#35937;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#65288;&#26465;&#20214;&#65289;&#29109;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#36825;&#31181;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#30340;&#33258;&#22238;&#24402;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#26469;&#20998;&#26512;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#19982;&#20043;&#21069;&#19978;&#19979;&#25991;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#26102;&#34920;&#29616;&#20248;&#20110;&#32039;&#23494;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20449;&#24687;&#20998;&#24067;&#22312;&#26631;&#35760;&#20043;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#25104;&#26412;&#25928;&#30410;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#26469;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03435</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#35780;&#20272;&#65306;&#27880;&#37325;&#38544;&#31169;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#25104;&#26412;&#25928;&#30410;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#26469;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20027;&#35201;&#30446;&#26631;&#26377;&#20004;&#20010;&#65306;&#31532;&#19968;&#65292;&#30830;&#23450;&#25903;&#25345;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#30340;&#20851;&#38190;&#25688;&#24405;&#65307;&#31532;&#20108;&#65292;&#24635;&#32467;&#26448;&#26009;&#20197;&#35777;&#23454;&#39044;&#20998;&#37197;&#30340;&#33258;&#26432;&#39118;&#38505;&#27700;&#24179;&#12290;&#35813;&#24037;&#20316;&#23616;&#38480;&#20110;&#20351;&#29992;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#30340;&#8220;&#24320;&#28304;&#8221;LLMs&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#23427;&#20248;&#20808;&#36873;&#29992;&#35745;&#31639;&#35201;&#27714;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#20197;&#20351;&#26377;&#38480;&#30340;&#35745;&#31639;&#39044;&#31639;&#30340;&#20010;&#20154;&#21644;&#26426;&#26500;&#33021;&#22815;&#20351;&#29992;&#12290;&#23454;&#26045;&#30340;&#31574;&#30053;&#20165;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#65292;&#20197;&#25351;&#23548;LLM&#30340;&#25991;&#26412;&#23436;&#25104;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#35780;&#20272;&#25351;&#26631;&#26174;&#31034;&#20986;&#26480;&#20986;&#30340;&#32467;&#26524;&#65292;&#20351;&#20043;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#27880;&#37325;&#38544;&#31169;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;2024 Computational Linguistics and Clinical Psychology (CLPsych)&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03396</link><description>&lt;p&gt;
UniTSyn&#65306;&#19968;&#20010;&#21487;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#20195;&#30721;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#36719;&#20214;&#27979;&#35797;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#19981;&#21306;&#20998;&#27979;&#35797;&#30446;&#30340;&#20195;&#30721;&#21644;&#20854;&#20182;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;UniTSyn&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;LLM&#22312;&#21333;&#20803;&#27979;&#35797;&#21512;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23558;&#27979;&#35797;&#19982;&#34987;&#27979;&#35797;&#20989;&#25968;&#36827;&#34892;&#20851;&#32852;&#23545;&#20110;LLM&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#21644;&#35201;&#39564;&#35777;&#30340;&#36923;&#36753;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#26381;&#21153;&#22120;&#21327;&#35758;&#65292;UniTSyn&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27599;&#20010;&#39033;&#30446;&#25191;&#34892;&#35774;&#32622;&#25110;&#26131;&#30862;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#27599;&#20010;&#35821;&#35328;&#21551;&#21457;&#24335;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#28966;&#28857;&#27979;&#35797;&#23545;&#30340;&#25361;&#25112;&#30446;&#26631;&#12290;&#23427;&#21253;&#21547;&#20102;&#20116;&#31181;&#20027;&#27969;&#32534;&#31243;&#35821;&#35328;&#30340;270&#19975;&#20010;&#28966;&#28857;&#27979;&#35797;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
&lt;/p&gt;</description></item><item><title>PixelGen&#26159;&#19968;&#20010;&#37325;&#26032;&#26500;&#24819;&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#24863;&#22120;&#12289;&#25910;&#21457;&#22120;&#21644;&#20302;&#20998;&#36776;&#29575;&#25668;&#20687;&#22836;&#21644;&#32418;&#22806;&#35270;&#35273;&#20256;&#24863;&#22120;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#24191;&#27867;&#30340;&#19990;&#30028;&#34920;&#36798;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;transformer-based&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31616;&#21333;&#30340;&#25968;&#25454;&#20063;&#21487;&#20197;&#20135;&#29983;&#20986;&#29615;&#22659;&#30340;&#26032;&#39062;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.03390</link><description>&lt;p&gt;
PixelGen&#65306;&#37325;&#26032;&#24605;&#32771;&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PixelGen: Rethinking Embedded Camera Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03390
&lt;/p&gt;
&lt;p&gt;
PixelGen&#26159;&#19968;&#20010;&#37325;&#26032;&#26500;&#24819;&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#24863;&#22120;&#12289;&#25910;&#21457;&#22120;&#21644;&#20302;&#20998;&#36776;&#29575;&#25668;&#20687;&#22836;&#21644;&#32418;&#22806;&#35270;&#35273;&#20256;&#24863;&#22120;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#26356;&#24191;&#27867;&#30340;&#19990;&#30028;&#34920;&#36798;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;transformer-based&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#31616;&#21333;&#30340;&#25968;&#25454;&#20063;&#21487;&#20197;&#20135;&#29983;&#20986;&#29615;&#22659;&#30340;&#26032;&#39062;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#26159;&#26080;&#32447;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20363;&#23376;&#12290;&#23427;&#20204;&#25429;&#25417;&#20102;&#19990;&#30028;&#30340;&#34920;&#36798; - &#30001;&#21487;&#35265;&#20809;&#25110;&#32418;&#22806;&#20809;&#29031;&#20142;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;&#30340;&#26550;&#26500;&#20173;&#28982;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#38480;&#21046;&#12290;&#23427;&#20204;&#21482;&#21487;&#35270;&#21270;&#20102;&#19990;&#30028;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#28040;&#32791;&#33021;&#37327;&#39640;&#65292;&#23548;&#33268;&#30005;&#27744;&#23551;&#21629;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PixelGen&#65292;&#23427;&#37325;&#26032;&#26500;&#24819;&#20102;&#23884;&#20837;&#24335;&#25668;&#20687;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PixelGen&#32467;&#21512;&#20102;&#20256;&#24863;&#22120;&#12289;&#25910;&#21457;&#22120;&#21644;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#32418;&#22806;&#35270;&#35273;&#20256;&#24863;&#22120;&#65292;&#20197;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#19990;&#30028;&#34920;&#36798;&#12290;&#23427;&#20204;&#34987;&#26377;&#24847;&#36873;&#25321;&#20986;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#31616;&#21333;&#12289;&#20302;&#27604;&#29305;&#29575;&#21644;&#21151;&#32791;&#20302;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#19968;&#31181;&#39640;&#33021;&#25928;&#30340;&#24179;&#21488;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#31616;&#21333;&#65292;&#25429;&#33719;&#30340;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#65292;&#29983;&#25104;&#29615;&#22659;&#30340;&#26032;&#39062;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedded camera systems are ubiquitous, representing the most widely deployed example of a wireless embedded system. They capture a representation of the world - the surroundings illuminated by visible or infrared light. Despite their widespread usage, the architecture of embedded camera systems has remained unchanged, which leads to limitations. They visualize only a tiny portion of the world. Additionally, they are energy-intensive, leading to limited battery lifespan. We present PixelGen, which re-imagines embedded camera systems. Specifically, PixelGen combines sensors, transceivers, and low-resolution image and infrared vision sensors to capture a broader world representation. They are deliberately chosen for their simplicity, low bitrate, and power consumption, culminating in an energy-efficient platform. We show that despite the simplicity, the captured data can be processed using transformer-based image and language models to generate novel representations of the environment. F
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03388</link><description>&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#34892;&#20026;&#29992;&#25143;&#20998;&#21106;&#20013;&#30340;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#32447;&#34892;&#20026;&#36275;&#36857;&#21487;&#20197;&#20351;&#20844;&#21496;&#21457;&#29616;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#32454;&#20998;&#65292;&#24182;&#21521;&#29992;&#25143;&#21457;&#36865;&#29305;&#23450;&#32454;&#20998;&#30340;&#20449;&#24687;&#12290;&#22312;&#21457;&#29616;&#32454;&#20998;&#20043;&#21518;&#65292;&#36890;&#36807;&#20687;Facebook&#21644;Google&#36825;&#26679;&#30340;&#39318;&#36873;&#23186;&#20307;&#28192;&#36947;&#21521;&#29992;&#25143;&#21457;&#36865;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21482;&#26377;&#37096;&#20998;&#34892;&#20026;&#32454;&#20998;&#20013;&#30340;&#29992;&#25143;&#22312;&#23186;&#20307;&#19978;&#25214;&#21040;&#21305;&#37197;&#65292;&#24182;&#19988;&#21482;&#26377;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#30475;&#21040;&#28040;&#24687;&#65288;&#26333;&#20809;&#65289;&#12290;&#21363;&#20351;&#39640;&#36136;&#37327;&#30340;&#21457;&#29616;&#20063;&#20250;&#22312;&#20256;&#36882;&#22833;&#36133;&#26102;&#21464;&#24471;&#26080;&#29992;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#31639;&#27861;&#29992;&#20110;&#21457;&#29616;&#34892;&#20026;&#32454;&#20998;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#20256;&#36882;&#32452;&#20214;&#12290;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#26159;&#22240;&#20026;&#65288;i&#65289;&#21457;&#29616;&#26159;&#22312;&#20844;&#21496;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#34892;&#20026;&#25968;&#25454;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#20256;&#36882;&#21017;&#26159;&#22522;&#20110;&#23186;&#20307;&#23450;&#20041;&#30340;&#38745;&#24577;&#25968;&#25454;&#31354;&#38388;&#65288;&#20363;&#22914;&#22320;&#29702;&#20301;&#32622;&#65292;&#24180;&#40836;&#65289;&#36827;&#34892;&#30340;&#65307;&#65288;ii&#65289;&#20844;&#21496;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#36890;&#36807;&#23558;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03386</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#65306;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;
&lt;/p&gt;
&lt;p&gt;
A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#36890;&#36807;&#23558;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26159;&#24314;&#27169;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#25968;&#23398;&#29305;&#24615;&#65292;&#26080;&#27861;&#20687;&#31070;&#32463;&#32593;&#32476;&#37027;&#26679;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#20998;&#23618;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#21644;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#21487;&#20197;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#23450;&#20041;&#19968;&#20010;&#22270;&#32467;&#26500;&#30340;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#24182;&#22312;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#65288;&#26080;&#38656;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#31216;&#20026;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;DGBF&#30340;&#29305;&#23450;&#22270;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;...&#65288;&#25688;&#35201;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
Tree ensemble algorithms as RandomForest and GradientBoosting are currently the dominant methods for modeling discrete or tabular data, however, they are unable to perform a hierarchical representation learning from raw data as NeuralNetworks does thanks to its multi-layered structure, which is a key feature for DeepLearning problems and modeling unstructured data. This limitation is due to the fact that tree algorithms can not be trained with back-propagation because of their mathematical nature. However, in this work, we demonstrate that the mathematical formulation of bagging and boosting can be combined together to define a graph-structured-tree-ensemble algorithm with a distributed representation learning process between trees naturally (without using back-propagation). We call this novel approach Distributed Gradient Boosting Forest (DGBF) and we demonstrate that both RandomForest and GradientBoosting can be expressed as particular graph architectures of DGBT. Finally, we see tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#29983;&#23384;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;</title><link>https://arxiv.org/abs/2402.03384</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#39044;&#27979;&#33014;&#36136;&#30244;&#30340;&#29983;&#23384;&#21644;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Survival and grade of the glioma prediction using transfer learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#29983;&#23384;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#19968;&#31181;&#39640;&#24230;&#24694;&#24615;&#30340;&#33041;&#32959;&#30244;&#65292;&#27809;&#26377;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#39044;&#26399;&#23551;&#21629;&#20165;&#20026;3&#33267;6&#20010;&#26376;&#12290;&#20934;&#30830;&#26816;&#27979;&#21644;&#39044;&#27979;&#20854;&#29983;&#23384;&#21644;&#31561;&#32423;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35814;&#23613;&#30340;&#20248;&#21270;&#65292;&#27979;&#35797;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;EfficientNet&#12289;ResNet&#12289;VGG16&#21644;Inception&#65292;&#20197;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#26550;&#26500;&#12290;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#65306;&#29983;&#23384;&#39044;&#27979;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#23558;&#24739;&#32773;&#20998;&#20026;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#29983;&#23384;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#32959;&#30244;&#31561;&#32423;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#65292;&#20934;&#30830;&#21306;&#20998;&#20302;&#31561;&#32423;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;(LGG)&#21644;&#39640;&#31561;&#32423;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;(HGG)&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#24402;&#22240;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma is a highly malignant brain tumor with a life expectancy of only 3 to 6 months without treatment. Detecting and predicting its survival and grade accurately are crucial. This study introduces a novel approach using transfer learning techniques. Various pre-trained networks, including EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive optimization to identify the most suitable architecture. Transfer learning was applied to fine-tune these models on a glioblastoma image dataset, aiming to achieve two objectives: survival and tumor grade prediction.The experimental results show 65% accuracy in survival prediction, classifying patients into short, medium, or long survival categories. Additionally, the prediction of tumor grade achieved an accuracy of 97%, accurately differentiating low-grade gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is attributed to the effectiveness of transfer learning, surpassing the current state-of-the
&lt;/p&gt;</description></item><item><title>&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#26041;&#27861;ECUP&#26088;&#22312;&#35299;&#20915;&#38142;&#36335;&#20559;&#24046;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#32447;&#33829;&#38144;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03379</link><description>&lt;p&gt;
&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#19982;&#19978;&#19979;&#25991;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#26234;&#33021;&#33829;&#38144;
&lt;/p&gt;
&lt;p&gt;
Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03379
&lt;/p&gt;
&lt;p&gt;
&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#26041;&#27861;ECUP&#26088;&#22312;&#35299;&#20915;&#38142;&#36335;&#20559;&#24046;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#32447;&#33829;&#38144;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#21319;&#24314;&#27169;&#22312;&#22312;&#32447;&#33829;&#38144;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#39044;&#27979;&#20010;&#20307;&#22788;&#29702;&#25928;&#26524;&#65288;ITE&#65289;&#26469;&#20934;&#30830;&#34913;&#37327;&#19981;&#21516;&#31574;&#30053;&#65288;&#22914;&#20248;&#24800;&#21048;&#25110;&#25240;&#25187;&#65289;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#22312;&#30005;&#23376;&#21830;&#21153;&#29615;&#22659;&#20013;&#65292;&#29992;&#25143;&#34892;&#20026;&#36981;&#24490;&#30830;&#23450;&#30340;&#39034;&#24207;&#38142;&#36335;&#65292;&#21253;&#25324;&#23637;&#31034;&#12289;&#28857;&#20987;&#21644;&#36716;&#21270;&#12290;&#33829;&#38144;&#31574;&#30053;&#22312;&#36825;&#20010;&#38142;&#36335;&#20013;&#30340;&#27599;&#20010;&#38454;&#27573;&#37117;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#19978;&#21319;&#25928;&#24212;&#65292;&#24433;&#21709;&#30528;&#28857;&#20987;&#29575;&#21644;&#36716;&#21270;&#29575;&#31561;&#25351;&#26631;&#12290;&#23613;&#31649;&#20854;&#23454;&#29992;&#24615;&#65292;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#29305;&#23450;&#22788;&#29702;&#20013;&#25152;&#26377;&#38454;&#27573;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#22788;&#29702;&#20449;&#24687;&#65292;&#21487;&#33021;&#32473;&#21518;&#32493;&#30340;&#33829;&#38144;&#20915;&#31574;&#24341;&#20837;&#20102;&#37325;&#22823;&#20559;&#24046;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#20010;&#38382;&#39064;&#31216;&#20026;&#38142;&#36335;&#20559;&#24046;&#38382;&#39064;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#22686;&#24378;&#23398;&#20064;&#30340;&#20840;&#38142;&#36335;&#19978;&#21319;&#26041;&#27861;&#65288;ECUP&#65289;&#12290;ECUP&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#21028;&#21035;&#22120;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03375</link><description>&lt;p&gt;
BetterV: &#36890;&#36807;&#26377;&#21306;&#20998;&#24230;&#30340;&#24341;&#23548;&#23454;&#29616;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BetterV: Controlled Verilog Generation with Discriminative Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#21028;&#21035;&#22120;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;Verilog&#29983;&#25104;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#20195;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#30005;&#36335;&#35774;&#35745;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#30828;&#20214;&#35774;&#35745;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#19981;&#26029;&#22686;&#22810;&#65292;&#20197;&#20415;&#20419;&#36827;&#35774;&#35745;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Verilog&#29983;&#25104;&#26694;&#26550;BetterV&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#32467;&#21512;&#29983;&#25104;&#21028;&#21035;&#22120;&#20197;&#25351;&#23548;&#29305;&#23450;&#35774;&#35745;&#38656;&#27714;&#12290;Verilog&#27169;&#22359;&#26159;&#20174;&#20114;&#32852;&#32593;&#20013;&#25910;&#38598;&#12289;&#36807;&#28388;&#21644;&#22788;&#29702;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#24178;&#20928;&#32780;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;Instruct-tuning&#26041;&#27861;&#65292;&#23545;LLMs&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20197;&#20102;&#35299;&#20851;&#20110;Verilog&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20016;&#23500;&#35757;&#32451;&#38598;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#21028;&#21035;&#22120;&#65292;&#20026;LLMs&#20248;&#21270;Verilog&#23454;&#29616;&#25552;&#20379;&#25351;&#23548;&#12290;BetterV&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#21644;&#21151;&#33021;&#19978;&#27491;&#30830;&#30340;Verilog&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tuned the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#26816;&#27979;&#31185;&#23398;&#35770;&#25991;&#20013;&#25305;&#21155;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#27979;&#20998;&#25968;&#30340;&#20256;&#25773;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26631;&#35760;&#24182;&#25552;&#21462;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20026;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.03370</link><description>&lt;p&gt;
&#26816;&#27979;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25305;&#21155;&#30701;&#35821;
&lt;/p&gt;
&lt;p&gt;
Detection of tortured phrases in scientific literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#26816;&#27979;&#31185;&#23398;&#35770;&#25991;&#20013;&#25305;&#21155;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#27979;&#20998;&#25968;&#30340;&#20256;&#25773;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26631;&#35760;&#24182;&#25552;&#21462;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20026;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#31181;&#33258;&#21160;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#25152;&#35859;&#30340;&#25305;&#21155;&#30701;&#35821;&#12290;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20363;&#22914;&#23558;"&#20449;&#21495;&#19982;&#22122;&#22768;"&#26367;&#25442;&#20026;"&#26071;&#24092;&#19982;&#21927;&#38393;"&#65292;&#26159;&#20351;&#29992;&#25913;&#20889;&#24037;&#20855;&#35268;&#36991;&#25220;&#34989;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#31574;&#30053;&#26469;&#26631;&#35760;&#20197;&#21069;&#26410;&#35760;&#24405;&#30340;&#25305;&#21155;&#30701;&#35821;&#12290;&#25552;&#20986;&#21644;&#27979;&#35797;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#22522;&#20110;&#23884;&#20837;&#30456;&#20284;&#24615;&#65292;&#35201;&#20040;&#22522;&#20110;&#25513;&#30721;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#39044;&#27979;&#24182;&#23558;&#20998;&#25968;&#20256;&#25773;&#21040;&#22359;&#32423;&#21035;&#30340;&#26041;&#27861;&#25928;&#26524;&#26368;&#22909;&#12290;&#20854;&#21484;&#22238;&#29575;&#20026;0.87&#65292;&#31934;&#30830;&#29575;&#20026;0.61&#65292;&#21487;&#20197;&#26816;&#32034;&#21040;&#26032;&#30340;&#25305;&#21155;&#30701;&#35821;&#20197;&#20379;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. These tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. We built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. The proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. We found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. With a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#21512;&#35843;&#26597;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22823;&#25968;&#25454;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20998;&#23618;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#35813;&#20998;&#31867;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.03368</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#25968;&#25454;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#30740;&#31350;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#21512;&#35843;&#26597;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22823;&#25968;&#25454;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20998;&#23618;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#35813;&#20998;&#31867;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#21512;&#35843;&#26597;&#35770;&#25991;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22823;&#25968;&#25454;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#28145;&#24230;&#21644;&#31934;&#30830;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#31649;&#40784;&#19979;&#30340;&#26041;&#27861;&#65306;&#23545;&#24403;&#21069;&#31639;&#27861;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20998;&#23618;&#30340;&#20998;&#31867;&#27861;&#20197;&#23454;&#29616;&#31934;&#30830;&#24402;&#31867;&#12290;&#35813;&#20998;&#31867;&#27861;&#22522;&#20110;&#19977;&#23618;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#36880;&#27493;&#32454;&#21270;&#20026;&#20855;&#20307;&#25216;&#26415;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20801;&#35768;&#23545;&#31639;&#27861;&#36827;&#34892;&#32467;&#26500;&#21270;&#21644;&#20840;&#38754;&#30340;&#20998;&#31867;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#35770;&#25991;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#31639;&#27861;&#65292;&#39318;&#20808;&#23558;&#31639;&#27861;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#20998;&#26512;&#31867;&#22411;&#65306;&#22522;&#20110;&#29992;&#25143;&#21644;&#39033;&#30446;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#12289;&#28151;&#21512;&#21644;&#32508;&#21512;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#31639;&#27861;&#26041;&#27861;&#12289;&#25968;&#23398;&#24314;&#27169;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#23376;&#31867;&#21035;&#21644;&#25216;&#26415;&#12290;&#35770;&#25991;&#34701;&#21512;&#20102;&#23454;&#35777;&#21644;&#23454;&#39564;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper provides a comprehensive analysis of big data algorithms in recommendation systems, addressing the lack of depth and precision in existing literature. It proposes a two-pronged approach: a thorough analysis of current algorithms and a novel, hierarchical taxonomy for precise categorization. The taxonomy is based on a tri-level hierarchy, starting with the methodology category and narrowing down to specific techniques. Such a framework allows for a structured and comprehensive classification of algorithms, assisting researchers in understanding the interrelationships among diverse algorithms and techniques. Covering a wide range of algorithms, this taxonomy first categorizes algorithms into four main analysis types: User and Item Similarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and Algorithmic Methods, and Mathematical Modeling Methods, with further subdivisions into sub-categories and techniques. The paper incorporates both empirical and experim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03366</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21487;&#35299;&#37322;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Explainable Recommendation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20869;&#25552;&#20379;&#35299;&#37322;&#33021;&#22815;&#25552;&#21319;&#29992;&#25143;&#28385;&#24847;&#24230;&#24182;&#24314;&#31435;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#35814;&#32454;&#35828;&#26126;&#20026;&#29992;&#25143;&#23450;&#21046;&#25512;&#33616;&#39033;&#30446;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#39046;&#22495;&#20013;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#23588;&#20026;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#25913;&#36827;LLMs&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#22312;&#23454;&#36341;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#25552;&#31034;&#32780;&#19981;&#26159;LLM&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;GPT-2&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#65292;&#20248;&#21270;&#25512;&#33616;&#20219;&#21153;&#21644;&#35299;&#37322;&#20219;&#21153;&#12290;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#65292;NanoNER&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27880;&#37322;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03362</link><description>&lt;p&gt;
NanoNER: &#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#36827;&#34892;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#65292;NanoNER&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27880;&#37322;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#12290;NER&#26159;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#29305;&#23450;&#23454;&#20307;&#30340;&#20219;&#21153;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20449;&#24687;&#25552;&#21462;&#20013;&#32463;&#24120;&#26159;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#30446;&#30340;&#26159;&#35782;&#21035;&#39046;&#22495;&#19987;&#23478;&#20043;&#21069;&#30830;&#23450;&#20026;&#35813;&#39046;&#22495;&#22522;&#26412;&#30693;&#35782;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20381;&#38752;&#26412;&#20307;&#35770;&#26469;&#25552;&#20379;&#39046;&#22495;&#35789;&#27719;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20351;&#19987;&#23478;&#33021;&#22815;&#30830;&#23450;&#19982;&#24403;&#21069;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#28982;&#21518;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#22312;NER&#20013;&#30340;&#28508;&#21147;&#65292;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#21487;&#20197;&#36890;&#36807;&#26368;&#23569;&#30340;&#20154;&#21147;&#22686;&#21152;&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#22312;&#21253;&#21547;&#36229;&#36807;120k&#23454;&#20307;&#20986;&#29616;&#27425;&#25968;&#30340;728&#31687;&#20840;&#25991;&#32435;&#31859;&#29983;&#29289;&#23398;&#25991;&#31456;&#30340;&#23436;&#25972;&#35821;&#26009;&#24211;&#19978;&#65292;NanoNER&#22312;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#30340;&#35782;&#21035;&#19978;&#33719;&#24471;&#20102;0.98&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here we present the training and evaluation of NanoNER, a Named Entity Recognition (NER) model for Nanobiology. NER consists in the identification of specific entities in spans of unstructured texts and is often a primary task in Natural Language Processing (NLP) and Information Extraction. The aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. Relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. We then delve into the potential of distant supervision learning in NER, supporting how this method can increase the quantity of annotated data with minimal additional manpower. On our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, NanoNER obtained a F1-score of 0.98 on the recognition of previously known entities. Our model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;NAGASIL&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20551;&#26032;&#38395;&#20943;&#36731;&#20013;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03357</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#25928;&#24212;&#20943;&#36731;&#20551;&#26032;&#38395;&#20256;&#25773;&#65306;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;
&lt;/p&gt;
&lt;p&gt;
Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;NAGASIL&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20551;&#26032;&#38395;&#20943;&#36731;&#20013;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#26368;&#23567;&#21270;&#20551;&#26032;&#38395;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#36825;&#34987;&#35774;&#23450;&#20026;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#38454;&#27573;&#36873;&#21462;&#19968;&#20010;&#29992;&#25143;&#20256;&#25773;&#30495;&#23454;&#30340;&#26032;&#38395;&#12290;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#19968;&#27425;&#24615;&#22870;&#21169;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#20132;&#32455;&#20449;&#24687;&#20256;&#25773;&#20013;&#65292;&#26080;&#27861;&#21306;&#20998;&#21333;&#20010;&#25581;&#38706;&#32773;&#30340;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#8220;&#20928;&#8221;&#25928;&#24212;&#65292;&#21482;&#33021;&#35266;&#23519;&#21040;&#26469;&#33258;&#20943;&#36731;&#21162;&#21147;&#30340;&#38598;&#20307;&#25928;&#26524;&#12290;&#29616;&#26377;&#30340;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#26041;&#27861;&#22312;&#20174;&#19968;&#27425;&#24615;&#22870;&#21169;&#20013;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#20551;&#26032;&#38395;&#20943;&#36731;&#24212;&#29992;&#20013;&#30001;&#20110;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#20551;&#26032;&#38395;&#20943;&#36731;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAGASIL - &#22522;&#20110;&#36127;&#37319;&#26679;&#21644;&#29366;&#24577;&#22686;&#24378;&#29983;&#25104;&#23545;&#25239;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#38024;&#23545;&#20551;&#26032;&#38395;&#20943;&#36731;&#30340;&#25913;&#36827;:&#20174;&#36127;&#26679;&#26412;&#20013;&#23398;&#20064;&#21644;&#29366;&#24577;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to minimize the influence of fake news on social networks by deploying debunkers to propagate true news. This is framed as a reinforcement learning problem, where, at each stage, one user is selected to propagate true news. A challenging issue is episodic reward where the "net" effect of selecting individual debunkers cannot be discerned from the interleaving information propagation on social networks, and only the collective effect from mitigation efforts can be observed. Existing Self-Imitation Learning (SIL) methods have shown promise in learning from episodic rewards, but are ill-suited to the real-world application of fake news mitigation because of their poor sample efficiency. To learn a more effective debunker selection policy for fake news mitigation, this study proposes NAGASIL - Negative sampling and state Augmented Generative Adversarial Self-Imitation Learning, which consists of two improvements geared towards fake news mitigation: learning from negative sa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#12289;&#20998;&#26512;&#20102;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.03355</link><description>&lt;p&gt;
&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#39564;&#21644;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#12289;&#20998;&#26512;&#20102;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#23545;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#25152;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#38024;&#23545;&#27599;&#31181;&#25216;&#26415;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#20854;&#26377;&#25928;&#24615;&#12289;&#23616;&#38480;&#24615;&#12289;&#25913;&#36827;&#28508;&#21147;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#29616;&#26377;&#20851;&#27880;&#20110;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#21644;&#39044;&#27979;&#29359;&#32618;&#31639;&#27861;&#30340;&#35843;&#26597;&#35770;&#25991;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#31639;&#27861;&#20998;&#20026;&#26356;&#35814;&#32454;&#30340;&#31867;&#21035;&#21644;&#20855;&#20307;&#25216;&#26415;&#12290;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#23545;&#19981;&#21516;&#25216;&#26415;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#30340;&#32467;&#21512;&#20351;&#20154;&#20204;&#33021;&#22815;&#20840;&#38754;&#32780;&#32454;&#33268;&#22320;&#20102;&#35299;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26126;&#26234;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20854;&#20182;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a thorough analysis of techniques and algorithms used in the identification of crime leaders within criminal networks. For each technique, the paper examines its effectiveness, limitations, potential for improvement, and future prospects. The main challenge faced by existing survey papers focusing on algorithms for identifying crime leaders and predicting crimes is effectively categorizing these algorithms. To address this limitation, this paper proposes a new methodological taxonomy that hierarchically classifies algorithms into more detailed categories and specific techniques. The paper includes empirical and experimental evaluations to rank the different techniques. The combination of the methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of the techniques and algorithms for identifying crime leaders, assisting researchers in making informed decisions. Moreover, the paper offers v
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#31181;&#24050;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.03349</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#19978;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22522;&#30784;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#31181;&#24050;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20195;&#34920;&#30528;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25215;&#35834;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#21019;&#36896;&#21512;&#25104;&#25968;&#25454;&#21644;&#36755;&#20986;&#12290; GAI&#26368;&#36817;&#22312;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#12289;&#25945;&#32946;&#12289;&#31435;&#27861;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#20026;&#20102;&#23454;&#29616;&#22686;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#29983;&#25104;AI&#30830;&#23454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#24046;&#24322;&#21270;&#22240;&#32032;&#65292;&#24182;&#25215;&#35834;&#22312;&#35813;&#39046;&#22495;&#24341;&#36215;&#27169;&#24335;&#36716;&#21464;&#12290; &#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290; &#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#19982;&#22320;&#29699;&#31185;&#23398;&#21644;&#22320;&#29699;&#31995;&#32479;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#39044;&#27979;&#38382;&#39064;&#12289;&#27169;&#25311;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#25361;&#25112;&#12290; &#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;&#20960;&#31181;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This paper explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03348</link><description>&lt;p&gt;
&#23562;&#37325;&#27169;&#22411;: &#32454;&#31890;&#24230;&#19988;&#40065;&#26834;&#30340;&#35299;&#37322;&#19982;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#33021;&#21542;&#30495;&#23454;&#38416;&#26126;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#30495;&#23454;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#29616;&#26377;&#26041;&#27861;&#20559;&#31163;&#20102;&#23545;&#27169;&#22411;&#30340;&#24544;&#23454;&#34920;&#36798;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#26041;&#27861;&#65292;&#31216;&#20026;SRD(&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;)&#65292;&#23427;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#20803;&#32423;&#21035;&#24378;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#26469;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#65292;&#31216;&#20026;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#65292;&#35753;&#25105;&#20204;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#27963;&#36291;&#21644;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SRD&#20801;&#35768;&#36882;&#24402;&#20998;&#35299;&#19968;&#20010;&#28857;&#29305;&#24449;&#21521;&#37327;(PFV)&#12290;
&lt;/p&gt;
&lt;p&gt;
The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Densenet201&#26550;&#26500;&#27169;&#22411;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36827;&#34892;&#22303;&#35910;&#21494;&#30149;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.03347</link><description>&lt;p&gt;
&#20351;&#29992;Densenet201&#26550;&#26500;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#22303;&#35910;&#21494;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Densenet201&#26550;&#26500;&#27169;&#22411;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36827;&#34892;&#22303;&#35910;&#21494;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#35910;&#26893;&#29289;&#23545;&#20154;&#31867;&#26377;&#30410;&#12290;&#20687;&#20854;&#20182;&#26893;&#29289;&#19968;&#26679;&#65292;&#22303;&#35910;&#26893;&#29289;&#20063;&#20250;&#29983;&#30149;&#65307;&#22914;&#26524;&#19981;&#21450;&#26102;&#27835;&#30103;&#65292;&#39135;&#29289;&#20135;&#37327;&#23558;&#26174;&#33879;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#65292;&#20197;&#20415;&#26377;&#25928;&#39640;&#25928;&#22320;&#36827;&#34892;&#30149;&#23475;&#25511;&#21046;&#12290;&#22303;&#35910;&#21494;&#30149;&#30340;&#20998;&#31867;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#65292;&#20294;&#30151;&#29366;&#24182;&#19981;&#33021;&#22987;&#32456;&#35299;&#37322;&#25915;&#20987;&#22303;&#35910;&#21494;&#30340;&#30149;&#23475;&#31867;&#22411;&#65292;&#22240;&#20026;&#26377;&#24456;&#22810;&#31181;&#30149;&#23475;&#30340;&#30151;&#29366;&#30475;&#36215;&#26469;&#19968;&#26679;&#12290;&#20154;&#31867;&#22312;&#30830;&#23450;&#22303;&#35910;&#21494;&#30149;&#37492;&#23450;&#32467;&#26524;&#26102;&#20063;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#27492;&#65292;&#26377;&#26102;&#20010;&#20307;&#38388;&#30340;&#37492;&#23450;&#32467;&#26524;&#21487;&#33021;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22303;&#35910;&#21494;&#30149;&#20998;&#31867;&#30340;&#26041;&#27861;&#26377;&#26395;&#32553;&#30701;&#26102;&#38388;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;DenseNet201&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potato plants are plants that are beneficial to humans. Like other plants in general, potato plants also have diseases; if this disease is not treated immediately, there will be a significant decrease in food production. Therefore, it is necessary to detect diseases quickly and precisely so that disease control can be carried out effectively and efficiently. Classification of potato leaf disease can be done directly. Still, the symptoms cannot always explain the type of disease that attacks potato leaves because there are many types of diseases with symptoms that look the same. Humans also have deficiencies in determining the results of identification of potato leaf disease, so sometimes the results of identification between individuals can be different. Therefore, the use of Deep Learning for the classification process of potato leaf disease is expected to shorten the time and have a high classification accuracy. This study uses a deep learning method with the DenseNet201 architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#23454;&#39564;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#21019;&#24314;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#30340;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#21644;&#23454;&#26045;&#27493;&#39588;&#20197;&#21450;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23548;&#33322;&#31639;&#27861;&#22312;&#30495;&#23454;&#33337;&#21482;&#19978;&#30340;&#24212;&#29992;&#20855;&#26377;&#30452;&#25509;&#30340;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03337</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24070;&#33337;&#65306;&#27169;&#25311;&#22120;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Reinforcement-learning robotic sailboats: simulator and preliminary results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#23454;&#39564;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#21019;&#24314;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#30340;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#21644;&#23454;&#26045;&#27493;&#39588;&#20197;&#21450;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23548;&#33322;&#31639;&#27861;&#22312;&#30495;&#23454;&#33337;&#21482;&#19978;&#30340;&#24212;&#29992;&#20855;&#26377;&#30452;&#25509;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#35299;&#20915;&#22312;&#20351;&#29992;&#26080;&#20154;&#34920;&#38754;&#33337;&#21482;(Unmanned Surface Vehicles, USV)&#30340;&#25968;&#23383;&#23402;&#29983;&#24320;&#23637;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#26469;&#22797;&#21046;&#30495;&#23454;&#23454;&#39564;&#26102;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;&#34394;&#25311;&#19990;&#30028;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#32771;&#34385;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;(RL)&#26234;&#33021;&#20307;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20027;&#35201;&#38382;&#39064;&#28041;&#21450;&#27169;&#25311;&#26041;&#31243;&#30340;&#23450;&#20041;(&#29289;&#29702;&#21644;&#25968;&#23398;)&#12289;&#23427;&#20204;&#30340;&#26377;&#25928;&#23454;&#26045;&#20197;&#21450;&#22914;&#20309;&#23558;&#29992;&#20110;RL&#30340;&#27169;&#25311;&#25511;&#21046;&#21644;&#24863;&#30693;&#31574;&#30053;(&#20256;&#24863;&#22120;)&#21253;&#25324;&#36827;&#26469;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#21019;&#24314;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#12289;&#23454;&#26045;&#27493;&#39588;&#21644;&#25361;&#25112;&#12290;&#35813;&#24212;&#29992;&#23558;&#31435;&#21363;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;RL&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#30495;&#23454;&#33337;&#21482;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the main challenges and problems in developing a virtual oceanic environment reproducing real experiments using Unmanned Surface Vehicles (USV) digital twins. We introduce the key features for building virtual worlds, considering using Reinforcement Learning (RL) agents for autonomous navigation and control. With this in mind, the main problems concern the definition of the simulation equations (physics and mathematics), their effective implementation, and how to include strategies for simulated control and perception (sensors) to be used with RL. We present the modeling, implementation steps, and challenges required to create a functional digital twin based on a real robotic sailing vessel. The application is immediate for developing navigation algorithms based on RL to be applied on real boats.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#34917;&#19969;&#26469;&#25913;&#21892;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#32463;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;Vision Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#36873;&#25321;&#38590;&#20197;&#37325;&#26500;&#30340;&#26174;&#33879;&#34917;&#19969;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03329</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26174;&#33879;&#24615;&#34917;&#19969;&#36873;&#25321;&#29992;&#20110;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Salient Patch Selection for Data-Efficient Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#34917;&#19969;&#26469;&#25913;&#21892;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#32463;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;Vision Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#36873;&#25321;&#38590;&#20197;&#37325;&#26500;&#30340;&#26174;&#33879;&#34917;&#19969;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SPIRL&#65292;&#29992;&#20110;&#33258;&#21160;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#34917;&#19969;&#12290;SPIRL&#22522;&#20110;&#32463;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;Vision Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#30340;&#34917;&#19969;&#23545;&#22270;&#20687;&#36827;&#34892;&#37325;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#36873;&#25321;&#26174;&#33879;&#30340;&#34917;&#19969;&#65292;&#36825;&#20123;&#34917;&#19969;&#22312;&#37051;&#36817;&#34917;&#19969;&#20013;&#38590;&#20197;&#37325;&#26500;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SPIRL&#20195;&#29702;&#21033;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#22788;&#29702;&#36873;&#20013;&#30340;&#26174;&#33879;&#34917;&#19969;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#19978;&#32463;&#39564;&#39564;&#35777;&#20102;SPIRL&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#20854;&#19982;&#30456;&#20851;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20123;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the sample efficiency of vision-based deep reinforcement learning (RL), we propose a novel method, called SPIRL, to automatically extract important patches from input images. Following Masked Auto-Encoders, SPIRL is based on Vision Transformer models pre-trained in a self-supervised fashion to reconstruct images from randomly-sampled patches. These pre-trained models can then be exploited to detect and select salient patches, defined as hard to reconstruct from neighboring patches. In RL, the SPIRL agent processes selected salient patches via an attention module. We empirically validate SPIRL on Atari games to test its data-efficiency against relevant state-of-the-art methods, including some traditional model-based methods and keypoint-based models. In addition, we analyze our model's interpretability capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.03328</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;AI&#27169;&#22411;&#32570;&#20047;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large-scale Generative AI Models Lack Visual Number Sense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#36731;&#26494;&#21028;&#26029;&#29289;&#20307;&#30340;&#25968;&#37327;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#35745;&#25968;&#65292;&#32780;&#19988;&#36825;&#31181;&#25216;&#33021;&#22312;&#21508;&#31181;&#21160;&#29289;&#29289;&#31181;&#21644;&#35821;&#35328;&#21457;&#23637;&#21644;&#27491;&#24335;&#23398;&#26657;&#25945;&#32946;&#20043;&#21069;&#30340;&#23156;&#20799;&#20013;&#37117;&#26377;&#35760;&#24405;&#12290;&#23545;&#20110;&#23567;&#30340;&#29289;&#20307;&#38598;&#65292;&#25968;&#23383;&#21028;&#26029;&#26159;&#26080;&#35823;&#30340;&#65292;&#32780;&#23545;&#20110;&#26356;&#22823;&#30340;&#38598;&#21512;&#65292;&#22238;&#24212;&#21464;&#24471;&#36817;&#20284;&#65292;&#24182;&#19988;&#21464;&#24322;&#24615;&#19982;&#30446;&#26631;&#25968;&#23383;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#23613;&#31649;&#29289;&#20307;&#29305;&#24449;&#65288;&#22914;&#39068;&#33394;&#25110;&#24418;&#29366;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#31181;&#22238;&#24212;&#27169;&#24335;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#29289;&#20307;&#19978;&#35266;&#23519;&#21040;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#20381;&#36182;&#20110;&#25968;&#23383;&#25968;&#37327;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21487;&#38752;&#22320;&#21629;&#21517;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#20013;&#30340;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#29289;&#21697;&#25968;&#37327;&#30340;&#22270;&#20687;&#65288;1-10&#33539;&#22260;&#20869;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#32771;&#34385;&#30340;&#25152;&#26377;&#22522;&#30784;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#19968;&#26679;&#30340;&#26041;&#24335;&#34920;&#29616;&#20986;&#26469;&#65306;&#21363;&#20351;&#26159;&#20855;&#26377;&#36739;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#29359;&#19979;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with sm
&lt;/p&gt;</description></item><item><title>Uni3D-LLM&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#21644;&#20462;&#25913;&#28857;&#20113;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03327</link><description>&lt;p&gt;
Uni3D-LLM&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03327
&lt;/p&gt;
&lt;p&gt;
Uni3D-LLM&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#21644;&#20462;&#25913;&#28857;&#20113;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Uni3D-LLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;3D&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#22320;&#22312;&#28857;&#20113;&#22330;&#26223;&#20013;&#25351;&#23450;&#20301;&#32622;&#29983;&#25104;&#21644;&#20462;&#25913;&#23545;&#35937;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22810;&#26679;&#24615;&#36827;&#34892;&#24341;&#23548;&#12290;Uni3D-LLM&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;3D&#23545;&#35937;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;Uni3D-LLM&#23454;&#29616;&#20102;&#36328;&#24212;&#29992;&#21151;&#33021;&#65292;&#33021;&#22815;&#26080;&#32541;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#31934;&#30830;&#23454;&#20363;&#21270;3D&#23545;&#35937;&#21040;&#20132;&#20114;&#35774;&#35745;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;Uni3D-LLM&#22312;&#28857;&#20113;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes. This framework empowers users to effortlessly generate and modify objects at specified locations within a scene, guided by the versatility of natural language descriptions. Uni3D-LLM harnesses the expressive power of natural language to allow for precise command over the generation and editing of 3D objects, thereby significantly enhancing operational flexibility and controllability. By mapping point cloud into the unified representation space, Uni3D-LLM achieves cross-application functionality, enabling the seamless execution of a wide array of tasks, ranging from the accurate instantiation of 3D objects to the diverse requirements of interactive design. Through a comprehensive suite of rigorous experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and editing of point cloud has
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23396;&#27874;&#21644;&#38750;&#32447;&#24615;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#23454;&#29616;&#20102;&#19968;&#31181;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20316;&#20026;&#20256;&#32479;RC&#31639;&#27861;&#30340;&#25216;&#26415;&#31616;&#21333;&#30828;&#20214;&#23545;&#24212;&#29289;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03319</link><description>&lt;p&gt;
&#30001;&#23396;&#27874;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#38750;&#32447;&#24615;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#23454;&#29616;&#30340;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Physical Reservoir Computing Enabled by Solitary Waves and Biologically-Inspired Nonlinear Transformation of Input Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23396;&#27874;&#21644;&#38750;&#32447;&#24615;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#23454;&#29616;&#20102;&#19968;&#31181;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20316;&#20026;&#20256;&#32479;RC&#31639;&#27861;&#30340;&#25216;&#26415;&#31616;&#21333;&#30828;&#20214;&#23545;&#24212;&#29289;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#22791;&#35745;&#31639; (RC) &#31995;&#32479;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#36830;&#25509;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#39044;&#27979;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#12290;RC&#31995;&#32479;&#30340;&#22810;&#21151;&#33021;&#24615;&#20419;&#20351;&#36827;&#19968;&#27493;&#30740;&#31350;&#20256;&#32479;RC&#31639;&#27861;&#30340;&#30828;&#20214;&#23545;&#24212;&#29289;&#21644;&#26356;&#39640;&#25928;&#30340;RC-like&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#29983;&#29289;&#33041;&#20013;&#30340;&#38750;&#32447;&#24615;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#22312;&#27969;&#21160;&#28082;&#20307;&#34180;&#33180;&#34920;&#38754;&#19978;&#28608;&#21457;&#30340;&#23396;&#31435;&#27874;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#19968;&#31181;&#29289;&#29702;RC&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#38750;&#32447;&#24615;&#25968;&#25454;&#36755;&#20837;&#36716;&#25442;&#20195;&#26367;&#20102;&#38543;&#26426;&#24615;&#30340;&#25928;&#26524;&#12290;&#21033;&#29992;&#20165;&#20855;&#26377;&#26368;&#23567;&#35745;&#31639;&#33021;&#21147;&#30340;&#24494;&#25511;&#21046;&#22120;&#36827;&#34892;&#25152;&#26377;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#35774;&#35745;&#30340;RC&#31995;&#32479;&#20316;&#20026;&#20256;&#32479;RC&#31639;&#27861;&#30340;&#8220;&#19979;&#19968;&#20195;&#8221;&#25913;&#36827;&#30340;&#25216;&#26415;&#31616;&#21333;&#30828;&#20214;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing (RC) systems can efficiently forecast chaotic time series using nonlinear dynamical properties of an artificial neural network of random connections. The versatility of RC systems has motivated further research on both hardware counterparts of traditional RC algorithms and more efficient RC-like schemes. Inspired by the nonlinear processes in a living biological brain and using solitary waves excited on the surface of a flowing liquid film, in this paper we experimentally validate a physical RC system that substitutes the effect of randomness for a nonlinear transformation of input data. Carrying out all operations using a microcontroller with a minimal computational power, we demonstrate that the so-designed RC system serves as a technically simple hardware counterpart to the `next-generation' improvement of the traditional RC algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#21644;&#21160;&#21147;&#23398;&#31995;&#32479;&#29702;&#35770;&#30340;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#33041;&#30005;&#27874;&#25968;&#25454;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.03316</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33041;&#30005;&#27874;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#28151;&#27788;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for EEG Prediction: Applied Chaos Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#21644;&#21160;&#21147;&#23398;&#31995;&#32479;&#29702;&#35770;&#30340;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#33041;&#30005;&#27874;&#25968;&#25454;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22797;&#26434;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#36328;32&#20010;EEG&#36890;&#36947;&#36827;&#34892;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#23558;&#24212;&#29992;&#28151;&#27788;&#29702;&#35770;&#21644;&#21160;&#21147;&#23398;&#31995;&#32479;&#29702;&#35770;&#30340;&#21407;&#21017;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#20102;&#19968;&#22871;&#26032;&#30340;&#29305;&#24449;&#38598;&#65292;&#20016;&#23500;&#20102;&#25105;&#20204;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#32463;&#36807;&#31934;&#24515;&#26657;&#20934;&#65292;&#33021;&#22815;&#25429;&#25417;EEG&#24207;&#21015;&#20013;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#21644;&#39640;&#32500;&#26102;&#24577;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#21512;&#29702;&#30340;&#26550;&#26500;&#35774;&#35745;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#22797;&#26434;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;EEG&#25968;&#25454;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20808;&#39537;&#65292;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;EEG&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#29702;&#35299;&#65292;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#26032;&#30340;&#33041;&#30005;&#27874;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present research, we delve into the intricate realm of electroencephalogram (EEG) data analysis, focusing on sequence-to-sequence prediction of data across 32 EEG channels. The study harmoniously fuses the principles of applied chaos theory and dynamical systems theory to engender a novel feature set, enriching the representational capacity of our deep learning model. The endeavour's cornerstone is a transformer-based sequence-to-sequence architecture, calibrated meticulously to capture the non-linear and high-dimensional temporal dependencies inherent in EEG sequences. Through judicious architecture design, parameter initialisation strategies, and optimisation techniques, we have navigated the intricate balance between computational expediency and predictive performance. Our model stands as a vanguard in EEG data sequence prediction, demonstrating remarkable generalisability and robustness. The findings not only extend our understanding of EEG data dynamics but also unveil a po
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01881</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Agent for Hyper-Parameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01881
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12289;&#22823;&#37327;&#23454;&#39564;&#20197;&#21450;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#23613;&#31649;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35797;&#39564;&#25928;&#29575;&#12289;&#35774;&#32622;&#22797;&#26434;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#31216;&#20026;AgentHPO&#65288;LLM Agent-based Hyperparameter Optimization&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AgentHPO&#33258;&#20027;&#22788;&#29702;&#20219;&#21153;&#20449;&#24687;&#65292;&#26681;&#25454;&#21382;&#21490;&#35797;&#39564;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;HPs&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#20248;&#21270;&#36807;&#31243;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#24182;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07237</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling Event Sequence Knowledge From Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#22312;&#20107;&#20214;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#34987;&#21457;&#29616;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#20016;&#23500;&#30340;&#39640;&#36136;&#37327;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#24178;&#20928;&#30340;&#32467;&#26500;&#21270;&#20107;&#20214;&#24207;&#21015;&#19981;&#21487;&#29992;&#65292;&#33258;&#21160;&#21270;&#24207;&#21015;&#25552;&#21462;&#23548;&#33268;&#30340;&#25968;&#25454;&#22826;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#27010;&#29575;&#20107;&#20214;&#27169;&#22411;&#26500;&#24314;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;LLMs&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#19968;&#31181;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26469;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;&#36755;&#20837;KG&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#30340;&#24207;&#21015;&#26469;&#21457;&#29616;&#26356;&#26377;&#29992;&#21644;&#26356;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00736</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#12289;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21644;&#19968;&#20999;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models, Image Super-Resolution And Everything: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00736
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20204;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#33021;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#26679;&#26412;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#39640;&#35745;&#31639;&#38656;&#27714;&#12289;&#21487;&#27604;&#24615;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#33394;&#24425;&#20559;&#31227;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#22823;&#37327;&#30340;&#20986;&#29256;&#29289;&#65292;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21465;&#36848;&#65292;&#38416;&#26126;&#20102;&#24212;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#20869;&#19982;&#20854;&#20182;&#32508;&#36848;&#25991;&#31456;&#19981;&#21516;&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#23545;DM&#30340;&#21407;&#21017;&#36827;&#34892;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#29702;&#35299;&#65292;&#24182;&#25506;&#32034;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21253;&#25324;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;OpenContra&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;&#65292;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#36739;&#39640;&#30340;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2401.00006</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#31574;&#30053;&#21452;&#21521;&#36866;&#24212;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;OpenContra&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26500;&#24314;&#24320;&#25918;&#24335;&#20855;&#36523;&#20195;&#29702;&#65292;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#36739;&#39640;&#30340;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#24320;&#25918;&#24335;&#23398;&#20064;&#20195;&#29702;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;LLM&#22312;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#23454;&#26102;&#20132;&#20114;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#32780;RL&#26041;&#27861;&#21017;&#38754;&#20020;&#25506;&#32034;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenContra&#65292;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#21327;&#21516;LLM&#21644;Goal-Conditioned&#24378;&#21270;&#23398;&#20064;&#65288;GRL&#65289;&#65292;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#30340;&#24320;&#25918;&#24335;&#20195;&#29702;&#12290;&#23454;&#29616;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#29992;LLM&#24494;&#35843;&#32763;&#35793;&#20154;&#31867;&#25351;&#20196;&#20026;&#32467;&#26500;&#21270;&#30446;&#26631;&#65292;&#20197;&#21450;&#35838;&#31243;&#35757;&#32451;&#30446;&#26631;&#26465;&#20214;&#30340;RL&#31574;&#30053;&#65292;&#25191;&#34892;&#20219;&#24847;&#30446;&#26631;&#65307;&#65288;2&#65289;&#21327;&#21516;&#35757;&#32451;LLM&#21644;RL&#31574;&#30053;&#30456;&#20114;&#36866;&#24212;&#65292;&#23454;&#29616;&#25351;&#20196;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#12290;&#25105;&#20204;&#22312;Contra&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#28216;&#25103;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#24191;&#38420;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#22823;&#36867;&#26432;FPS&#28216;&#25103;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;OpenContra&#35757;&#32451;&#30340;&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#20219;&#24847;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#20197;&#39640;&#23436;&#25104;&#29575;&#23436;&#25104;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.15122</link><description>&lt;p&gt;
&#25193;&#23637;&#23601;&#26159;&#19968;&#20999;&#65306;&#20351;&#29992;JAX&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#35270;&#39057;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#26368;&#20248;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36816;&#34892;&#24517;&#35201;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#38750;&#24120;&#22256;&#38590;&#12290;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#36827;&#34892;&#20998;&#24067;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#19978;&#25910;&#38598;&#32463;&#39564;&#20174;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#19988;&#30495;&#23454;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#30495;&#23454;&#39550;&#39542;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33021;&#21147;&#38598;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#24182;&#19981;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.14215</link><description>&lt;p&gt;
SimLM&#65306;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
SimLM: Can Language Models Infer Parameters of Physical Systems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#24182;&#19981;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#25110;&#25512;&#29702;&#22797;&#26434;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25512;&#29702;&#30340;&#24120;&#35265;&#31532;&#19968;&#27493;&#26159;&#20174;&#31995;&#32479;&#34892;&#20026;&#30340;&#35266;&#23519;&#20013;&#25512;&#26029;&#31995;&#32479;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29289;&#29702;&#31995;&#32479;&#19978;&#25191;&#34892;&#21442;&#25968;&#25512;&#26029;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#20204;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#35813;&#26041;&#21521;&#28041;&#21450;&#21040;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;LLMs&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;LLMs&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#19978;&#30340;&#24615;&#33021;&#65292;&#26377;&#26080;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several machine learning methods aim to learn or reason about complex physical systems. A common first-step towards reasoning is to infer system parameters from observations of its behavior. In this paper, we investigate the performance of Large Language Models (LLMs) at performing parameter inference in the context of physical systems. Our experiments suggest that they are not inherently suited to this task, even for simple systems. We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs. We assess and compare the performance of different LLMs on a simple example with and without access to physical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;textSCF&#65292;&#36890;&#36807;&#23558;&#31354;&#38388;&#21327;&#21464;&#28388;&#27874;&#22120;&#21644;&#30001;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30340;&#35299;&#21078;&#25991;&#26412;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#21319;&#20102;&#22270;&#20687;&#37197;&#20934;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.15607</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#31354;&#38388;&#21327;&#21464;&#22270;&#20687;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Spatially Covariant Image Registration with Text Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15607
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;textSCF&#65292;&#36890;&#36807;&#23558;&#31354;&#38388;&#21327;&#21464;&#28388;&#27874;&#22120;&#21644;&#30001;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30340;&#35299;&#21078;&#25991;&#26412;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#21319;&#20102;&#22270;&#20687;&#37197;&#20934;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#36890;&#24120;&#20197;&#20854;&#32467;&#26500;&#21270;&#30340;&#35299;&#21078;&#34920;&#31034;&#21644;&#31354;&#38388;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#32780;&#33879;&#31216;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#21078;&#20808;&#39564;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#25928;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#28982;&#32780;&#22312;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;&#21364;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;textSCF&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31354;&#38388;&#21327;&#21464;&#28388;&#27874;&#22120;&#21644;&#30001;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30340;&#35299;&#21078;&#25991;&#26412;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36825;&#31181;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#38544;&#24335;&#20989;&#25968;&#65292;&#23558;&#35299;&#21078;&#21306;&#22495;&#30340;&#25991;&#26412;&#23884;&#20837;&#19982;&#28388;&#27874;&#22120;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#25918;&#23485;&#20102;&#20856;&#22411;&#30340;&#21367;&#31215;&#36816;&#31639;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#32422;&#26463;&#12290;textSCF&#19981;&#20165;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#21487;&#20197;&#20445;&#30041;&#25110;&#25552;&#39640;&#37197;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35299;&#21078;&#21306;&#22495;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30456;&#20114;&#20316;&#29992;&#65292;&#23427;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21306;&#22495;&#38388;&#21487;&#20256;&#36882;&#24615;&#21644;&#39044;&#22791;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical images are often characterized by their structured anatomical representations and spatially inhomogeneous contrasts. Leveraging anatomical priors in neural networks can greatly enhance their utility in resource-constrained clinical settings. Prior research has harnessed such information for image segmentation, yet progress in deformable image registration has been modest. Our work introduces textSCF, a novel method that integrates spatially covariant filters and textual anatomical prompts encoded by visual-language models, to fill this gap. This approach optimizes an implicit function that correlates text embeddings of anatomical regions to filter weights, relaxing the typical translation-invariance constraint of convolutional operations. TextSCF not only boosts computational efficiency but can also retain or improve registration accuracy. By capturing the contextual interplay between anatomical regions, it offers impressive inter-regional transferability and the ability to pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21512;&#35268;&#35282;&#24230;&#20998;&#26512;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#35201;&#27714;&#24037;&#31243;&#38454;&#27573;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#30340;&#21512;&#35268;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#27431;&#30431;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#23545;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#23450;&#12290;</title><link>https://arxiv.org/abs/2311.13871</link><description>&lt;p&gt;
&#27861;&#24459;&#35201;&#27714;&#20998;&#26512;&#65306;&#20174;&#21512;&#35268;&#30340;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Legal Requirements Analysis: A Regulatory Compliance Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21512;&#35268;&#35282;&#24230;&#20998;&#26512;&#27861;&#24459;&#35201;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#35201;&#27714;&#24037;&#31243;&#38454;&#27573;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#30340;&#21512;&#35268;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#27431;&#30431;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#23545;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36719;&#20214;&#24050;&#25104;&#20026;&#35768;&#22810;&#23398;&#31185;&#21644;&#24212;&#29992;&#29615;&#22659;&#20013;&#26085;&#24120;&#27963;&#21160;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23454;&#29616;&#26234;&#33021;&#33258;&#21160;&#21270;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;AI&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#24402;&#21151;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#25968;&#25454;&#30340;&#22686;&#21152;&#21487;&#29992;&#24615;&#12290;&#27431;&#27954;&#32852;&#30431;&#65288;EU&#65289;&#30340;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#30340;&#20986;&#21488;&#26159;&#20026;&#20102;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#30340;&#20445;&#25252;&#12290;&#25910;&#38598;&#12289;&#22788;&#29702;&#25110;&#20998;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#36719;&#20214;&#31995;&#32479;&#24517;&#39035;&#36981;&#23432;&#36825;&#20123;&#27861;&#35268;&#30340;&#21512;&#35268;&#24615;&#35201;&#27714;&#12290;&#24320;&#21457;&#31526;&#21512;&#27861;&#35268;&#35201;&#27714;&#30340;&#36719;&#20214;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#35201;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#38454;&#27573;&#20013;&#38656;&#35201;&#37325;&#35270;&#22788;&#29702;&#27861;&#24459;&#35201;&#27714;&#12290;RE&#20851;&#27880;&#30340;&#26159;&#25351;&#23450;&#21644;&#32500;&#25252;&#19968;&#20010;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#21253;&#25324;&#27861;&#24459;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern software has been an integral part of everyday activities in many disciplines and application contexts. Introducing intelligent automation by leveraging artificial intelligence (AI) led to break-throughs in many fields. The effectiveness of AI can be attributed to several factors, among which is the increasing availability of data. Regulations such as the general data protection regulation (GDPR) in the European Union (EU) are introduced to ensure the protection of personal data. Software systems that collect, process, or share personal data are subject to compliance with such regulations. Developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (RE) phase of the software development process. RE is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. Legal agreements which describe the policies organizations implement f
&lt;/p&gt;</description></item><item><title>CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2311.08588</link><description>&lt;p&gt;
CodeScope:&#19968;&#20010;&#22522;&#20110;&#25191;&#34892;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08588
&lt;/p&gt;
&lt;p&gt;
CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32534;&#30721;&#30456;&#20851;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#24110;&#21161;&#20154;&#31867;&#32534;&#31243;&#21644;&#20419;&#36827;&#32534;&#31243;&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#22522;&#20934;&#23384;&#22312;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#29421;&#31364;&#33539;&#22260;&#20869;&#30340;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#21644;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#23454;&#38469;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#38656;&#35201;&#23454;&#29616;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#12290;&#23454;&#38469;&#32534;&#31243;&#23454;&#36341;&#36824;&#24378;&#28872;&#26399;&#26395;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20197;&#20840;&#38754;&#21644;&#31283;&#20581;&#22320;&#27979;&#35797;LLMs&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#20063;&#26410;&#32771;&#34385;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#21644;&#25191;&#34892;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#29616;&#26377;&#22522;&#20934;&#19982;&#23454;&#38469;&#24212;&#29992;&#26399;&#26395;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CodeScope&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20174;&#25968;&#25454;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#19977;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#20102;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21482;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#26102;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21551;&#21457;&#20102;GNN4LP&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2310.00793</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38142;&#25509;&#39044;&#27979;: &#19968;&#20010;&#25968;&#25454;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Link Prediction: A Data Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20174;&#25968;&#25454;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#19977;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#20102;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21482;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#26102;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#38142;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21551;&#21457;&#20102;GNN4LP&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#39033;&#22522;&#20110;&#22270;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#20363;&#22914;&#26379;&#21451;&#25512;&#33616;&#12289;&#34507;&#30333;&#36136;&#20998;&#26512;&#21644;&#33647;&#29289;&#20114;&#20316;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#20010;&#39046;&#22495;&#65292;&#23427;&#20204;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#38142;&#25509;&#24418;&#25104;&#26426;&#21046;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35777;&#25454;&#24378;&#35843;&#20102;&#19968;&#20010;&#26222;&#36941;&#36866;&#29992;&#20110;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31639;&#27861;&#30340;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35270;&#35282;&#25506;&#32034;&#38142;&#25509;&#39044;&#27979;&#30340;&#21407;&#21017;&#65292;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#23545;&#38142;&#25509;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#22240;&#32032;:&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#12289;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#21644;&#29305;&#24449;&#25509;&#36817;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20854;&#20013; (i)&#21482;&#26377;&#22312;&#23616;&#37096;&#32467;&#26500;&#25509;&#36817;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20840;&#23616;&#32467;&#26500;&#25509;&#36817;&#24615;&#25165;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290; (ii)&#29305;&#24449;&#21644;&#32467;&#26500;&#25509;&#36817;&#24615;&#20043;&#38388;&#23384;&#22312;&#19981;&#20860;&#23481;&#24615;&#12290;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#23548;&#33268;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN4LP) &#25345;&#32493;&#22320;
&lt;/p&gt;
&lt;p&gt;
Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#21270;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#36890;&#36807;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#25200;&#21160;&#22270;&#24418;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.09844</link><description>&lt;p&gt;
CC-SGG: &#20351;&#29992;&#23398;&#20064;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#21270;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#36890;&#36807;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#25200;&#21160;&#22270;&#24418;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#23545;&#20110;&#27979;&#35797;&#21644;&#39564;&#35777;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#36825;&#20123;&#22330;&#26223;&#22312;&#33258;&#28982;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#36890;&#24120;&#19981;&#36275;&#65292;&#22240;&#27492;&#20351;&#29992;&#21512;&#25104;&#30340;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;AV&#22312;&#29420;&#29305;&#24773;&#20917;&#19979;&#30340;&#23433;&#20840;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#12289;&#30495;&#23454;&#20294;&#21448;&#36924;&#30495;&#30340;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#25442;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#26368;&#23567;&#21270;&#23545;&#20854;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#25913;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#26469;&#23398;&#20064;&#25200;&#21160;&#36825;&#20123;&#22270;&#24418;&#20197;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#12290;&#36755;&#20837;&#21644;&#25200;&#21160;&#21518;&#30340;&#22270;&#24418;&#20877;&#27425;&#23548;&#20837;&#27169;&#25311;&#22120;&#20197;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#20174;&#26222;&#36890;&#22330;&#26223;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into the simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;</title><link>https://arxiv.org/abs/2307.16806</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#27979;&#35797;ChatGPT&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#30340;&#33021;&#21147;&#65306;GPT3.5&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;ASCII-Art&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;
&lt;/p&gt;
&lt;p&gt;
Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#24067;&#21518;&#30340;&#20843;&#20010;&#26376;&#37324;&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#26131;&#20110;&#20351;&#29992;&#65292;ChatGPT&#21450;&#20854;&#24213;&#23618;&#27169;&#22411;GPT3.5&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#19968;&#25209;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#33539;&#22260;&#30340;&#35770;&#25991;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#25152;&#25509;&#25910;&#21644;&#25552;&#21462;&#30340;&#20449;&#24687;&#35201;&#20040;&#26159;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#35201;&#20040;&#26159;&#31867;&#20284;&#20195;&#30721;&#30340;&#39118;&#26684;&#21270;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#23545;&#19968;&#20010;&#30495;&#27491;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#22810;&#20010;&#20449;&#21495;&#27169;&#24577;&#19978;&#20855;&#22791;&#30340;&#33021;&#21147;&#30340;&#21551;&#31034;&#65292;&#32771;&#23519;&#20102;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#36755;&#20837;&#20197;ASCII-Art&#24418;&#24335;&#25552;&#20379;&#20869;&#23481;&#65292;&#27809;&#26377;&#26126;&#26174;&#30340;&#35821;&#35328;&#21270;&#24635;&#32467;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#35813;&#27169;&#22411;&#22312;&#32463;&#36807;&#20856;&#22411;&#30340;&#35270;&#35273;&#35774;&#32622;&#19979;&#30340;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#20854;&#23545;&#22270;&#20687;&#37096;&#20998;&#30340;&#30693;&#35782;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2305.14998</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14998
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#22914;CLIPScore&#65288;Hessel&#31561;&#65292;2021&#65289;&#65292;UMIC&#65288;Lee&#31561;&#65292;2021&#65289;&#21644;PAC-S&#65288;Sarto&#31561;&#65292;2023&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#26080;&#21442;&#32771;&#35780;&#20272;&#22270;&#20687;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#22312;&#38656;&#35201;&#21306;&#20998;&#20855;&#26377;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#20004;&#20010;&#26631;&#39064;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;CLIPScore&#12289;UMIC&#21644;PAC-S&#24456;&#38590;&#35782;&#21035;&#32454;&#31890;&#24230;&#38169;&#35823;&#12290;&#34429;&#28982;&#25152;&#26377;&#25351;&#26631;&#23545;&#35270;&#35273;&#38169;&#35823;&#25935;&#24863;&#65292;&#20294;&#23545;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#30340;&#25935;&#24863;&#24615;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#25152;&#26377;&#25351;&#26631;&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#22823;&#23567;&#21464;&#21270;&#25935;&#24863;&#65292;&#32780;CLIPScore&#21644;PAC-S&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#25968;&#37327;&#20063;&#25935;&#24863;&#12290;&#20851;&#20110;&#26631;&#39064;&#30340;&#35821;&#35328;&#26041;&#38754;&#65292;&#25152;&#26377;&#25351;&#26631;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2302.07200</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#35797;&#22270;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#35299;&#37322;&#24615;&#12289;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#23558;&#31526;&#21495;&#25512;&#29702;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;&#38543;&#30528;&#30693;&#35782;&#22270;&#35889;&#25104;&#20026;&#34920;&#31034;&#24322;&#26500;&#21644;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#65292;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#24320;&#22987;&#36981;&#24490;&#36825;&#31181;&#31070;&#32463;&#31526;&#21495;&#33539;&#24335;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#35201;&#20040;&#29983;&#25104;&#20195;&#34920;&#24615;&#30340;&#25968;&#20540;&#23884;&#20837;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#20986;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#24357;&#21512;&#36825;&#31181;&#20108;&#20803;&#23545;&#31435;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#12289;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#21147;&#24182;&#38598;&#25104;&#19987;&#23478;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#25191;&#34892;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#65288;1&#65289;&#36923;&#36753;&#20449;&#24687;&#23884;&#20837;&#26041;&#27861;&#65292;&#65288;2&#65289;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#19982;&#36923;&#36753;&#19968;&#33268;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. As knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. Therefore, we survey methods that perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel taxonomy by which we can classify them. Specifically, we propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical cons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.10718</link><description>&lt;p&gt;
&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Whole Page Unbiased Learning to Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.10718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#39029;&#38754;&#21576;&#29616;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#28857;&#20987;&#34892;&#20026;&#26041;&#38754;&#30340;&#20559;&#24046;&#65292;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#20351;&#29992;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#26469;&#25913;&#36827;&#25490;&#24207;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(ULTR)&#31639;&#27861;&#65292;&#36890;&#36807;&#20559;&#24046;&#28857;&#20987;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#26080;&#20559;&#30340;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#20449;&#20219;&#20559;&#24046;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;(SERP)&#20013;&#20854;&#20182;&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#30001;&#22810;&#23186;&#20307;&#24341;&#21457;&#30340;&#21560;&#24341;&#20559;&#24046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#20559;&#24046;&#22312;&#24037;&#19994;&#31995;&#32479;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(WP-ULTR)&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#25972;&#39029;SERP&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#12290;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65306;(1) &#24456;&#38590;&#25214;&#21040;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411; (&#29992;&#25143;&#34892;&#20026;&#20551;&#35774;)&#65307;(2) &#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank~(ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;QAOA&#26159;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#21892;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2207.06294</link><description>&lt;p&gt;
&#21152;&#24378;&#23398;&#20064;&#36741;&#21161;&#30340;&#36882;&#24402;QAOA
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Assisted Recursive QAOA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06294
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;QAOA&#26159;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#21892;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#20043;&#31867;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#22240;&#20026;&#23427;&#20204;&#26377;&#26395;&#21033;&#29992;NISQ&#35774;&#22791;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22312;&#20302;&#28145;&#24230;&#19979;&#65292;QAOA&#30340;&#26576;&#20123;&#23616;&#37096;&#24615;&#32422;&#26463;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#21363;&#36882;&#24402;QAOA&#65288;RQAOA&#65289;&#65292;&#20197;&#25552;&#39640;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;RQAOA&#30456;&#23545;&#20110;QAOA&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#20154;&#20204;&#23545;&#20854;&#20102;&#35299;&#36739;&#23569;&#65292;&#20363;&#22914;&#22312;&#21738;&#20123;&#23454;&#20363;&#26063;&#19978;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25105;&#20204;&#22788;&#29702;&#30340;&#26159;NP&#38590;&#38382;&#39064;&#65288;&#20855;&#20307;&#32780;&#35328;&#26159;&#20234;&#36763;&#33258;&#26059;&#27169;&#22411;&#65289;&#65292;&#39044;&#35745;RQAOA&#20250;&#22833;&#36133;&#65292;&#36825;&#24341;&#21457;&#20102;&#35774;&#35745;&#26356;&#22909;&#30340;&#37327;&#23376;&#31639;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20998;&#26512;&#20102;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms such as the Quantum Approximation Optimization Algorithm (QAOA) in recent years have gained popularity as they provide the hope of using NISQ devices to tackle hard combinatorial optimization problems. It is, however, known that at low depth, certain locality constraints of QAOA limit its performance. To go beyond these limitations, a non-local variant of QAOA, namely recursive QAOA (RQAOA), was proposed to improve the quality of approximate solutions. The RQAOA has been studied comparatively less than QAOA, and it is less understood, for instance, for what family of instances it may fail to provide high quality solutions. However, as we are tackling $\mathsf{NP}$-hard problems (specifically, the Ising spin model), it is expected that RQAOA does fail, raising the question of designing even better quantum algorithms for combinatorial optimization. In this spirit, we identify and analyze cases where RQAOA fails and, based on this, propose a reinforcement le
&lt;/p&gt;</description></item><item><title>IM-META&#26159;&#19968;&#31181;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#21644;&#26597;&#35810;&#20449;&#24687;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#12289;&#26500;&#24314;&#22686;&#24378;&#22270;&#20197;&#21450;&#20351;&#29992;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2106.02926</link><description>&lt;p&gt;
IM-META: &#20351;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.02926
&lt;/p&gt;
&lt;p&gt;
IM-META&#26159;&#19968;&#31181;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#21644;&#26597;&#35810;&#20449;&#24687;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#12289;&#26500;&#24314;&#22686;&#24378;&#22270;&#20197;&#21450;&#20351;&#29992;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#24213;&#23618;&#32593;&#32476;&#30340;&#19968;&#37096;&#20998;&#26469;&#30830;&#23450;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#65292;&#32473;&#23450;&#33410;&#28857;&#26597;&#35810;&#30340;&#23567;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IM-META&#65292;&#19968;&#31181;&#35299;&#20915;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26597;&#35810;&#21644;&#33410;&#28857;&#20803;&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#30001;&#20110;&#20351;&#29992;&#36825;&#26679;&#30340;&#20803;&#25968;&#25454;&#24182;&#19981;&#26159;&#27809;&#26377;&#39118;&#38505;&#65292;&#22240;&#20026;&#20803;&#25968;&#25454;&#30340;&#22122;&#22768;&#29305;&#24615;&#21644;&#36830;&#36890;&#24615;&#25512;&#26029;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;IM&#38382;&#39064;&#65292;&#26088;&#22312;&#25214;&#21040;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;&#22312;IM-META&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19979;&#19977;&#20010;&#27493;&#39588;&#36845;&#20195;&#36827;&#34892;&#65306;1&#65289;&#25105;&#20204;&#36890;&#36807;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25910;&#38598;&#21040;&#30340;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#65292;2&#65289;&#25105;&#20204;&#36873;&#25321;&#19968;&#23450;&#25968;&#37327;&#30340;&#25512;&#26029;&#20986;&#30340;&#21487;&#20449;&#36793;&#26469;&#26500;&#24314;&#19968;&#20010;&#22686;&#24378;&#22270;&#65292;3&#65289;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25512;&#26029;&#30340;&#24433;&#21709;&#25193;&#25955;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#26597;&#35810;&#30340;&#33410;&#28857;&#12290;&#36890;&#36807;&#23545;IM-META&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the structure of complex networks is often unknown, we may identify the most influential seed nodes by exploring only a part of the underlying network, given a small budget for node queries. We propose IM-META, a solution to influence maximization (IM) in networks with unknown topology by retrieving information from queries and node metadata. Since using such metadata is not without risk due to the noisy nature of metadata and uncertainties in connectivity inference, we formulate a new IM problem that aims to find both seed nodes and queried nodes. In IM-META, we develop an effective method that iteratively performs three steps: 1) we learn the relationship between collected metadata and edges via a Siamese neural network, 2) we select a number of inferred confident edges to construct a reinforced graph, and 3) we identify the next node to query by maximizing the inferred influence spread using our topology-aware ranking strategy. Through experimental evaluation of IM-META on fou
&lt;/p&gt;</description></item><item><title>SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15270</link><description>&lt;p&gt;
SimFair&#65306;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#19982;&#27169;&#25311;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15270
&lt;/p&gt;
&lt;p&gt;
SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#24050;&#32463;&#25104;&#20026;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#22522;&#30784;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19981;&#24179;&#31561;&#26159;&#30001;&#20110;&#19981;&#21516;&#21306;&#22495;&#20998;&#24067;&#30340;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#25552;&#39640;&#20844;&#24179;&#21487;&#36801;&#31227;&#24615;&#30340;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#26469;&#33258;&#26032;&#21306;&#22495;&#30340;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#36825;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#23581;&#35797;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22522;&#20110;&#29289;&#29702;&#26426;&#21046;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimFair&#65292;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#29289;&#29702;&#35268;&#21017;&#30340;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#21040;&#35757;&#32451;&#35774;&#35745;&#20013;&#26469;&#24357;&#34917;&#25968;&#25454;&#38480;&#21046;&#12290;&#20197;&#28201;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SimFair&#22312;&#20445;&#25345;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03301</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#20160;&#20040;&#20419;&#36827;&#20102;&#23545;&#20110;&#24207;&#36125;&#21494;&#26031;&#20915;&#31574;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22312;&#21033;&#29992;&#65288;&#20540;&#65289;&#20989;&#25968;&#36924;&#36817;&#30340;&#21516;&#26102;&#20139;&#21463;&#26679;&#26412;&#25928;&#29575;&#30340;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#25324;&#31163;&#32447;RL&#20013;&#35206;&#30422;&#24230;&#37327;&#30340;&#20808;&#21069;&#27010;&#24565;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#23558;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#65288;VS&#65289;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;RO&#65289;&#21644;&#21518;&#39564;&#37319;&#26679;&#65288;PS&#65289;&#30340;&#19977;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#36827;&#34892;&#32479;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#65292;&#22522;&#20110;VS&#12289;&#22522;&#20110;RO&#21644;&#22522;&#20110;PS&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;\emph{&#21487;&#27604;}&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#24674;&#22797;&#20102;&#22312;&#26377;&#38480;&#21644;&#32447;&#24615;&#27169;&#22411;&#31867;&#21035;&#19979;&#30340;&#26368;&#20248;&#24615;&#30340;&#26631;&#20934;&#20551;&#35774;&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#19981;&#20855;&#26377;&#26377;&#21033;&#24615;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.01344</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#38405;&#35835;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#26159;AI&#31995;&#32479;&#23433;&#20840;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#28857;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35828;&#65292;&#26550;&#26500;&#26159;&#23545;&#25163;&#35797;&#22270;&#24674;&#22797;&#30340;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#19968;&#31995;&#21015;&#37325;&#22797;&#35745;&#31639;&#22359;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#29983;&#29420;&#29305;&#30340;&#20391;&#20449;&#36947;&#27844;&#38706;&#12290;&#24403;&#30446;&#26631;&#24179;&#21488;&#22312;&#29289;&#29702;&#19978;&#21487;&#35775;&#38382;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#36947;&#27844;&#38706;&#26469;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;&#36890;&#36807;&#32467;&#21512;&#23545;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#30340;&#29702;&#35770;&#30693;&#35782;&#21644;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29616;&#24211;&#65288;ARM CMSIS-NN&#65289;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#22238;&#31572;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36890;&#36807;&#31616;&#21333;&#22320;&#26816;&#26597;&#19968;&#20010;EM&#20391;&#20449;&#36947;&#36319;&#36394;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#22810;&#36828;&#30340;&#26550;&#26500;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;MLP&#21644;CNN&#27169;&#22411;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#31471;32&#20301;&#24494;&#25511;&#21046;&#22120;&#65288;Cortex-M7&#65289;&#19978;&#36816;&#34892;&#65292;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#27169;&#24335;&#35782;&#21035;&#20998;&#26512;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#22768;&#31216;&#65292;&#19982;&#21442;&#25968;&#25552;&#21462;&#30456;&#21453;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#20013;&#25552;&#21462;&#20986;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19917</link><description>&lt;p&gt;
&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#20559;&#35265;&#26816;&#27979;&#21644;&#32531;&#35299;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#21307;&#30103;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#32508;&#36848;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#12290;&#26041;&#27861;&#65306;&#36981;&#24490;Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)&#20934;&#21017;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#20174;PubMed&#12289;Web of Science&#21644;&#30005;&#27668;&#21644;&#30005;&#23376;&#24037;&#31243;&#24072;&#23398;&#20250;&#20013;&#26816;&#32034;&#20102;2010&#24180;1&#26376;1&#26085;&#33267;2022&#24180;10&#26376;31&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;&#32467;&#26524;&#65306;&#22312;&#26816;&#32034;&#21040;&#30340;252&#31687;&#25991;&#31456;&#20013;&#65292;&#26377;20&#31687;&#31526;&#21512;&#26368;&#32456;&#32508;&#36848;&#30340;&#32435;&#20837;&#26631;&#20934;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#20845;&#31181;&#20559;&#35265;&#20013;&#30340;&#20116;&#31181;&#65306;&#20843;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#36873;&#25321;&#20559;&#35265;&#65307;&#20845;&#39033;&#30740;&#31350;&#38024;&#23545;&#38544;&#24615;&#20559;&#35265;&#65307;&#20116;&#39033;&#30740;&#31350;&#23545;&#28151;&#26434;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#22235;&#39033;&#30740;&#31350;&#23545;&#27979;&#37327;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#20004;&#39033;&#30740;&#31350;&#23545;&#31639;&#27861;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#26041;&#38754;&#65292;&#26377;&#21313;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13033</link><description>&lt;p&gt;
LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13033
&lt;/p&gt;
&lt;p&gt;
LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24182;&#34892;SGD&#26159;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#36890;&#20449;&#29942;&#39048;&#26159;&#20854;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#21387;&#32553;&#26041;&#26696;&#35201;&#20040;&#20551;&#35774;&#36890;&#20449;&#38142;&#36335;&#26080;&#22122;&#22768;&#65292;&#35201;&#20040;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#20171;&#32461;&#20102;LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;&#12290;LASER&#21033;&#29992;&#26799;&#24230;&#30340;&#22266;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#12290;&#23613;&#31649;&#20139;&#21463;&#19982;&#32463;&#20856;SGD&#30456;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;LASER&#22312;&#21508;&#31181;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25345;&#32493;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#22522;&#20934;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#19978;&#33719;&#24471;&#20102;50-64%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#36890;&#29992;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#23545;&#26538;&#25903;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.07975</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;&#19978;&#30340;&#26538;&#25903;&#36208;&#31169;&#27963;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#36890;&#29992;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#23545;&#26538;&#25903;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#33258;&#21160;&#21270;&#22320;&#20998;&#31867;&#26538;&#25903;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#29616;&#23454;&#20219;&#21153;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#28041;&#21450;&#20844;&#20849;&#31354;&#38388;&#23433;&#20840;&#12289;&#24773;&#25253;&#25910;&#38598;&#21644;&#25191;&#27861;&#35843;&#26597;&#12290;&#24403;&#24212;&#29992;&#20110;&#20174;&#20840;&#29699;&#33539;&#22260;&#20869;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#22270;&#20687;&#65288;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#21644;&#26263;&#32593;&#31449;&#65289;&#26102;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#20998;&#26512;&#24320;&#25918;&#28304;&#24773;&#25253;&#20013;&#30340;&#22823;&#25968;&#25454;&#30340;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20197;&#35797;&#22270;&#35782;&#21035;&#29359;&#32618;&#26538;&#25903;&#36208;&#31169;&#32593;&#32476;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#24120;&#35265;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#22312;&#22823;&#35268;&#27169;&#30340;&#36890;&#29992;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet-1k&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#36827;&#34892;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#65292;&#28982;&#21518;&#22312;&#36739;&#23567;&#30340;&#12289;&#27880;&#37322;&#30340;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#23545;DNN&#36827;&#34892;&#24494;&#35843;&#20197;&#36827;&#34892;&#26538;&#25903;&#35270;&#35273;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;AI&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#23454;&#20363;&#30340;UQ&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10852</link><description>&lt;p&gt;
&#20351;&#29992;AI&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Using AI Uncertainty Quantification to Improve Human Decision-Making. (arXiv:2309.10852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;AI&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#23454;&#20363;&#30340;UQ&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26377;&#21487;&#33021;&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#26377;&#29992;&#30340;&#27010;&#29575;&#20449;&#24687;&#65292;&#36229;&#36234;&#21333;&#32431;&#30340;AI&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#36827;&#20154;&#31867;&#20915;&#31574;&#12290;&#36807;&#21435;&#30340;&#22823;&#37096;&#20998;&#20851;&#20110;AI&#21644;&#20154;&#31867;&#20915;&#31574;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#19978;&#12290;&#25105;&#20204;&#36816;&#29992;&#22522;&#20110;&#23454;&#20363;&#30340;UQ&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#20998;&#31867;AI&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22312;&#32473;&#23450;&#23454;&#20363;&#38468;&#36817;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#21019;&#24314;&#20102;UQ&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#20005;&#26684;&#30340;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#23545;&#35745;&#31639;&#20986;&#30340;UQ&#36827;&#34892;&#20102;&#26657;&#20934;&#65292;&#20316;&#20026;UQ&#30340;&#36136;&#37327;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#39044;&#20808;&#27880;&#20876;&#30340;&#22312;&#32447;&#34892;&#20026;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;AI&#20449;&#24687;&#26465;&#20214;&#19979;&#23458;&#35266;&#20154;&#31867;&#20915;&#31574;&#34920;&#29616;&#65292;&#21253;&#25324;UQ&#12290;&#22312;&#23454;&#39564;1&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#20351;&#29992;AI&#65288;&#23545;&#29031;&#32452;&#65289;&#12289;&#20165;&#20351;&#29992;AI&#39044;&#27979;&#21644;&#20351;&#29992;AI&#39044;&#27979;&#21450;UQ&#21487;&#35270;&#21270;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#21457;&#29616;UQ&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#20915;&#31574;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional useful probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability. We implemented instance-based UQ for three real datasets. To achieve this, we trained different AI models for classification for each dataset, and used random samples generated around the neighborhood of the given instance to create confidence intervals for UQ. The computed UQ was calibrated using a strictly proper scoring rule as a form of quality assurance for UQ. We then conducted two preregistered online behavioral experiments that compared objective human decision-making performance under different AI information conditions, including UQ. In Experiment 1, we compared decision-making for no AI (control), AI prediction alone, and AI prediction with a visualization of UQ. We found UQ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.04146</link><description>&lt;p&gt;
NESTLE&#65306;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04146
&lt;/p&gt;
&lt;p&gt;
NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27861;&#24459;&#35265;&#35299;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#20998;&#26512;&#65292;&#38656;&#35201;&#20351;&#29992;&#25991;&#26723;&#26816;&#32034;&#24037;&#20855;&#36873;&#25321;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#65292;&#20351;&#29992;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#31995;&#32479;&#23545;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#20197;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#27599;&#20010;&#36807;&#31243;&#37117;&#38656;&#35201;&#19987;&#19994;&#24037;&#20855;&#25110;&#32534;&#31243;&#25216;&#33021;&#65292;&#28982;&#32780;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#21487;&#29992;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;IE&#65292;&#22914;&#26524;IE&#31995;&#32479;&#30340;&#26412;&#20307;&#20013;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#37027;&#20040;&#38656;&#35201;&#33258;&#24049;&#26500;&#24314;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NESTLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#12290;&#36890;&#36807;NESTLE&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;GUI&#36827;&#34892;&#32454;&#33268;&#32423;&#21035;&#30340;&#25511;&#21046;&#26469;&#21487;&#35270;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;NESTLE&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;IE&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#23558;&#21508;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02908</link><description>&lt;p&gt;
DECODE: &#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#25968;&#25454;&#39537;&#21160;&#33021;&#32791;&#39044;&#27979;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#30340;&#33021;&#32791;&#39044;&#27979;&#22312;&#26377;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#20248;&#21270;&#30340;&#33021;&#32791;&#21644;&#30005;&#32593;&#20998;&#37197;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#33021;&#28304;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;LSTM&#27169;&#22411;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#33021;&#32791;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;LSTM&#27169;&#22411;&#19982;&#32447;&#24615;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#24050;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#25552;&#20986;&#30340;LSTM&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;R2&#24471;&#20998;&#20026;0.97&#65292;&#26368;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.007&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
&lt;/p&gt;</description></item><item><title>&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01516</link><description>&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65306;&#20026;&#21487;&#25193;&#23637;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#35843;&#25972;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01516
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#19987;&#38376;&#30340;&#20219;&#21153;&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#23396;&#31435;&#12289;&#31351;&#20030;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#32463;&#24120;&#24573;&#35270;&#27169;&#24577;&#23545;&#40784;&#65292;&#20165;&#20851;&#27880;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#8220;&#23545;&#40784;&#22686;&#24378;&#22120;&#8221;&#65292;&#21487;&#20197;&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#24230;&#30340;&#21487;&#36716;&#31227;&#24615;&#32780;&#26080;&#38656;&#35843;&#25972;&#39044;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21521;LMMs&#28155;&#21152;&#20102;&#19981;&#21040;1.25%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#20197;BEiT-3&#27169;&#22411;&#20026;&#20363;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#39640;&#36798;57%&#30340;&#24494;&#35843;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11776</link><description>&lt;p&gt;
WS-SfMLearner: &#33258;&#21161;&#30417;&#30563;&#24335;&#22312;&#26410;&#30693;&#30456;&#26426;&#21442;&#25968;&#24773;&#20917;&#19979;&#36827;&#34892;&#25163;&#26415;&#35270;&#39057;&#30340;&#21333;&#30446;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters. (arXiv:2308.11776v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#22312;&#35768;&#22810;&#22270;&#20687;&#24341;&#23548;&#25163;&#26415;&#31243;&#24207;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#20142;&#24230;&#21644;&#22122;&#22768;&#19981;&#19968;&#33268;&#65292;&#21019;&#24314;&#28145;&#24230;&#22320;&#22270;&#30495;&#23454;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#19968;&#20010;&#20934;&#30830;&#19988;&#40065;&#26834;&#30340;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#24341;&#36215;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#30340;&#26356;&#22810;&#20851;&#27880;&#12290;&#23613;&#31649;&#19968;&#20123;&#33258;&#21161;&#30417;&#30563;&#26041;&#27861;&#20943;&#36731;&#20102;&#23545;&#30495;&#23454;&#28145;&#24230;&#22320;&#22270;&#21644;&#20301;&#23039;&#30340;&#38656;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#38656;&#35201;&#24050;&#30693;&#30340;&#30456;&#26426;&#20869;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#36890;&#24120;&#32570;&#22833;&#25110;&#26410;&#35760;&#24405;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#20013;&#30456;&#26426;&#20869;&#21442;&#25968;&#39044;&#27979;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#20307;&#31215;&#30340;&#30417;&#30563;&#26041;&#24335;&#26469;&#25552;&#20379;&#31995;&#32479;&#30340;&#33258;&#25105;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the sy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01285</link><description>&lt;p&gt;
Flows: &#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Flows: Building Blocks of Reasoning and Collaborating AI. (arXiv:2308.01285v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01285
&lt;/p&gt;
&lt;p&gt;
Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#31995;&#32479;&#12290;&#36825;&#20026;&#32467;&#26500;&#21270;&#25512;&#29702;&#20197;&#21450;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#20316;&#21019;&#36896;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#36935;&#12290;&#20026;&#20102;&#20805;&#20998;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#65292;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#30740;&#31350;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27969;&#31243;&#30340;&#27010;&#24565;&#26694;&#26550;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#24314;&#27169;&#22797;&#26434;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#27969;&#31243;&#26159;&#35745;&#31639;&#30340;&#33258;&#21253;&#21547;&#26500;&#24314;&#27169;&#22359;&#65292;&#20855;&#26377;&#29420;&#31435;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#22522;&#20110;&#28040;&#24687;&#30340;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;&#27969;&#31243;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20219;&#24847;&#23884;&#22871;&#30340;&#20132;&#20114;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#20219;&#20309;&#20132;&#20114;&#37117;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#21253;&#25324;&#20043;&#21069;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;-&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;-&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#12289;&#25552;&#31034;&#24037;&#31243;&#26041;&#26696;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#31243;&#22312;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.07286</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching. (arXiv:2307.07286v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#39592;&#26550;&#21305;&#37197;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#26088;&#22312;&#36890;&#36807;&#21333;&#20010;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#27169;&#22411;&#65292;&#30001;&#20110;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#35268;&#27169;&#39592;&#26550;&#21160;&#20316;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#23427;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#27604;&#36739;&#29305;&#24449;&#21521;&#37327;&#26469;&#21305;&#37197;&#39592;&#26550;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#39592;&#26550;&#25968;&#25454;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#26102;&#38388;&#39034;&#24207;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#27425;&#24615;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26102;&#31354;&#29305;&#24449;&#21305;&#37197;&#26469;&#22788;&#29702;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#34920;&#31034;&#39592;&#26550;&#25968;&#25454;&#65292;&#24182;&#20174;&#20004;&#20010;&#35282;&#24230;&#23454;&#29616;&#26368;&#20248;&#29305;&#24449;&#21305;&#37197;&#12290;&#31532;&#19968;&#31181;&#26159;&#22810;&#23610;&#24230;&#21305;&#37197;&#65292;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#39592;&#26550;&#25968;&#25454;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#31532;&#20108;&#31181;&#26159;&#36328;&#23610;&#24230;&#21305;&#37197;&#65292;&#36890;&#36807;&#25429;&#25417;&#26679;&#26412;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#36816;&#21160;&#24133;&#24230;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18593</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#24314;&#27169;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#25104;&#20026;&#22522;&#20110;&#23494;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#23588;&#20854;&#26159;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#22791;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#36890;&#36807;&#31616;&#21270;DDPM&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24341;&#20986;&#21478;&#19968;&#31181;&#31216;&#20026;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65288;DTPM&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;DTPM&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#30340;&#25193;&#25955;&#26102;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27492;&#21518;&#39564;&#20998;&#24067;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;ADBenh&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;</title><link>http://arxiv.org/abs/2305.14330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#24103;&#32423;&#23548;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#33539;&#24335;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#19978;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#32500;&#25252;&#19968;&#33268;&#30340;&#21465;&#36848;&#21644;&#22788;&#29702;&#20174;&#21333;&#20010;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#24555;&#36895;&#22330;&#26223;&#32452;&#21512;&#25110;&#23545;&#35937;&#20301;&#32622;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DirecT2V&#65292;&#23427;&#21033;&#29992;&#38024;&#23545;&#25351;&#20196;&#26657;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#21333;&#20010;&#25277;&#35937;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#36880;&#24103;&#25551;&#36848;&#12290;DirecT2V&#21033;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#20010;&#24103;&#30340;&#21333;&#29420;&#25552;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21253;&#21547;&#26102;&#38388;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#20415;&#20110;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20026;&#20102;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#26144;&#23556;&#26041;&#27861;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;DirecT2V&#26694;&#26550;&#22312;&#38646;&#26679;&#26412;T2V&#29983;&#25104;&#20013;&#20135;&#29983;&#30340;&#35270;&#35273;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2305.06841</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#34913;&#37327;&#28040;&#38500;&#38382;&#31572;&#27169;&#22411;&#39044;&#27979;&#24555;&#25463;&#26041;&#24335;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06841
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#20027;&#23548;&#20102;&#22823;&#37096;&#20998;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24314;&#27169;&#20551;&#28151;&#28102;&#30340;&#25903;&#25345;&#19979;, &#22240;&#27492;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26576;&#20123;&#36825;&#20123;&#32467;&#26524;&#26159;&#30001;&#24314;&#27169;&#34394;&#20551;&#30456;&#20851;&#24615;&#23454;&#29616;&#30340;&#12290;&#20316;&#32773;&#24120;&#24120;&#36890;&#36807;&#35780;&#20272;&#21516;&#19968;&#20219;&#21153;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#23610;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#30340;&#25253;&#21578;OOD&#25910;&#30410;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#26377;&#20559;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;OOD&#27169;&#22411;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#20559;&#24046;&#29305;&#24449;&#65292;&#19982;ID&#27169;&#22411;&#30456;&#24403;&#65292;&#36827;&#32780;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#25512;&#21160;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10750</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#65288;Grounded Language Understanding&#65289;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10750
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36890;&#24120;&#34987;&#35270;&#20026;&#21333;&#27493;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#25509;&#25910;&#19968;&#20010;&#25351;&#20196;&#65292;&#25191;&#34892;&#23427;&#65292;&#28982;&#21518;&#26681;&#25454;&#26368;&#32456;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#25105;&#20204;&#20027;&#24352;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#20063;&#24212;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#20154;&#31867;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20195;&#29702;&#21487;&#20197;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;Help Feedback&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
&lt;/p&gt;</description></item><item><title>LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18013</link><description>&lt;p&gt;
LaCViT&#65306;&#19968;&#31181;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;
&lt;/p&gt;
&lt;p&gt;
LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18013
&lt;/p&gt;
&lt;p&gt;
LaCViT&#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#21508;&#21521;&#31561;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#39640;&#20854;&#34920;&#31034;&#31354;&#38388;&#31561;&#24615;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273; Transformer &#24050;&#32463;&#22312;&#22788;&#29702;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#27169;&#25311;&#38271;&#26102;&#38388;&#30340;&#29305;&#24449;&#20381;&#36182;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#21508;&#31181;&#33258;&#30417;&#30563;&#20449;&#21495;&#65288;&#20363;&#22914;&#65292;&#36974;&#34109;&#38543;&#26426;&#22359;&#65289;&#65292;&#35270;&#35273; Transformer &#22312; ImageNet-1k &#21644; CIFAR-10 &#31561;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#36890;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21482;&#33021;&#20135;&#29983;&#21508;&#21521;&#24322;&#24615;&#34920;&#31034;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38754;&#21521;&#26631;&#31614;&#30340;&#23545;&#27604;&#35757;&#32451;&#26694;&#26550; LaCViT&#65292;&#23427;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#30340;&#31561;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LaCViT&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#24182;&#24314;&#31435;&#36215;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09197</link><description>&lt;p&gt;
&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporality and Causality in Abstract Argumentation. (arXiv:2303.09197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25277;&#35937;&#35770;&#35777;&#20013;&#30340;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#24182;&#24314;&#31435;&#36215;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25277;&#35937;&#35770;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32771;&#34385;&#26102;&#38388;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#22909;&#22788;&#65292;&#21363;&#35770;&#25454;&#34987;&#38472;&#36848;&#30340;&#39034;&#24207;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#26080;&#29615;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#27010;&#24565;&#37325;&#26032;&#32534;&#20889;&#25104;&#19968;&#20010;&#34892;&#21160;&#35821;&#35328;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#19990;&#30028;&#30340;&#28436;&#21270;&#65292;&#24182;&#24314;&#31435;&#35770;&#25454;&#38472;&#36848;&#21644;&#23427;&#20204;&#30340;&#30452;&#25509;&#25110;&#38388;&#25509;&#21518;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Answer Set&#32534;&#31243;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of abstract argumentation, we present the benefits of considering temporality, i.e. the order in which arguments are enunciated, as well as causality. We propose a formal method to rewrite the concepts of acyclic abstract argumentation frameworks into an action language, that allows us to model the evolution of the world, and to establish causal relationships between the enunciation of arguments and their consequences, whether direct or indirect. An Answer Set Programming implementation is also proposed, as well as perspectives towards explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.00487</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65306;&#29702;&#35770;&#12289;&#26041;&#27861;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#33719;&#21462;&#12289;&#26356;&#26032;&#12289;&#31215;&#32047;&#21644;&#21033;&#29992;&#30693;&#35782;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65292;&#20026;AI&#31995;&#32479;&#33258;&#36866;&#24212;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26032;&#20219;&#21153;&#36890;&#24120;&#20250;&#23548;&#33268;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#26029;&#28044;&#29616;&#30340;&#21508;&#31181;&#36827;&#23637;&#22823;&#22823;&#25193;&#23637;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65292;&#26088;&#22312;&#36830;&#25509;&#22522;&#26412;&#35774;&#32622;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#20195;&#34920;&#24615;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#27010;&#25324;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65306;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21644;&#32456;&#36523;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;&#23454;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#12289;&#22522;&#20110;&#22238;&#25918;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item><item><title>pyRDDLGym&#26159;&#19968;&#20010;Python&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#12290;&#23427;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;RDDL&#20013;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#65292;&#24182;&#25903;&#25345;&#31616;&#21333;&#30340;&#29615;&#22659;&#20462;&#25913;&#21644;&#25193;&#23637;&#12290;&#23427;&#20855;&#26377;&#29420;&#29305;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#24555;&#36895;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.05939</link><description>&lt;p&gt;
pyRDDLGym&#65306;&#20174;RDDL&#21040;Gym&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
pyRDDLGym: From RDDL to Gym Environments. (arXiv:2211.05939v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05939
&lt;/p&gt;
&lt;p&gt;
pyRDDLGym&#26159;&#19968;&#20010;Python&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#12290;&#23427;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;RDDL&#20013;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#65292;&#24182;&#25903;&#25345;&#31616;&#21333;&#30340;&#29615;&#22659;&#20462;&#25913;&#21644;&#25193;&#23637;&#12290;&#23427;&#20855;&#26377;&#29420;&#29305;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#24555;&#36895;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#24182;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;pyRDDLGym&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;RDDL&#22768;&#26126;&#24615;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;OpenAI Gym&#29615;&#22659;&#30340;Python&#26694;&#26550;&#12290;RDDL&#20013;&#30340;&#21464;&#37327;&#30340;&#31163;&#25955;&#26102;&#38388;&#27493;&#36827;&#28436;&#21270;&#30001;&#26465;&#20214;&#27010;&#29575;&#20989;&#25968;&#25551;&#36848;&#65292;&#36825;&#19982;Gym&#27493;&#39588;&#26041;&#26696;&#33258;&#28982;&#22865;&#21512;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;RDDL&#26159;&#19968;&#20010;&#25277;&#35937;&#25551;&#36848;&#65292;&#23558;&#29615;&#22659;&#36827;&#34892;&#20462;&#25913;&#21644;&#25193;&#23637;&#20197;&#25903;&#25345;&#22810;&#20010;&#23454;&#20307;&#21644;&#19981;&#21516;&#37197;&#32622;&#21464;&#24471;&#31616;&#21333;&#32780;&#19981;&#26159;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#24076;&#26395;pyRDDLGym&#33021;&#22815;&#30001;&#20110;RDDL&#30340;&#29420;&#29305;&#34920;&#36798;&#33021;&#21147;&#65292;&#25104;&#20026;&#24378;&#21270;&#23398;&#20064;&#31038;&#21306;&#20013;&#19968;&#32929;&#26032;&#30340;&#21147;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26131;&#20110;&#24555;&#36895;&#24320;&#21457;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#23545;RDDL&#25551;&#36848;&#20013;&#30340;&#27169;&#22411;&#30340;&#26126;&#30830;&#35775;&#38382;&#65292;pyRDDLGym&#36824;&#21487;&#20197;&#20419;&#36827;&#21033;&#29992;&#27169;&#22411;&#30693;&#35782;&#36827;&#34892;&#20132;&#20114;&#24615;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;pyRDDLGym&#30340;&#35774;&#35745;&#21644;&#20869;&#32622;&#31034;&#20363;&#65292;&#20197;&#21450;&#34701;&#20837;RDDL&#35821;&#35328;&#30340;&#38468;&#21152;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present pyRDDLGym, a Python framework for auto-generation of OpenAI Gym environments from RDDL declerative description. The discrete time step evolution of variables in RDDL is described by conditional probability functions, which fits naturally into the Gym step scheme. Furthermore, since RDDL is a lifted description, the modification and scaling up of environments to support multiple entities and different configurations becomes trivial rather than a tedious process prone to errors. We hope that pyRDDLGym will serve as a new wind in the reinforcement learning community by enabling easy and rapid development of benchmarks due to the unique expressive power of RDDL. By providing explicit access to the model in the RDDL description, pyRDDLGym can also facilitate research on hybrid approaches for learning from interaction while leveraging model knowledge. We present the design and built-in examples of pyRDDLGym, and the additions made to the RDDL language that were incorporated into t
&lt;/p&gt;</description></item></channel></rss>