<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#20351;&#29992;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RT-Trajectory&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#22312;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2311.01977</link><description>&lt;p&gt;
RT-Trajectory: &#36890;&#36807;&#22238;&#39038;&#36712;&#36857;&#33609;&#22270;&#23454;&#29616;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches. (arXiv:2311.01977v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RT-Trajectory&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#22312;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#30340;&#26368;&#37325;&#35201;&#24895;&#26223;&#20043;&#19968;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#12289;&#35821;&#20041;&#27010;&#24565;&#25110;&#35270;&#35273;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;&#25342;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23558;&#26080;&#27861;&#27867;&#21270;&#21040;&#25240;&#21472;&#20219;&#21153;&#65292;&#21363;&#20351;&#25240;&#21472;&#30340;&#33218;&#37096;&#36712;&#36857;&#19982;&#25342;&#21462;&#21644;&#25918;&#32622;&#31867;&#20284;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#36890;&#36807;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;&#36825;&#31181;&#27867;&#21270;&#23558;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31895;&#31961;&#36712;&#36857;&#33609;&#22270;&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#31216;&#20026;RT-Trajectory&#65292;&#23427;&#26159;&#23454;&#29992;&#30340;&#12289;&#26131;&#20110;&#25351;&#23450;&#30340;&#65292;&#21487;&#20197;&#20351;&#31574;&#30053;&#26377;&#25928;&#22320;&#25191;&#34892;&#21407;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01723</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#21487;&#20197;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#65292;&#20294;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#40065;&#26834;&#24494;&#35843;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#20197;&#21450;&#24494;&#35843;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#36825;&#19968;&#26631;&#20934;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#32454;&#35843;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26657;&#20934;&#30340;&#25285;&#24551;&#65292;&#24182;&#36890;&#36807;&#26174;&#31034;&#26222;&#36890;&#24494;&#35843;&#29978;&#33267;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#36896;&#25104;&#20102;&#25439;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#65292;&#23427;&#22312;&#26657;&#20934;&#21644;&#40065;&#26834;&#24615;&#19978;&#25552;&#20379;&#20102;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.01378</link><description>&lt;p&gt;
Vision-Language Foundation Models&#20316;&#20026;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#27169;&#20223;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#23427;&#20204;&#29702;&#35299;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#25805;&#20316;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#35270;&#35273;&#35821;&#35328;&#25805;&#20316;&#26694;&#26550;&#65292;&#21517;&#20026;RoboFlamingo&#65292;&#23427;&#24314;&#31435;&#22312;&#24320;&#28304;&#30340;VLMs&#65292;OpenFlamingo&#20043;&#19978;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;RoboFlamingo&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;VLMs&#36827;&#34892;&#21333;&#27493;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#20351;&#29992;&#26174;&#24335;&#31574;&#30053;&#22836;&#27169;&#25311;&#39034;&#24207;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#21482;&#22312;&#35821;&#35328;&#26465;&#20214;&#30340;&#25805;&#20316;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#20998;&#35299;&#20026;RoboFlamingo&#25552;&#20379;&#20102;&#22312;&#20302;&#24615;&#33021;&#24179;&#21488;&#19978;&#36827;&#34892;&#24320;&#29615;&#25511;&#21046;&#21644;&#37096;&#32626;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#22312;&#27979;&#35797;&#22522;&#20934;&#19978;&#22823;&#24133;&#36229;&#36807;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RoboFlamingo&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective an
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12290;&#36890;&#36807;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21452;&#27169;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20107;&#25925;&#23545;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#24182;&#30830;&#23450;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.00634</link><description>&lt;p&gt;
&#22522;&#20110;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#30340;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#21452;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline. (arXiv:2311.00634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12290;&#36890;&#36807;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21452;&#27169;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20107;&#25925;&#23545;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#24182;&#30830;&#23450;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20107;&#20214;&#30340;&#38543;&#26426;&#24615;&#65292;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20934;&#30830;&#30340;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#21487;&#20197;&#20026;&#36890;&#21220;&#32773;&#36873;&#25321;&#26368;&#20339;&#36335;&#32447;&#21644;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#35299;&#20915;&#38750;&#32463;&#24120;&#24615;&#25317;&#22581;&#38382;&#39064;&#24102;&#26469;&#24040;&#22823;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#20174;&#20132;&#36890;&#20107;&#25925;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#20102;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12289;&#36947;&#36335;&#29366;&#20917;&#21644;&#27668;&#35937;&#25968;&#25454;&#65292;&#20197;&#26816;&#26597;&#22312;&#27809;&#26377;&#20107;&#25925;&#19978;&#19979;&#25991;&#20449;&#24687;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#20005;&#37325;&#24615;&#21644;&#25991;&#26412;&#25551;&#36848;&#65289;&#30340;&#24773;&#20917;&#19979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#31649;&#36947;&#30340;&#21487;&#34892;&#24615;&#12290;&#37319;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#36947;&#36335;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#28982;&#21518;&#21033;&#29992;&#21452;&#27169;&#24335;&#26041;&#27861;&#30830;&#23450;&#20107;&#25925;&#24433;&#21709;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20108;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21487;&#20197;&#20197;83%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#30701;&#26399;&#21644;&#38271;&#26399;&#24433;&#21709;&#65292;&#32780;LightGBM&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00447</link><description>&lt;p&gt;
&#20851;&#20110;&#32511;&#33394;&#35745;&#31639;&#30340;&#26426;&#36935;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities of Green Computing: A Survey. (arXiv:2311.00447v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#25216;&#26415;&#21644;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#35821;&#38899;&#21512;&#25104;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#20851;&#27880;&#20110;&#36861;&#27714;&#26032;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#32467;&#26524;&#65292;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#23545;&#20110;&#39640;&#35745;&#31639;&#33021;&#21147;&#30340;&#38656;&#27714;&#23548;&#33268;&#26356;&#39640;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#36890;&#36807;&#38459;&#27490;&#36164;&#37329;&#26377;&#38480;&#30340;&#23567;&#22411;&#25110;&#20013;&#22411;&#30740;&#31350;&#26426;&#26500;&#21644;&#20844;&#21496;&#21442;&#19982;&#30740;&#31350;&#26469;&#30772;&#22351;&#30740;&#31350;&#20844;&#24179;&#24615;&#12290;&#20026;&#24212;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#65292;&#32511;&#33394;&#35745;&#31639;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#32511;&#33394;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;G&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has achieved significant advancements in technology and research with the development over several decades, and is widely used in many areas including computing vision, natural language processing, time-series analysis, speech synthesis, etc. During the age of deep learning, especially with the arise of Large Language Models, a large majority of researchers' attention is paid on pursuing new state-of-the-art (SOTA) results, resulting in ever increasing of model size and computational complexity. The needs for high computing power brings higher carbon emission and undermines research fairness by preventing small or medium-sized research institutions and companies with limited funding in participating in research. To tackle the challenges of computing resources and environmental impact of AI, Green Computing has become a hot research topic. In this survey, we give a systematic overview of the technologies used in Green Computing. We propose the framework of G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.20381</link><description>&lt;p&gt;
GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#19981;&#21516;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#20010;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#22522;&#20934;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#32473;&#20986;&#32467;&#26500;&#33391;&#22909;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4V&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#25581;&#31034;&#20102;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;(&#22914;CIDEr)&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21307;&#23398;VQA&#39046;&#22495;&#65292;GPT-4V&#22312;&#21306;&#20998;&#38382;&#39064;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#65292;&#20294;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#19981;&#21450;&#29616;&#26377;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#24120;&#35268;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#20998;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#21628;&#21505;&#24320;&#21457;&#26356;&#22909;&#30340;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.20327</link><description>&lt;p&gt;
&#20174;&#32858;&#31867;&#35270;&#35282;&#25913;&#36827;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;EBTTA&#26041;&#27861;&#26469;&#35828;&#65292;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#20026;&#20854;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#30340;&#36739;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#39046;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23436;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#36935;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#29109;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;EBTTA&#65289;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#26368;&#23567;&#21270;&#39044;&#27979;&#30340;&#29109;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32858;&#31867;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;EBTTA&#30340;&#26032;&#35270;&#35282;&#21644;&#35299;&#37322;&#12290;&#36825;&#26159;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65306;1&#65289;&#22312;&#20998;&#37197;&#27493;&#39588;&#20013;&#65292;EBTTA&#27169;&#22411;&#30340;&#21069;&#21521;&#36807;&#31243;&#26159;&#20026;&#36825;&#20123;&#27979;&#35797;&#26679;&#26412;&#20998;&#37197;&#26631;&#31614;&#65307;2&#65289;&#22312;&#26356;&#26032;&#27493;&#39588;&#20013;&#65292;&#21453;&#21521;&#36807;&#31243;&#26159;&#36890;&#36807;&#24050;&#20998;&#37197;&#30340;&#26679;&#26412;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;EBTTA&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#29109;&#25439;&#22833;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#26368;&#22823;&#30340;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#35299;&#37322;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;EBTTA&#26041;&#27861;&#22312;&#32858;&#31867;&#20219;&#21153;&#19978;&#27604;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20195</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#19979;&#29983;&#25104;&#24310;&#32493;
&lt;/p&gt;
&lt;p&gt;
Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24815;&#29992;&#25110;&#23383;&#38754;&#22810;&#35789;&#34920;&#36798;&#26159;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#21253;&#21547;&#24815;&#29992;&#65288;&#25110;&#23383;&#38754;&#65289;&#34920;&#36798;&#30340;&#21465;&#36848;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24310;&#32493;&#30340;&#20219;&#21153;&#21487;&#20197;&#35753;&#25105;&#20204;&#27979;&#35797;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29702;&#35299;&#38750;&#32452;&#21512;&#24615;&#27604;&#21947;&#25991;&#26412;&#30340;&#32420;&#32454;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#19979;&#65288;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65289;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#23383;&#38754;&#19978;&#19979;&#25991;&#30340;&#24310;&#32493;&#26102;&#30053;&#20248;&#20110;&#24815;&#29992;&#19978;&#19979;&#25991;&#65292;&#20294;&#24046;&#36317;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;&#27169;&#22411;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#29983;&#25104;&#27169;&#22411;&#22312;&#25191;&#34892;&#27492;&#20219;&#21153;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
&lt;/p&gt;</description></item><item><title>BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19975</link><description>&lt;p&gt;
BioInstruct:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19975
&lt;/p&gt;
&lt;p&gt;
BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21482;&#21457;&#34920;&#20102;&#24456;&#23569;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;25,000&#20010;&#31034;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#20010;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#20197;GPT-4&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#20248;&#21270;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA LLMs (1&amp;2,7B&amp;13B)&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25351;&#20196;&#22914;&#20309;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24310;&#36831;&#21453;&#39304;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18919</link><description>&lt;p&gt;
&#24310;&#36831;&#21453;&#39304;&#30340;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24310;&#36831;&#21453;&#39304;&#23545;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#20989;&#25968;&#36924;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#39640;&#25928;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21363;&#26102;&#21453;&#39304;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#25361;&#25112;&#65292;&#39318;&#20808;&#20171;&#32461;&#20102;Delayed-PSVI&#31639;&#27861;&#65292;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#20013;&#30340;&#22122;&#22768;&#25200;&#21160;&#26377;&#25928;&#22320;&#25506;&#32034;&#20215;&#20540;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24310;&#36831;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#20013;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#30340;&#39318;&#27425;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.15177</link><description>&lt;p&gt;
&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization. (arXiv:2310.15177v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30340;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#26085;&#30410;&#27969;&#34892;&#65292;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#65292;&#25104;&#20026;&#20102;&#25152;&#35859;"&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;"&#30340;&#28909;&#38376;&#20195;&#34920;&#12290;&#38500;&#20102;&#32473;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#20043;&#22806;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#36824;&#32473;&#35748;&#30693;&#31185;&#23398;&#25552;&#20986;&#20102;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#26088;&#22312;&#25581;&#31034;&#26500;&#25104;&#24605;&#32500;&#21644;&#22823;&#33041;&#36807;&#31243;&#30340;&#26412;&#36136;&#65292;&#20197;&#21450;&#29702;&#35299;&#36825;&#31181;&#21151;&#33021;&#22914;&#20309;&#22312;&#29983;&#29289;&#65288;&#25110;&#20154;&#24037;&#65289;&#22522;&#36136;&#20013;&#33719;&#21462;&#21644;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#38271;&#26399;&#36884;&#24452;&#22312;&#20110;&#35774;&#35745;&#35748;&#30693;&#26550;&#26500;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#20256;&#32479;&#65292;&#22522;&#26412;&#19978;&#26159;&#26681;&#25454;&#31070;&#32463;&#27169;&#20223;&#30340;&#29983;&#25104;&#24615;&#26500;&#24314;&#27169;&#22359;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;COGitive NEural GENerative&#31995;&#32479;&#65292;&#23427;&#20197;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, large neural generative models, capable of synthesizing intricate sequences of words or producing complex image patterns, have recently emerged as a popular representation of what has come to be known as "generative artificial intelligence" (generative AI). Beyond opening the door to new opportunities as well as challenges for the domain of statistical machine learning, the rising popularity of generative AI brings with it interesting questions for Cognitive Science, which seeks to discover the nature of the processes that underpin minds and brains as well as to understand how such functionality might be acquired and instantiated in biological (or artificial) substrate. With this goal in mind, we argue that a promising long-term pathway lies in the crafting of cognitive architectures, a long-standing tradition of the field, cast fundamentally in terms of neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive Neural GENerative system, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#35780;&#20272;&#30340;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#35758;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#35780;&#20272;&#20154;&#21592;&#20043;&#38388;&#24314;&#31435;&#32852;&#21512;&#20307;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#20154;&#21592;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#31561;&#21327;&#35843;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.14455</link><description>&lt;p&gt;
&#19968;&#20010;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35780;&#20272;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI. (arXiv:2310.14455v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#35780;&#20272;&#30340;&#22269;&#38469;&#21512;&#20316;&#26426;&#26500;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#35758;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#35780;&#20272;&#20154;&#21592;&#20043;&#38388;&#24314;&#31435;&#32852;&#21512;&#20307;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#20154;&#21592;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#31561;&#21327;&#35843;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#21644;&#30417;&#31649;&#26041;&#26696;&#30340;&#21019;&#24314;&#21644;&#23454;&#26045;&#24212;&#35813;&#20248;&#20808;&#32771;&#34385;&#24182;&#36827;&#34892;&#22823;&#37327;&#25237;&#36164;&#12290;&#28982;&#32780;&#65292;&#29616;&#29366;&#26159;&#38590;&#20197;&#32500;&#25345;&#30340;&#65292;&#32780;&#19988;&#21361;&#38505;&#12290;&#30417;&#31649;&#32570;&#21475;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#21487;&#20197;&#22312;&#24456;&#23569;&#30417;&#30563;&#19979;&#36827;&#34892;&#30740;&#31350;&#12289;&#24320;&#21457;&#21644;&#37096;&#32626;&#27963;&#21160;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35780;&#20272;&#20316;&#20026;&#35780;&#20272;&#21069;&#27839;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#35780;&#20272;&#29983;&#24577;&#31995;&#32479;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#21327;&#35843;&#25361;&#25112;&#65292;&#20363;&#22914;&#35780;&#20272;&#20154;&#21592;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12289;&#21162;&#21147;&#20998;&#37197;&#19981;&#29702;&#24819;&#21644;&#28608;&#21169;&#26426;&#21046;&#39072;&#20498;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#30001;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#21644;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#20154;&#21592;&#32452;&#25104;&#30340;&#22269;&#38469;&#32852;&#21512;&#20307;&#26469;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#32852;&#21512;&#20307;&#21487;&#20197;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given rapid progress toward advanced AI and risks from frontier AI systems (advanced AI systems pushing the boundaries of the AI capabilities frontier), the creation and implementation of AI governance and regulatory schemes deserves prioritization and substantial investment. However, the status quo is untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to conduct research, development, and deployment activities with minimal oversight. In response, frontier AI system evaluations have been proposed as a way of assessing risks from the development and deployment of frontier AI systems. Yet, the budding AI risk evaluation ecosystem faces significant coordination challenges, such as a limited diversity of evaluators, suboptimal allocation of effort, and perverse incentives. This paper proposes a solution in the form of an international consortium for AI risk evaluations, comprising both AI developers and third-party AI risk evaluators. Such a consortium could play a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14017</link><description>&lt;p&gt;
&#20840;&#38754;&#23545;&#27604;&#65306;&#38754;&#21521;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20943;&#23569;&#20102;&#23545;&#21171;&#21160;&#23494;&#38598;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#25968;&#25454;&#23618;&#38754;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMET&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20013;&#25152;&#26377;&#20869;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#25429;&#25417;&#20102;&#26469;&#33258;&#22235;&#20010;&#28508;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#65306;&#35266;&#27979;&#12289;&#26679;&#26412;&#12289;&#35797;&#39564;&#21644;&#24739;&#32773;&#23618;&#32423;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#21040;&#20445;&#25345;&#20840;&#38754;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#23454;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#26368;&#22823;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#31435;&#24739;&#32773;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23558;COMET&#19982;&#20845;&#20010;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#65292;&#20026;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.04755</link><description>&lt;p&gt;
Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Pairwise GUI Dataset Construction Between Android Phones and Tablets. (arXiv:2310.04755v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#65292;&#20026;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#26222;&#21450;&#24615;&#25163;&#26426;&#21644;&#24179;&#26495;&#30340;&#29615;&#22659;&#20013;&#65292;&#24212;&#29992;&#31243;&#24207;&#32463;&#24120;&#23384;&#22312;&#20110;&#20004;&#20010;&#24179;&#21488;&#19978;&#12290;&#34429;&#28982;&#24212;&#29992;&#31243;&#24207;&#22312;&#25163;&#26426;&#21644;&#24179;&#26495;&#19978;&#20849;&#20139;&#22823;&#37096;&#20998;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#21151;&#33021;&#65292;&#20294;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#20026;&#24179;&#26495;&#29256;&#26412;&#37325;&#26032;&#26500;&#24314;&#65292;&#22686;&#21152;&#20102;&#24320;&#21457;&#25104;&#26412;&#24182;&#28010;&#36153;&#20102;&#29616;&#26377;&#30340;&#35774;&#35745;&#36164;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#27491;&#23581;&#35797;&#25910;&#38598;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#20013;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#30446;&#21069;&#23384;&#22312;&#19968;&#20123;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25163;&#26426;GUI&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#20294;&#27809;&#26377;&#20851;&#20110;&#25163;&#26426;&#21644;&#24179;&#26495;&#20043;&#38388;&#25104;&#23545;GUI&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#23545;&#20110;&#22312;&#33258;&#21160;&#21270;GUI&#24320;&#21457;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Papt&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;Android&#25163;&#26426;&#21644;&#24179;&#26495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;&#25104;&#23545;GUI&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;5593&#20010;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#23545;&#30340;10035&#20010;&#25163;&#26426;-&#24179;&#26495;GUI&#39029;&#38754;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25104;&#23545;GUI&#25910;&#38598;&#26041;&#27861;&#26469;&#26500;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms. Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers' productivity. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02066</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;Transformer&#36827;&#34892;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#26469;&#35299;&#20915;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#29983;&#25104;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#30446;&#26631;&#23646;&#24615;&#12290;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#21644;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#36890;&#36807;&#32852;&#21512;Transformer&#29983;&#25104;&#30340;&#26032;&#39062;&#20998;&#23376;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#38656;&#35201;&#21516;&#26102;&#29983;&#25104;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39062;&#20998;&#23376;&#24182;&#39044;&#27979;&#20854;&#30446;&#26631;&#23646;&#24615;&#65292;&#36825;&#23545;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#33392;&#24040;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;Transformer&#65292;&#23427;&#23558;Transformer&#30340;&#35299;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;&#21644;&#39044;&#27979;&#22120;&#32467;&#21512;&#20026;&#19968;&#20010;&#20855;&#26377;&#20849;&#20139;&#26435;&#37325;&#30340;&#32852;&#21512;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24809;&#32602;&#23545;&#25968;&#20284;&#28982;&#30446;&#26631;&#26469;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#20165;&#24494;&#35843;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#38477;&#20302;&#20102;&#26032;&#37319;&#26679;&#20998;&#23376;&#30340;&#39044;&#27979;&#35823;&#24046;42%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32852;&#21512;Transformer&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#30446;&#26631;&#23646;&#24615;&#30340;&#26032;&#39062;&#20998;&#23376;&#65292;&#30456;&#27604;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;SMILES&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.01775</link><description>&lt;p&gt;
STAMP&#65306;&#36890;&#36807;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#21487;&#24494;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01775
&lt;/p&gt;
&lt;p&gt;
STAMP&#26159;&#19968;&#31181;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#20351;&#29992;&#24037;&#20855;&#25110;&#35013;&#37197;&#38646;&#20214;&#65292;&#24448;&#24448;&#38656;&#35201;&#31526;&#21495;&#21644;&#20960;&#20309;&#25512;&#29702;&#12290;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#39640;&#32423;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#26641;&#25628;&#32034;&#24182;&#26816;&#26597;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#21487;&#34892;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#65292;&#22240;&#20026;&#20854;&#26102;&#38388;&#22797;&#26434;&#24615;&#38543;&#21487;&#33021;&#21160;&#20316;&#21644;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#33021;&#25214;&#21040;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#21487;&#34892;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stein&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;STAMP&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#24182;&#34892;&#21270;&#21644;&#21487;&#24494;&#20223;&#30495;&#26469;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#35745;&#21010;&#12290;STAMP&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;TAMP&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#35299;&#20915;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#19968;&#31181;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17425</link><description>&lt;p&gt;
&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35757;&#32451;&#38598;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#24182;&#20026;&#35821;&#35328;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#37319;&#38598;&#20173;&#28982;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#65292;&#20294;&#25968;&#25454;&#31574;&#21010;&#24448;&#24448;&#20173;&#28982;&#26159;&#20020;&#26102;&#30340;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#22823;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#27492;&#20505;&#36873;&#27744;&#31579;&#36873;&#21040;&#23454;&#38469;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65288;DFN&#65289;&#29992;&#20110;&#31579;&#36873;&#22823;&#22411;&#26410;&#31574;&#21010;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#29992;&#20110;&#31579;&#36873;&#30340;&#32593;&#32476;&#30340;&#36136;&#37327;&#19982;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#65306;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;ImageNet&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27604;&#19968;&#20010;&#22312;ImageNet&#19978;&#20934;&#30830;&#29575;&#36739;&#20302;&#20294;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24046;&#30340;&#35757;&#32451;&#38598;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#36807;&#28388;&#32593;&#32476;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#25968;&#25454;&#38598;DFN-5B&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16223</link><description>&lt;p&gt;
GInX-Eval: &#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#20102;&#31361;&#20986;&#22270;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#26377;&#36129;&#29486;&#30340;&#36793;&#21644;&#33410;&#28857;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20174;&#20154;&#31867;&#25110;&#27169;&#22411;&#30340;&#35282;&#24230;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#12290;&#24403;&#21069;&#35780;&#20272;&#36807;&#31243;&#20013;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#29942;&#39048;&#38382;&#39064;&#26159;&#35299;&#37322;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#27969;&#34892;&#30340;&#24544;&#23454;&#24230;&#25110;&#20445;&#30495;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GInX-Eval&#65288;&#22270;&#20869;&#20998;&#24067;&#35299;&#37322;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#35299;&#37322;&#30340;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#24544;&#23454;&#24230;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35299;&#37322;&#26041;&#27861;&#30340;&#26032;&#35265;&#35299;&#12290;&#20351;&#29992;&#37325;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;GInX&#24471;&#20998;&#21487;&#34913;&#37327;&#24050;&#31227;&#38500;&#36793;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#20197;&#21450;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
&lt;/p&gt;</description></item><item><title>RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10160</link><description>&lt;p&gt;
RadOnc-GPT&#65306;&#19968;&#31181;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT: A Large Language Model for Radiation Oncology. (arXiv:2309.10160v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10160
&lt;/p&gt;
&lt;p&gt;
RadOnc-GPT&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#29983;&#25104;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#30103;&#27861;&#21644;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;&#31561;&#20851;&#38190;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RadOnc-GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#35843;&#25972;&#26041;&#27861;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#32959;&#30244;&#23398;&#12290;RadOnc-GPT&#22312;Mayo Clinic&#30340;&#22823;&#37327;&#25918;&#23556;&#32959;&#30244;&#23398;&#24739;&#32773;&#35760;&#24405;&#21644;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#20219;&#21153;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21253;&#25324;&#29983;&#25104;&#25918;&#23556;&#27835;&#30103;&#26041;&#26696;&#12289;&#30830;&#23450;&#26368;&#20339;&#25918;&#23556;&#30103;&#27861;&#20197;&#21450;&#22522;&#20110;&#24739;&#32773;&#35786;&#26029;&#32454;&#33410;&#25552;&#20379;&#35786;&#26029;&#25551;&#36848;/ICD&#20195;&#30721;&#12290;&#36890;&#36807;&#23558;&#25918;&#23556;&#32959;&#30244;&#23398;&#21307;&#29983;&#27604;&#36739;RadOnc-GPT&#30340;&#21360;&#35937;&#19982;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#34920;&#26126;RadOnc-GPT&#29983;&#25104;&#30340;&#36755;&#20986;&#22312;&#28165;&#26224;&#24230;&#12289;&#29305;&#24322;&#24230;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#20687;RadOnc-GPT&#36825;&#26679;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#31561;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#21307;&#30103;&#39046;&#22495;&#23454;&#29616;&#21464;&#38761;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic. The model employs instruction tuning on three key tasks generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by having radiation oncologists compare RadOnc-GPT impressions to general large language model impressions showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08138</link><description>&lt;p&gt;
&#23547;&#25214;&#20320;&#24819;&#35201;&#30340;&#65306;&#23398;&#20064;&#23454;&#29616;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation. (arXiv:2309.08138v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#35937;&#23548;&#33322;&#65288;VON&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22330;&#26223;&#20013;&#23450;&#20301;&#29305;&#23450;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#25104;&#21151;&#23436;&#25104;VON&#20219;&#21153;&#65292;&#24517;&#39035;&#28385;&#36275;&#20004;&#20010;&#22522;&#26412;&#26465;&#20214;&#65306;1&#65289;&#29992;&#25143;&#24517;&#39035;&#30693;&#36947;&#25152;&#38656;&#23545;&#35937;&#30340;&#21517;&#31216;&#65307;2&#65289;&#29992;&#25143;&#25351;&#23450;&#30340;&#23545;&#35937;&#24517;&#39035;&#30830;&#23454;&#23384;&#22312;&#20110;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#65292;&#27169;&#25311;&#22120;&#21487;&#20197;&#23558;&#39044;&#23450;&#20041;&#30340;&#23545;&#35937;&#21517;&#31216;&#21644;&#20301;&#32622;&#32435;&#20837;&#22330;&#26223;&#30340;&#20803;&#25968;&#25454;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#24448;&#24448;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#38476;&#29983;&#30340;&#29615;&#22659;&#20013;&#65292;&#20154;&#20204;&#21487;&#33021;&#19981;&#30693;&#36947;&#22330;&#26223;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#35937;&#65292;&#25110;&#32773;&#20182;&#20204;&#21487;&#33021;&#38169;&#35823;&#22320;&#25351;&#23450;&#19968;&#20010;&#23454;&#38469;&#19978;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#20154;&#20204;&#20173;&#28982;&#21487;&#33021;&#23545;&#19968;&#20010;&#23545;&#35937;&#26377;&#38656;&#27714;&#65292;&#36825;&#20010;&#38656;&#27714;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#20854;&#20182;&#23545;&#35937;&#20197;&#31561;&#25928;&#30340;&#26041;&#24335;&#26469;&#28385;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#65288;DDN&#65289;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user'
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.06038</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#25235;&#21462;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#26088;&#22312;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#24110;&#21161;&#29992;&#25143;&#25235;&#21462;&#29289;&#20307;&#30340;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#28789;&#24039;&#25235;&#21462;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#30528;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#31574;&#30053;&#38656;&#35201;&#36866;&#24212;&#19981;&#21516;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#30001;&#20004;&#20010;&#23376;&#27169;&#22359;&#32452;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#19968;&#31181;&#25163;-&#29289;&#20307;&#26465;&#20214;&#25235;&#21462;&#21407;&#35821;&#31216;&#20026;Grasping Gradient Field&#65288;GraspGF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#12290;GraspGF&#36890;&#36807;&#20272;&#35745;&#26469;&#33258;&#25104;&#21151;&#25235;&#21462;&#31034;&#20363;&#38598;&#30340;&#26799;&#24230;&#26469;&#23398;&#20064;&#8220;&#22914;&#20309;&#8221;&#25235;&#21462;&#65292;&#32780;&#27531;&#24046;&#31574;&#30053;&#26681;&#25454;&#36712;&#36857;&#21382;&#21490;&#30830;&#23450;&#8220;&#20309;&#26102;&#8221;&#21644;&#20197;&#20309;&#31181;&#36895;&#24230;&#25191;&#34892;&#25235;&#21462;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field~(GraspGF), and a history-conditional residual policy. GraspGF learns `how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines `when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demons
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20195;&#29702;&#30340;&#23398;&#20064;&#21151;&#33021;&#12290;&#23427;&#21253;&#21547;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#22312;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.12486</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Model of Sequential Learning based on Non-Axiomatic Logic. (arXiv:2308.12486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20195;&#29702;&#30340;&#23398;&#20064;&#21151;&#33021;&#12290;&#23427;&#21253;&#21547;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#22312;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#23398;&#20064;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#12290;&#23398;&#20064;&#36807;&#31243;&#21253;&#25324;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#24403;&#21069;&#35774;&#35745;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential learning is a fundamental function of an intelligent agent. This technical report introduces a model of sequential learning, which is interpretable through Non-Axiomatic Logic. The learning procedure includes three steps, hypothesizing, revising, and recycling, and can work under the Assumption of Insufficient Knowledge and Resources. Although there are limitations for the current design, the model has been proven effective in some simple cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09544</link><description>&lt;p&gt;
&#36866;&#24212;&#24744;&#30340;&#25945;&#24072;: &#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#25945;&#24072;&#30340;&#26041;&#27861;&#65288;TA&#65289;&#29992;&#20110;&#26080;&#26679;&#26412;&#36830;&#32493;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#26679;&#26412;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#26088;&#22312;&#38450;&#27490;&#36951;&#24536;&#12290;KD&#26041;&#27861;&#22312;CIL&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#24456;&#38590;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#26174;&#33879;&#34920;&#31034;&#36716;&#25442;&#12290;&#36825;&#23548;&#33268;KD&#25439;&#22833;&#25104;&#20998;&#20013;&#20986;&#29616;&#36739;&#22823;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;CIL&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#24072;&#36866;&#24212;&#65288;TA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#26356;&#26032;&#25945;&#24072;&#21644;&#20027;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;KD&#30340;CIL&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#26080;&#26679;&#26412;CIL&#22522;&#20934;&#27979;&#35797;&#20013;&#25345;&#32493;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.06197</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#22522;&#26412;&#29305;&#24449;&#36827;&#34892;&#22797;&#26434;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06197
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#39062;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#35748;&#30693;&#20219;&#21153;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20854;&#34920;&#29616;&#22312;&#19982;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#30456;&#31561;&#25110;&#20197;&#19978;&#30340;&#20854;&#20182;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#30340;&#31243;&#24230;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#36739;&#20026;&#22256;&#38590;&#30340;&#12290;&#30001;&#20110;&#20154;&#33080;&#34920;&#36798;&#30340;&#24773;&#32490;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#29305;&#21035;&#22256;&#38590;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#22312;&#36825;&#19968;&#39046;&#22495;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#23427;&#21487;&#33021;&#38656;&#35201;&#23454;&#26102;&#21512;&#25104;&#30693;&#35782;&#24182;&#29702;&#35299;&#26032;&#27010;&#24565;&#12290;&#20154;&#31867;&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#20960;&#20010;&#20363;&#23376;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20174;&#35760;&#24518;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#24182;&#20002;&#24323;&#20854;&#20313;&#20449;&#24687;&#12290;&#21516;&#26679;&#65292;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#30041;&#24050;&#30693;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#23398;&#20064;&#26032;&#31867;&#21035;&#65292;&#32780;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#21644;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20934;&#30830;&#22320;&#35782;&#21035;&#26032;&#30340;&#22797;&#21512;&#34920;&#24773;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.04451</link><description>&lt;p&gt;
AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#28431;&#27934;&#65306;&#25506;&#32034;&#38024;&#23545;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#35780;&#20272;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21363;&#36890;&#36807;&#23558;&#24694;&#24847;&#26679;&#26412;&#27880;&#20837;&#35757;&#32451;&#25968;&#25454;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26469;&#27602;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#25968;&#25454;&#27602;&#21270;&#65292;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#19981;&#20250;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.
&lt;/p&gt;</description></item><item><title>MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.00352</link><description>&lt;p&gt;
MetaGPT: &#20803;&#32534;&#31243;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00352
&lt;/p&gt;
&lt;p&gt;
MetaGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#21327;&#20316;&#20013;&#12290;&#23427;&#37319;&#29992;&#20803;&#32534;&#31243;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#65292;&#24182;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36825;&#31181;&#26694;&#26550;&#21033;&#29992;&#20102;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#65292;&#33258;&#21160;&#20219;&#21153;&#35299;&#20915;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#32570;&#20047;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#25506;&#32034;&#21644;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#24187;&#35273;&#22312;&#22810;&#20010;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#26102;&#34987;&#26080;&#38480;&#25918;&#22823;&#65292;&#23548;&#33268;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#37319;&#29992;&#26377;&#25928;&#30340;&#20154;&#24037;&#24037;&#20316;&#27969;&#20316;&#20026;&#20803;&#32534;&#31243;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaGPT&#39318;&#20808;&#23558;&#26631;&#20934;&#25805;&#20316;&#35268;&#31243;&#65288;SOPs&#65289;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#20419;&#36827;&#32467;&#26500;&#21270;&#21327;&#35843;&#12290;&#28982;&#21518;&#65292;&#23427;&#36827;&#19968;&#27493;&#35201;&#27714;&#27169;&#22359;&#21270;&#36755;&#20986;&#65292;&#36171;&#20104;&#26234;&#33021;&#20307;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#24179;&#34892;&#39564;&#35777;&#36755;&#20986;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MetaGPT&#21033;&#29992;&#27969;&#27700;&#32447;&#24037;&#20316;&#27169;&#24335;&#26469;&#20998;&#37197;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2307.16104</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#20840;&#29699;&#21487;&#38752;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26410;&#32463;&#27979;&#37327;&#27969;&#22495;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#27946;&#27700;&#39044;&#35686;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27700;&#26159;&#26368;&#24120;&#35265;&#21644;&#24433;&#21709;&#26368;&#22823;&#30340;&#33258;&#28982;&#28798;&#23475;&#20043;&#19968;&#65292;&#23545;&#21457;&#23637;&#20013;&#22269;&#23478;&#23588;&#20854;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22269;&#23478;&#24448;&#24448;&#32570;&#20047;&#23494;&#38598;&#30340;&#27700;&#27969;&#30417;&#27979;&#32593;&#32476;&#12290;&#20934;&#30830;&#21450;&#26102;&#30340;&#39044;&#35686;&#23545;&#20110;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20934;&#30830;&#30340;&#27700;&#25991;&#27169;&#25311;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26681;&#25454;&#27599;&#20010;&#24212;&#29992;&#30340;&#27969;&#22495;&#20013;&#30340;&#38271;&#26102;&#38388;&#25968;&#25454;&#35760;&#24405;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;7&#22825;&#20869;&#30340;&#26497;&#31471;&#27700;&#25991;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#22823;&#27954;&#12289;&#21069;&#23548;&#26102;&#38388;&#21644;&#37325;&#29616;&#26399;&#20013;&#22343;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20840;&#29699;&#27700;&#25991;&#27169;&#22411;&#65288;Copernicus&#24212;&#24613;&#31649;&#29702;&#26381;&#21153;&#20840;&#29699;&#27946;&#27700;&#24847;&#35782;&#31995;&#32479;&#65289;&#12290;AI&#22312;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#20013;&#30340;&#39044;&#27979;&#23588;&#20854;&#26377;&#25928;&#65292;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20840;&#29699;&#21482;&#26377;&#30334;&#20998;&#20043;&#20960;&#30340;&#27969;&#22495;&#20855;&#26377;&#27969;&#37327;&#35266;&#27979;&#31449;&#65292;&#32780;&#21457;&#23637;&#20013;&#22269;&#23478;&#30340;&#26410;&#32463;&#27979;&#37327;&#30340;&#27969;&#22495;&#25968;&#37327;&#21344;&#27604;&#24456;&#39640;&#65292;&#23545;&#20154;&#31867;&#29305;&#21035;&#33030;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Floods are one of the most common and impactful natural disasters, with a disproportionate impact in developing countries that often lack dense streamflow monitoring networks. Accurate and timely warnings are critical for mitigating flood risks, but accurate hydrological simulation models typically must be calibrated to long data records in each watershed where they are applied. We developed an Artificial Intelligence (AI) model to predict extreme hydrological events at timescales up to 7 days in advance. This model significantly outperforms current state of the art global hydrology models (the Copernicus Emergency Management Service Global Flood Awareness System) across all continents, lead times, and return periods. AI is especially effective at forecasting in ungauged basins, which is important because only a few percent of the world's watersheds have stream gauges, with a disproportionate number of ungauged basins in developing countries that are especially vulnerable to the human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06870</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38271;&#26102;&#38388;&#20869;&#37096;&#32626;&#22312;&#23478;&#20013;&#30340;&#26426;&#22120;&#20154;&#38754;&#20020;&#30528;&#30495;&#27491;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#12290;&#20316;&#20026;&#26426;&#22120;&#20154;&#23547;&#27714;&#20026;&#29992;&#25143;&#25552;&#20379;&#24110;&#21161;&#65292;&#23427;&#24212;&#35813;&#21033;&#29992;&#20219;&#20309;&#31215;&#32047;&#30340;&#32463;&#39564;&#26469;&#25913;&#36827;&#33258;&#24049;&#30340;&#30693;&#35782;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#29087;&#32451;&#30340;&#21161;&#25163;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#36825;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#20026;&#35268;&#21010;&#22120;&#29983;&#25104;&#20505;&#36873;&#36830;&#32493;&#21442;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#25968;&#25454;&#22914;&#20309;&#22312;&#20219;&#21153;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#20915;&#23450;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#22312;&#32447;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65292;&#36825;&#20123;&#20219;&#21153;&#20316;&#20026;&#27599;&#20010;&#27169;&#22411;&#23545;&#29366;&#24577;&#30340;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15136</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#30495;&#27491;&#37325;&#35201;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15136
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#22312;&#30830;&#20445;&#23433;&#20840;&#21644;&#20419;&#36827;&#24179;&#31283;&#23548;&#33322;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#22120;&#20934;&#30830;&#24615;&#19982;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#39550;&#39542;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#24403;&#21069;&#36712;&#36857;&#39044;&#27979;&#35780;&#20272;&#21327;&#35758;&#20013;&#24573;&#35270;&#20102;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#25968;&#25454;&#38598;&#19982;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65307;2&#65289;&#39044;&#27979;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#31639;&#27861;&#24433;&#21709;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#65292;&#36827;&#32780;&#25913;&#21464;&#36947;&#36335;&#19978;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#20114;&#21160;&#20135;&#29983;&#20102;&#38024;&#23545;&#39044;&#27979;&#22120;&#30340;&#29305;&#23450;&#21160;&#21147;&#23398;&#65292;&#30452;&#25509;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#21453;&#24212;&#22312;&#25968;&#25454;&#38598;&#19978;&#26159;&#39044;&#20808;&#30830;&#23450;&#30340;&#65292;&#22240;&#27492;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#26080;&#27861;&#28385;&#36275;&#23545;&#39044;&#27979;&#22120;&#21160;&#24577;&#34892;&#20026;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the autonomous driving system, trajectory prediction plays a vital role in ensuring safety and facilitating smooth navigation. However, we observe a substantial discrepancy between the accuracy of predictors on fixed datasets and their driving performance when used in downstream tasks. This discrepancy arises from two overlooked factors in the current evaluation protocols of trajectory prediction: 1) the dynamics gap between the dataset and real driving scenario; and 2) the computational efficiency of predictors. In real-world scenarios, prediction algorithms influence the behavior of autonomous vehicles, which, in turn, alter the behaviors of other agents on the road. This interaction results in predictor-specific dynamics that directly impact prediction results. As other agents' responses are predetermined on datasets, a significant dynamics gap arises between evaluations conducted on fixed datasets and actual driving scenarios. Furthermore, focusing solely on accuracy fails to ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.14111</link><description>&lt;p&gt;
RLHF&#26159;&#21542;&#27604;&#26631;&#20934;RL&#26356;&#22256;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#20174;&#20559;&#22909;&#20449;&#21495;&#23398;&#20064;&#65292;&#32780;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#30452;&#25509;&#20174;&#22870;&#21169;&#20449;&#21495;&#23398;&#20064;&#12290;&#20559;&#22909;&#20449;&#21495;&#21487;&#33021;&#21253;&#21547;&#30340;&#20449;&#24687;&#27604;&#22870;&#21169;&#20449;&#21495;&#23569;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22870;&#21169;&#27010;&#29575;&#27169;&#22411;&#30340;&#20559;&#22909;&#65292;&#27492;&#26102;&#21487;&#20197;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23481;&#24525;&#22870;&#21169;&#23567;&#35823;&#24046;&#30340;&#40065;&#26834;&#22870;&#21169;RL&#38382;&#39064;&#65307;&#65288;2&#65289;&#23545;&#20110;&#19968;&#33324;&#30340;&#20219;&#24847;&#20559;&#22909;&#19988;&#30446;&#26631;&#26159;&#25214;&#21040;von Neumann&#33719;&#32988;&#32773;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#22810;&#26234;&#33021;&#20307;&#22870;&#21169;RL&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#19979;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22240;&#23376;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#21518;&#19968;&#31181;&#24773;&#20917;&#21487;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#25104;&#23545;&#20851;&#31995;&#30340;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#20013;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11739</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#19982;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior. (arXiv:2306.11739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#29366;&#20808;&#39564;&#30340;&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#20013;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#29289;&#20307;&#37325;&#24314;&#22312;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#12289;&#36974;&#25377;&#21644;&#22122;&#22768;&#31561;&#38382;&#39064;&#65292;&#20174;&#21333;ocular&#22270;&#20687;&#30452;&#25509;&#37325;&#24314;&#35814;&#32454;&#30340;&#19977;&#32500;&#24418;&#29366;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#29983;&#25104;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#24847;&#35782;&#21040;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#23545;&#35937;&#34920;&#31034;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#34920;&#31034;&#20174;&#22823;&#22411;3D&#23545;&#35937;&#27169;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#20998;&#24067;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#22312;&#34920;&#31034;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#32534;&#30721;&#22120;&#65292;&#30452;&#25509;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#28508;&#22312;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#23558;&#28508;&#22312;&#20195;&#30721;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;SDF&#20540;&#20013;&#65292;&#24182;&#20026;&#27599;&#20010;&#32593;&#26684;&#32452;&#20214;&#29983;&#25104;&#24102;&#26377;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#30340;&#19977;&#32500;&#23545;&#35937;&#32593;&#26684;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#28176;&#36827;&#34701;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#35282;&#24230;&#23545;&#35937;&#30340;&#28508;&#22312;&#20195;&#30721;&#34701;&#21512;&#25104;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#23436;&#25972;3D&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#35270;&#35282;&#21644;&#22810;&#35270;&#35282;3D&#37325;&#24314;&#22522;&#20934;&#27979;&#35797;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#38752;&#19988;&#23450;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view ob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#65292;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08687</link><description>&lt;p&gt;
&#22522;&#20110;&#33539;&#25968;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#28508;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Norm-guided latent space exploration for text-to-image generation. (arXiv:2306.08687v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35266;&#23519;&#21040;&#24403;&#21069;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#65292;&#36825;&#23545;&#20110;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#21508;&#31181;&#27010;&#24565;&#30340;&#26032;&#26500;&#22270;&#21644;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21021;&#22987;&#31181;&#23376;&#30340;&#28508;&#31354;&#38388;&#20173;&#19981;&#34987;&#24456;&#22909;&#29702;&#35299;&#65292;&#24182;&#19988;&#20854;&#32467;&#26500;&#24050;&#34987;&#35777;&#26126;&#20250;&#24433;&#21709;&#21508;&#31181;&#27010;&#24565;&#30340;&#29983;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#27431;&#20960;&#37324;&#24471;&#25110;&#29699;&#38754;&#24230;&#37327;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25554;&#20540;&#21644;&#23547;&#25214;&#31181;&#23376;&#38598;&#30340;&#36136;&#24515;&#31561;&#31616;&#21333;&#25805;&#20316;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#35266;&#27979;&#21040;&#20855;&#26377;&#29421;&#31364;&#33539;&#20540;&#30340;&#36755;&#20837;&#12290;&#36825;&#23545;&#20110;&#20381;&#36182;&#20110;&#31181;&#23376;&#25805;&#20316;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#21644;&#38271;&#23614;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#31181;&#23376;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#35777;&#26126;&#23427;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#25968;&#30340;&#20808;&#39564;&#30340;&#26032;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models show great potential in synthesizing a large variety of concepts in new compositions and scenarios. However, the latent space of initial seeds is still not well understood and its structure was shown to impact the generation of various concepts. Specifically, simple operations like interpolation and finding the centroid of a set of seeds perform poorly when using standard Euclidean or spherical metrics in the latent space. This paper makes the observation that, in current training procedures, diffusion models observed inputs with a narrow range of norm values. This has strong implications for methods that rely on seed manipulation for image generation, with applications to few-shot and long-tail learning tasks. To address this issue, we propose a novel method for interpolating between two seeds and demonstrate that it defines a new non-Euclidean metric that takes into account a norm-based prior on seeds. We describe a simple yet efficient algorithm for ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2306.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#32479;&#35745;&#23398;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20869;&#37096;&#26159;&#21542;&#21019;&#24314;&#21644;&#20351;&#29992;&#31616;&#21333;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#21457;&#29616;LDM&#20869;&#37096;&#28608;&#27963;&#25552;&#20379;&#20102;&#20851;&#20110;3D&#28145;&#24230;&#25968;&#25454;&#21644;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#20998;&#31163;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#20855;&#26377;&#22240;&#26524;&#20316;&#29992;, &#21487;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#23637;&#29616;&#20102;&#20135;&#29983;&#36924;&#30495;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#31070;&#31192;&#12290;&#21363;&#20351;&#22312;&#27809;&#26377;&#26174;&#24335;&#28145;&#24230;&#20449;&#24687;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#36890;&#24120;&#20063;&#20250;&#36755;&#20986;&#19968;&#33268;&#30340;3D&#22330;&#26223;&#22270;&#29255;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#26412;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65306;LDM&#26159;&#21542;&#21019;&#24314;&#24182;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22330;&#26223;&#20960;&#20309;&#20869;&#37096;&#34920;&#31034;&#65311;&#20351;&#29992;&#32447;&#24615;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;LDM&#30340;&#20869;&#37096;&#28608;&#27963;&#32534;&#30721;&#20102;&#32447;&#24615;&#34920;&#31034;&#65292;&#26082;&#21253;&#25324;3D&#28145;&#24230;&#25968;&#25454;&#65292;&#21448;&#21253;&#25324;&#31361;&#20986;&#23545;&#35937;/&#32972;&#26223;&#21306;&#21035;&#12290;&#36825;&#20123;&#34920;&#31034;&#20284;&#20046;&#22312;&#21435;&#22122;&#36807;&#31243;&#30340;&#26089;&#26399;&#23601;&#20986;&#29616;&#20102;&#8212;&#8212;&#22312;&#20154;&#31867;&#33021;&#36731;&#26131;&#29702;&#35299;&#22024;&#26434;&#30340;&#22270;&#20687;&#20043;&#21069;&#12290;&#24178;&#39044;&#24615;&#23454;&#39564;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22270;&#20687;&#21512;&#25104;&#20013;&#25198;&#28436;&#22240;&#26524;&#20316;&#29992;&#65292;&#24182;&#19988;&#21487;&#33021;&#29992;&#20110;LDM&#36755;&#20986;&#30340;&#31616;&#21333;&#39640;&#32423;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM encode linear representations of both 3D depth data and a salient-object / background distinction. These representations appear surprisingly early in the denoising process$-$well before a human can easily make sense of the noisy images. Intervention experiments further indicate these representations play a causal role in image synthesis, and may be used for simple high-level editing of an LDM's output.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03937</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24341;&#23548;&#26368;&#21518;&#19968;&#23618;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#25311;&#21512;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Fl)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36328;&#22810;&#20010;&#21442;&#19982;&#32773;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#32771;&#34385;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29616;&#26377;FL&#31639;&#27861;&#30340;&#21021;&#22987;&#21270;&#28857;&#30340;&#24433;&#21709;; &#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#38598;&#20013;&#24335;&#23398;&#20064;&#35774;&#32622;&#20013;&#22823;&#37327;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#25991;&#29486;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;FL&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#32452;&#35745;&#31639;&#26426;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20165;&#25311;&#21512;&#32447;&#24615;&#20998;&#31867;&#22836;&#26159;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#26368;&#36817;&#31867;&#22343;&#20540;(NCM)&#25311;&#21512;&#20998;&#31867;&#22120;&#21487;&#20197;&#31934;&#30830;&#22320;&#19988;&#27604;&#29616;&#26377;&#25552;&#35758;&#39640;&#25928;&#22320;&#23436;&#25104;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#33719;&#24471;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging paradigm that allows a model to be trained across a number of participants without sharing data. Recent works have begun to consider the effects of using pre-trained models as an initialization point for existing FL algorithms; however, these approaches ignore the vast body of efficient transfer learning literature from the centralized learning setting. Here we revisit the problem of FL from a pre-trained model considered in prior work and expand it to a set of computer vision transfer learning problems. We first observe that simply fitting a linear classification head can be efficient and effective in many cases. We then show that in the FL setting, fitting a classifier using the Nearest Class Means (NCM) can be done exactly and orders of magnitude more efficiently than existing proposals, while obtaining strong performance. Finally, we demonstrate that using a two-phase approach of obtaining the classifier and then fine-tuning the model can yiel
&lt;/p&gt;</description></item><item><title>SALE&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#65292;TD7&#31639;&#27861;&#24341;&#20837;&#20102;&#35813;&#26041;&#27861;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.02451</link><description>&lt;p&gt;
&#24453;&#21806;&#65306;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
For SALE: State-Action Representation Learning for Deep Reinforcement Learning. (arXiv:2306.02451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02451
&lt;/p&gt;
&lt;p&gt;
SALE&#26159;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#65292;TD7&#31639;&#27861;&#24341;&#20837;&#20102;&#35813;&#26041;&#27861;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#34920;&#31034;&#23398;&#20064;&#26159;&#22788;&#29702;&#22797;&#26434;&#22522;&#20110;&#22270;&#20687;&#20219;&#21153;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#36890;&#24120;&#34987;&#24573;&#30053;&#20102;&#20302;&#32423;&#29366;&#24577;&#65288;&#20363;&#22914;&#29289;&#29702;&#25511;&#21046;&#38382;&#39064;&#65289;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SALE&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#23884;&#20837;&#26469;&#24314;&#27169;&#29366;&#24577;&#21644;&#21160;&#20316;&#20043;&#38388;&#24494;&#22937;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#20302;&#32423;&#29366;&#24577;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#36825;&#20123;&#23884;&#20837;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#24182;&#24378;&#35843;&#20102;&#37325;&#35201;&#30340;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;SALE&#21644;RL&#30340;&#26816;&#26597;&#28857;&#33258;&#36866;&#24212;&#26041;&#27861;&#25972;&#21512;&#21040;TD3&#20013;&#65292;&#24418;&#25104;TD7&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;OpenAI gym&#22522;&#20934;&#20219;&#21153;&#20013;&#65292;TD7&#22312;300k&#21644;5M&#26102;&#38388;&#27493;&#39588;&#19979;&#30340;&#24179;&#22343;&#24615;&#33021;&#22686;&#30410;&#20998;&#21035;&#20026;276.7&#65285;&#21644;50.7&#65285;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of reinforcement learning (RL), representation learning is a proven tool for complex image-based tasks, but is often overlooked for environments with low-level states, such as physical control problems. This paper introduces SALE, a novel approach for learning embeddings that model the nuanced interaction between state and action, enabling effective representation learning from low-level states. We extensively study the design space of these embeddings and highlight important design considerations. We integrate SALE and an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which significantly outperforms existing continuous control algorithms. On OpenAI gym benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over TD3 at 300k and 5M time steps, respectively, and works in both the online and offline settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#21010;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#34920;&#29616;&#26356;&#26377;&#21069;&#36884;&#12290;</title><link>http://arxiv.org/abs/2305.15771</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#8212;&#8212;&#19968;&#39033;&#20851;&#38190;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Planning Abilities of Large Language Models -- A Critical Investigation. (arXiv:2305.15771v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#21010;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#34920;&#29616;&#26356;&#26377;&#21069;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#35268;&#21010;&#20219;&#21153;&#20013;&#20855;&#26377;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#30340;&#22768;&#31216;&#30340;&#21551;&#21457;&#65292;&#26088;&#22312;&#30740;&#31350;&#20854;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;LLMs&#22312;&#33258;&#20027;&#29983;&#25104;&#24120;&#35782;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#35745;&#21010;&#25928;&#26524;&#21644;LLMs&#20316;&#20026;&#20854;&#20182;&#26234;&#33021;&#20307;&#65288;AI&#35268;&#21010;&#32773;&#65289;&#22312;&#20854;&#35268;&#21010;&#20219;&#21153;&#20013;&#21551;&#21457;&#24615;&#25351;&#23548;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#19968;&#22871;&#19982;&#22269;&#38469;&#35745;&#21010;&#31454;&#36187;&#20013;&#20351;&#29992;&#30340;&#30456;&#20284;&#39046;&#22495;&#30340;&#23454;&#20363;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24335;&#19979;&#35780;&#20272;LLMs&#65306;&#33258;&#20027;&#21644;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#30456;&#24403;&#26377;&#38480;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#39046;&#22495;&#20013;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#32422;&#20026;12%&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#26377;&#21069;&#36884;&#30340;&#36857;&#35937;&#12290;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#20197;&#25913;&#21892;&#22522;&#30784;&#35268;&#21010;&#22120;&#30340;&#25628;&#32034;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and addition
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.15269</link><description>&lt;p&gt;
&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35777;&#26126;&#31354;&#38388;&#30340;&#24222;&#22823;&#65292;&#20219;&#20309;&#20855;&#26377;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#29702;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32473;&#23450;&#25512;&#29702;&#38142;&#26465;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26576;&#20123;&#25277;&#35937;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#26159;&#22312;&#20351;&#29992;&#33707;&#24503;&#26031;&#22374;&#26031;&#25110;&#29305;&#23450;&#22823;&#23567;&#30340;&#35777;&#26126;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19988;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#20026;&#20102;&#34913;&#37327;LLM&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#25512;&#29702;&#26356;&#22797;&#26434;&#35777;&#26126;&#30340;&#33021;&#21147;&#65292;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#27867;&#21270;&#12289;&#23485;&#24230;&#27867;&#21270;&#21644;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#20415;&#20110;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#21644;&#21487;&#32534;&#31243;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#28436;&#32462;&#35268;&#21017;&#21644;&#35777;&#26126;&#22797;&#26434;&#24615;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03017</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;BERT&#21644;Query-Aware LSH&#25552;&#39640;&#38750;&#27491;&#24335;&#25991;&#26723;&#20013;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Stack Overflow&#19978;&#30340;Java&#32534;&#31243;&#35821;&#35328;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21644;&#26368;&#36817;&#19968;&#30452;&#22312;&#36827;&#34892;&#20195;&#30721;&#31034;&#20363;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#22312;&#20114;&#32852;&#32593;&#19978;&#23547;&#25214;&#30456;&#20851;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#21033;&#29992;&#24320;&#28304;&#39033;&#30446;&#21644;&#38750;&#27491;&#24335;&#25991;&#26723;&#12290;&#20026;&#20102;&#25214;&#21040;&#26377;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#65292;&#38750;&#27491;&#24335;&#25991;&#26723;&#65288;&#22914;Stack Overflow&#35752;&#35770;&#21644;&#35770;&#22363;&#65289;&#21487;&#20197;&#38750;&#24120;&#23453;&#36149;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;Stack Overflow&#65292;&#23427;&#26159;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35752;&#35770;&#19981;&#21516;&#20027;&#39064;&#30340;&#27969;&#34892;&#36164;&#28304;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#33616;&#20195;&#30721;&#31034;&#20363;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#25512;&#33616;&#20102;Java&#32534;&#31243;&#35821;&#35328;&#20013;&#26368;&#20339;&#30340;&#20195;&#30721;&#31034;&#20363;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERT&#26469;&#36827;&#34892;&#22788;&#29702;&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#20351;&#29992;BERT&#23558;&#20195;&#30721;&#31034;&#20363;&#36716;&#25442;&#20026;&#25968;&#20540;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02305</link><description>&lt;p&gt;
&#26657;&#20934;&#21270;&#35299;&#37322;&#65306;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#21644;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;Calibrated Explanations (CE)&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20449;&#24687;&#65292;&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#21487;&#38752;&#19988;&#24378;&#20581;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#39046;&#22495;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21487;&#33021;&#23548;&#33268;&#28389;&#29992;&#25110;&#19981;&#20351;&#29992;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#21019;&#24314;&#21487;&#20197;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20010;&#21035;&#39044;&#27979;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#20294;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#65292;&#26657;&#20934;&#21270;&#35299;&#37322;(Calibrated Explanations&#65292;CE)&#65292;&#23427;&#22522;&#20110; Venn-Abers&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#30340;&#21516;&#26102;&#26657;&#20934;&#24213;&#23618;&#27169;&#22411;&#12290;CE&#19981;&#20165;&#25552;&#20379;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#21644;&#24378;&#20581;&#30340;&#35299;&#37322;&#65292;&#36824;&#25552;&#20379;&#27010;&#29575;&#20272;&#35745;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20855;&#26377;&#26131;&#20110;&#29702;&#35299;&#30340;&#26465;&#20214;&#35268;&#21017;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become an integral part of decision support systems (DSSs) in various domains, but the lack of transparency in the predictive models used in AI-based DSSs can lead to misuse or disuse. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance, but they suffer from drawbacks such as instability. To address these issues, we propose a new feature importance explanation method, Calibrated Explanations (CE), which is based on Venn-Abers and calibrates the underlying model while generating feature importance explanations. CE provides fast, reliable, stable, and robust explanations, along with uncertainty quantification of the probability estimates and feature importance weights. Furthermore, the method is model agnostic with easily understood conditional rules and can also genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12958</link><description>&lt;p&gt;
&#23545;&#39640;&#27700;&#24179;&#26426;&#22120;&#20154;&#35299;&#37322;&#20013;&#30340;&#22870;&#21169;&#20998;&#35299;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20154;&#31867;&#35299;&#37322;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38590;&#20197;&#29702;&#35299;&#30340;&#33258;&#20307;&#24863;&#29366;&#24577;&#12289;&#22810;&#21464;&#30340;&#20013;&#38388;&#30446;&#26631;&#21644;&#20854;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19968;&#27493;&#35299;&#37322;&#21487;&#33021;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#22312;&#27599;&#20010;&#36716;&#25442;&#26102;&#32771;&#34385;&#21040;&#20195;&#29702;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26144;&#23556;&#21040;&#20219;&#21153;&#29305;&#23450;&#22522;&#20803;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#31227;&#21160;&#23618;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#22870;&#21169;&#20998;&#35299;&#65288;RD&#65289;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#20219;&#21153;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#26126;&#30830;&#30340;&#39640;&#23618;&#27425;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;RD&#35299;&#37322;&#30340;&#36755;&#20986;&#25991;&#29289;&#20013;&#30340;&#21487;&#35270;&#21644;&#25991;&#26412;&#35299;&#37322;&#65292;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09823</link><description>&lt;p&gt;
ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Future of ChatGPT-enabled Labor Market: A Preliminary Study. (arXiv:2304.09823v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#38750;&#20961;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25104;&#21151;&#24182;&#36234;&#26469;&#36234;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;ChatGPT&#26679;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26159;&#21542;&#20250;&#21462;&#20195;&#20154;&#31867;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#32780;&#19981;&#26159;&#23545;&#31435;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#30340;&#21021;&#27493;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20013;&#22269;&#26368;&#22823;&#30340;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;BOSS&#30452;&#32856;&#20013;&#30340;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#32422;&#26377;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#32844;&#19994;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#32844;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.06813</link><description>&lt;p&gt;
&#27169;&#22411;&#29305;&#23450;&#35270;&#35282;&#19979;&#30340;&#32479;&#19968;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#24182;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#27979;&#35797;&#26679;&#20363;&#12290;&#34429;&#28982;&#24050;&#26377;&#22823;&#37327;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#21482;&#20851;&#27880;&#26469;&#33258;&#35821;&#20041;&#36716;&#25442;&#65288;&#22914;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#65289;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21487;&#33021;&#30340;&#21407;&#22240;&#65288;&#22914;&#21327;&#21464;&#37327;&#36716;&#25442;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#30740;&#31350;OOD&#26816;&#27979;&#12290;&#25105;&#20204;&#24314;&#35758;&#19981;&#26159;&#26816;&#27979;&#29305;&#23450;&#21407;&#22240;&#23548;&#33268;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#26159;&#26816;&#27979;&#24050;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#22120;&#65289;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26159;&#21542;&#24212;&#35813;&#26816;&#27979;&#21644;&#25298;&#32477;&#27979;&#35797;&#20363;&#23376;&#26159;&#8220;&#27169;&#22411;&#29305;&#23450;&#8221;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#30001;&#35821;&#20041;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21464;&#21270;&#24341;&#36215;&#30340;OOD&#20363;&#23376;&#30340;&#26816;&#27979;&#65292;&#24182;&#23494;&#20999;&#20851;&#27880;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01246</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#23433;&#20840;&#20998;&#26512;&#65306;&#32842;&#22825;GPT&#22312;STPA&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;BERT&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23545;&#35805;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20855;&#26377;&#35814;&#32454;&#21644;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#27491;&#22312;&#24341;&#39046;&#19968;&#22330;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#28909;&#28526;&#12290;&#34429;&#28982;LLMs&#27491;&#22312;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#25105;&#20204;&#23545;&#20197;&#19979;&#38382;&#39064;&#24863;&#20852;&#36259;&#65306;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;LLMs&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#30340;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;STPA&#26159;&#26368;&#26222;&#36941;&#30340;&#21361;&#38505;&#20998;&#26512;&#25216;&#26415;&#20043;&#19968;&#65292;&#20294;&#23427;&#23384;&#22312;&#35832;&#22810;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#39640;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#20132;&#20114;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#23558;ChatGPT&#32435;&#20837;STPA&#20013;&#30340;&#26041;&#27861;&#65306;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#12289;&#37325;&#22797;&#21333;&#24037;&#20132;&#20114;&#21644;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;ChatGPT&#19981;&#33021;&#20026;STPA&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#23545;STPA&#26377;&#24110;&#21161;&#65292;&#20294;&#19981;&#22914;&#37325;&#22797;&#20132;&#20114;&#26377;&#25928;&#65307;&#65288;iii&#65289;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;AEB&#20197;&#22806;&#30340;&#20854;&#20182;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. While LLMs are being quickly applied to many AI application domains, we are interested in the following question: Can safety analysis for safety-critical systems make use of LLMs? To answer, we conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. Comparative results reveal that: (i) using ChatGPT without human exp
&lt;/p&gt;</description></item><item><title>xASTNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;&#23427;&#30340;&#20248;&#28857;&#21253;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#19977;&#20010;&#35774;&#35745;&#26469;&#20445;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07104</link><description>&lt;p&gt;
xASTNN&#65306;&#29992;&#20110;&#24037;&#19994;&#23454;&#36341;&#30340;&#25913;&#36827;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xASTNN: Improved Code Representations for Industrial Practice. (arXiv:2303.07104v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07104
&lt;/p&gt;
&lt;p&gt;
xASTNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;&#23427;&#30340;&#20248;&#28857;&#21253;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#19977;&#20010;&#35774;&#35745;&#26469;&#20445;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#24320;&#21457;&#39640;&#36136;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#20960;&#24180;&#30740;&#31350;&#30028;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37096;&#32626;&#22256;&#38590;&#21644;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;xASTNN&#65292;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#36825;&#31181;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;xASTNN&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#39318;&#20808;&#65292;xASTNN&#23436;&#20840;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;AST&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#35774;&#35745;&#26469;&#20445;&#35777;xASTNN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#29992;&#20110;&#20195;&#30721;&#33258;&#28982;&#24615;&#30340;&#35821;&#21477;&#23376;&#26641;&#24207;&#21015;&#12289;&#29992;&#20110;&#21477;&#27861;&#24314;&#27169;&#30340;&#38376;&#25511;&#36882;&#24402;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning techniques in software engineering becomes increasingly popular. One key problem is developing high-quality and easy-to-use source code representations for code-related tasks. The research community has acquired impressive results in recent years. However, due to the deployment difficulties and performance bottlenecks, seldom these approaches are applied to the industry. In this paper, we present xASTNN, an eXtreme Abstract Syntax Tree (AST)-based Neural Network for source code representation, aiming to push this technique to industrial practice. The proposed xASTNN has three advantages. First, xASTNN is completely based on widely-used ASTs and does not require complicated data pre-processing, making it applicable to various programming languages and practical scenarios. Second, three closely-related designs are proposed to guarantee the effectiveness of xASTNN, including statement subtree sequence for code naturalness, gated recursive unit for syntacti
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09495</link><description>&lt;p&gt;
Topical: &#20351;&#29992;Attention&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09495
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#28304;&#20195;&#30721;&#20013;&#23398;&#20064;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#21644;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#30340;&#26426;&#22120;&#23398;&#20064;(MLOnCode)&#25215;&#35834;&#25913;&#21464;&#36719;&#20214;&#20132;&#20184;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#25366;&#25496;&#36719;&#20214;&#24037;&#20214;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21644;&#20851;&#31995;&#65292;MLOnCode&#36890;&#36807;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#12289;&#20195;&#30721;&#25512;&#33616;&#12289;&#20195;&#30721;&#33258;&#21160;&#26631;&#35760;&#21644;&#20854;&#20182;&#25968;&#25454;&#39537;&#21160;&#22686;&#24378;&#26469;&#22686;&#24378;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#20195;&#30721;&#30340;&#33050;&#26412;&#32423;&#34920;&#31034;&#24050;&#32463;&#36275;&#22815;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#21508;&#31181;&#20381;&#36182;&#20851;&#31995;&#21644;&#23384;&#20648;&#24211;&#32467;&#26500;&#30340;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#65292;&#20026;&#23384;&#20648;&#24211;&#33258;&#21160;&#26631;&#35760;&#20027;&#39064;&#25110;&#33258;&#21160;&#35760;&#24405;&#23384;&#20648;&#24211;&#20195;&#30721;&#31561;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#23384;&#20648;&#24211;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(a) &#20381;&#36182;&#20110;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;(&#20363;&#22914;README&#25991;&#20214;)&#65307;(b) &#36890;&#36807;&#20018;&#32852;&#25110;&#24179;&#22343;&#31561;&#26041;&#27861;&#23545;&#26041;&#27861;/&#33050;&#26412;&#32423;&#34920;&#31034;&#36827;&#34892;&#31616;&#21333;&#32858;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Topical&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20844;&#20849;&#23384;&#20648;&#24211;&#32423;&#23884;&#20837;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven enhancements. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27010;&#29575;&#29702;&#35770;(GPT)&#30340;&#32852;&#24819;&#35760;&#24518;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#26356;&#22909;&#22320;&#22788;&#29702;&#20855;&#26377;&#25351;&#25968;&#32423;&#20248;&#21183;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2201.12305</link><description>&lt;p&gt;
&#21518;&#37327;&#23376;&#32852;&#24819;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
A Post-Quantum Associative Memory. (arXiv:2201.12305v2 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27010;&#29575;&#29702;&#35770;(GPT)&#30340;&#32852;&#24819;&#35760;&#24518;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#26356;&#22909;&#22320;&#22788;&#29702;&#20855;&#26377;&#25351;&#25968;&#32423;&#20248;&#21183;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#24819;&#35760;&#24518;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#20449;&#24687;&#26816;&#32034;&#23436;&#25972;&#20449;&#24687;&#30340;&#35774;&#22791;&#12290;&#22312;&#24191;&#20041;&#27010;&#29575;&#35770;&#65288;GPT&#65289;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#24819;&#35760;&#24518;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#25152;&#21463;&#21040;&#30340;&#26497;&#38480;&#38480;&#21046;&#12290;&#22312;GPT&#30340;&#35268;&#23450;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#20197;&#23481;&#32435;$2^m$&#20010;&#29366;&#24577;&#19988;&#20854;&#20013;&#30340;&#20219;&#24847;$N$&#20010;&#29366;&#24577;&#37117;&#26159;&#23436;&#20840;&#21487;&#21306;&#20998;&#30340;&#26368;&#23567;GPT&#32500;&#24230;$d(N,m)$&#12290;&#36890;&#36807;&#24341;&#29992;Danzer&#21644;Gr&#252;nbaum&#30340;&#32769;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;$m$&#20026;2&#26102;$d(2,m)=m+1$&#65292;&#32780;&#22312;&#38656;&#35201;GPT&#20998;&#21035;&#20026;&#32463;&#20856;&#25110;&#37327;&#23376;&#29702;&#35770;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#32500;&#24230;&#20998;&#21035;&#20026;$O(2^m)$&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#20363;&#23376;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;GPT&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#37117;&#26377;&#25351;&#25968;&#32423;&#30340;&#20248;&#21183;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22266;&#23450;$N$&#21644;&#28176;&#36817;&#22823;&#30340;$m$&#30340;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;$d(N,m) \leq m^{1+o_N(1)}$&#65288;&#24403;$m\to\infty$&#26102;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Associative memories are devices storing information that can be fully retrieved given partial disclosure of it. We examine a toy model of associative memory and the ultimate limitations it is subjected to within the framework of general probabilistic theories (GPTs), which represent the most general class of physical theories satisfying some basic operational axioms. We ask ourselves how large the dimension of a GPT should be so that it can accommodate $2^m$ states with the property that any $N$ of them are perfectly distinguishable. Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and Gr\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the GPT is required to be either classical or quantum. This yields an example of a task where GPTs outperform both classical and quantum theory exponentially. More generally, we resolve the case of fixed $N$ and asymptotically large $m$, proving that $d(N,m) \leq m^{1+o_N(1)}$ (as $m\to\infty$) for every 
&lt;/p&gt;</description></item></channel></rss>