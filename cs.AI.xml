<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.20156</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;MDPs&#20013;&#36890;&#36807;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#19982;&#31579;&#36873;&#22686;&#24378;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;CAESAR&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20156
&lt;/p&gt;
&lt;p&gt;
CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20540;&#20026;&#22522;&#30784;&#20195;&#29702;&#22312;&#19981;&#21516;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20043;&#38388;&#36816;&#34892;&#26102;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FedRL&#65289;&#12290;&#29616;&#26377;&#30340;FedRL&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#20195;&#29702;&#30340;&#20540;&#20989;&#25968;&#36827;&#34892;&#24179;&#22343;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#32858;&#21512;&#31574;&#30053;&#22312;&#20195;&#29702;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#20010;&#20307;&#20195;&#29702;&#36328;&#21508;&#31181;MDPs&#23398;&#20064;&#30340;Convergence-AwarE SAmpling with scReening&#65288;CAESAR&#65289;&#32858;&#21512;&#26041;&#26696;&#12290;CAESAR&#26159;&#26381;&#21153;&#22120;&#20351;&#29992;&#30340;&#19968;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30456;&#21516;MDP&#20013;&#20195;&#29702;&#25910;&#25947;&#21040;&#30456;&#21516;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20107;&#23454;&#65292;CAESAR&#20351;&#24471;&#33021;&#22815;&#20174;&#26356;&#29087;&#32451;&#30340;&#21516;&#34892;&#37027;&#37324;&#26377;&#36873;&#25321;&#22320;&#21560;&#25910;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20156v1 Announce Type: cross  Abstract: In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.17089</link><description>&lt;p&gt;
GOLF&#65306;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#22686;&#24378;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#21327;&#21161;&#29992;&#25143;&#22788;&#29702;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#20449;&#24687;&#33719;&#21462;&#36807;&#31243;&#12290;&#21033;&#29992;LLMs&#20316;&#20026;&#25628;&#32034;&#24341;&#25806;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#26681;&#25454;&#20854;&#26597;&#35810;&#23450;&#21046;&#30340;&#25688;&#35201;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#23548;&#33322;&#22823;&#37327;&#20449;&#24687;&#36164;&#28304;&#26102;&#25152;&#24102;&#26469;&#30340;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#31181;&#36716;&#21464;&#20984;&#26174;&#20102;LLMs&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#24687;&#33719;&#21462;&#33539;&#24335;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#20219;&#21153;&#28966;&#28857;&#20449;&#24687;&#26816;&#32034;&#21644;LLMs&#30340;&#20219;&#21153;&#35268;&#21010;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#23558;LLMs&#30340;&#33021;&#21147;&#33539;&#22260;&#25193;&#23637;&#21040;&#25903;&#25345;&#29992;&#25143;&#23548;&#33322;&#38271;&#26399;&#21644;&#37325;&#35201;&#30340;&#29983;&#27963;&#20219;&#21153;&#12290;&#23427;&#24341;&#20837;&#20102;GOLF&#26694;&#26550;&#65288;&#30446;&#26631;&#23548;&#21521;&#30340;&#38271;&#26399;&#29983;&#27963;&#20219;&#21153;&#65289;&#65292;&#20391;&#37325;&#20110;&#22686;&#24378;LLMs&#36890;&#36807;&#30446;&#26631;&#23450;&#21521;&#21644;&#38271;&#26399;&#35268;&#21010;&#26469;&#21327;&#21161;&#29992;&#25143;&#20570;&#20986;&#37325;&#35201;&#30340;&#29983;&#27963;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#35770;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.00824</link><description>&lt;p&gt;
&#20449;&#24687;&#27969;&#36335;&#30001;&#65306;&#33258;&#21160;&#35299;&#37322;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Information Flow Routes: Automatically Interpreting Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#23454;&#29616;&#30340;&#26426;&#21046;&#65292;&#20449;&#24687;&#36890;&#36807;&#32593;&#32476;&#20869;&#37096;&#30340;&#36335;&#30001;&#36827;&#34892;&#20256;&#36755;&#12290;&#36825;&#20123;&#36335;&#30001;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#23545;&#24212;&#20110;&#26631;&#35760;&#34920;&#31034;&#65292;&#36793;&#23545;&#24212;&#20110;&#32593;&#32476;&#20869;&#37096;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#20197;&#33258;&#39030;&#21521;&#19979;&#30340;&#26041;&#24335;&#33258;&#21160;&#26500;&#24314;&#36825;&#20123;&#22270;&#65292;&#38024;&#23545;&#27599;&#19968;&#20010;&#39044;&#27979;&#21482;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#28608;&#27963;&#20462;&#34917;&#30340;&#24037;&#20316;&#27969;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#24402;&#22240;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65306;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26377;&#25928;&#22320;&#25581;&#31034;&#29616;&#26377;&#30340;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20462;&#34917;&#65306;&#25105;&#20204;&#19981;&#38656;&#35201;&#20154;&#31867;&#20180;&#32454;&#35774;&#35745;&#39044;&#27979;&#27169;&#26495;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#39044;&#27979;&#25552;&#21462;&#20449;&#24687;&#27969;&#36335;&#30001;&#65288;&#19981;&#20165;&#20165;&#26159;&#22312;&#20801;&#35768;&#30340;&#27169;&#26495;&#20043;&#38388;&#30340;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23601;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#19968;&#33324;&#24615;&#35752;&#35770;&#65292;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#25110;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;Llama 2&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00824v1 Announce Type: cross  Abstract: Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the rol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65292;&#22312;&#20445;&#25345;&#19982;&#30495;&#23454;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24863;&#30693;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.16749</link><description>&lt;p&gt;
MISC: &#30001;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#39537;&#21160;&#30340;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65292;&#22312;&#20445;&#25345;&#19982;&#30495;&#23454;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24863;&#30693;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23384;&#20648;&#21644;&#36890;&#20449;&#21327;&#35758;&#30340;&#28436;&#21464;&#65292;&#36229;&#20302;&#27604;&#29305;&#29575;&#22270;&#20687;&#21387;&#32553;&#24050;&#25104;&#20026;&#19968;&#20010;&#26497;&#20855;&#38656;&#27714;&#30340;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21387;&#32553;&#31639;&#27861;&#24517;&#39035;&#22312;&#36229;&#20302;&#27604;&#29305;&#29575;&#19979;&#35201;&#20040;&#29306;&#29298;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#35201;&#20040;&#29306;&#29298;&#24863;&#30693;&#36136;&#37327;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#24179;&#34913;&#36825;&#20004;&#20010;&#30446;&#26631;&#21464;&#24471;&#21487;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#27169;&#24577;&#22270;&#20687;&#35821;&#20041;&#21387;&#32553;&#65288;MISC&#65289;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;LMM&#32534;&#30721;&#22120;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#20449;&#24687;&#12289;&#19968;&#20010;&#22320;&#22270;&#32534;&#30721;&#22120;&#29992;&#20110;&#23450;&#20301;&#19982;&#35821;&#20041;&#23545;&#24212;&#30340;&#21306;&#22495;&#12289;&#19968;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#29983;&#25104;&#26497;&#24230;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#65292;&#20197;&#21450;&#19968;&#20010;&#35299;&#30721;&#22120;&#26681;&#25454;&#19978;&#36848;&#20449;&#24687;&#37325;&#26500;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MISC&#36866;&#29992;&#20110;&#21387;&#32553;&#20256;&#32479;&#33258;&#28982;&#24863;&#30693;&#22270;&#20687;&#65288;NSIs&#65289;&#21644;&#26032;&#20852;AI&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16749v2 Announce Type: replace-cross  Abstract: With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>https://arxiv.org/abs/2402.16472</link><description>&lt;p&gt;
mEdIT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
mEdIT: Multilingual Text Editing via Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;CoEdIT&#26159;&#26368;&#36817;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#65292;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#12290;mEdIT&#27169;&#22411;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#12290;&#23427;&#20204;&#26088;&#22312;&#25509;&#25910;&#29992;&#25143;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#23646;&#24615;&#30340;&#25351;&#20196;&#65292;&#20363;&#22914;Grammatik korrigieren&#65288;&#24503;&#35821;&#65289;&#25110;Parafrasee la oraci&#243;n&#65288;&#35199;&#29677;&#29273;&#35821;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25991;&#26412;&#32534;&#36753;&#25968;&#25454;&#38598;&#20013;&#31574;&#21010;&#25968;&#25454;&#65292;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35821;&#31995;&#30340;&#22810;&#35821;&#35328;&#65292;&#20026;&#19977;&#20010;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#65288;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#12289;&#25991;&#26412;&#31616;&#21270;&#21644;&#25913;&#20889;&#65289;&#26500;&#24314;&#20102;mEdIT&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;mEdIT&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#38598;&#19978;&#19982;&#20854;&#20182;&#22810;&#35821;&#35328;LLMs&#30456;&#27604;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;mEdIT gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16472v1 Announce Type: cross  Abstract: We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT gen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.15903</link><description>&lt;p&gt;
&#39640;&#25928;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#26080;&#32447;&#35774;&#22791;&#19978;
&lt;/p&gt;
&lt;p&gt;
ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#31471;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#24182;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#65288;&#20998;&#24067;&#24335;&#35774;&#22791;&#65289;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#35774;&#22791;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#30340;&#36164;&#28304;&#26159;&#19968;&#20010;&#26497;&#20855;&#21560;&#24341;&#21147;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ESFL&#65289;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20013;&#22830;&#26381;&#21153;&#22120;&#22312;&#20855;&#26377;&#24322;&#26500;&#31471;&#35774;&#22791;&#65288;EDs&#65289;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;EDs&#20043;&#38388;&#23558;&#27169;&#22411;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#30340;&#24322;&#36136;&#24615;&#20849;&#21516;&#20248;&#21270;&#29992;&#25143;&#31471;&#24037;&#20316;&#37327;&#21644;&#26381;&#21153;&#22120;&#31471;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#25972;&#20010;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65292;&#36825;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36845;&#20195;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#33719;&#24471;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15903v1 Announce Type: cross  Abstract: Federated learning (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split federated learning algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split federated learning framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users' heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive simulations have been c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12219</link><description>&lt;p&gt;
&#37325;&#26032;&#26684;&#24335;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Reformatted Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24494;&#35843;&#25968;&#25454;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#36153;&#21147;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;LLM&#24187;&#35273;&#24341;&#36215;&#30340;&#20107;&#23454;&#38169;&#35823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#25552;&#21319;&#29616;&#26377;&#25351;&#23548;&#25968;&#25454;&#36136;&#37327;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#23558;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26356;&#31526;&#21512;&#39044;&#20808;&#24314;&#31435;&#26631;&#20934;&#21644;&#32534;&#35793;&#35777;&#25454;&#30340;&#26684;&#24335;&#12290;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#20154;&#31867;&#27880;&#37322;&#12289;&#24187;&#35273;&#21644;&#25193;&#23637;&#22256;&#38590;&#65292;&#19982;&#29616;&#26377;&#23545;&#40784;&#25216;&#26415;&#27491;&#20132;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReAlign&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25972;&#20307;&#23545;&#40784;&#33021;&#21147;&#12289;&#25968;&#23398;&#25512;&#29702;&#12289;&#20107;&#23454;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#25110;&#20808;&#36827;&#35757;&#32451;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#21709;&#24212;&#65292;LLaMA-2-13
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.11444</link><description>&lt;p&gt;
&#22312;&#32654;&#22269;&#35780;&#20272;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20844;&#20247;&#25509;&#21463;&#24230;
&lt;/p&gt;
&lt;p&gt;
Gauging Public Acceptance of Conditionally Automated Cars in the United States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26234;&#33021;&#22478;&#24066;&#30340;&#19968;&#20010;&#20803;&#32032;&#65292;&#21363;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;SAE Level 3&#65289;&#65292;&#30740;&#31350;&#20102;&#24433;&#21709;&#32654;&#22269;&#20844;&#20247;&#25509;&#21463;&#24230;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;UTUAT2&#27169;&#22411;&#30340;&#25913;&#32534;&#29256;&#12290;&#36890;&#36807;&#23454;&#39564;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21521;&#20182;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#36848;L3&#25216;&#26415;&#30340;&#30701;&#31687;&#25925;&#20107;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#20182;&#20204;&#23545;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#24863;&#30693;&#12290;&#37319;&#29992;PLS-SEM&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25216;&#26415;&#30340;&#25509;&#21463;&#24230;&#65292;&#25353;&#37325;&#35201;&#24615;&#36882;&#20943;&#30340;&#39034;&#24207;&#65292;&#21463;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20139;&#21463;&#21160;&#26426;&#12289;&#31038;&#20250;&#24433;&#21709;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#37117;&#23545;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#24863;&#30693;&#26377;&#31215;&#26497;&#24433;&#21709;&#65307;&#20415;&#21033;&#26465;&#20214;&#12289;&#20139;&#21463;&#21160;&#26426;&#21644;&#31038;&#20250;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11444v1 Announce Type: cross  Abstract: In this work we look at an element of smart cities, conditionally automated cars (SAE Level 3), investigating the factors influencing public acceptance in the United States. We apply an adaptation of the UTUAT2 model. Taking an experimental approach study 358 participants in the US were presented with a vignette outlining the L3 technology followed by a series of questions to capture their perceptions of conditionally automated cars. PLS-SEM was used to analyze the collected data. The results reveal that the acceptance of the technology, in order of decreasing importance, was determined by social influence, performance expectancy, hedonic motivation, facilitating conditions, and effort expectancy. Furthermore, hedonic motivation, social influence, facilitating conditions and effort expectancy all have a positive influence on the perception of how useful the technology is; facilitating conditions, hedonic motivation, and social influenc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11176</link><description>&lt;p&gt;
KnowTuning&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#29983;&#25104;&#19981;&#23436;&#25972;&#12289;&#38750;&#20107;&#23454;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#31572;&#26696;&#31561;&#38480;&#21046;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;LLMs&#22312;&#26222;&#36890;&#24494;&#35843;&#26399;&#38388;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#65288;KnowTuning&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#21644;&#38544;&#24335;&#22320;&#25913;&#21892;LLMs&#30340;&#30693;&#35782;&#35748;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24335;&#30693;&#35782;&#24863;&#30693;&#29983;&#25104;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#24335;&#30693;&#35782;&#24863;&#30693;&#27604;&#36739;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23436;&#25972;&#24615;&#12289;&#20107;&#23454;&#24615;&#21644;&#36923;&#36753;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#23545;&#36890;&#29992;&#21644;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
&lt;/p&gt;</description></item><item><title>UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10381</link><description>&lt;p&gt;
UMAIR-FPS&#65306;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10381
&lt;/p&gt;
&lt;p&gt;
UMAIR-FPS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#22686;&#24378;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#27493;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21160;&#28459;&#25554;&#30011;&#12290;&#22312;&#28023;&#37327;&#25968;&#25454;&#20013;&#21521;&#29992;&#25143;&#25512;&#33616;&#25554;&#30011;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21160;&#28459;&#25512;&#33616;&#31995;&#32479;&#20391;&#37325;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#20294;&#20173;&#38656;&#35201;&#25972;&#21512;&#22270;&#20687;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#25512;&#33616;&#30740;&#31350;&#21463;&#21040;&#32039;&#23494;&#32806;&#21512;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#21160;&#28459;&#25554;&#30011;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#32472;&#30011;&#39118;&#26684;&#30340;&#29992;&#25143;&#24863;&#30693;&#22810;&#27169;&#24577;&#21160;&#30011;&#25554;&#30011;&#25512;&#33616;&#34701;&#21512;&#65288;UMAIR-FPS&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#23545;&#20110;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#39318;&#27425;&#32467;&#21512;&#22270;&#20687;&#32472;&#30011;&#39118;&#26684;&#29305;&#24449;&#19982;&#35821;&#20041;&#29305;&#24449;&#26469;&#26500;&#24314;&#21452;&#36755;&#20986;&#22270;&#20687;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#34920;&#31034;&#12290;&#23545;&#20110;&#25991;&#26412;&#29305;&#24449;&#65292;&#25105;&#20204;&#22522;&#20110;Fine-tuning Sentence-Transformers&#33719;&#24471;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10381v1 Announce Type: cross  Abstract: The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime recommendation systems have focused on text features but still need to integrate image features. In addition, most multi-modal recommendation research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For text features, we obtain text embeddings based on fine-tuning Sentence-Transformers by incorporating domain knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;</title><link>https://arxiv.org/abs/2402.06590</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#65306;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Predictive representations: building blocks of intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06590
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#35268;&#23450;&#20102;&#20160;&#20040;&#26679;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#26159;&#26377;&#29992;&#30340;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#23427;&#20204;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#29702;&#35770;&#35266;&#28857;&#19982;&#35748;&#30693;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#32487;&#20219;&#32773;&#34920;&#24449;&#65288;SR&#65289;&#21450;&#20854;&#24191;&#20041;&#24418;&#24335;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#22823;&#33041;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#34701;&#21512;&#34920;&#26126;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.06079</link><description>&lt;p&gt;
DiscDiff: DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiscDiff: Latent Diffusion Model for DNA Sequence Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNA&#24207;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;DiscDiff&#65292;&#19968;&#31181;&#20026;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#32780;&#23450;&#21046;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#65292;&#20197;&#21450;Absorb-Escape&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#36825;&#20123;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;Absorb-Escape&#36890;&#36807;&#32416;&#27491;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#8220;&#33293;&#20837;&#35823;&#24046;&#8221;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#32780;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;EPD-GenDNA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;15&#20010;&#29289;&#31181;&#30340;&#12289;&#32508;&#21512;&#24615;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;160,000&#20010;&#29420;&#29305;&#24207;&#21015;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting `round errors' inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28595;&#26032;&#38134;&#34892;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#32452;&#32455;&#20013;&#65292;&#23558;AI&#24037;&#20855;GitHub Copilot&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#35813;&#24037;&#20855;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;&#21518;&#23545;&#29983;&#20135;&#21147;&#30340;&#25913;&#21892;&#26159;&#26174;&#33879;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.05636</link><description>&lt;p&gt;
AI&#24037;&#20855;&#23545;&#28595;&#26032;&#38134;&#34892;&#24037;&#31243;&#30340;&#24433;&#21709;&#8212;&#8212;&#20851;&#20110;GitHub Copilot&#22312;&#20225;&#19994;&#29615;&#22659;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28595;&#26032;&#38134;&#34892;&#36825;&#26679;&#19968;&#20010;&#22823;&#22411;&#32452;&#32455;&#20013;&#65292;&#23558;AI&#24037;&#20855;GitHub Copilot&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#35813;&#24037;&#20855;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;&#21518;&#23545;&#29983;&#20135;&#21147;&#30340;&#25913;&#21892;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30340;&#26085;&#30410;&#27969;&#34892;&#24050;&#32463;&#23545;&#21253;&#25324;&#36719;&#20214;&#24037;&#31243;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#32452;&#32455;&#20013;&#23558;AI&#24037;&#20855;&#25972;&#21512;&#21040;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#28595;&#26032;&#38134;&#34892;&#65292;&#35813;&#38134;&#34892;&#25317;&#26377;&#36229;&#36807;5000&#21517;&#24037;&#31243;&#24072;&#65292;&#28085;&#30422;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20351;&#29992;&#19968;&#27454;&#33879;&#21517;&#30340;AI&#24037;&#20855;GitHub Copilot&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#23454;&#38469;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#22312;&#22823;&#35268;&#27169;&#37319;&#29992;GitHub Copilot&#21518;&#35266;&#23519;&#21040;&#30340;&#29983;&#20135;&#21147;&#25913;&#21892;&#30340;&#21021;&#27493;&#21457;&#29616;&#65292;&#32422;&#26377;1000&#21517;&#24037;&#31243;&#24072;&#22312;&#20351;&#29992;&#35813;&#24037;&#20855;&#12290;&#28595;&#26032;&#38134;&#34892;&#23545;GitHub Copilot&#36827;&#34892;&#20102;&#20026;&#26399;&#20845;&#21608;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#21608;&#30340;&#20934;&#22791;&#21644;&#22235;&#21608;&#30340;&#20027;&#21160;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#20197;&#21450;&#35813;&#24037;&#20855;&#23545;&#29983;&#20135;&#21147;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#21442;&#19982;&#32773;&#20351;&#29992;GitHub Copilot&#36827;&#34892;&#36845;&#20195;&#21644;&#25913;&#36827;&#65292;&#21516;&#26102;&#35760;&#24405;&#20854;&#21453;&#39304;&#21644;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a large organization. We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. Initially, participants used GitHub
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02286</link><description>&lt;p&gt;
&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#19979;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#12290;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#65292;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#21644;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#21516;&#26102;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#22914;&#33258;&#20027;&#23548;&#33322;&#21644;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#21516;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#21644;&#36882;&#24402;&#23545;&#40784;&#32593;&#32476;&#65288;MFARANet&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#20998;&#21106;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#20316;&#20026;&#39592;&#24178;&#26469;&#20445;&#35777;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#26469;&#24357;&#34917;&#27973;&#39592;&#24178;&#24341;&#36215;&#30340;&#27169;&#22411;&#23481;&#37327;&#20943;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#22810;&#32423;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65288;MFAM&#65289;&#65292;&#23558;&#32534;&#30721;&#22120;&#20013;&#30340;&#20998;&#23618;&#29305;&#24449;&#32858;&#21512;&#21040;&#27599;&#20010;&#23610;&#24230;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#23545;&#40784;&#21644;&#22810;&#23610;&#24230;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#27969;&#30340;&#23545;&#40784;&#26469;&#24314;&#31435;&#36882;&#24402;&#23545;&#40784;&#27169;&#22359;&#65288;RAM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a crucial research for real-world applications. However, many methods lay particular emphasis on reducing the computational complexity and model size, while largely sacrificing the accuracy. In some scenarios, such as autonomous navigation and driver assistance system, accuracy and speed are equally important. To tackle this problem, we propose a novel Multi-level Feature Aggregation and Recursive Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at real-time inference speed. We employ ResNet-18 as the backbone to ensure efficiency, and propose three core components to compensate for the reduced model capacity due to the shallow backbone. Specifically, we first design Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical features in the encoder to each scale to benefit subsequent spatial alignment and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by combining the flow-based alignm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;</title><link>https://arxiv.org/abs/2312.07401</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#22810;&#26679;&#21270;&#20559;&#22909;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Diversified Preferences of Large Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#34987;&#35748;&#20026;&#26159;&#25552;&#39640;LLMs&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#22810;&#20803;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#26631;&#27880;&#32773;&#30340;&#19981;&#21516;&#20559;&#22909;&#65292;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#20250;&#22810;&#26679;&#21270;&#65292;&#36825;&#38459;&#30861;&#20102;LLM&#23545;&#40784;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#24120;&#29992;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#38598;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#22810;&#26679;&#21270;&#20559;&#22909;&#23545;&#22870;&#21169;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RMs&#65289;&#30340;&#26657;&#20934;&#24615;&#33021;&#19982;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22810;&#26679;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#20154;&#31867;&#20849;&#20139;&#20559;&#22909;&#65288;&#22914;&#8220;&#26080;&#23475;&#21644;&#26377;&#24110;&#21161;&#8221;&#65289;&#19978;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;LLMs&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#31181;&#26080;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#23398;&#20064;&#26041;&#27861;&#65288;MORE&#65289;&#20197;&#22686;&#24378;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \textit{Harmless\&amp;Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance 
&lt;/p&gt;</description></item><item><title>LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03740</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting in Autoregressive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03740
&lt;/p&gt;
&lt;p&gt;
LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26684;&#23616;&#12290;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#24050;&#32463;&#21462;&#20195;&#20102;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#24120;&#35268;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#12290;&#36825;&#31181;&#36716;&#21464;&#20027;&#35201;&#24471;&#30410;&#20110;LLMs&#21644;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#12290;LLMs&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#22823;&#37327;&#21442;&#25968;&#21644;&#24222;&#22823;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#24517;&#39035;&#24341;&#23548;&#23427;&#20204;&#30340;&#36755;&#20986;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#65292;&#21363;&#25552;&#20379;&#29305;&#23450;&#30340;&#36755;&#20837;&#25110;&#25351;&#20196;&#26469;&#24341;&#23548;LLMs&#26397;&#30528;&#39044;&#26399;&#36755;&#20986;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24050;&#34987;&#24212;&#29992;&#26469;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#30340;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#25552;&#31034;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#24182;&#26681;&#25454;&#20854;&#36827;&#34892;&#20102;&#31616;&#26126;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03740v1 Announce Type: cross  Abstract: Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.00493</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#28151;&#21512;&#31383;&#21475;&#27880;&#24847;&#21147;&#65306;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FWin&#30340;&#24555;&#36895;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;Informer&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#21152;&#36895;&#25512;&#26029;&#36895;&#24230;&#65292;&#21516;&#26102;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#19982;Softmax&#20840;&#27880;&#24847;&#21147;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26412;&#22320;&#20840;&#23616;&#31383;&#21475;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;Informer&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#31383;&#21475;&#27880;&#24847;&#21147;&#26159;&#23616;&#37096;&#30340;&#21644;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#20294;&#23427;&#32570;&#20047;&#25429;&#33719;&#20840;&#23616;&#20196;&#29260;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36825;&#36890;&#36807;&#21518;&#32493;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#22359;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;FWin&#65292;&#19981;&#20381;&#36182;&#20110;Informer&#30340;ProbSparse&#27880;&#24847;&#21147;&#20013;&#30340;&#26597;&#35810;&#31232;&#30095;&#24615;&#20551;&#35774;&#21644;&#32463;&#39564;&#24615;&#36817;&#20284;&#12290;&#36890;&#36807;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FWin transformers&#21487;&#20197;&#25552;&#39640;Informer&#30340;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#23558;&#20854;&#25512;&#26029;&#36895;&#24230;&#21152;&#36895;40%&#33267;50%&#12290;&#25105;&#20204;&#36824;&#22312;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;FWin&#31867;&#22411;&#27880;&#24847;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36890;&#36807;&#20174;Informer&#27169;&#22411;&#30340;&#20840;&#27880;&#24847;&#21147;&#23618;&#20013;&#25552;&#21462;&#30340;&#20851;&#38190;&#21521;&#37327;&#26469;&#36924;&#36817;&#29978;&#33267;&#32988;&#36807;&#22522;&#20110;Softmax&#20840;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00493v2 Announce Type: replace-cross  Abstract: We study a fast local-global window-based attention method to accelerate Informer for long sequence time-series forecasting. While window attention is local and a considerable computational saving, it lacks the ability to capture global token information which is compensated by a subsequent Fourier transform block. Our method, named FWin, does not rely on query sparsity hypothesis and an empirical approximation underlying the ProbSparse attention of Informer. Through experiments on univariate and multivariate datasets, we show that FWin transformers improve the overall prediction accuracies of Informer while accelerating its inference speeds by 40 to 50 %. We also show in a nonlinear regression model that a learned FWin type attention approaches or even outperforms softmax full attention based on key vectors extracted from an Informer model's full attention layer acting on time series data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2209.04589</link><description>&lt;p&gt;
&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#22240;&#26524;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causal Intervention for Fairness in Multi-behavior Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.04589
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#65292;&#22788;&#29702;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20174;&#21508;&#31181;&#29992;&#25143;&#34892;&#20026;&#20013;&#23398;&#20064;&#29992;&#25143;&#20852;&#36259;&#65292;&#21253;&#25324;&#28857;&#20987;&#21644;&#28857;&#20987;&#21518;&#30340;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#28857;&#36190;&#21644;&#25910;&#34255;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34892;&#20026;&#19981;&#21487;&#36991;&#20813;&#22320;&#34920;&#29616;&#20986;&#27969;&#34892;&#24230;&#20559;&#24046;&#65292;&#23548;&#33268;&#19968;&#20123;&#19981;&#20844;&#24179;&#38382;&#39064;&#65306;1&#65289;&#23545;&#20110;&#30456;&#20284;&#36136;&#37327;&#30340;&#29289;&#21697;&#65292;&#26356;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#65307;2&#65289;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#27969;&#34892;&#24230;&#36739;&#20302;&#30340;&#21463;&#27426;&#36814;&#29289;&#21697;&#21487;&#33021;&#20250;&#33719;&#24471;&#26356;&#22810;&#26333;&#20809;&#12290;&#29616;&#26377;&#24037;&#20316;&#22312;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#24046;&#26041;&#38754;&#30450;&#30446;&#28040;&#38500;&#20559;&#35265;&#65292;&#36890;&#24120;&#24573;&#30053;&#29289;&#21697;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#65288;&#20363;&#22914;&#36716;&#21270;&#29575;&#65289;&#30340;&#20851;&#31995;&#23454;&#38469;&#19978;&#21453;&#26144;&#20102;&#29289;&#21697;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22788;&#29702;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#22810;&#31181;&#29992;&#25143;&#34892;&#20026;&#26469;&#20943;&#36731;&#27969;&#34892;&#24230;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#34892;&#20026;&#25512;&#33616;&#20013;&#20132;&#20114;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.04589v2 Announce Type: replace-cross  Abstract: Recommender systems usually learn user interests from various user behaviors, including clicks and post-click behaviors (e.g., like and favorite). However, these behaviors inevitably exhibit popularity bias, leading to some unfairness issues: 1) for items with similar quality, more popular ones get more exposure; and 2) even worse the popular items with lower popularity might receive more exposure. Existing work on mitigating popularity bias blindly eliminates the bias and usually ignores the effect of item quality. We argue that the relationships between different user behaviors (e.g., conversion rate) actually reflect the item quality. Therefore, to handle the unfairness issues, we propose to mitigate the popularity bias by considering multiple user behaviors.   In this work, we examine causal relationships behind the interaction generation procedure in multi-behavior recommendation. Specifically, we find that: 1) item popula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.13138</link><description>&lt;p&gt;
&#23545;AI&#20195;&#29702;&#30340;&#21487;&#35265;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#25919;&#24220;&#21644;&#20010;&#20154;&#27963;&#21160;&#22996;&#25176;&#32473;&#20855;&#26377;&#26377;&#38480;&#30417;&#30563;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#29616;&#26377;&#30340;&#31038;&#20250;&#39118;&#38505;&#24182;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#23545;&#29616;&#26377;&#27835;&#29702;&#32467;&#26500;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20462;&#35746;&#21644;&#35843;&#25972;&#65292;&#24182;&#30830;&#20445;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#23558;AI&#20195;&#29702;&#30340;&#20351;&#29992;&#22320;&#28857;&#12289;&#21407;&#22240;&#12289;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#32773;&#31561;&#20449;&#24687;&#31216;&#20026;&#8220;&#21487;&#35265;&#24615;&#8221;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#12290;&#23545;&#20110;&#27599;&#19968;&#31181;&#25514;&#26045;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#65292;&#36825;&#20123;&#26041;&#24335;&#22312;&#20405;&#20837;&#24615;&#21644;&#20449;&#24687;&#24615;&#26041;&#38754;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2312.14972</link><description>&lt;p&gt;
&#22312;&#29983;&#20135;&#20013;&#29992;&#24320;&#28304;SLMs&#26367;&#20195;&#19987;&#26377;LLMs&#30340;&#26435;&#34913;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production. (arXiv:2312.14972v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#19987;&#26377;LLMs&#21644;&#24320;&#28304;SLMs&#30340;&#26435;&#34913;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;SLaM&#26469;&#27979;&#35797;&#20135;&#21697;&#21151;&#33021;&#12290;&#22312;&#23454;&#38469;&#20135;&#21697;&#21151;&#33021;&#26367;&#25442;&#26102;&#65292;&#23545;&#20110;&#29616;&#26377;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#34987;&#24320;&#28304;SLMs&#20195;&#26367;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#20381;&#36182;&#20110;&#31649;&#29702;&#30340;AI&#27169;&#22411;&#30340;API&#65292;&#22914;OpenAI&#30340;GPT-4&#65292;&#20197;&#22312;&#20854;&#20135;&#21697;&#20013;&#21019;&#24314;AI&#22686;&#24378;&#20307;&#39564;&#12290;&#38500;&#20102;&#20351;&#29992;&#20415;&#21033;&#21644;&#32553;&#30701;&#29983;&#20135;&#26102;&#38388;&#30340;&#22909;&#22788;&#22806;&#65292;&#20381;&#36182;&#19987;&#26377;API&#36824;&#20855;&#26377;&#27169;&#22411;&#25511;&#21046;&#12289;&#24615;&#33021;&#21487;&#38752;&#24615;&#12289;&#19978;&#32447;&#21487;&#39044;&#27979;&#24615;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#32570;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#28044;&#29616;&#20102;&#35768;&#22810;&#20379;&#21830;&#19994;&#20351;&#29992;&#30340;&#24320;&#28304;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26367;&#20195;&#29616;&#26377;&#33021;&#21147;&#30340;&#20934;&#22791;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#65292;&#24182;&#19988;&#27809;&#26377;&#29616;&#25104;&#30340;&#31995;&#32479;&#26041;&#27861;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#29616;&#20195;&#24320;&#28304;SLMs&#21450;&#20854;&#22312;&#26367;&#20195;&#30495;&#23454;&#20135;&#21697;&#21151;&#33021;&#30340;&#19987;&#26377;LLM APIs&#26102;&#25152;&#20570;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;SLaM&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#24471;&#21487;&#20197;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#27979;&#35797;&#20351;&#29992;&#20219;&#24847;SLMs&#30340;&#20135;&#21697;&#21151;&#33021;&#12290;&#20351;&#29992;SLaM&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine bot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11700</link><description>&lt;p&gt;
&#21333;&#35270;&#35282;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Runner re-identification from single-view video in the open-world setting. (arXiv:2310.11700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#36305;&#32773;&#30340;&#20877;&#35782;&#21035;&#23545;&#20110;&#33258;&#21160;&#35270;&#39057;&#22788;&#29702;&#21644;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#35270;&#35282;&#25110;&#21333;&#35270;&#35282;&#20307;&#32946;&#35270;&#39057;&#20013;&#36305;&#32773;&#20877;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23553;&#38381;&#19990;&#30028;&#35774;&#23450;&#30340;&#20877;&#35782;&#21035;&#19978;&#65292;&#32780;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36827;&#34892;&#33258;&#21160;&#35270;&#39057;&#20998;&#26512;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#24182;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#22312;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24517;&#39035;&#30452;&#25509;&#22788;&#29702;&#35270;&#39057;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#21363;&#20351;&#36305;&#32773;&#20986;&#29616;&#22810;&#27425;&#34987;&#26694;&#36873;&#20986;&#65292;&#31995;&#32479;&#20063;&#33021;&#36827;&#34892;&#35782;&#21035;&#12290;&#23545;&#20110;&#33258;&#21160;&#22788;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;YOLOv8&#21644;&#24494;&#35843;&#30340;EfficientNet&#26469;&#26816;&#27979;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#12290;&#28982;&#21518;&#20351;&#29992;ByteTrack&#26469;&#36319;&#36394;&#36305;&#32773;&#65292;&#24182;&#20351;&#29992;&#24494;&#35843;&#30340;YOLO&#26469;&#26816;&#27979;&#20182;&#20204;&#30340;&#38795;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YO
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01132</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;BoWs&#33258;&#21160;&#35780;&#20272;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65306;&#23558;&#20840;&#23616;&#39044;&#27979;&#19982;&#20855;&#20307;&#21453;&#39304;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#25945;&#24072;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#26356;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20272;&#35745;&#8220;&#25945;&#23398;&#25903;&#25345;&#8221;&#39046;&#22495;&#30340;CLASS&#35838;&#22530;&#35780;&#20272;&#24471;&#20998;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#35266;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;Meta&#30340;Llama2&#30340;&#38646;-shot&#25552;&#31034;&#65292;&#21644;/&#25110;&#32463;&#20856;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25945;&#24072;&#35328;&#35821;&#30340;&#20010;&#21035;&#35805;&#35821;&#65288;&#20351;&#29992;OpenAI&#30340;Whisper&#36827;&#34892;&#33258;&#21160;&#36716;&#24405;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#25945;&#23398;&#25903;&#25345;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#35805;&#35821;&#32423;&#30340;&#21028;&#26029;&#32467;&#26524;&#22312;&#25972;&#20010;15&#20998;&#38047;&#30340;&#35266;&#23519;&#20250;&#35805;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;CLASS&#24471;&#20998;&#12290;&#22312;&#24188;&#20799;&#22253;&#21644;&#23398;&#21069;&#29677;&#25945;&#23460;&#30340;&#20004;&#20010;&#32463;&#36807;CLASS&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33258;&#21160;&#20272;&#35745;CLASS&#25945;&#23398;&#25903;&#25345;&#30340;&#20934;&#30830;&#24615;&#65288;Pearson R&#39640;&#36798;0.47&#65289;&#25509;&#36817;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65288;&#26368;&#39640;R=0.55&#65289;&#65307;&#65288;2&#65289;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#23567;&#29677;&#25945;&#23460;&#20013;&#30340;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00347</link><description>&lt;p&gt;
&#35299;&#38145;&#20559;&#35265;&#26816;&#27979;&#65306;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20869;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis. (arXiv:2310.00347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20559;&#35265;&#23545;&#20110;&#24378;&#21270;&#36127;&#38754;&#21051;&#26495;&#21360;&#35937;&#12289;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20915;&#31574;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#20559;&#35265;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#38598;&#33539;&#22260;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Contextualized Bi-Directional Dual Transformer&#65288;CBDT&#65289;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21033;&#29992;&#20102;&#20004;&#20010;&#21327;&#21516;&#24037;&#20316;&#30340;Transformer&#32593;&#32476;&#65306;Context Transformer&#21644;Entity Transformer&#65292;&#26088;&#22312;&#22686;&#24378;&#20559;&#35265;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20934;&#22791;&#36981;&#24490;FAIR&#21407;&#21017;&#65292;&#30830;&#20445;&#25968;&#25454;&#20351;&#29992;&#20855;&#26377;&#36947;&#24503;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;CBDT&#23637;&#31034;&#20102;&#20854;&#22312;&#21306;&#20998;&#26377;&#20559;&#35265;&#19982;&#20013;&#31435;&#38472;&#36848;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25351;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;2-4&#65285;&#30340;&#25552;&#21319;&#12290;&#36825;&#20026;&#23558;CBDT&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#36866;&#24212;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. Current language models often fall short in generalizing beyond their training sets. In response, we introduce the Contextualized Bi-Directional Dual Transformer (CBDT) Classifier. This novel architecture utilizes two synergistic transformer networks: the Context Transformer and the Entity Transformer, aiming for enhanced bias detection. Our dataset preparation follows the FAIR principles, ensuring ethical data usage. Through rigorous testing on various datasets, CBDT showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. Our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. This opens avenues for adapting the CBDT model across diverse linguistic and cultural landscapes.
&lt;/p&gt;</description></item><item><title>&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15178</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative World Models. (arXiv:2309.15178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15178
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#25215;&#35834;&#22312;&#31163;&#32447;&#39044;&#35757;&#32451;&#38454;&#27573;&#21518;&#65292;&#25552;&#20379;&#33021;&#22815;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#21069;&#21521;-&#21518;&#21521;&#65288;FB&#65289;&#34920;&#31034;&#22312;&#36825;&#20010;&#29702;&#24819;&#30340;&#23454;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#36798;&#21040;&#29305;&#23450;&#20219;&#21153;&#20195;&#29702;&#30340;85%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#23545;&#20110;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#38382;&#39064;&#26080;&#27861;&#26399;&#26395;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;FB&#24615;&#33021;&#22914;&#20309;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#20445;&#23432;&#24615;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23478;&#26063;&#65292;&#22312;&#24635;&#20307;&#19978;&#36798;&#21040;&#20102;150%&#30340;&#26222;&#36890;FB&#24615;&#33021;&#12290;&#26377;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20445;&#23432;&#30340;FB&#31639;&#27861;&#22312;&#27809;&#26377;&#35775;&#38382;&#22870;&#21169;&#26631;&#31614;&#19988;&#38656;&#35201;&#32500;&#25252;&#25152;&#26377;&#20219;&#21153;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15098</link><description>&lt;p&gt;
&#28385;&#36275;&#20851;&#27880;&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#38169;&#35823;&#30340;&#32422;&#26463;&#28385;&#36275;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#25991;&#26412;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#26597;&#35810;&#24314;&#27169;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#19982;&#20107;&#23454;&#32422;&#26463;&#36827;&#34892;&#20869;&#37096;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20854;&#21709;&#24212;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#23384;&#22312;&#24378;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;11&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#24635;&#35745;&#36229;&#36807;40,000&#20010;&#25552;&#31034;&#30340;&#31934;&#24515;&#31574;&#21010;&#22871;&#35013;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Llama-2&#31995;&#21015;&#22312;&#25152;&#26377;&#35268;&#27169;&#65288;7B&#65292;13B&#65292;70B&#65289;&#19978;&#39044;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAT Probe&#65292;&#19968;&#31181;&#25506;&#26597;&#33258;&#27880;&#24847;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#12290;&#36825;&#19968;&#26041;&#27861;&#21644;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;LLM&#20013;&#20107;&#23454;&#24615;&#30340;&#26426;&#26800;&#29702;&#35299;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.03798</link><description>&lt;p&gt;
CLIPMasterPrints: &#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#28436;&#21270;&#27450;&#39575;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20026;&#20195;&#34920;&#30340;&#21516;&#26102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25152;&#35859;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#26159;&#33030;&#24369;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#33021;&#22815;&#26368;&#22823;&#21270;CLIP&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21516;&#26102;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28436;&#21270;&#31574;&#30053;&#25110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25628;&#32034;&#27450;&#39575;&#20027;&#22270;&#20687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25366;&#25496;&#20986;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#35757;&#32451;&#30340;&#22270;&#20687;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#23545;&#27450;&#39575;&#20027;&#20363;&#23376;&#30340;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;</title><link>http://arxiv.org/abs/2306.08251</link><description>&lt;p&gt;
GBSD: &#24102;&#26377;&#38454;&#27573;&#25193;&#25955;&#30340;&#29983;&#25104;&#34394;&#21270;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bokeh&#25928;&#26524;&#26159;&#19968;&#31181;&#33402;&#26415;&#25216;&#24039;&#65292;&#21487;&#20197;&#20351;&#29031;&#29255;&#20013;&#30340;&#22833;&#28966;&#21306;&#22495;&#27169;&#31946;&#65292;&#36817;&#26399;&#30001;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#21644;&#29031;&#29255;&#20998;&#20139;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#31181;&#25928;&#26524;&#22791;&#21463;&#20851;&#27880;&#12290;&#20197;&#24448;&#30340;&#34394;&#21270;&#25928;&#26524;&#28210;&#26579;&#24037;&#20316;&#37117;&#26159;&#38024;&#23545;&#24050;&#26377;&#29031;&#29255;&#36827;&#34892;&#20107;&#21518;&#22270;&#20687;&#22788;&#29702;&#65292;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25110;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#20135;&#29983;&#31867;&#20284;&#30340;&#27169;&#31946;&#25928;&#26524;&#65292;&#20294;&#26159;&#24448;&#24448;&#23384;&#22312;&#28145;&#24230;&#19981;&#36830;&#32493;&#30340;&#22270;&#20687;&#20266;&#24433;&#65292;&#32780;&#19988;&#38480;&#21046;&#20110;&#32451;&#20064;&#25968;&#25454;&#27979;&#35797;&#30340;&#34394;&#21270;&#25928;&#26524;&#12290;&#36817;&#26399;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#20855;&#26377;&#33402;&#26415;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#20294;&#26159;&#35201;&#27714;&#29983;&#25104;&#39640;&#32500;&#24230;&#25513;&#27169;&#25110;&#32773;&#36827;&#34892;&#26114;&#36149;&#30340;&#35843;&#25972;&#65292;&#21487;&#33021;&#24433;&#21709;&#25972;&#20307;&#22270;&#20687;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GBSD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#20013;&#36880;&#27493;&#21512;&#25104;&#22270;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;</title><link>http://arxiv.org/abs/2305.18505</link><description>&lt;p&gt;
&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#20154;&#31867;&#21453;&#39304;&#26597;&#35810;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#21453;&#39304;&#30340;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#33539;&#20363;&#65292;&#22312;&#27492;&#33539;&#20363;&#19979;&#65292;RL&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#23545;&#36712;&#36857;&#30340;&#25104;&#23545;&#20248;&#20808;&#32423;&#21453;&#39304;&#26469;&#26368;&#20248;&#21270;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#26126;&#30830;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#23613;&#31649;RLHF&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#29992;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#23454;&#35777;&#30740;&#31350;&#24182;&#26410;&#35299;&#20915;&#22914;&#20309;&#39640;&#25928;&#37319;&#26679;&#36712;&#36857;&#23545;&#20197;&#26597;&#35810;&#20154;&#31867;&#21453;&#39304;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#25506;&#32034;&#24615;&#36712;&#36857;&#65292;&#22312;&#25910;&#38598;&#20219;&#20309;&#20154;&#31867;&#21453;&#39304;&#20043;&#21069;&#65292;&#20351;&#23398;&#20064;&#38544;&#34255;&#30340;&#22870;&#21169;&#20989;&#25968;&#26356;&#21152;&#20934;&#30830;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;&#21442;&#25968;&#21270;&#21644;&#26410;&#30693;&#36807;&#28193;&#30340;&#22522;&#20110;&#20559;&#22909;&#27169;&#22411;&#19979;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#26356;&#23569;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#32435;&#20837;&#32447;&#24615;&#21644;&#20302;&#31209;MDPs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#34892;&#21160;&#27604;&#36739;&#30340;&#21453;&#39304;&#30340;RLHF&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#22312;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#21453;&#39304;&#30340;&#20219;&#21153;&#26102;&#33719;&#24471;&#25506;&#32034;&#24615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16351</link><description>&lt;p&gt;
WeiAvg&#65306;&#20419;&#36827;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20026;&#21033;&#29992;&#22823;&#35268;&#27169;&#31169;&#26377;&#36793;&#32536;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#31561;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21442;&#19982;&#32773;&#23545;&#32852;&#37030;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#65288;WeiAvg&#65289;&#30340;&#26694;&#26550;&#65292;&#30528;&#37325;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.18062</link><description>&lt;p&gt;
&#35299;&#20915;&#24418;&#24577;&#23398;&#31867;&#27604;&#38382;&#39064;&#65306;&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#31181;&#38750;&#20961;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#38590;&#20197;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290; &#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#65288;AR&#65289;&#21463;&#21040;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20854;&#28508;&#21147;&#65292;&#20363;&#22914;&#20998;&#31867;&#65292;&#20915;&#31574;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;AR&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#22312;&#25972;&#20010;Siganalogies&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#21333;&#35789;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#31867;&#27604;&#27604;&#20363;&#65288;APs&#65289;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290; &#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNc&#65289;&#21644;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNr&#65289;&#65292;&#20197;&#21450;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#21333;&#35789;&#19978;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#24635;&#32467;&#24182;&#25193;&#23637;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#22312;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#19981;&#23384;&#22312;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;AST&#21644;&#25511;&#21046;&#27969;&#22270;&#12289;&#25968;&#25454;&#20381;&#36182;&#22270;&#21450;&#25511;&#21046;&#20381;&#36182;&#22270;&#19978;&#36827;&#34892;&#30456;&#24212;&#30340;&#25506;&#27979;&#20219;&#21153;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.10017</link><description>&lt;p&gt;
&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?. (arXiv:2212.10017v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;AST&#21644;&#25511;&#21046;&#27969;&#22270;&#12289;&#25968;&#25454;&#20381;&#36182;&#22270;&#21450;&#25511;&#21046;&#20381;&#36182;&#22270;&#19978;&#36827;&#34892;&#30456;&#24212;&#30340;&#25506;&#27979;&#20219;&#21153;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#23454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#31243;&#24207;&#35821;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#30740;&#31350;&#22312;&#20998;&#26512;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#21463;&#21040;&#39640;&#32500;&#24230;&#19979;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#20041;&#30340;&#30740;&#31350;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#26412;&#25991;&#26088;&#22312;&#36827;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#23398;&#21040;&#30340;&#20195;&#30721;&#29305;&#24449;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#21363;CodeBERT&#21644;GraphCodeBERT&#65289;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#25506;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of pre-trained code models also has revealed that they can effectively learn program syntax. However, these works are limited in analyzing code syntax and their distance-based approaches are not accurate due to the curse of high dimensionality. Furthermore, the study of the learnt program semantics of these models is rarely discussed. To further understand the code features learnt by these models, in this paper, we target two well-known representative code pre-trained models (i.e., CodeBERT and GraphCodeBERT) and devise a set of probing tasks for the syntax and semantics analysis. Specifically, on one hand, we design two probing tasks (i.e., syntax pair node prediction and token tagging prediction) to manipulate AST for the understanding of learnt program syntax. On the other hand, we design two tasks (i.e., semantic relationship prediction and semantic propagation prediction(inGraph) ) on the constructed control flow graph (CFG), data dependency graph (DDG) and control depend
&lt;/p&gt;</description></item></channel></rss>