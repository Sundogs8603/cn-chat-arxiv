<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08251</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#31038;&#20250;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Social Norms in Large Language Model-based Agent Societies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;&#21560;&#24341;&#20102;&#31038;&#20250;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;Creation &amp; Representation&#12289;Spreading&#12289;Evaluation&#21644;Compliance&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22788;&#29702;&#20102;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#32039;&#24613;&#36807;&#31243;&#65306;(i)&#31038;&#20250;&#35268;&#33539;&#30340;&#26469;&#28304;&#65292;(ii)&#23427;&#20204;&#22914;&#20309;&#34987;&#27491;&#24335;&#34920;&#31034;&#65292;(iii)&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;Agent&#30340;&#20132;&#27969;&#21644;&#35266;&#23519;&#20256;&#25773;&#65292;(iv)&#22914;&#20309;&#36890;&#36807;&#21512;&#29702;&#26816;&#26597;&#36827;&#34892;&#26816;&#26597;&#24182;&#22312;&#38271;&#26399;&#20869;&#36827;&#34892;&#32508;&#21512;&#65292;(v)&#22914;&#20309;&#34987;&#32435;&#20837;Agent&#30340;&#35745;&#21010;&#21644;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#22312;Smallville&#27801;&#30418;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08251v1 Announce Type: cross  Abstract: The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our ar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05110</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#36890;&#36807;&#32452;&#21512;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Collection for Robotic Manipulation via Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05110
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#22320;&#25910;&#38598;&#25968;&#25454;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#32570;&#20047;&#24456;&#22810;&#29702;&#35299;&#12290;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#25910;&#38598;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#21464;&#21270;&#20102;&#35768;&#22810;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#29289;&#20307;&#31867;&#22411;&#21644;&#26700;&#38754;&#32441;&#29702;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#35797;&#22270;&#28085;&#30422;&#21508;&#31181;&#21508;&#26679;&#30340;&#22330;&#26223;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#21040;&#22522;&#20110;&#25968;&#25454;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#30340;&#22797;&#21512;&#33021;&#21147;&#12290;&#22914;&#26524;&#26426;&#22120;&#20154;&#31574;&#30053;&#33021;&#22815;&#20174;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32452;&#21512;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#65288;&#20363;&#22914;&#29289;&#20307;&#31867;&#22411;&#12289;&#26700;&#38754;&#39640;&#24230;&#65289;&#20197;&#22312;&#36935;&#21040;&#30475;&#19981;&#35265;&#30340;&#22240;&#32032;&#32452;&#21512;&#26102;&#25104;&#21151;&#65292;&#37027;&#20040;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#36991;&#20813;&#20026;&#22797;&#21512;&#22788;&#29702;&#30340;&#24773;&#20917;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05110v1 Announce Type: cross  Abstract: Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary a wide range of environmental factors during data collection, such as object types and table textures. While these works attempt to cover a diverse variety of scenarios, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies are able to compose different environmental factors of variation (e.g., object types, table heights) from their training data to succeed when encountering unseen factor combinations, then we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03348</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Learning to Maximize Mutual Information for Chain-of-Thought Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24605;&#32500;&#38142;&#33976;&#39311;&#20013;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19982;&#30693;&#35782;&#38598;&#25104;&#19981;&#36275;&#38382;&#39064;&#30340;&#21464;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#36739;&#23567;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#26159;&#23454;&#29616;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142; (CoT) &#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#36880;&#27493;&#33976;&#39311; (DSS)&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20026;&#36739;&#23567;&#27169;&#22411;&#36171;&#20104;&#20854;&#36739;&#22823;&#21516;&#34892;&#30340;&#20248;&#36234;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22312;DSS&#20013;&#65292;&#33976;&#39311;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#33719;&#24471;&#29983;&#25104;&#29702;&#30001;&#21644;&#39044;&#27979;&#26631;&#31614;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DSS&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#65292;&#23548;&#33268;CoT&#30693;&#35782;&#19982;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#25972;&#21512;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#34920;&#36848;&#20026;&#26368;&#22823;&#21270;&#20004;&#20010;&#20219;&#21153;&#30340;&#34920;&#31034;&#29305;&#24449;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03348v1 Announce Type: cross  Abstract: Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01221</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25104;&#26412;&#25928;&#29575;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#32467;&#26524;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25512;&#33616;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25805;&#20316;&#30340;&#36755;&#20837;&#26356;&#25913;&#65292;&#23558;&#19981;&#33391;&#31995;&#32479;&#36755;&#20986;&#36716;&#21464;&#20026;&#26399;&#26395;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;&#35299;&#37322;&#21333;&#20010;&#23454;&#20363;&#65292;&#20294;&#19968;&#20123;&#30495;&#23454;&#30340;&#29992;&#20363;&#65288;&#22914;&#23458;&#25143;&#28385;&#24847;&#24230;&#65289;&#38656;&#35201;&#35782;&#21035;&#33021;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;&#23454;&#20363;&#65288;&#20363;&#22914;&#23458;&#25143;&#65289;&#30340;&#21333;&#19968;&#21453;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25214;&#21040;&#36825;&#26679;&#30340;&#23454;&#20363;&#32452;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01221v1 Announce Type: cross  Abstract: Counterfactual explanations constitute among the most popular methods for analyzing the predictions of black-box systems since they can recommend cost-efficient and actionable changes to the input to turn an undesired system's output into a desired output. While most of the existing counterfactual methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single counterfactual that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance counterfactual explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19267</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#25351;&#23548;&#65306;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#20196;&#20154;&#22256;&#25200;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#21487;&#20197;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#32763;&#35793;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#21477;&#23376;&#12290;&#33719;&#24471;&#21644;&#32763;&#35793;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#34429;&#28982;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#23545;&#20110;&#39640;&#36136;&#37327;&#32763;&#35793;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#25214;&#21040;&#26368;&#8220;&#26377;&#25928;&#8221;&#30340;&#25968;&#25454;&#25104;&#20026;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#8220;&#36866;&#24403;&#22256;&#38590;&#30340;&#25968;&#25454;&#8221;&#26469;&#25214;&#21040;&#36825;&#20123;&#26377;&#25928;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#19981;&#24212;&#36807;&#20110;&#22256;&#38590;&#25110;&#36807;&#20110;&#31616;&#21333;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24314;&#31435;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26631;&#20934;&#20173;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#8220;&#36866;&#24403;&#22256;&#38590;&#24230;&#8221;&#21487;&#33021;&#22240;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#39046;&#22495;&#32780;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#65292;&#8216;Capturing Perplexing Named Entities&#8217;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19267v1 Announce Type: cross  Abstract: Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexi
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18028</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25903;&#25345;&#39044;&#26399;&#27835;&#29702;: &#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Supporting Anticipatory Governance using LLMs: Evaluating and Aligning Large Language Models with the News Media to Anticipate the Negative Impacts of AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#26032;&#38395;&#23186;&#20307;&#23545;&#40784;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;LLMs&#27169;&#22411;&#22312;&#29983;&#25104;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21457;&#23637;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#39044;&#27979;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20351;&#29992;LLMs&#22686;&#24378;&#21644;&#25351;&#23548;&#36825;&#19968;&#36807;&#31243;&#26159;&#19968;&#31181;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;LLMs&#21644;&#35780;&#20272;&#25351;&#26631;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#32771;&#34385;&#20559;&#24046;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;LLMs&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#24341;&#21457;&#20102;&#20851;&#20110;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#31867;&#21035;&#30340;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20016;&#23500;&#30340;&#21253;&#21547;&#23545;&#26032;&#20852;&#25216;&#26415;&#30340;&#35268;&#33539;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#8212;&#8212;&#26032;&#38395;&#23186;&#20307;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35745;&#31639;&#20998;&#26512;&#20840;&#29699;&#25968;&#30334;&#20010;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#21457;&#24067;&#30340;&#25968;&#21315;&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#31867;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;LLMs&#27169;&#22411;&#65288;GPT-4&#31561;&#65289;&#21644;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#29983;&#25104;&#30340;&#24433;&#21709;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the negative impacts of emerging AI technologies is a challenge, especially in the early stages of development. An understudied approach to such anticipation is the use of LLMs to enhance and guide this process. Despite advancements in LLMs and evaluation metrics to account for biases in generated text, it is unclear how well these models perform in anticipatory tasks. Specifically, the use of LLMs to anticipate AI impacts raises questions about the quality and range of categories of negative impacts these models are capable of generating. In this paper we leverage news media, a diverse data source that is rich with normative assessments of emerging technologies, to formulate a taxonomy of impacts to act as a baseline for comparing against. By computationally analyzing thousands of news articles published by hundreds of online news domains around the world, we develop a taxonomy consisting of ten categories of AI impacts. We then evaluate both instruction-based (GPT-4 and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.07656</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24615;&#33021;&#26356;&#22909;&#30340;POMDP&#31574;&#30053;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#35760;&#24518;&#12290;&#19968;&#31181;&#34920;&#31034;&#36825;&#31181;&#35760;&#24518;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;L*&#31639;&#27861;&#23398;&#20064;&#31574;&#30053;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#31574;&#30053;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#20307;&#31215;&#26174;&#33879;&#26356;&#23567;&#65292;&#22240;&#27492;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#19982;&#30452;&#25509;&#20174;POMDP&#21512;&#25104;&#33258;&#21160;&#26426;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#21487;&#27604;&#25311;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.03365</link><description>&lt;p&gt;
&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#65306;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24314;&#31569;&#29289;&#30340;&#33021;&#28304;&#28040;&#32791;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#24050;&#25104;&#20026;&#20943;&#23569;&#20840;&#29699;&#30899;&#25490;&#25918;&#21644;&#38480;&#21046;&#27668;&#20505;&#21464;&#21270;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#20197;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#30340;&#28909;&#33298;&#36866;&#24230;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#25110;&#32773;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#38656;&#27714;&#21709;&#24212;&#31639;&#27861;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#22312;&#26827;&#30424;&#28216;&#25103;&#65288;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#65289;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#25104;&#21151;&#30340;RL&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24314;&#31569;&#25511;&#21046;&#26041;&#38754;&#65292;MCTS&#20173;&#28982;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#38376;&#30740;&#31350;&#20102;MCTS&#22312;&#24314;&#31569;&#38656;&#27714;&#21709;&#24212;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20854;&#33258;&#28982;&#30340;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#20248;&#21270;&#65292;&#38544;&#24335;&#22320;&#38598;&#25104;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65288;&#19982;&#20256;&#32479;&#30340;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65289;&#65292;&#20351;MCTS&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.09262</link><description>&lt;p&gt;
&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20854;&#27969;&#34892;&#24230;&#21644;&#36129;&#29486;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#22522;&#20110;&#38745;&#24577;&#22270;&#26469;&#20272;&#35745;&#21407;&#22987;&#24341;&#29992;&#35745;&#25968;&#65292;&#26410;&#33021;&#20174;&#32454;&#24494;&#30340;&#35282;&#24230;&#21306;&#20998;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#65288;&#31216;&#20026;DPPDCC&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#35770;&#25991;&#65292;DPPDCC&#22312;&#26500;&#24314;&#30340;&#21160;&#24577;&#24322;&#26500;&#22270;&#20013;&#32534;&#30721;&#20102;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35770;&#25991;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#20849;&#24341;/&#34987;&#24341;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24555;&#29031;&#28436;&#21270;&#30340;&#32858;&#21512;&#12290;&#20026;&#20102;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#22686;&#24378;&#22270;&#26469;&#25552;&#21462;&#20256;&#25773;&#30340;&#26412;&#36136;&#65292;&#24182;&#39044;&#27979;&#32047;&#31215;&#30340;&#24341;&#29992;&#20998;&#32452;&#20197;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#27599;&#20010;&#35282;&#24230;&#30340;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#20854;&#22266;&#26377;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.02041</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26368;&#36817;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#25152;&#25215;&#35834;&#30340;&#20248;&#21183;&#20381;&#36182;&#20110;&#23558;&#37327;&#23376;&#25805;&#20316;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#29289;&#29702;&#23454;&#29616;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#20197;&#20419;&#36827;&#36825;&#31181;&#36716;&#21270;&#12290;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DMs&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#65292;&#36825;&#26159;&#20808;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#12290;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30005;&#36335;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25903;&#25345;&#20856;&#22411;&#30340;DM&#25193;&#23637;&#65292;&#20363;&#22914;&#25513;&#30721;&#21644;&#32534;&#36753;&#65292;&#20197;&#20351;&#30005;&#36335;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#37327;&#23376;&#35774;&#22791;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#37327;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#65292;&#20351;&#29992;&#20102;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;StratiFOILed&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#21644;&#20840;&#23616;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.11636</link><description>&lt;p&gt;
&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
A Symbolic Language for Interpreting Decision Trees. (arXiv:2310.11636v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#65292;&#20351;&#29992;&#20102;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;StratiFOILed&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#21644;&#20840;&#23616;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#21457;&#23637;&#30340;&#27491;&#24335;&#21487;&#35299;&#37322;&#30340;AI&#25361;&#25112;&#20102;&#8220;&#20915;&#31574;&#26641;&#26159;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#8221;&#30340;&#27969;&#34892;&#35828;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#20915;&#31574;&#26641;&#19978;&#36827;&#34892;&#35299;&#37322;&#24615;&#26597;&#35810;&#30340;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#35299;&#37322;&#24615;&#26597;&#35810;&#25110;&#35780;&#20998;&#36866;&#29992;&#20110;&#27599;&#20010;&#24773;&#22659;&#21644;&#26368;&#32456;&#29992;&#25143;&#12290;&#36825;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#35821;&#35328;&#8221;&#30340;&#21487;&#33021;&#24615;&#65292;&#20854;&#20013;&#21487;&#20197;&#34920;&#36798;&#21508;&#31181;&#26597;&#35810;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#23450;&#21046;&#26597;&#35810;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20171;&#32461;&#20102;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#12290;ExplainDT&#26681;&#26893;&#20110;&#25105;&#20204;&#31216;&#20043;&#20026;StratiFOILed&#30340;&#31934;&#24515;&#26500;&#24314;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;&#12290;StratiFOILed&#24179;&#34913;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#35780;&#20272;&#22797;&#26434;&#24230;&#65292;&#20801;&#35768;&#35745;&#31639;&#20986;&#35768;&#22810;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#35748;&#20026;&#21644;&#21453;&#21521;&#25512;&#29702;&#65289;&#21644;&#20840;&#23616;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#25512;&#24191;&#21644;&#23545;&#25239;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of formal explainable AI has disputed the folklore claim that "decision trees are readily interpretable models", showing different interpretability queries that are computationally hard on decision trees, as well as proposing different methods to deal with them in practice. Nonetheless, no single explainability query or score works as a "silver bullet" that is appropriate for every context and end-user. This naturally suggests the possibility of "interpretability languages" in which a wide variety of queries can be expressed, giving control to the end-user to tailor queries to their particular needs. In this context, our work presents ExplainDT, a symbolic language for interpreting decision trees. ExplainDT is rooted in a carefully constructed fragment of first-ordered logic that we call StratiFOILed. StratiFOILed balances expressiveness and complexity of evaluation, allowing for the computation of many post-hoc explanations--both local (e.g., abductive and contr
&lt;/p&gt;</description></item><item><title>MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09833</link><description>&lt;p&gt;
MIR2:&#38754;&#21521;&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#21487;&#35777;&#26126;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09833
&lt;/p&gt;
&lt;p&gt;
MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#23545;&#20110;&#26410;&#30693;&#30431;&#21451;&#30340;&#19981;&#30830;&#23450;&#25110;&#26368;&#22351;&#24773;&#20917;&#34892;&#21160;&#38656;&#35201;&#20855;&#22791;&#24377;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;MARL&#20013;&#30340;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#25216;&#26415;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#30340;&#23545;&#25163;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#38590;&#20197;&#25805;&#20316;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#35797;&#22270;&#31616;&#21270;&#36825;&#31181;&#22797;&#26434;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#20110;&#24754;&#35266;&#30340;&#31574;&#30053;&#12289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#19981;&#36275;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#20154;&#31867;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#34892;&#20026;&#26102;&#33258;&#28982;&#32780;&#28982;&#22320;&#19981;&#38656;&#35201;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIR2&#65292;&#23427;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#35270;&#20026;&#19968;&#20010;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21382;&#21490;&#21644;&#34892;&#21160;&#20043;&#38388;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#40065;&#26834;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.03318</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03318
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#65292;&#37325;&#28857;&#26159;&#22312;Gymnasium&#65288;&#28216;&#25103;&#65289;&#27169;&#25311;&#22120;&#20013;&#30340;&#36827;&#21270;&#20195;&#29702;&#19978; - &#22312;&#36825;&#37324;&#36866;&#24212;&#24230;&#35745;&#31639;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#37319;&#26679;&#20010;&#20307;&#21450;&#20854;&#23454;&#38469;&#36866;&#24212;&#24230;&#24471;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#25972;&#20010;&#36827;&#21270;&#36807;&#31243;&#20013;&#19981;&#26029;&#26356;&#26032;&#19968;&#20010;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;1&#65289;&#22312;&#23454;&#38469;&#36866;&#24212;&#24230;&#21644;&#36817;&#20284;&#36866;&#24212;&#24230;&#20043;&#38388;&#20999;&#25442;&#65292;2&#65289;&#23545;&#31181;&#32676;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#21450;3&#65289;&#21152;&#26435;&#37319;&#26679;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24212;&#24230;&#35745;&#31639;&#30340;&#36817;&#20284;&#27604;&#20363;&#21462;&#20915;&#20110;&#23436;&#20840;&#36816;&#34892;GA&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;GA&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.03813</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#20302;&#20998;&#36776;&#29575;&#28857;&#20113;&#23436;&#25104;&#21464;&#25442;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#37117;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#20154;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#39045;&#39592;&#20260;&#23475;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#26893;&#20837;&#29289;&#65292;&#25163;&#24037;&#35774;&#35745;&#26114;&#36149;&#19988;&#36153;&#26102;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19987;&#29992;&#31995;&#32479;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#24517;&#35201;&#12290;&#33258;&#21160;&#39045;&#39592;&#32570;&#25439;&#37325;&#24314;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#24418;&#29366;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#19987;&#29992;&#28145;&#24230;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30446;&#21069;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20307;&#31215;&#34920;&#31034;&#27861;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19979;&#37325;&#24314;&#39045;&#32570;&#25439;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.06328</link><description>&lt;p&gt;
Offline RL&#30340;&#39044;&#31639;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06328
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#28508;&#22312;&#21160;&#20316;&#39046;&#22495;&#20869;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#22256;&#22659;&#25152;&#24341;&#36215;&#65306;&#22914;&#26524;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#34892;&#21160;&#20250;&#24590;&#20040;&#26679;&#65311;&#36825;&#20123;&#24773;&#20917;&#36890;&#24120;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#32047;&#31215;&#30340;&#22806;&#25512;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#35748;&#35782;&#21040;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20915;&#31574;&#27493;&#39588;&#23545;&#26368;&#32456;&#32467;&#26524;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#24182;&#22312;&#25919;&#31574;&#21046;&#23450;&#20013;&#39044;&#31639;&#21453;&#20107;&#23454;&#20915;&#31574;&#30340;&#25968;&#37327;&#20197;&#25511;&#21046;&#22806;&#25512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25919;&#31574;&#25110;&#20540;&#20989;&#25968;&#19978;&#20351;&#29992;&#35268;&#21017;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26126;&#30830;&#38480;&#21046;&#35757;&#32451;&#26399;&#38388;&#30340;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20915;&#23450;&#22312;&#21738;&#37324;&#36827;&#34892;&#22806;&#25512;&#21644;&#22312;&#21738;&#37324;&#19981;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#23545;&#20915;&#31574;&#30340;&#19978;&#38480;&#19981;&#21516;&#20110;&#34892;&#20026;&#31574;&#30053;&#12290;&#23427;&#22312;&#28508;&#22312;&#25913;&#36827;&#30340;&#28508;&#21147;&#21644;&#22806;&#25512;&#25511;&#21046;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.01829</link><description>&lt;p&gt;
t-SMILES&#65306;&#29992;&#20110;&#20840;&#26032;&#20998;&#23376;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#12289;&#22522;&#20110;&#30862;&#29255;&#30340;&#22810;&#23610;&#24230;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65288;&#22522;&#20110;&#26641;&#30340;SMILES&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#31639;&#27861;&#65306;TSSA&#65288;&#24102;&#26377;&#20849;&#20139;&#21407;&#23376;&#30340;t-SMILES&#65289;&#12289;TSDY&#65288;&#24102;&#26377;&#34394;&#25311;&#21407;&#23376;&#30340;t-SMILES&#65289;&#21644;TSID&#65288;&#24102;&#26377;ID&#30340;t-SMILES&#65289;&#12290;&#23427;&#20351;&#29992;&#20174;&#20998;&#23376;&#22270;&#30340;&#30862;&#29255;&#24418;&#25104;&#30340;&#20840;&#20108;&#21449;&#26641;&#19978;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#24471;&#21040;&#30340;SMILES&#31867;&#22411;&#23383;&#31526;&#20018;&#26469;&#25551;&#36848;&#20998;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;JTVAE&#12289;BRICS&#12289;MMPA&#21644;Scaffold&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#26500;&#24314;&#22810;&#20195;&#30721;&#20998;&#23376;&#25551;&#36848;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#21508;&#31181;&#25551;&#36848;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#35770;&#27169;&#22411;&#26159;&#21407;&#22987;&#30340;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#36824;&#26159;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#12290;&#23427;&#22312;goa&#31561;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;SMILES&#12289;DeepSMILES&#12289;SELFIES&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective representation of molecules is a crucial factor affecting the performance of artificial intelligence models. This study introduces a flexible, fragment-based, multiscale molecular representation framework called t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It describes molecules using SMILES-type strings obtained by performing a breadth-first search on a full binary tree formed from a fragmented molecular graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the feasibility to construct a multi-code molecular description system, where various descriptions complement each other, enhancing the overall performance. Additionally, it exhibits impressive performance on low-resource datasets, whether the model is original, data augmented, or pre-training fine-tuned. It significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline models in goa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;(MORN)&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04873</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition. (arXiv:2212.04873v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;(MORN)&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21407;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36981;&#24490;ProtoNet&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#21407;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25928;&#26524;&#34987;&#24573;&#30053;&#65292;&#20363;&#22914;&#26631;&#31614;&#25991;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;&#65288;MORN&#65289;&#65292;&#23427;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20923;&#32467;&#30340;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#22810;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#22312;&#35270;&#35273;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26102;&#38388;&#20851;&#31995;&#20132;&#21449;&#21464;&#25442;&#22120;(TRX)&#27169;&#22359;&#35745;&#31639;&#35270;&#35273;&#21407;&#22411;&#12290;&#22312;&#25991;&#26412;&#27969;&#31243;&#20013;&#65292;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;(SE)&#27169;&#22359;&#21644;&#19968;&#20010;&#25193;&#24352;&#25805;&#20316;&#26469;&#33719;&#21462;&#25991;&#26412;&#21407;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;(MPE)&#27169;&#22359;&#35745;&#31639;&#26368;&#32456;&#30340;&#22810;&#27169;&#24577;&#21407;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21407;&#22411;&#30456;&#20284;&#24615;&#24046;&#24322;(PRIDE)&#26469;&#35780;&#20272;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for few-shot action recognition mainly fall into the metric learning framework following ProtoNet, which demonstrates the importance of prototypes. Although they achieve relatively good performance, the effect of multimodal information is ignored, e.g. label texts. In this work, we propose a novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic information of label texts as multimodal information to enhance prototypes. A CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain features with good multimodal initialization. Then in the visual flow, visual prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module for example. In the text flow, a semantic-enhanced (SE) module and an inflating operation are used to obtain text prototypes. The final multimodal prototypes are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we define a PRototype SImilarity DiffErence (PRIDE) to evaluate the qu
&lt;/p&gt;</description></item></channel></rss>