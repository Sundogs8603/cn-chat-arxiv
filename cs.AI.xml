<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17169</link><description>&lt;p&gt;
NUMTEMP&#65306;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#24102;&#26377;&#32479;&#35745;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#30340;&#35770;&#28857;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17169
&lt;/p&gt;
&lt;p&gt;
NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#22312;&#25968;&#23383;&#26102;&#20195;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#19987;&#27880;&#20110;&#32500;&#22522;&#30334;&#31185;&#19978;&#30340;&#21512;&#25104;&#35770;&#28857;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#35770;&#28857;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Numtemp&#65292;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22810;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20851;&#27880;&#25968;&#23383;&#35770;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#32479;&#35745;&#21644;&#22810;&#26679;&#21270;&#26041;&#38754;&#30340;&#32454;&#31890;&#24230;&#20803;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#27844;&#38706;&#30340;&#35777;&#25454;&#25910;&#38598;&#12290;&#36825;&#35299;&#20915;&#20102;&#39564;&#35777;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#35770;&#28857;&#22797;&#26434;&#65292;&#24448;&#24448;&#32570;&#20047;&#31934;&#30830;&#20449;&#24687;&#65292;&#36825;&#26159;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#35770;&#28857;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#39564;&#35777;&#25968;&#23383;&#35770;&#28857;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;&#35770;&#28857;&#20998;&#35299;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#25968;&#23383;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#22522;&#32447;&#23454;&#29616;&#20102;58.32&#30340;&#23439;F1&#20998;&#25968;&#12290;&#36825;&#35777;&#26126;&#20102;Numtemp&#30340;&#20851;&#38190;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.08828</link><description>&lt;p&gt;
&#24403;&#35299;&#37322;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#26102;&#65292;&#20154;&#20204;&#20250;&#32473;&#20104;&#20854;&#23646;&#24615;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08828
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#20248;&#31168;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26631;&#24535;&#26159;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#24182;&#37319;&#21462;&#34892;&#21160;&#30340;&#35299;&#37322;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#38656;&#35201;&#31995;&#32479;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#22240;&#26524;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#35748;&#30693;&#31185;&#23398;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#29992;&#25143;&#21487;&#33021;&#26399;&#26395;&#30340;&#35299;&#37322;&#31867;&#22411;&#65292;&#20197;&#21450;&#22312;&#21738;&#31181;&#26684;&#24335;&#19979;&#21576;&#29616;&#36825;&#20123;&#35299;&#37322;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;&#35748;&#30693;&#31185;&#23398;&#35299;&#37322;&#26041;&#38754;&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#30446;&#30340;&#35770;&#65292;&#21363;&#20197;&#36798;&#21040;&#30446;&#30340;&#20026;&#35299;&#37322;&#20915;&#31574;&#30340;&#20542;&#21521;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#20204;&#22914;&#20309;&#20026;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#20135;&#29983;&#35299;&#37322;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#39033;&#35843;&#26597;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;n = 54&#65289;&#35266;&#30475;&#20102;&#36947;&#36335;&#22330;&#26223;&#30340;&#35270;&#39057;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#36710;&#36742;&#30340;&#34892;&#20026;&#29983;&#25104;&#26426;&#26800;&#30340;&#12289;&#21453;&#20107;&#23454;&#30340;&#25110;&#30446;&#30340;&#35770;&#30340;&#35328;&#35821;&#35299;&#37322;&#12290;&#22312;&#31532;&#20108;&#39033;&#35843;&#26597;&#20013;&#65292;&#21478;&#19968;&#32452;&#21442;&#19982;&#32773;&#65288;n = 356&#65289;&#23545;&#36825;&#20123;&#36827;&#34892;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08828v1 Announce Type: cross  Abstract: A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. In the second survey, a different set of participants (n=356) rated these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26725;&#25509;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#19982;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.05593</link><description>&lt;p&gt;
&#24341;&#20837;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#65306;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#32676;&#20307;&#21160;&#24577;&#21644;&#22312;TeNP-&#38142;&#31038;&#20250;&#21160;&#24577;&#27169;&#25311;&#20013;&#26550;&#35774;&#31038;&#20250;&#29616;&#35937;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26725;&#25509;&#37327;&#23376;&#21147;&#23398;&#21407;&#29702;&#19982;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#35770;&#65292;&#23427;&#26550;&#36215;&#20102;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#21407;&#29702;&#19982;&#26448;&#26009;&#30740;&#31350;&#65288;&#22914;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#65289;&#20197;&#21450;&#31038;&#20250;&#31995;&#32479;&#22797;&#26434;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#22312;&#20110;&#30898;&#32435;&#31859;&#39063;&#31890;&#21644;&#30707;&#22696;&#28911;&#30340;&#32467;&#26500;&#29305;&#24449;&#19982;&#31038;&#20250;&#32676;&#20307;&#34892;&#20026;&#27169;&#24335;&#20043;&#38388;&#30340;&#38544;&#21947;&#23545;&#24212;&#12290;&#30898;&#32435;&#31859;&#39063;&#31890;&#20855;&#26377;&#29420;&#29305;&#30340;&#23646;&#24615;&#65292;&#22914;&#25552;&#39640;&#30898;&#38142;&#20013;&#20849;&#20215;&#38190;&#30340;&#24378;&#21270;&#21644;&#24341;&#21457;&#27425;&#29983;&#32467;&#26500;&#30772;&#22351;&#23548;&#33268;&#36825;&#20123;&#38142;&#30340;&#20998;&#31163;&#12290;&#36825;&#31867;&#20284;&#20110;&#31038;&#20250;&#32676;&#20307;&#20869;&#37096;&#20957;&#32858;&#21147;&#21152;&#24378;&#21644;&#19981;&#21516;&#20998;&#32452;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#30707;&#22696;&#28911;&#30340;&#20986;&#33394;&#29305;&#24615;&#65292;&#22914;&#39640;&#30005;&#23548;&#24615;&#12289;&#24378;&#24230;&#21644;&#26580;&#38887;&#24615;&#65292;&#20026;&#29702;&#35299;&#31038;&#20250;&#32676;&#20307;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26356;&#22810;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05593v1 Announce Type: cross  Abstract: This note considers an innovative interdisciplinary methodology that bridges the gap between the fundamental principles of quantum mechanics applied to the study of materials such as tellurium nanoparticles (TeNPs) and graphene and the complex dynamics of social systems. The basis for this approach lies in the metaphorical parallels drawn between the structural features of TeNPs and graphene and the behavioral patterns of social groups in the face of misinformation. TeNPs exhibit unique properties such as the strengthening of covalent bonds within telluric chains and the disruption of secondary structure leading to the separation of these chains. This is analogous to increased cohesion within social groups and disruption of information flow between different subgroups, respectively. . Similarly, the outstanding properties of graphene, such as high electrical conductivity, strength, and flexibility, provide additional aspects for unders
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#23396;&#23376;&#35299;&#26041;&#27861;&#30740;&#31350;&#20102;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;&#21508;&#31867;&#21160;&#32773;&#22312;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#38450;&#27490;&#20551;&#26032;&#38395;&#25193;&#25955;&#12290;</title><link>https://arxiv.org/abs/2403.05585</link><description>&lt;p&gt;
&#24748;&#28014;&#22797;&#20849;&#25391;&#27169;&#22411;&#65306;&#20351;&#29992;&#23396;&#23376;&#35299;&#30740;&#31350;&#31532;&#19977;&#26041;&#24178;&#39044;&#19979;&#38750;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#20551;&#26032;&#38395;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Plasmon Resonance Model: Investigation of Analysis of Fake News Diffusion Model with Third Mover Intervention Using Soliton Solution in Non-Complete Information Game under Repeated Dilemma Condition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#23396;&#23376;&#35299;&#26041;&#27861;&#30740;&#31350;&#20102;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;&#21508;&#31867;&#21160;&#32773;&#22312;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#29702;&#35299;&#21644;&#38450;&#27490;&#20551;&#26032;&#38395;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#30740;&#31350;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#34394;&#20551;&#26032;&#38395;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#26694;&#26550;&#20869;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#34920;&#31034;&#24748;&#28014;&#22797;&#20849;&#25391;&#29616;&#35937;&#65292;&#20854;&#20013;&#34394;&#20551;&#26032;&#38395;&#30340;&#25193;&#25955;&#22312;&#29305;&#23450;&#31038;&#20132;&#32676;&#20307;&#25110;&#36890;&#20449;&#32593;&#32476;&#20869;&#34987;&#36805;&#36895;&#25918;&#22823;&#65292;&#24182;&#36890;&#36807;&#23396;&#23376;&#35299;&#26041;&#27861;&#20998;&#26512;&#20854;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#22836;&#37096;&#21160;&#32773;&#12289;&#27425;&#22836;&#21160;&#32773;&#21644;&#31532;&#19977;&#26041;&#24178;&#39044;&#31574;&#30053;&#22914;&#20309;&#22312;&#36825;&#20010;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#34394;&#20551;&#26032;&#38395;&#30340;&#25193;&#25955;&#36215;&#21040;&#25918;&#22823;&#25110;&#25233;&#21046;&#30340;&#20316;&#29992;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#26426;&#21046;&#65292;&#24182;&#25552;&#20379;&#38450;&#27490;&#25110;&#25171;&#20987;&#20854;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#32467;&#21512;&#31038;&#20250;&#31185;&#23398;&#21644;&#33258;&#28982;&#31185;&#23398;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#35797;&#22270;&#20026;&#24403;&#20170;&#30340;&#20551;&#26032;&#38395;&#38382;&#39064;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05585v1 Announce Type: cross  Abstract: In this research note, we propose a new approach to model the fake news diffusion process within the framework of incomplete information games. In particular, we use nonlinear partial differential equations to represent the phenomenon of plasmon resonance, in which the diffusion of fake news is rapidly amplified within a particular social group or communication network, and analyze its dynamics through a soliton solution approach. In addition, we consider how first mover, second mover, and third mover strategies interact within this nonlinear system and contribute to the amplification or suppression of fake news diffusion. The model aims to understand the mechanisms of fake news proliferation and provide insights into how to prevent or combat it. By combining concepts from the social sciences and the physical sciences, this study attempts to develop a new theoretical framework for the contemporary problem of fake news.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05000</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#34920;&#31034;&#36827;&#34892;&#21307;&#23398;&#35328;&#35821;&#30151;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Medical Speech Symptoms Classification via Disentangled Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 &#20844;&#21578;&#31867;&#22411;:new &#25688;&#35201;: &#22312;&#29616;&#26377;&#24037;&#20316;&#20013;&#65292;&#24847;&#22270;&#34987;&#23450;&#20041;&#29992;&#20110;&#29702;&#35299;&#21475;&#22836;&#35821;&#35328;&#12290;&#21307;&#23398;&#35328;&#35821;&#20013;&#28041;&#21450;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;&#22768;&#23398;&#29305;&#24449;&#22343;&#21253;&#21547;&#24847;&#22270;&#65292;&#36825;&#23545;&#20110;&#30151;&#29366;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290; &#36890;&#36807;&#24847;&#22270;&#32534;&#30721;&#22120;&#25552;&#21462;&#25991;&#26412;&#22495;&#21644;Mel-&#39057;&#35889;&#22270;&#22495;&#30340;&#24847;&#22270;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#20004;&#20010;&#20132;&#25442;&#33719;&#21462;&#37325;&#26500;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;Mel-&#39057;&#35889;&#22270;&#29305;&#24449;&#12290;&#22312;&#23558;&#20004;&#20010;&#22495;&#30340;&#24847;&#22270;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#34920;&#31034;&#21518;&#65292;&#32508;&#21512;&#24847;&#22270;&#34920;&#31034;&#34987;&#36755;&#20837;&#20915;&#31574;&#23618;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#33719;&#24471;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 Announce Type: new  Abstract: Intent is defined for understanding spoken language in existing works. Both textual features and acoustic features involved in medical speech contain intent, which is important for symptomatic diagnosis. In this paper, we propose a medical speech classification model named DRSC that automatically learns to disentangle intent and content representations from textual-acoustic data for classification. The intent representations of the text domain and the Mel-spectrogram domain are extracted via intent encoders, and then the reconstructed text feature and the Mel-spectrogram feature are obtained through two exchanges. After combining the intent from two domains into a joint representation, the integrated intent representation is fed into a decision layer for classification. Experimental results show that our model obtains an average accuracy rate of 95% in detecting 25 different medical symptoms.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20013;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#29702;&#35299;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#23545;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03239</link><description>&lt;p&gt;
&#27880;&#24847;&#65306;&#21033;&#29992;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#25968;&#23383;&#39046;&#22495;&#30340;&#24067;&#40065;&#26031;&#29305;&#35282;&#21644;&#24503;&#40065;&#24503;&#27169;&#22411;&#20013;&#31561;&#31163;&#28608;&#20803;&#20849;&#25391;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#21560;&#38468;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#30340;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon Resonance, in the Context of Brewster's Angle and the Drude Model for Fake News Adsorption in Incomplete Information Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20013;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#29702;&#35299;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#30740;&#31350;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#23545;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31508;&#35760;&#25506;&#35752;&#20102;&#23396;&#23376;&#29702;&#35770;&#21644;&#31561;&#31163;&#23376;&#29616;&#35937;&#22312;&#24314;&#27169;&#25968;&#23383;&#20581;&#24247;&#24179;&#21488;&#20869;&#29992;&#25143;&#34892;&#20026;&#21644;&#21442;&#19982;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#23396;&#23376;&#35299;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35299;&#38543;&#26102;&#38388;&#31283;&#23450;&#30340;&#20581;&#24247;&#25913;&#21892;&#34892;&#20026;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#21450;&#20854;&#22312;&#21560;&#38468;&#20551;&#26032;&#38395;&#20013;&#30340;&#31561;&#31163;&#23376;&#29305;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#29992;&#25143;&#20114;&#21160;&#21644;&#21442;&#19982;&#27700;&#24179;&#12290;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#19982;&#30802;&#21270;&#38217;&#32435;&#31859;&#39063;&#31890;&#30340;&#29420;&#29305;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#65292;&#20197;&#20102;&#35299;&#25968;&#23383;&#20581;&#24247;&#29615;&#22659;&#20013;&#29992;&#25143;&#21442;&#19982;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#23396;&#23376;&#29702;&#35770;&#22312;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#21160;&#24577;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#32780;&#31561;&#31163;&#23376;&#29616;&#35937;&#30340;&#24212;&#29992;&#20026;&#25552;&#39640;&#28789;&#25935;&#24230;&#21644;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03239v1 Announce Type: cross  Abstract: This note explores the innovative application of soliton theory and plasmonic phenomena in modeling user behavior and engagement within digital health platforms. By introducing the concept of soliton solutions, we present a novel approach to understanding stable patterns of health improvement behaviors over time. Additionally, we delve into the role of tellurium nanoparticles and their plasmonic properties in adsorbing fake news, thereby influencing user interactions and engagement levels. Through a theoretical framework that combines nonlinear dynamics with the unique characteristics of tellurium nanoparticles, we aim to provide new insights into the dynamics of user engagement in digital health environments. Our analysis highlights the potential of soliton theory in capturing the complex, nonlinear dynamics of user behavior, while the application of plasmonic phenomena offers a promising avenue for enhancing the sensitivity and effec
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18002</link><description>&lt;p&gt;
&#32771;&#34385;&#23545;&#31216;&#24615;&#30340;&#36719;&#33109;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#26426;&#22120;&#20154;&#35013;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#36719;&#33109;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#20195;&#34920;&#24615;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23500;&#25509;&#35302;PEG-IN-HOLE&#20219;&#21153;&#65292;&#35813;&#36719;&#33109;&#21487;&#20197;&#27604;&#21018;&#24615;&#33109;&#37096;&#26356;&#23433;&#20840;&#22320;&#25805;&#20316;&#24182;&#23481;&#24525;&#36739;&#20302;&#39057;&#29575;&#30340;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#20844;&#24335;&#19981;&#21516;&#65292;&#35813;&#20844;&#24335;&#38656;&#35201;&#22806;&#37096;&#35774;&#32622;&#25110;&#20272;&#35745;&#22120;&#26469;&#33719;&#21462;PEG-TO-HOLE&#23039;&#24577;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#20844;&#24335;&#21644;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#23436;&#20840;&#22522;&#20110;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#20449;&#21495;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#34701;&#21512;&#28508;&#22312;&#39046;&#22495;&#23545;&#31216;&#24615;&#65292;&#22240;&#27492;&#24517;&#39035;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#24182;&#26500;&#24314;&#36741;&#21161;&#25439;&#22833;&#26469;&#24378;&#36843;&#20195;&#29702;&#36981;&#23432;&#23545;&#31216;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#23545;&#31216;PEG&#24418;&#29366;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#21487;&#20197;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18002v1 Announce Type: cross  Abstract: This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to 
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>PANDAS&#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#34920;&#31034;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#30784;&#31867;&#21035;&#30340;&#22522;&#30784;&#19978;&#26816;&#27979;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.17420</link><description>&lt;p&gt;
PANDAS: &#22522;&#20110;&#21407;&#22411;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PANDAS: Prototype-based Novel Class Discovery and Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17420
&lt;/p&gt;
&lt;p&gt;
PANDAS&#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#34920;&#31034;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#30784;&#31867;&#21035;&#30340;&#22522;&#30784;&#19978;&#26816;&#27979;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#22312;&#22266;&#23450;&#30340;&#19968;&#32452;&#31867;&#21035;&#19978;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23553;&#38381;&#19990;&#30028;&#30340;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#20026;&#22312;&#26816;&#27979;&#22120;&#37096;&#32626;&#22312;&#37326;&#22806;&#21518;&#65292;&#26032;&#30340;&#31867;&#21035;&#24517;&#28982;&#20250;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#23637;&#35757;&#32451;&#20026;&#19968;&#32452;&#22522;&#30784;&#31867;&#21035;&#30340;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815; i) &#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#23384;&#22312;&#65292;&#24182; ii) &#33258;&#21160;&#20016;&#23500;&#20854;&#24211;&#20197;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#20197;&#21450;&#22522;&#30784;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; PANDAS&#65292;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#23427;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#26469;&#34920;&#31034;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#22120;&#20351;&#29992;&#36825;&#20123;&#21407;&#22411;&#20026;&#27599;&#20010;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#23454;&#20363;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;VOC 2012&#21644;COCO-to-LVIS&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;PANDAS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17420v1 Announce Type: cross  Abstract: Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12147</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35268;&#27169;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
End-to-end multilingual fact-checking at scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Factiverse AI&#27169;&#22411;&#22312;100&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#24615;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;GPT-4&#12289;GPT-3.5-Turbo&#21644;Mistral-7b&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.10142</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Changing Probabilities via Dynamic Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#39044;&#27979;&#22120;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#31163;&#25955;&#39033;&#30446;&#12290;&#39044;&#27979;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#27010;&#29575;&#22810;&#31867;&#21035;&#39044;&#27979;&#65292;&#21363;&#36890;&#36807;&#36755;&#20986;&#26377;&#38646;&#20010;&#25110;&#22810;&#20010;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#25509;&#19979;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#39033;&#30446;&#65292;&#28982;&#21518;&#25581;&#31034;&#23454;&#38469;&#39033;&#30446;&#24182;&#20174;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36755;&#20986;&#27010;&#29575;&#65292;&#39044;&#27979;&#22120;&#20250;&#36319;&#36394;&#20854;&#25152;&#35265;&#39033;&#30446;&#30340;&#27604;&#20363;&#12290;&#39044;&#27979;&#22120;&#20855;&#26377;&#24658;&#23450;&#65288;&#26377;&#38480;&#65289;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#23547;&#27714;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#26356;&#26032;&#25216;&#26415;&#65306;&#27969;&#26159;&#26080;&#30028;&#30340;&#65292;&#39033;&#30446;&#30340;&#38598;&#21512;&#23545;&#39044;&#27979;&#22120;&#26159;&#26410;&#30693;&#30340;&#65292;&#23427;&#20204;&#30340;&#24635;&#25968;&#20063;&#21487;&#33021;&#26080;&#38480;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#65306;&#39033;&#30446;&#30340;&#28508;&#22312;&#39057;&#29575;&#21487;&#33021;&#20250;&#19981;&#26102;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#26032;&#39033;&#30446;&#21487;&#33021;&#24320;&#22987;&#20986;&#29616;&#65292;&#19968;&#20123;&#24403;&#21069;&#39057;&#32321;&#20986;&#29616;&#30340;&#39033;&#30446;&#21487;&#33021;&#20877;&#27425;&#20572;&#27490;&#20986;&#29616;&#12290;&#30001;&#20110;&#26377;&#31354;&#38388;&#38480;&#21046;&#65292;&#39044;&#27979;&#22120;&#21482;&#38656;&#35201;&#25552;&#20379;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#20107;&#25925;&#30340;&#35299;&#21078;&#65306;&#20174;Cruise&#34892;&#20154;&#25302;&#25341;&#20107;&#25925;&#20013;&#21560;&#21462;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#22312;&#26087;&#37329;&#23665;&#65292;&#19968;&#36742;&#36890;&#29992;&#27773;&#36710;Cruise&#30340;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#19968;&#21517;&#34892;&#20154;&#30456;&#25758;&#65292;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#20260;&#23475;&#65292;&#21516;&#26102;&#20063;&#23545;&#35813;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#36825;&#24456;&#21487;&#33021;&#20250;&#23545;&#25972;&#20010;&#34892;&#19994;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#38382;&#39064;&#19981;&#20165;&#20165;&#28304;&#20110;&#20107;&#25925;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;Cruise&#22312;&#22788;&#29702;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#25758;&#21040;&#34892;&#20154;&#21518;&#34987;&#25302;&#34892;&#30340;&#36807;&#31243;&#20013;&#30340;&#22833;&#35823;&#12290;&#20004;&#20221;&#22806;&#37096;&#35843;&#26597;&#25253;&#21578;&#25552;&#20379;&#20102;&#25551;&#36848;&#20107;&#20214;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#24182;&#20174;&#30417;&#31649;&#20114;&#21160;&#30340;&#35282;&#24230;&#25209;&#35780;&#20102;&#20844;&#21496;&#30340;&#21453;&#24212;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#28508;&#22312;&#30340;&#23433;&#20840;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25253;&#21578;&#26448;&#26009;&#26469;&#24378;&#35843;&#20855;&#20307;&#30340;&#20107;&#23454;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#23558;&#25253;&#21578;&#26448;&#26009;&#30340;&#19981;&#21516;&#37096;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#23545;&#20107;&#20214;&#30340;&#21453;&#24212;&#26041;&#38754;&#21487;&#33021;&#21487;&#20197;&#23398;&#21040;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. The issues stem not just from the crash facts themselves, but also how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. A pair of external investigation reports provide raw material describing the incident and critique the company response from a regulatory interaction point of view, but did not include potential safety recommendations in scope. We use that report material to highlight specific facts and relationships between events by tying together different pieces of the report material. We then explore safety lessons that might be learned with regard to technology, operational safety practices, and organizational reaction to incidents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.03305</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#35821;&#20041;&#26377;&#24847;&#20041;&#21644;&#39640;&#25928;&#30340;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#26465;&#20214;DDPMs&#23398;&#20064;&#29983;&#25104;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#30340;&#20851;&#38190;&#65292;&#20135;&#29983;&#20102;&#19982;&#19981;&#21516;&#38454;&#27573;&#23545;&#24212;&#30340; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#19981;&#23547;&#24120;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20363;&#22914;&#23431;&#33322;&#21592;&#39569;&#22312;&#26376;&#29699;&#19978;&#30340;&#39532;&#65292;&#24182;&#19988;&#26377;&#27491;&#30830;&#30340;&#38452;&#24433;&#12290;&#36825;&#20123;&#36755;&#20986;&#34920;&#26126;&#20102;&#27169;&#22411;&#20855;&#26377;&#32452;&#21512;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#27169;&#22411;&#26159;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#30340;&#21602;&#65311;&#25105;&#20204;&#22312;&#26465;&#20214;DDPMs&#19978;&#36827;&#34892;&#20102;&#25511;&#21046;&#23454;&#39564;&#65292;&#23398;&#20064;&#29983;&#25104;&#20197;&#25351;&#23450;&#30340;$x$&#21644;$y$&#20301;&#32622;&#20026;&#20013;&#24515;&#30340;2D&#29699;&#24418;&#39640;&#26031;&#20984;&#36215;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#23545;&#20110;&#23454;&#29616;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#34920;&#31034;&#38454;&#27573;&#65306;(A&#38454;&#27573;)&#27809;&#26377;&#28508;&#22312;&#32467;&#26500;&#65292;(B&#38454;&#27573;)&#19968;&#20010;&#28151;&#20081;&#29366;&#24577;&#30340;2D&#27969;&#24418;&#65292;&#20197;&#21450;(C&#38454;&#27573;)&#19968;&#20010;&#26377;&#24207;&#30340;2D&#27969;&#24418;&#12290;&#23545;&#24212;&#20110;&#36825;&#20123;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102; qualitatively &#19981;&#21516;&#30340;&#29983;&#25104;&#34892;&#20026;&#65306;1&#65289;&#29983;&#25104;&#22810;&#20010;&#20984;&#36215;&#65292;2&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#65292;&#20294;$x$&#21644;$y$&#20301;&#32622;&#19981;&#20934;&#30830;&#65292;3&#65289;&#29983;&#25104;&#19968;&#20010;&#20984;&#36215;&#19988;&#20301;&#32622;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;SMT&#31574;&#30053;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20102;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#65292;&#20197;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#21457;&#29616;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.17159</link><description>&lt;p&gt;
&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#29992;&#20110;SMT&#31574;&#30053;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;SMT&#31574;&#30053;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20102;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#65292;&#20197;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#21457;&#29616;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;SMT&#27714;&#35299;&#22120;&#65288;&#20363;&#22914;Z3&#65289;&#25552;&#20379;&#29992;&#25143;&#21487;&#25511;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#23454;&#20363;&#38598;&#23450;&#21046;&#27714;&#35299;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27714;&#35299;&#22120;&#22312;&#29305;&#23450;&#29992;&#20363;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#23450;&#21046;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#20026;&#19968;&#31867;SMT&#23454;&#20363;&#25163;&#24037;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#30340;&#31574;&#30053;&#20173;&#28982;&#26159;&#27714;&#35299;&#22120;&#24320;&#21457;&#20154;&#21592;&#21644;&#29992;&#25143;&#38754;&#20020;&#30340;&#22797;&#26434;&#21644;&#33392;&#24040;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#33258;&#21160;&#36827;&#34892;SMT&#31574;&#30053;&#21512;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31574;&#30053;&#21512;&#25104;&#35270;&#20026;&#19968;&#31181;&#24207;&#21015;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#25628;&#32034;&#26641;&#23545;&#24212;&#20110;&#31574;&#30053;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;MCTS&#26469;&#36941;&#21382;&#36825;&#20010;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#25214;&#21040;&#26377;&#25928;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#30340;&#29702;&#24565;&#12290;&#36825;&#20123;&#26032;&#39062;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#31574;&#30053;&#31354;&#38388;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#26356;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.   In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decision-making process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enablin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16043</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#32447;&#24615;&#20998;&#31867;&#30340;&#25193;&#23637;&#38750;&#23545;&#31216;sigmoid&#21644;&#24863;&#30693;&#26426;(SIGTRON)
&lt;/p&gt;
&lt;p&gt;
An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;(SIGTRON)&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#20854;&#20276;&#38543;&#30340;SIC&#27169;&#22411;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#21442;&#25968;&#21270;sigmoid&#20989;&#25968;&#65292;&#31216;&#20026;SIGTRON&#65292;&#23427;&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#38750;&#23545;&#31216;sigmoid&#20989;&#25968;&#21644;&#24863;&#30693;&#26426;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#23427;&#30340;&#20276;&#38543;&#20984;&#27169;&#22411;SIGTRON-&#19981;&#24179;&#34913;&#20998;&#31867;(SIC)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#34394;&#25311;SIGTRON&#20135;&#29983;&#30340;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;$\pi$-&#21152;&#26435;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;SIC&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#27809;&#26377;&#22806;&#37096;&#30340;$\pi$-&#26435;&#37325;&#65292;&#32780;&#26159;&#22312;&#34394;&#25311;&#30340;SIGTRON&#20135;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#26377;&#20869;&#37096;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#24403;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25509;&#36817;&#33391;&#22909;&#24179;&#34913;&#30340;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SIC&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26356;&#21152;&#36866;&#24212;&#65292;&#27604;&#22914;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#27604;&#20363;&#19981;&#24179;&#34913;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#36866;&#24212;&#26159;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20542;&#26012;&#30340;&#36229;&#24179;&#38754;&#26041;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25311;&#29275;&#39039;&#20248;&#21270;(L-BFGS)&#26694;&#26550;&#30340;&#34394;&#25311;&#20984;&#25439;&#22833;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#20108;&#20998;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new polynomial parameterized sigmoid called SIGTRON, which is an extended asymmetric sigmoid with Perceptron, and its companion convex model called SIGTRON-imbalanced classification (SIC) model that employs a virtual SIGTRON-induced convex loss function. In contrast to the conventional $\pi$-weighted cost-sensitive learning model, the SIC model does not have an external $\pi$-weight on the loss function but has internal parameters in the virtual SIGTRON-induced loss function. As a consequence, when the given training dataset is close to the well-balanced condition, we show that the proposed SIC model is more adaptive to variations of the dataset, such as the inconsistency of the scale-class-imbalance ratio between the training and test datasets. This adaptation is achieved by creating a skewed hyperplane equation. Additionally, we present a quasi-Newton optimization(L-BFGS) framework for the virtual convex loss by developing an interval-based bisection line sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.14831</link><description>&lt;p&gt;
&#26426;&#22120;&#35270;&#35273;&#20912;&#23665;&#30340;&#35299;&#37322;&#65306;&#36890;&#36807;&#32771;&#34385;&#20840;&#38754;&#29615;&#22659;&#26465;&#20214;&#25512;&#36827;&#21160;&#24577;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#35270;&#35273;&#27979;&#35797;&#26159;&#21542;&#20250;&#24102;&#26469;&#20912;&#23665;&#30340;&#21361;&#38505;&#65311;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#35270;&#35273;&#65288;MV&#65289;&#27979;&#35797;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#39640;&#24230;&#33258;&#21160;&#39550;&#39542;&#65288;HAD&#65289;&#31995;&#32479;&#20013;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#20511;&#21161;&#21521;&#20912;&#23665;&#33322;&#34892;&#30340;&#38544;&#21947;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#27979;&#35797;&#31574;&#30053;&#20013;&#28508;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#22914;&#20309;&#22788;&#29702;MV&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19981;&#36879;&#26126;&#21151;&#33021;&#30340;&#26356;&#28145;&#20837;&#20102;&#35299;&#30340;&#32039;&#36843;&#38656;&#35201;&#65292;&#22240;&#20026;&#24573;&#35270;&#20102;&#36825;&#20123;&#32771;&#34385;&#21487;&#33021;&#20250;&#36896;&#25104;&#29983;&#21629;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31890;&#24230;&#31561;&#32423;&#12290;&#35813;&#27169;&#22411;&#40723;&#21169;&#23545;MV&#25805;&#20316;&#29615;&#22659;&#26465;&#20214;&#30340;&#21508;&#20010;&#23618;&#27425;&#36827;&#34892;&#31934;&#32454;&#25506;&#32034;&#65292;&#20174;&#20010;&#20307;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21040;&#25972;&#20010;&#29615;&#22659;&#22330;&#26223;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#21487;&#33021;&#24433;&#21709;MV&#21151;&#33021;&#30340;&#25152;&#26377;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13639</link><description>&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#65306;&#23398;&#20064;&#29992;&#25143;&#21453;&#39304;&#32780;&#26080;&#38656;RL&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Learning: Learning from Human Feedback without RL. (arXiv:2310.13639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13639
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#30340;RLHF&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#40784;&#27169;&#22411;&#12290;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#26159;&#26681;&#25454;&#22870;&#21169;&#20998;&#24067;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23427;&#20204;&#36981;&#24490;&#29992;&#25143;&#26368;&#20339;&#31574;&#30053;&#19979;&#30340;&#36951;&#25022;&#12290;&#22240;&#27492;&#65292;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19981;&#20165;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#38169;&#35823;&#20551;&#35774;&#65292;&#36824;&#23548;&#33268;&#20102;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#25110;RL&#38454;&#27573;&#30340;&#33258;&#21161;&#27861;&#24341;&#36215;&#30340;&#26840;&#25163;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;&#36825;&#20123;&#20248;&#21270;&#25361;&#25112;&#65292;&#24403;&#20195;&#30340;RLHF&#26041;&#27861;&#38480;&#21046;&#33258;&#24049;&#21482;&#33021;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#25110;&#38480;&#21046;&#20102;&#35266;&#27979;&#32500;&#24230;&#65288;&#22914;&#22522;&#20110;&#29366;&#24577;&#30340;&#26426;&#22120;&#20154;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new famil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#23545;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#19981;&#21516;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#21767;&#35835;&#29305;&#24449;&#65292;&#25552;&#39640;&#21767;&#35835;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05058</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading. (arXiv:2310.05058v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#23545;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#19981;&#21516;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#21767;&#35835;&#29305;&#24449;&#65292;&#25552;&#39640;&#21767;&#35835;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#20010;&#20307;&#30340;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#20182;/&#22905;&#30340;&#20960;&#24352;&#38754;&#37096;&#22270;&#20687;&#65292;&#29978;&#33267;&#26159;&#19968;&#24352;&#27973;&#23618;&#32593;&#32476;&#30340;&#21333;&#19968;&#22270;&#20687;&#26469;&#20934;&#30830;&#25551;&#32472;&#65292;&#32780;&#19982;&#35762;&#35805;&#38754;&#37096;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#29305;&#24449;&#21017;&#38656;&#35201;&#28145;&#23618;&#24207;&#21015;&#32593;&#32476;&#26469;&#20934;&#30830;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27973;&#23618;&#21644;&#28145;&#23618;&#20998;&#21035;&#22788;&#29702;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#21767;&#35835;&#24773;&#22659;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#20010;&#20307;&#29420;&#29305;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#31361;&#20986;&#30340;&#21475;&#33108;&#21644;&#19979;&#39052;&#65289;&#23545;&#20110;&#19981;&#21516;&#30340;&#21333;&#35789;&#21644;&#21457;&#38899;&#30340;&#21767;&#35835;&#34920;&#29616;&#20855;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#29305;&#24449;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#21767;&#35835;&#12290;&#22522;&#20110;&#36825;&#20004;&#28857;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#21160;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#21487;&#20998;&#31163;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for speaker adaptation in lip reading, motivated by two observations. Firstly, a speaker's own characteristics can always be portrayed well by his/her few facial images or even a single image with shallow networks, while the fine-grained dynamic features associated with speech content expressed by the talking face always need deep sequential networks to represent accurately. Therefore, we treat the shallow and deep layers differently for speaker adaptive lip reading. Secondly, we observe that a speaker's unique characteristics ( e.g. prominent oral cavity and mandible) have varied effects on lip reading performance for different words and pronunciations, necessitating adaptive enhancement or suppression of the features for robust lip reading. Based on these two observations, we propose to take advantage of the speaker's own characteristics to automatically learn separable hidden unit contributions with different targets for shallow layers and de
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.03718</link><description>&lt;p&gt;
&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#21644;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#27169;&#22359;&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19987;&#27880;&#20110;&#35757;&#32451;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#32422;&#26463;&#26465;&#20214;&#19979;&#33021;&#22815;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#23433;&#20840;&#32422;&#26463;&#35201;&#27714;&#19988;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#22810;&#21151;&#33021;&#23433;&#20840;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#20026;&#26410;&#24320;&#21457;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#20010;&#20027;&#35201;&#38656;&#27714;&#65306;&#35757;&#32451;&#25928;&#29575;&#21644;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conditioned Constrained Policy Optimization&#65288;CCPO&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#65288;1&#65289;&#22810;&#21151;&#33021;&#20540;&#20272;&#35745;&#65288;VVE&#65289;&#65292;&#29992;&#20110;&#22312;&#26410;&#35265;&#36807;&#30340;&#38408;&#20540;&#26465;&#20214;&#19979;&#36817;&#20284;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#65288;2&#65289;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#25512;&#29702;&#65288;CVI&#65289;&#65292;&#29992;&#20110;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#32534;&#30721;&#20219;&#24847;&#32422;&#26463;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CCPO&#22312;&#23433;&#20840;&#21644;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#19981;&#21516;&#32422;&#26463;&#30340;&#38646;-shot&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
&lt;/p&gt;</description></item><item><title>AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17288</link><description>&lt;p&gt;
AutoAgents: &#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26234;&#33021;&#20195;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoAgents: A Framework for Automatic Agent Generation. (arXiv:2309.17288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17288
&lt;/p&gt;
&lt;p&gt;
AutoAgents&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#33258;&#21160;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;AI&#22242;&#38431;&#65292;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#20219;&#21153;&#35299;&#20915;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20195;&#29702;&#26469;&#22788;&#29702;&#31616;&#21333;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoAgents&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#33258;&#36866;&#24212;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#20197;&#26500;&#24314;AI&#22242;&#38431;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoAgents&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;&#25152;&#38656;&#20195;&#29702;&#26469;&#32806;&#21512;&#20219;&#21153;&#21644;&#35282;&#33394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#26681;&#25454;&#29983;&#25104;&#30340;&#19987;&#23478;&#20195;&#29702;&#20026;&#24403;&#21069;&#20219;&#21153;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#30456;&#20114;&#21327;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#35266;&#23519;&#32773;&#35282;&#33394;&#34987;&#32435;&#20837;&#26694;&#26550;&#20013;&#20197;&#21453;&#24605;&#25351;&#23450;&#30340;&#35745;&#21010;&#21644;&#20195;&#29702;&#30340;&#21709;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AutoAgents&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#20219;&#21153;&#36866;&#24212;&#24615;&#29983;&#25104;&#21644;&#21327;&#35843;&#22810;&#20010;&#19987;&#19994;&#20195;&#29702;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that Au
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#30495;&#23454;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;ConvNets&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#24182;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.12106</link><description>&lt;p&gt;
&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#21450;&#20854;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#30340;ConvNets&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Human labeling errors and their impact on ConvNets for satellite image scene classification. (arXiv:2305.12106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#35823;&#24046;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#30495;&#23454;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#36825;&#20123;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;ConvNets&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#24182;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNets)&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#12290;&#23545;&#20110;ConvNets&#25191;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#20154;&#24037;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21355;&#26143;&#22270;&#20687;&#30340;&#22797;&#26434;&#24615;&#65292;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20013;&#30340;&#35823;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#22312;&#21355;&#26143;&#22270;&#20687;&#19978;&#30340;&#20998;&#24067;&#21450;&#20854;&#23545;ConvNets&#30340;&#24433;&#21709;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#25910;&#38598;&#20102;&#26469;&#33258;32&#20301;&#21442;&#19982;&#32773;&#30340;&#30495;&#23454;&#26631;&#31614;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#35823;&#24046;&#22914;&#20309;&#24433;&#21709;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#19977;&#20010;ConvNets&#65288;VGG16&#65292;GoogleNet&#21644;ResNet-50&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#20855;&#26377;&#26174;&#30528;&#30340;&#31867;&#21644;&#23454;&#20363;&#20381;&#36182;&#24615;&#65292;&#36825;&#19982;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#27169;&#25311;&#22122;&#22768;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#65288;2&#65289;&#20851;&#20110;&#25152;&#26377;&#31867;&#30340;&#24635;&#20307;&#31934;&#24230;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#20154;&#24037;&#26631;&#27880;&#35823;&#24046;&#22686;&#21152;1&#20010;&#21333;&#20301;&#26102;&#65292;ConvNets&#30340;&#24635;&#20307;&#31934;&#24230;&#20250;&#38477;&#20302;0.67%~1.02%&#65307;&#65288;3&#65289;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#23545;&#19981;&#21516;CNN&#32467;&#26500;&#30340;&#24433;&#21709;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#20180;&#32454;&#26816;&#26597;&#21355;&#26143;&#22270;&#20687;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20154;&#31867;&#26631;&#27880;&#35823;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21628;&#21505;&#24320;&#21457;&#24378;&#22823;&#30340;ConvNets&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (ConvNets) have been successfully applied to satellite image scene classification. Human-labeled training datasets are essential for ConvNets to perform accurate classification. Errors in human-labeled training datasets are unavoidable due to the complexity of satellite images. However, the distribution of human labeling errors on satellite images and their impact on ConvNets have not been investigated. To fill this research gap, this study, for the first time, collected real-world labels from 32 participants and explored how their errors affect three ConvNets (VGG16, GoogleNet and ResNet-50) for high-resolution satellite image scene classification. We found that: (1) human labeling errors have significant class and instance dependence, which is fundamentally different from the simulation noise in previous studies; (2) regarding the overall accuracy of all classes, when human labeling errors in training data increase by one unit, the overall accuracy of Co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06221</link><description>&lt;p&gt;
&#24102;&#28145;&#24230;&#21010;&#20998;&#30340;&#22810;&#25552;&#31034;&#27169;&#24577;&#20132;&#21449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Prompt with Depth Partitioned Cross-Modal Learning. (arXiv:2305.06221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#24494;&#35843;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23436;&#25104;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#26631;&#35760;&#19982;&#31867;&#21035;&#26631;&#35760;&#32452;&#21512;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#21442;&#25968;&#34987;&#20923;&#32467;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#20351;&#29992;&#21333;&#19968;&#25552;&#31034;&#26469;&#25551;&#36848;&#31867;&#21035;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#31867;&#21035;&#30340;&#22810;&#26679;&#23646;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#21487;&#23398;&#20064;&#25552;&#31034;&#25193;&#23637;&#21040;&#22810;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#28145;&#24230;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#36830;&#25509;&#21040;&#20998;&#31163;&#30340;&#35270;&#35273;&#28145;&#24230;&#19978;&#65292;&#20351;&#19981;&#21516;&#25552;&#31034;&#33021;&#22815;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#23618;&#27425;&#19978;&#19979;&#25991;&#28145;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22810;&#25552;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#21644;&#21487;&#23398;&#20064;&#30340;&#22810;&#25552;&#31034;&#30340;&#20808;&#39564;&#20449;&#24687;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabiliti
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2207.10170</link><description>&lt;p&gt;
&#24187;&#35273;&#25915;&#20987;&#65306;&#23545;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#20013;&#21487;&#26816;&#27979;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10170
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#33258;&#20027;&#20195;&#29702;&#38656;&#35201;&#23545;&#24863;&#23448;&#36755;&#20837;&#30340;&#25932;&#23545;&#25915;&#20987;&#20855;&#22791;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#24378;&#21270;&#20195;&#29702;&#31574;&#30053;&#38656;&#35201;&#39044;&#27979;&#21487;&#33021;&#30340;&#26368;&#24378;&#25915;&#20987;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35266;&#27979;&#31354;&#38388;&#25915;&#20987;&#20855;&#26377;&#20849;&#21516;&#30340;&#24369;&#28857;&#65306;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#25163;&#27573;&#25110;&#20154;&#24037;&#26816;&#26597;&#26469;&#26816;&#27979;&#12290;&#23545;&#20110;&#25932;&#25163;&#26469;&#35828;&#65292;&#21487;&#26816;&#27979;&#24615;&#26159;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#24341;&#21457;&#23433;&#20840;&#20107;&#24577;&#21319;&#32423;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#32654;&#30340;&#24187;&#35273;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#21152;&#28789;&#27963;&#30340;R-attack&#65292;&#20854;&#29983;&#25104;&#30340;&#35266;&#27979;&#36716;&#25442;&#19982;&#26080;&#25932;&#23545;&#29615;&#22659;&#30340;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#19968;&#33268;&#19988;&#21487;&#20197;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#25915;&#20987;&#30456;&#27604;&#65292;R-attack&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, 
&lt;/p&gt;</description></item></channel></rss>