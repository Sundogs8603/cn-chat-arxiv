<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>DeepSeek-VL&#26159;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#12289;&#30495;&#23454;&#22330;&#26223;&#35206;&#30422;&#21644;&#39640;&#25928;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;</title><link>https://arxiv.org/abs/2403.05525</link><description>&lt;p&gt;
DeepSeek-VL:&#36208;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-VL: Towards Real-World Vision-Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05525
&lt;/p&gt;
&lt;p&gt;
DeepSeek-VL&#26159;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#12289;&#30495;&#23454;&#22330;&#26223;&#35206;&#30422;&#21644;&#39640;&#25928;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DeepSeek-VL&#65292;&#19968;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#24320;&#28304;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#23637;&#24320;&#65306;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#21270;&#12289;&#21487;&#25193;&#23637;&#24615;&#24378;&#65292;&#24182;&#24191;&#27867;&#28085;&#30422;&#21253;&#25324;&#32593;&#32476;&#25130;&#22270;&#12289;PDF&#12289;OCR&#12289;&#22270;&#34920;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#20869;&#23481;&#22312;&#20869;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#20197;&#20840;&#38754;&#34920;&#24449;&#23454;&#38469;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#29992;&#25143;&#22330;&#26223;&#21019;&#24314;&#20102;&#29992;&#20363;&#20998;&#31867;&#27861;&#65292;&#24182;&#30456;&#24212;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#32771;&#34385;&#21040;&#25928;&#29575;&#21644;&#22823;&#22810;&#25968;&#30495;&#23454;&#22330;&#26223;&#30340;&#38656;&#27714;&#65292;DeepSeek-VL&#25972;&#21512;&#20102;&#19968;&#20010;&#28151;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65288;1024 x 1024&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#23545;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05525v1 Announce Type: new  Abstract: We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions:   We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's abilit
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.05518</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#22686;&#24378;&#19968;&#33268;&#24615;&#35757;&#32451;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05518
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CoT&#65289;&#26377;&#28508;&#21147;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#31995;&#32479;&#24615;&#22320;&#27498;&#26354;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#22240;&#32032;--&#27604;&#22914;&#65292;&#21512;&#29702;&#21270;&#31572;&#26696;&#20197;&#31526;&#21512;&#29992;&#25143;&#24847;&#35265;&#32780;&#19981;&#25552;&#21450;&#27492;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#20559;&#35265;&#25512;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20559;&#24046;&#22686;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#65288;BCT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26041;&#26696;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#20559;&#32622;&#29305;&#24449;&#30340;&#25552;&#31034;&#19979;&#36827;&#34892;&#19968;&#33268;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#21333;&#20803;&#65292;&#38024;&#23545;&#19971;&#20010;&#38382;&#31572;&#20219;&#21153;&#27979;&#35797;&#20102;&#20061;&#31181;&#24418;&#24335;&#30340;&#26377;&#20559;&#25512;&#29702;&#65292;&#21457;&#29616;&#23558;BCT&#24212;&#29992;&#20110;&#24102;&#26377;&#19968;&#31181;&#20559;&#35265;&#30340;GPT-3.5-Turbo&#21487;&#20197;&#23558;&#26377;&#20559;&#25512;&#29702;&#30340;&#27604;&#20363;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#38477;&#20302;86%&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#24418;&#24335;&#30340;&#20559;&#35265;&#65292;&#24179;&#22343;&#23558;&#26410;&#30693;&#20559;&#35265;&#19978;&#30340;&#26377;&#20559;&#25512;&#29702;&#20943;&#23569;&#20102;37%&#12290;&#30001;&#20110;BCT&#23558;&#26410;&#30693;&#20559;&#35265;&#27867;&#21270;&#24182;&#19988;&#19981;&#38656;&#35201;&#37329;&#26631;&#31614;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05518v1 Announce Type: cross  Abstract: While chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning, it can systematically misrepresent the factors influencing models' behavior--for example, rationalizing answers in line with a user's opinion without mentioning this bias. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37%. As BCT generalizes to held-out biases and does not require gold labels, this method may h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.05490</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Poly-View Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#20248;&#21270;&#21305;&#37197;&#22810;&#20010;&#30456;&#20851;&#35270;&#22270;&#65292;&#22312;ImageNet1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;SimCLR&#27169;&#22411;&#65292;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#25968;&#21644;&#26356;&#23567;&#30340;&#25209;&#22823;&#23567;&#19979;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#20250;&#21305;&#37197;&#19968;&#32452;&#19981;&#30456;&#20851;&#30340;&#36127;&#35270;&#22270;&#20013;&#30456;&#20851;&#35270;&#22270;&#30340;&#37197;&#23545;&#12290;&#35270;&#22270;&#21487;&#20197;&#26159;&#29983;&#25104;&#30340;&#65288;&#20363;&#22914;&#36890;&#36807;&#22686;&#24378;&#65289;&#25110;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#22810;&#20110;&#20004;&#20010;&#30456;&#20851;&#35270;&#22270;&#26102;&#30340;&#21305;&#37197;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#35270;&#22270;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20449;&#24687;&#26368;&#22823;&#21270;&#21644;&#20805;&#20998;&#32479;&#35745;&#23548;&#20986;&#20102;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#26080;&#38480;&#26102;&#65292;&#24212;&#26368;&#22823;&#21270;&#30456;&#20851;&#35270;&#22270;&#30340;&#25968;&#37327;&#65307;&#32780;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#29420;&#29305;&#26679;&#26412;&#30340;&#25968;&#37327;&#21516;&#26102;&#22686;&#21152;&#36825;&#20123;&#26679;&#26412;&#30340;&#35270;&#22270;&#25968;&#37327;&#26159;&#26377;&#30410;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#20197;256&#30340;&#25209;&#22823;&#23567;&#35757;&#32451;128&#36718;&#30340;&#22810;&#35270;&#22270;&#23545;&#27604;&#27169;&#22411;&#22312;ImageNet1k&#19978;&#34920;&#29616;&#20248;&#20110;&#22312;&#25209;&#22823;&#23567;&#20026;4096&#19988;&#36827;&#34892;1024&#36718;&#35757;&#32451;&#30340;SimCLR&#27169;&#22411;&#65292;&#25361;&#25112;&#20102;&#23545;&#27604;&#27169;&#22411;&#38656;&#35201;&#22823;&#25209;&#22823;&#23567;&#21644;&#22810;&#27425;&#35757;&#32451;&#36718;&#25968;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05490v1 Announce Type: cross  Abstract: Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#65292;&#21487;&#20197;&#36816;&#34892;&#24182;&#29609;1993&#24180;&#30340;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#65292;&#24182;&#19988;&#33021;&#22815;&#25191;&#34892;&#38376;&#25805;&#20316;&#12289;&#20987;&#36133;&#25932;&#20154;&#21644;&#35268;&#21010;&#36335;&#24452;&#65292;&#36825;&#26377;&#26395;&#25299;&#23637;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.05468</link><description>&lt;p&gt;
GPT-4&#20250;&#36816;&#34892;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will GPT-4 Run DOOM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05468
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#65292;&#21487;&#20197;&#36816;&#34892;&#24182;&#29609;1993&#24180;&#30340;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#65292;&#24182;&#19988;&#33021;&#22815;&#25191;&#34892;&#38376;&#25805;&#20316;&#12289;&#20987;&#36133;&#25932;&#20154;&#21644;&#35268;&#21010;&#36335;&#24452;&#65292;&#36825;&#26377;&#26395;&#25299;&#23637;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#39046;&#22495;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;1993&#24180;&#31532;&#19968;&#20154;&#31216;&#23556;&#20987;&#28216;&#25103;&#12298;&#27585;&#28781;&#25112;&#22763;&#12299;&#12290;&#36825;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20165;&#20973;&#23569;&#25968;&#25351;&#20196;&#21644;&#26469;&#33258;&#23631;&#24149;&#25130;&#22270;&#30340;&#25991;&#26412;&#25551;&#36848;&#65288;&#30001;&#27169;&#22411;&#26412;&#36523;&#29983;&#25104;&#65289;&#26469;&#36816;&#34892;&#21644;&#29609;&#28216;&#25103;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#21487;&#20197;&#20197;&#21450;&#33021;&#22815;&#21442;&#19982;&#28216;&#25103;&#65306;&#23427;&#33021;&#22815;&#25805;&#20316;&#38376;&#12289;&#19982;&#25932;&#20154;&#20316;&#25112;&#24182;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#28041;&#21450;&#22810;&#27425;&#27169;&#22411;&#35843;&#29992;&#30340;&#26356;&#22797;&#26434;&#25552;&#31034;&#31574;&#30053;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#24037;&#20316;&#26469;&#35753;&#36825;&#20010;LLM&#29609;&#24471;&#20687;&#20854;&#32463;&#20856;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;GPT-4&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#32780;&#26159;&#20381;&#38752;&#33258;&#36523;&#30340;&#25512;&#29702;&#21644;&#35266;&#23519;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#21160;&#20102;&#22522;&#20110;&#26234;&#33021;LLM&#20195;&#29702;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#30340;&#36793;&#30028;&#12290;&#25105;&#20204;&#26368;&#32456;&#35752;&#35770;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05468v1 Announce Type: cross  Abstract: We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex prompting strategies involving multiple model calls provide better results. While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities. We hope our work pushes the boundaries on intelligent, LLM-based agents in video games. We conclude by discussing the ethical implications of our work.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05465</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#30340;&#31639;&#27861;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;DNN&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05465
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#32534;&#30721;&#65288;LP&#65289;&#21644;LP&#37327;&#21270;&#65288;LPQ&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#22240;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;LP&#21442;&#25968;&#65292;&#35774;&#35745;&#20102;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#21160;&#24577;&#36866;&#24212;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#26041;&#27861;&#20351;&#29992;&#25972;&#25968;&#12289;&#23450;&#28857;&#25110;&#28014;&#28857;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24448;&#24448;&#38590;&#20197;&#22312;&#20302;&#31934;&#24230;&#19979;&#25429;&#25417;&#19981;&#21516;&#30340;DNN&#21442;&#25968;&#20998;&#24067;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30789;&#24320;&#38144;&#21644;&#23494;&#38598;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25968;&#27491;&#23450;&#65288;LP&#65289;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#27491;&#23450;&#21551;&#21457;&#30340;&#33258;&#36866;&#24212;&#12289;&#30828;&#20214;&#21451;&#22909;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;LP&#20301;&#22495;&#21160;&#24577;&#36866;&#24212;DNN&#26435;&#37325;/&#28608;&#27963;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;LP&#37327;&#21270;&#65288;LPQ&#65289;&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20248;&#30340;&#36880;&#23618;LP&#21442;&#25968;&#65292;&#21516;&#26102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;-&#23616;&#37096;&#23545;&#27604;&#30446;&#26631;&#20943;&#23569;&#37327;&#21270;&#21644;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#24615;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28151;&#21512;&#31934;&#24230;LP&#21152;&#36895;&#22120;&#65288;LPA&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#23558;LP&#32435;&#20837;&#35745;&#31639;&#25968;&#25454;&#36890;&#36335;&#20013;&#30340;&#22788;&#29702;&#21333;&#20803;&#65288;PEs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#33041;&#32593;&#32476;&#20013;&#30830;&#23450;&#28385;&#36275;&#20851;&#38190;&#22240;&#26524;&#20805;&#20998;&#24615;&#38656;&#27714;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05407</link><description>&lt;p&gt;
&#31639;&#27861;&#35782;&#21035;&#22823;&#33041;&#32593;&#32476;&#22240;&#26524;&#20805;&#20998;&#24615;&#20013;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#33041;&#32593;&#32476;&#20013;&#30830;&#23450;&#28385;&#36275;&#20851;&#38190;&#22240;&#26524;&#20805;&#20998;&#24615;&#38656;&#27714;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#20219;&#20309;&#22240;&#26524;&#26426;&#21046;&#65292;&#22914;&#22823;&#33041;&#30340;&#22240;&#26524;&#32593;&#32476;&#26102;&#65292;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20551;&#35774;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26126;&#26174;&#22320;&#65292;&#24573;&#35270;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#22823;&#33041;&#32593;&#32476;&#30340;&#22240;&#26524;&#20998;&#26512;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#28385;&#36275;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20851;&#38190;&#22806;&#28304;&#33410;&#28857;&#65292;&#20197;&#22312;&#27492;&#31867;&#30740;&#31350;&#20013;&#36981;&#24490;&#23427;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#25429;&#25417;Peter-Clark (PC)&#31639;&#27861;&#30340;&#26412;&#36136;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#20869;&#30340;&#21306;&#22495;&#23545;&#20197;&#21450;&#23545;&#26469;&#33258;&#20854;&#20182;&#32593;&#32476;&#33410;&#28857;&#26465;&#20214;&#30340;&#30456;&#21516;&#23545;&#36827;&#34892;&#29420;&#31435;&#24615;&#26816;&#39564;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#21306;&#20998;&#20505;&#36873;&#28151;&#26434;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#22240;&#23376;&#21270;&#21487;&#35782;&#21035;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05407v1 Announce Type: new  Abstract: In the investigation of any causal mechanisms, such as the brain's causal networks, the assumption of causal sufficiency plays a critical role. Notably, neglecting this assumption can result in significant errors, a fact that is often disregarded in the causal analysis of brain networks. In this study, we propose an algorithmic identification approach for determining essential exogenous nodes that satisfy the critical need for causal sufficiency to adhere to it in such inquiries. Our approach consists of three main steps: First, by capturing the essence of the Peter-Clark (PC) algorithm, we conduct independence tests for pairs of regions within a network, as well as for the same pairs conditioned on nodes from other networks. Next, we distinguish candidate confounders by analyzing the differences between the conditional and unconditional results, using the Kolmogorov-Smirnov test. Subsequently, we utilize Non-Factorized identifiable Vari
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HTV-Trans&#30340;Hierarchical Time series Variational Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#21644;Transformer&#65292;&#33021;&#22815;&#26377;&#25928;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22238;&#22797;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.05406</link><description>&lt;p&gt;
&#32771;&#34385;&#38750;&#24179;&#31283;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#23618;&#27425;&#21464;&#20998;Transformer
&lt;/p&gt;
&lt;p&gt;
Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05406
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HTV-Trans&#30340;Hierarchical Time series Variational Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#21644;Transformer&#65292;&#33021;&#22815;&#26377;&#25928;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#22238;&#22797;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#30340;&#39044;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#36328;&#36234;&#38271;&#26102;&#38388;&#27493;&#30340;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#24179;&#31283;&#21270;&#26041;&#27861;&#26469;&#20943;&#24369;&#21407;&#22987;&#31995;&#21015;&#30340;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24635;&#26159;&#37319;&#29992;&#24179;&#31283;&#21270;&#30340;&#31995;&#21015;&#65292;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#38543;&#26426;&#24615;&#65292;&#24456;&#38590;&#23545;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;MTS&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#23618;&#27425;&#27010;&#29575;&#29983;&#25104;&#27169;&#22359;&#65292;&#32771;&#34385;&#20102;MTS&#20013;&#30340;&#38750;&#24179;&#31283;&#24615;&#21644;&#38543;&#26426;&#29305;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;Transformer&#32467;&#21512;&#65292;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;Hierarchical Time series Variational Transformer&#65288;HTV-Trans&#65289;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#21464;&#20998;&#29983;&#25104;&#21160;&#24577;&#27169;&#22411;&#65292;&#23558;&#20869;&#22312;&#30340;&#38750;&#24179;&#31283;&#20449;&#24687;&#24674;&#22797;&#21040;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05406v1 Announce Type: cross  Abstract: The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability. However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling MTS with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastic characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being a po
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#33258;&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.05379</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#23454;&#20363;&#23398;&#20064;&#29992;&#20110;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05379
&lt;/p&gt;
&lt;p&gt;
&#33258;&#26412;&#30740;&#31350;&#21457;&#29616;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30142;&#30149;&#35786;&#26029;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20381;&#36182;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#27169;&#22411;&#35757;&#32451;&#12290;&#24613;&#24615;&#39635;&#32454;&#32990;&#30333;&#34880;&#30149;&#65288;AML&#65289;&#31561;&#30142;&#30149;&#30001;&#20110;&#22312;&#21333;&#20010;&#32454;&#32990;&#27700;&#24179;&#19978;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#26631;&#27880;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#35299;&#20915;&#20102;&#24369;&#26631;&#35760;&#22330;&#26223;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#29992;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#22522;&#20110;MIL&#30340;AML&#20122;&#22411;&#20998;&#31867;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#34880;&#28034;&#29255;&#20013;&#21435;&#38500;&#20102;&#32534;&#30721;&#22120;&#35757;&#32451;&#26399;&#38388;&#30340;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;SSL&#26041;&#27861;SimCLR&#12289;SwAV&#21644;DINO&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;SSL&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;SSL&#22312;MIL&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#19988;&#33410;&#32422;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05379v1 Announce Type: cross  Abstract: Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#35745;&#31639;&#35748;&#30693;&#31185;&#23398;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35843;&#35797;&#24515;&#26234;&#27169;&#22411;&#35299;&#37322;&#20196;&#20154;&#22256;&#24785;&#31243;&#24207;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05334</link><description>&lt;p&gt;
WatChat&#65306;&#36890;&#36807;&#35843;&#35797;&#24515;&#26234;&#27169;&#22411;&#35299;&#37322;&#20196;&#20154;&#22256;&#24785;&#30340;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
WatChat: Explaining perplexing programs by debugging mental models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#35745;&#31639;&#35748;&#30693;&#31185;&#23398;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35843;&#35797;&#24515;&#26234;&#27169;&#22411;&#35299;&#37322;&#20196;&#20154;&#22256;&#24785;&#31243;&#24207;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#35299;&#37322;&#31243;&#24207;&#24847;&#22806;&#34892;&#20026;&#30340;&#19968;&#20010;&#22909;&#26041;&#27861;&#26159;&#31243;&#24207;&#21592;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#20294;&#26377;&#26102;&#65292;&#19968;&#20010;&#26356;&#22909;&#30340;&#35299;&#37322;&#26159;&#31243;&#24207;&#21592;&#23545;&#25152;&#20351;&#29992;&#35821;&#35328;&#30340;&#24515;&#26234;&#27169;&#22411;&#20013;&#23384;&#22312;&#38169;&#35823;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#35843;&#35797;&#24403;&#21069;&#20195;&#30721;&#65288;&#8220;&#32473;&#31243;&#24207;&#21592;&#19968;&#26465;&#40060;&#8221;&#65289;&#65292;&#32780;&#26159;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#30452;&#25509;&#35843;&#35797;&#25105;&#20204;&#30340;&#24515;&#26234;&#27169;&#22411;&#65288;&#8220;&#25945;&#20250;&#31243;&#24207;&#21592;&#22914;&#20309;&#25429;&#40060;&#8221;&#65289;&#12290;&#26412;&#25991;&#23558;&#35745;&#31639;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#24212;&#29992;&#21040;&#20854;&#20013;&#65292;&#23545;&#20196;&#20154;&#22256;&#24785;&#30340;&#31243;&#24207;&#65292;&#25105;&#20204;&#20351;&#29992;&#31243;&#24207;&#32508;&#21512;&#25216;&#26415;&#33258;&#21160;&#25512;&#26029;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#23545;&#31243;&#24207;&#34892;&#20026;&#24863;&#21040;&#24778;&#35766;&#30340;&#35823;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#35823;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#31616;&#26126;&#12289;&#26377;&#29992;&#30340;&#31243;&#24207;&#34892;&#20026;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#34987;&#21453;&#36716;&#65292;&#20197;&#32508;&#21512;&#25945;&#23398;&#31034;&#33539;&#31243;&#24207;&#26469;&#35786;&#26029;&#21644;&#32416;&#27491;&#23398;&#29983;&#30340;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05334v1 Announce Type: cross  Abstract: Often, a good explanation for a program's unexpected behavior is a bug in the programmer's code. But sometimes, an even better explanation is a bug in the programmer's mental model of the language they are using. Instead of merely debugging our current code ("giving the programmer a fish"), what if our tools could directly debug our mental models ("teaching the programmer to fish")? In this paper, we apply ideas from computational cognitive science to do exactly that. Given a perplexing program, we use program synthesis techniques to automatically infer potential misconceptions that might cause the user to be surprised by the program's behavior. By analyzing these misconceptions, we provide succinct, useful explanations of the program's behavior. Our methods can even be inverted to synthesize pedagogical example programs for diagnosing and correcting misconceptions in students.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#25913;&#21892;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#21512;&#27861;&#24615;&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.05318</link><description>&lt;p&gt;
&#23637;&#26395;&#36991;&#20813;&#36831;&#21040;&#65306;&#35299;&#20915;&#30828;&#32422;&#26463;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#25913;&#21892;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#21512;&#27861;&#24615;&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#23450;&#24335;&#20026;&#20855;&#26377;&#32422;&#26463;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#24120;&#22797;&#26434;&#32780;&#19988;&#25968;&#37327;&#20247;&#22810;&#65292;&#20351;&#24471;&#35299;&#20915;TSP&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#22797;&#26434;&#32422;&#26463;&#30340;&#25968;&#37327;&#22686;&#38271;&#26102;&#65292;&#20256;&#32479;&#21551;&#21457;&#24335;&#31639;&#27861;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#20197;&#36991;&#20813;&#19981;&#21512;&#27861;&#32467;&#26524;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36719;&#26041;&#24335;&#26469;&#35299;&#20915;TSP&#38382;&#39064;&#65292;&#21516;&#26102;&#25903;&#25345;GPU&#21152;&#36895;&#20197;&#24555;&#36895;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36719;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#30828;&#32422;&#26463;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#21512;&#27861;&#24615;&#21644;&#26368;&#20248;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#20248;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#24182;&#23545;&#25239;&#30828;&#32422;&#26463;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21521;&#21069;&#23637;&#26395;&#20449;&#24687;&#20316;&#20026;&#29305;&#24449;&#26469;&#25913;&#36827;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#65288;TSPTW&#65289;&#30340;TSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#27861;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05318v1 Announce Type: new  Abstract: Many real-world problems can be formulated as a constrained Traveling Salesman Problem (TSP). However, the constraints are always complex and numerous, making the TSPs challenging to solve. When the number of complicated constraints grows, it is time-consuming for traditional heuristic algorithms to avoid illegitimate outcomes. Learning-based methods provide an alternative to solve TSPs in a soft manner, which also supports GPU acceleration to generate solutions quickly. Nevertheless, the soft manner inevitably results in difficulty solving hard-constrained problems with learning algorithms, and the conflicts between legality and optimality may substantially affect the optimality of the solution. To overcome this problem and to have an effective solution against hard constraints, we proposed a novel learning-based method that uses looking-ahead information as the feature to improve the legality of TSP with Time Windows (TSPTW) solutions.
&lt;/p&gt;</description></item><item><title>RAT&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#65292;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24187;&#35273;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.05313</link><description>&lt;p&gt;
RAT&#65306;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#24341;&#21457;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05313
&lt;/p&gt;
&lt;p&gt;
RAT&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#65292;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20013;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24187;&#35273;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#36845;&#20195;&#20462;&#35746;&#19968;&#31995;&#21015;&#24605;&#32500;&#65292;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#35270;&#35282;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#26497;&#22823;&#20943;&#36731;&#20102;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#8212;&#8212;*&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;* (RAT)&#8212;&#8212;&#22312;&#29983;&#25104;&#21021;&#22987;&#30340;&#38646;&#23556; CoT &#21518;&#65292;&#36880;&#27493;&#20462;&#35746;&#27599;&#20010;&#24605;&#32500;&#27493;&#39588;&#65292;&#19982;&#20219;&#21153;&#26597;&#35810;&#12289;&#24403;&#21069;&#21644;&#36807;&#21435;&#30340;&#24605;&#32500;&#27493;&#39588;&#30456;&#20851;&#30340;&#26816;&#32034;&#20449;&#24687;&#12290;&#23558; RAT &#24212;&#29992;&#20110; GPT-3.5&#12289;GPT-4 &#21644; CodeLLaMA-7b&#65292;&#22312;&#21508;&#31181;&#38271;&#35270;&#35282;&#29983;&#25104;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#24179;&#22343;&#32780;&#35328;&#65292;&#20195;&#30721;&#29983;&#25104;&#35780;&#20998;&#22686;&#21152;&#20102; 13.63%&#65292;&#25968;&#23398;&#25512;&#29702;&#22686;&#21152;&#20102; 16.96%&#65292;&#21019;&#24847;&#20889;&#20316;&#22686;&#21152;&#20102; 19.2%&#65292;&#20855;&#35937;&#20219;&#21153;&#35268;&#21010;&#22686;&#21152;&#20102; 42.78%&#12290;&#28436;&#31034;&#39029;&#38754;&#38142;&#25509;&#65306;https://craftjarvis.github.io/RAT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05313v1 Announce Type: cross  Abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT
&lt;/p&gt;</description></item><item><title>Tapilot-Crossing&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26032;&#22522;&#20934;&#65292;&#36890;&#36807;&#32463;&#27982;&#22411;&#22810;&#20195;&#29702;&#29615;&#22659;&#21644;&#33258;&#36866;&#24212;&#20132;&#20114;&#21453;&#24605;&#31574;&#30053;&#36827;&#34892;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#20984;&#26174;&#20102;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.05307</link><description>&lt;p&gt;
Tapilot-Crossing&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#21457;&#23637;LLM&#20197;&#23454;&#29616;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05307
&lt;/p&gt;
&lt;p&gt;
Tapilot-Crossing&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26032;&#22522;&#20934;&#65292;&#36890;&#36807;&#32463;&#27982;&#22411;&#22810;&#20195;&#29702;&#29615;&#22659;&#21644;&#33258;&#36866;&#24212;&#20132;&#20114;&#21453;&#24605;&#31574;&#30053;&#36827;&#34892;&#26500;&#24314;&#21644;&#35780;&#20272;&#65292;&#20984;&#26174;&#20102;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#65292;&#20154;&#31867;&#19982;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20026;&#26126;&#26234;&#20915;&#31574;&#25552;&#20379;&#20102;&#23454;&#26102;&#25968;&#25454;&#25506;&#32034;&#12290;&#25910;&#38598;&#36924;&#30495;&#30340;&#20132;&#20114;&#24335;&#26085;&#24535;&#20197;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#21644;&#25104;&#26412;&#38459;&#30861;&#20102;&#23545;LLM&#20195;&#29702;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Tapilot-Crossing&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#12290;Tapilot-Crossing&#21253;&#21547;1024&#20010;&#20132;&#20114;&#65292;&#28085;&#30422;4&#20010;&#23454;&#38469;&#22330;&#26223;&#65306;&#27491;&#24120;&#12289;&#21160;&#20316;&#12289;&#31169;&#20154;&#21644;&#31169;&#20154;&#21160;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Tapilot-Crossing&#26159;&#30001;&#19968;&#20010;&#32463;&#27982;&#22411;&#22810;&#20195;&#29702;&#29615;&#22659;Decision Company&#26500;&#24314;&#30340;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;Tapilot-Crossing&#20013;&#30693;&#21517;&#21644;&#20808;&#36827;&#30340;LLM&#20195;&#29702;&#65292;&#20984;&#26174;&#20102;&#20132;&#20114;&#24335;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20132;&#20114;&#21453;&#24605;&#65288;AIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#24605;&#31574;&#30053;&#65292;&#25351;&#23548;LLM&#20195;&#29702;&#20174;&#20013;&#21560;&#21462;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05307v1 Announce Type: new  Abstract: Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from succ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05265</link><description>&lt;p&gt;
MMoE: &#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#39046;&#22495;&#24863;&#30693;&#19987;&#23478;&#28151;&#21512;&#30340;&#40065;&#26834;&#21095;&#36879;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#30005;&#24433;&#35780;&#35770;&#32593;&#31449;&#23545;&#20110;&#30005;&#24433;&#20449;&#24687;&#21644;&#35752;&#35770;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#21095;&#36879;&#35780;&#35770;&#20250;&#24433;&#21709;&#35266;&#24433;&#20307;&#39564;&#65292;&#22240;&#27492;&#21095;&#36879;&#26816;&#27979;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24573;&#30053;&#20102;&#24179;&#21488;&#20013;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;MMoE&#39318;&#20808;&#20174;&#29992;&#25143;-&#30005;&#24433;&#32593;&#32476;&#20013;&#25552;&#21462;&#22270;&#34920;&#12289;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#29305;&#24449;&#65292;&#20998;&#21035;&#20174;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#21644;&#35780;&#35770;&#30340;&#20803;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#22788;&#29702;&#29305;&#23450;&#31867;&#22411;&#30005;&#24433;&#35780;&#35770;&#20013;&#30340;&#21095;&#36879;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05265v1 Announce Type: new  Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#20197;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05260</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#29305;&#24449;&#36827;&#34892;&#23545;&#25239;&#24615;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for Adversarial Multi-source Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#30340;&#23884;&#20837;&#65292;&#20197;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;&#27979;&#24207;&#25216;&#26415;&#30340;&#21457;&#23637;&#25512;&#21160;&#20102;&#22823;&#37327;&#21333;&#32454;&#32990;&#36716;&#24405;&#35889;&#30340;&#29983;&#25104;&#65292;&#20026;&#25506;&#32034;&#32959;&#30244;&#20013;&#32784;&#33647;&#32454;&#32990;&#20122;&#32676;&#25552;&#20379;&#20102;&#23453;&#36149;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#21333;&#32454;&#32990;&#27700;&#24179;&#30340;&#33647;&#29289;&#25935;&#24863;&#24615;&#25968;&#25454;&#20173;&#28982;&#31232;&#32570;&#65292;&#36843;&#20999;&#38656;&#35201;&#23545;&#20010;&#20307;&#32454;&#32990;&#30340;&#33647;&#29289;&#25935;&#24863;&#24615;&#36827;&#34892;&#35745;&#31639;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#39033;&#32039;&#36843;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;scAdaDrug&#30340;&#22810;&#28304;&#33258;&#36866;&#24212;&#21152;&#26435;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21333;&#32454;&#32990;&#33647;&#29289;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#19982;&#33647;&#29289;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26435;&#37325;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20135;&#29983;&#37325;&#35201;&#24615;&#24863;&#30693;&#21644;&#30456;&#20114;&#29420;&#31435;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#33410;&#27599;&#20010;&#26679;&#26412;&#22312;&#28304;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#32500;&#24230;&#32423;&#21035;&#23884;&#20837;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05260v1 Announce Type: new  Abstract: The development of single-cell sequencing technology had promoted the generation of a large amount of single-cell transcriptional profiles, providing valuable opportunities to explore drug-resistant cell subpopulations in a tumor. However, the drug sensitivity data in single-cell level is still scarce to date, pressing an urgent and highly challenging task for computational prediction of the drug sensitivity to individual cells. This paper proposed scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug sensitivity. We used an autoencoder to extract domain-invariant features related to drug sensitivity from multiple source domains by exploiting adversarial domain adaptation. Especially, we introduced an adaptive weight generator to produce importance-aware and mutual independent weights, which could adaptively modulate the embedding of each sample in dimension-level for both source and target domains. Extensive exp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05245</link><description>&lt;p&gt;
&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21152;&#36895;MRI&#30340;&#31283;&#20581;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;&#20250;&#36880;&#27493;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#19968;&#33268;&#24615;&#20197;&#37325;&#24314;&#28508;&#22312;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;MRI&#37319;&#38598;&#24050;&#32463;&#21253;&#21547;&#30001;&#28909;&#28072;&#33853;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#12290;&#20351;&#29992;&#36229;&#24555;&#36895;&#12289;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#24207;&#21015;&#36827;&#34892;&#39640;&#32423;&#30740;&#31350;&#65292;&#25110;&#32773;&#20351;&#29992;&#20302;&#22330;&#31995;&#32479;&#65288;&#21463;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#38738;&#30544;&#65289;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20854;&#26126;&#26174;&#12290;&#36825;&#20123;&#24120;&#35265;&#22330;&#26223;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#24314;&#25216;&#26415;&#24615;&#33021;&#20122;&#20248;&#25110;&#23436;&#20840;&#22833;&#36133;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38543;&#30528;&#36880;&#28176;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#22266;&#26377;&#30340;MRI&#22122;&#22768;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#65292;&#20351;&#23454;&#38469;&#22122;&#22768;&#27700;&#24179;&#19982;&#39044;&#23450;&#20041;&#21435;&#22122;&#26102;&#38388;&#34920;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#23548;&#33268;&#22270;&#20687;&#37325;&#24314;&#19981;&#20934;&#30830;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05245v1 Announce Type: cross  Abstract: In general, diffusion model-based MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images. However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations. This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries. These common scenarios can lead to sub-optimal performance or complete failure of existing diffusion model-based reconstruction techniques. Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction. To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#34701;&#20837;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#24378;&#21270;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;</title><link>https://arxiv.org/abs/2403.05239</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#20013;&#20154;&#24615;&#20808;&#39564;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#34701;&#20837;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#24378;&#21270;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#21319;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vanilla&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#23548;&#33268;&#19981;&#23436;&#32654;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#23039;&#21183;&#25110;&#32930;&#20307;&#19981;&#25104;&#27604;&#20363;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#39069;&#22806;&#22270;&#20687;&#25110;&#28155;&#21152;&#39069;&#22806;&#25511;&#21046;&#65288;&#22914;&#23039;&#21183;&#25110;&#28145;&#24230;&#22270;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#25511;&#21046;&#20027;&#35201;&#22312;&#22270;&#20687;&#29983;&#25104;&#38454;&#27573;&#24341;&#20837;&#20154;&#24615;&#20808;&#39564;&#12290;&#26412;&#25991;&#25506;&#35752;&#23558;&#36825;&#20123;&#20154;&#24615;&#20808;&#39564;&#30452;&#25509;&#38598;&#25104;&#21040;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#38454;&#27573;&#28040;&#38500;&#39069;&#22806;&#26465;&#20214;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20154;&#24615;&#23545;&#40784;&#25439;&#22833;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65292;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#24378;&#21270;&#20154;&#20307;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24341;&#20837;&#20855;&#26377;&#35268;&#27169;&#24863;&#30693;&#21644;&#36880;&#27493;&#32422;&#26463;&#30340;&#22240;&#32032;&#65292;&#20197;&#30830;&#20445;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#32454;&#33410;&#20016;&#23500;&#24615;&#21644;&#20154;&#20307;&#32467;&#26500;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05239v1 Announce Type: cross  Abstract: Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross
&lt;/p&gt;</description></item><item><title>FAIM&#26159;&#19968;&#20010;&#29992;&#20110;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#30028;&#38754;&#35782;&#21035;&#26368;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.05235</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#21487;&#35299;&#37322;&#24314;&#27169;&#65288;FAIM&#65289;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine Learning in Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05235
&lt;/p&gt;
&lt;p&gt;
FAIM&#26159;&#19968;&#20010;&#29992;&#20110;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#30028;&#38754;&#35782;&#21035;&#26368;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#26426;&#22120;&#23398;&#20064;&#19981;&#26029;&#34701;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#25552;&#20986;&#20102;&#37325;&#35201;&#20851;&#20999;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#26694;&#26550; - &#20844;&#24179;&#24863;&#30693;&#21487;&#35299;&#37322;&#24314;&#27169;&#65288;FAIM&#65289;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#20854;&#29305;&#28857;&#26159;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;&#39640;&#24615;&#33021;&#27169;&#22411;&#20013;&#35782;&#21035;&#20986;&#19968;&#20010;&#8220;&#26356;&#20844;&#24179;&#8221;&#30340;&#27169;&#22411;&#65292;&#24182;&#20419;&#36827;&#25968;&#25454;&#39537;&#21160;&#35777;&#25454;&#19982;&#20020;&#24202;&#19987;&#23478;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;&#24773;&#22659;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#24211;MIMIC-IV-ED&#21644;SGH-ED&#19978;&#39044;&#27979;&#21307;&#38498;&#20837;&#38498;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;FAIM&#22312;&#20943;&#23569;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;FAIM&#27169;&#22411;&#19981;&#20165;&#23637;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#27495;&#35270;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#24050;&#24314;&#31435;&#30340;&#20844;&#24179;&#24615;&#24230;&#37327;&#26126;&#26174;&#20943;&#36731;&#20102;&#20559;&#35265;&#65292;&#20248;&#20110;&#24120;&#29992;&#30340;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05235v1 Announce Type: cross  Abstract: The escalating integration of machine learning in high-stakes fields such as healthcare raises substantial concerns about model fairness. We propose an interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to improve model fairness without compromising performance, featuring an interactive interface to identify a "fairer" model from a set of high-performing models and promoting the integration of data-driven evidence and clinical expertise to enhance contextualized fairness. We demonstrated FAIM's value in reducing sex and race biases by predicting hospital admission with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both datasets, FAIM models not only exhibited satisfactory discriminatory performance but also significantly mitigated biases as measured by well-established fairness metrics, outperforming commonly used bias-mitigation methods. Our approach demonstrates the feasibility of improving f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#31435;&#22810;&#31449;&#28857;&#29983;&#23384;&#32467;&#26524;&#30340;&#32852;&#37030;&#35780;&#20998;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;&#65292;&#30830;&#20445;&#20102;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.05229</link><description>&lt;p&gt;
&#21033;&#29992;&#24322;&#26500;&#29616;&#23454;&#19990;&#30028;&#29983;&#23384;&#25968;&#25454;&#24320;&#21457;&#32852;&#37030;&#26102;&#38388;&#20107;&#20214;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Developing Federated Time-to-Event Scores Using Heterogeneous Real-World Survival Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#31435;&#22810;&#31449;&#28857;&#29983;&#23384;&#32467;&#26524;&#30340;&#32852;&#37030;&#35780;&#20998;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;&#65292;&#30830;&#20445;&#20102;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#27963;&#20998;&#26512;&#22312;&#35768;&#22810;&#21307;&#30103;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#22522;&#30784;&#32452;&#20214;&#30340;&#35282;&#33394;&#65292;&#23545;&#24739;&#32773;&#29305;&#23450;&#20107;&#20214;&#30340;&#26102;&#38388;&#65288;&#22914;&#26576;&#31181;&#30142;&#30149;&#30340;&#21457;&#20316;&#25110;&#27515;&#20129;&#65289;&#30340;&#30830;&#23450;&#23545;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#35780;&#20998;&#31995;&#32479;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26500;&#24314;&#29983;&#23384;&#35780;&#20998;&#31995;&#32479;&#30340;&#26041;&#27861;&#20551;&#23450;&#25968;&#25454;&#28304;&#33258;&#21333;&#19968;&#26469;&#28304;&#65292;&#36825;&#22312;&#19982;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#21512;&#20316;&#26102;&#23384;&#22312;&#38544;&#31169;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#22810;&#31449;&#28857;&#29983;&#23384;&#32467;&#26524;&#30340;&#32852;&#37030;&#35780;&#20998;&#31995;&#32479;&#65292;&#30830;&#20445;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26032;&#21152;&#22369;&#21644;&#32654;&#22269;&#24613;&#35786;&#23460;&#21508;&#33258;&#26469;&#28304;&#30340;&#24322;&#26500;&#29983;&#23384;&#25968;&#25454;&#30340;&#31449;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#31449;&#28857;&#29420;&#31435;&#24320;&#21457;&#20102;&#26412;&#22320;&#35780;&#20998;&#12290;&#22312;&#27599;&#20010;&#21442;&#19982;&#32773;&#31449;&#28857;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#35780;&#20998;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05229v1 Announce Type: new  Abstract: Survival analysis serves as a fundamental component in numerous healthcare applications, where the determination of the time to specific events (such as the onset of a certain disease or death) for patients is crucial for clinical decision-making. Scoring systems are widely used for swift and efficient risk prediction. However, existing methods for constructing survival scores presume that data originates from a single source, posing privacy challenges in collaborations with multiple data owners. We propose a novel framework for building federated scoring systems for multi-site survival outcomes, ensuring both privacy and communication efficiency. We applied our approach to sites with heterogeneous survival data originating from emergency departments in Singapore and the United States. Additionally, we independently developed local scores at each site. In testing datasets from each participant site, our proposed federated scoring system 
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#29983;&#25104;&#30340;&#37197;&#23545;&#20449;&#24687;&#26174;&#33879;&#25913;&#21892;&#20102;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#35823;&#24046;&#20943;&#23567;&#20998;&#21035;&#36798;&#21040;4.4&#20493;&#21644;5.6&#20493;</title><link>https://arxiv.org/abs/2403.05220</link><description>&lt;p&gt;
&#21512;&#25104;&#29305;&#26435;&#20449;&#24687;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Privileged Information Enhances Medical Image Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05220
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#25104;&#30340;&#37197;&#23545;&#20449;&#24687;&#26174;&#33879;&#25913;&#21892;&#20102;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#65292;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#35823;&#24046;&#20943;&#23567;&#20998;&#21035;&#36798;&#21040;4.4&#20493;&#21644;5.6&#20493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#34987;&#35777;&#26126;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#29983;&#29289;&#23398;&#30456;&#20851;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#19981;&#23384;&#22312;&#37197;&#23545;&#25968;&#25454;&#25110;&#21482;&#26377;&#23569;&#37327;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#24456;&#22909;&#22320;&#24037;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#25214;&#21040;&#26410;&#37197;&#23545;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#29983;&#25104;&#26377;&#25928;&#26080;&#38480;&#37327;&#30340;&#37197;&#23545;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#37197;&#23545;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#34920;&#31034;&#23398;&#20064;&#65292;&#19982;&#21333;&#27169;&#24577;&#35757;&#32451;&#65288;&#35823;&#24046;&#20943;&#23567;&#36798;&#21040;4.4&#20493;&#65289;&#25110;&#30495;&#23454;&#22810;&#27169;&#24577;&#37197;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65288;&#35823;&#24046;&#20943;&#23567;&#36798;&#21040;5.6&#20493;&#65289;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05220v1 Announce Type: cross  Abstract: Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMQA&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#37325;&#35282;&#33394;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05217</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35282;&#33394;&#33021;&#21147;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMQA&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#37325;&#35282;&#33394;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#24050;&#32463;&#25104;&#20026;&#20449;&#24687;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#28966;&#28857;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36981;&#24490;&#20004;&#31181;&#33539;&#24335;&#26469;&#25910;&#38598;&#35777;&#25454;&#65306;&#65288;1&#65289;\textit{&#26816;&#32034;-&#28982;&#21518;&#38405;&#35835;}&#33539;&#24335;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65307;&#21644;&#65288;2&#65289;\textit{&#29983;&#25104;-&#28982;&#21518;&#38405;&#35835;}&#33539;&#24335;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#19981;&#33021;&#23436;&#20840;&#28385;&#36275;&#35777;&#25454;&#30340;&#22810;&#26041;&#38754;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMQA&#65292;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;ODQA&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#22522;&#26412;&#27493;&#39588;&#65306;&#26597;&#35810;&#25193;&#23637;&#65292;&#25991;&#26723;&#36873;&#25321;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#32467;&#21512;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#35777;&#25454;&#30340;&#20248;&#21183;&#12290;&#30001;&#20110;LLMs&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#26469;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#25105;&#20204;&#25351;&#23548;LLMs&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#25198;&#28436;&#29983;&#25104;&#22120;&#12289;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#35780;&#20272;&#22120;&#31561;&#22810;&#31181;&#35282;&#33394;&#65292;&#20351;&#23427;&#20204;&#34701;&#21512;&#22312;ODQA&#36807;&#31243;&#20013;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05217v1 Announce Type: cross  Abstract: Open-domain question answering (ODQA) has emerged as a pivotal research spotlight in information systems. Existing methods follow two main paradigms to collect evidence: (1) The \textit{retrieve-then-read} paradigm retrieves pertinent documents from an external corpus; and (2) the \textit{generate-then-read} paradigm employs large language models (LLMs) to generate relevant documents. However, neither can fully address multifaceted requirements for evidence. To this end, we propose LLMQA, a generalized framework that formulates the ODQA process into three basic steps: query expansion, document selection, and answer generation, combining the superiority of both retrieval-based and generation-based evidence. Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProUD&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05209</link><description>&lt;p&gt;
&#20811;&#26381;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Overcoming Data Inequality across Domains with Semi-Supervised Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProUD&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#20013;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#20010;&#26469;&#28304;&#21644;&#20154;&#32676;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#36825;&#31181;&#19981;&#24179;&#31561;&#22312;&#20026;&#25968;&#25454;&#26377;&#38480;&#30340;&#20154;&#24314;&#27169;&#26102;&#24102;&#26469;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#28145;&#21051;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#36328;&#39046;&#22495;&#25968;&#25454;&#19981;&#24179;&#31561;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#20195;&#34920;&#24615;&#26696;&#20363;&#65292;&#21363;&#21322;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#65288;SSDG&#65289;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#39046;&#22495;&#34987;&#26631;&#35760;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#27809;&#26377;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;ProUD&#65292;&#36890;&#36807;&#39046;&#22495;&#24863;&#30693;&#21407;&#22411;&#21644;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#33258;&#36866;&#24212;&#28151;&#21512;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#39046;&#22495;&#30340;&#28176;&#36827;&#27867;&#21270;&#65292;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ProUD&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#21253;&#25324;&#21333;&#19968;&#39046;&#22495;&#27867;&#21270;&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05209v1 Announce Type: cross  Abstract: While there have been considerable advancements in machine learning driven by extensive datasets, a significant disparity still persists in the availability of data across various sources and populations. This inequality across domains poses challenges in modeling for those with limited data, which can lead to profound practical and ethical concerns. In this paper, we address a representative case of data inequality problem across domains termed Semi-Supervised Domain Generalization (SSDG), in which only one domain is labeled while the rest are unlabeled. We propose a novel algorithm, ProUD, which can effectively learn domain-invariant features via domain-aware prototypes along with progressive generalization via uncertainty-adaptive mixing of labeled and unlabeled domains. Our experiments on three different benchmark datasets demonstrate the effectiveness of ProUD, outperforming all baseline models including single domain generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36861;&#36394;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26469;&#28304;&#65292;&#21457;&#29616;&#20102;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#20026;&#21306;&#20998;&#23427;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20984;&#26174;&#20102;&#22312;&#22810;&#35821;&#35328;LMs&#20013;&#20445;&#25345;&#19968;&#33268;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;ML-LMs&#20013;&#25913;&#36827;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05189</link><description>&lt;p&gt;
&#36861;&#36394;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26681;&#28304;&#65306;&#29420;&#31435;&#30340;&#12289;&#20849;&#20139;&#30340;&#21644;&#36716;&#31227;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36861;&#36394;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30340;&#26469;&#28304;&#65292;&#21457;&#29616;&#20102;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#20026;&#21306;&#20998;&#23427;&#20204;&#25552;&#20986;&#20102;&#26041;&#27861;&#65292;&#20984;&#26174;&#20102;&#22312;&#22810;&#35821;&#35328;LMs&#20013;&#20445;&#25345;&#19968;&#33268;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#22312;ML-LMs&#20013;&#25913;&#36827;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#35821;&#35328;LMs&#65288;ML-LMs&#65289;&#20013;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-LMs&#22914;&#20309;&#33719;&#21462;&#21644;&#34920;&#31034;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22810;&#35821;&#35328;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#25968;&#25454;&#38598;mLAMA&#23545;ML-LMs&#65288;&#29305;&#21035;&#26159;&#22810;&#35821;&#35328;BERT&#65289;&#36827;&#34892;&#31070;&#32463;&#20803;&#35843;&#26597;&#12290;&#28982;&#21518;&#25105;&#20204;&#36861;&#28335;&#20107;&#23454;&#30340;&#26681;&#28304;&#65288;&#32500;&#22522;&#30334;&#31185;&#65289;&#65292;&#20197;&#30830;&#23450;ML-LMs&#33719;&#21462;&#29305;&#23450;&#20107;&#23454;&#30340;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;ML-LMs&#33719;&#21462;&#21644;&#34920;&#31034;&#20107;&#23454;&#30340;&#19977;&#31181;&#27169;&#24335;&#65306;&#35821;&#35328;&#29420;&#31435;&#12289;&#36328;&#35821;&#35328;&#20849;&#20139;&#21644;&#36716;&#31227;&#65292;&#24182;&#21046;&#23450;&#20102;&#21306;&#20998;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#36328;&#35821;&#35328;&#20445;&#25345;&#19968;&#33268;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;&#22312;ML-LMs&#20013;&#36827;&#34892;&#26356;&#22909;&#30340;&#20107;&#23454;&#34920;&#31034;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05189v1 Announce Type: cross  Abstract: Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05175</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Continual Learning and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05175
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#19968;&#38382;&#39064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#25345;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#31456;&#33410;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#21363;&#20174;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#25345;&#32493;&#23398;&#20064;&#26159;&#20154;&#33041;&#30340;&#19968;&#31181;&#33258;&#28982;&#25216;&#33021;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#21364;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22312;&#23398;&#20064;&#26032;&#30693;&#35782;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#24448;&#24448;&#20250;&#36805;&#36895;&#32780;&#24443;&#24213;&#22320;&#24536;&#35760;&#20197;&#21069;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25345;&#32493;&#23398;&#20064;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#26412;&#20070;&#31456;&#33410;&#22238;&#39038;&#20102;&#36825;&#19968;&#39046;&#22495;&#20135;&#29983;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05175v1 Announce Type: cross  Abstract: This book chapter delves into the dynamics of continual learning, which is the process of incrementally learning from a non-stationary stream of data. Although continual learning is a natural skill for the human brain, it is very challenging for artificial neural networks. An important reason is that, when learning something new, these networks tend to quickly and drastically forget what they had learned before, a phenomenon known as catastrophic forgetting. Especially in the last decade, continual learning has become an extensively studied topic in deep learning. This book chapter reviews the insights that this field has generated.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05171</link><description>&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#20811;&#26381;&#20102;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;AdvPO&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25913;&#36827;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;Anthropic HH&#21644;TL;DR&#25688;&#35201;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdvPO&#22312;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05171v1 Announce Type: cross  Abstract: We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.05168</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#35299;&#38145;&#22810;&#27169;&#24577;&#32479;&#19968;&#31163;&#25955;&#34920;&#31034;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#32479;&#19968;&#30721;&#26412;&#30340;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;DCID&#65289;&#27169;&#22411;&#22312;&#23454;&#29616;&#32454;&#31890;&#24230;&#34920;&#31034;&#21644;&#36328;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#21463;&#21040;&#23545;&#25152;&#26377;&#36890;&#36947;&#30340;&#22343;&#31561;&#23545;&#24453;&#20197;&#21450;&#24573;&#35270;&#27425;&#35201;&#20107;&#20214;&#20449;&#24687;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;&#26469;&#33258;&#26080;&#20851;&#36890;&#36947;&#30340;&#24178;&#25200;&#24182;&#22312;&#32454;&#31890;&#24230;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#65288;TOC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#36873;&#25321;&#37325;&#35201;&#36890;&#36947;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;H-DCID&#65289;&#26041;&#27861;&#23558;&#20449;&#24687;&#20998;&#31163;&#21644;&#23545;&#40784;&#25193;&#23637;&#21040;&#20004;&#20010;&#32423;&#21035;&#65292;&#25429;&#25417;&#26356;&#22810;&#36328;&#27169;&#24577;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05168v1 Announce Type: cross  Abstract: Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31867;&#20284;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05164</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#31995;&#32479;&#36776;&#35782;&#65306;&#21033;&#29992;&#31867;&#20284;&#31995;&#32479;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation for system identification: leveraging knowledge transfer from similar systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31867;&#20284;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#36807;&#25311;&#21512;&#30340;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#21516;&#19968;&#31867;&#31995;&#32479;&#20013;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#30340;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#20803;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35813;&#20803;&#27169;&#22411;&#25551;&#36848;&#20102;&#20551;&#23450;&#25152;&#20851;&#27880;&#30340;&#31995;&#32479;&#25152;&#23646;&#30340;&#19968;&#31867;&#31995;&#32479;&#12290;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#20004;&#20010;&#30446;&#30340;&#65306;&#39318;&#20808;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#20803;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#29992;&#20197;&#36776;&#21035;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20854;&#34892;&#20026;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#36755;&#20837;&#24207;&#21015;&#30340;&#21512;&#25104;&#36755;&#20986;&#24207;&#21015;&#65307;&#20854;&#27425;&#65292;&#19982;&#21512;&#25104;&#25968;&#25454;&#19968;&#36215;&#65292;&#29992;&#20110;&#23450;&#20041;&#29992;&#20110;&#27169;&#22411;&#20272;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#39564;&#35777;&#25968;&#25454;&#38598;&#29992;&#20110;&#35843;&#25972;&#19968;&#20010;&#26631;&#37327;&#36229;&#21442;&#25968;&#65292;&#24179;&#34913;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05164v1 Announce Type: cross  Abstract: This paper addresses the challenge of overfitting in the learning of dynamical systems by introducing a novel approach for the generation of synthetic data, aimed at enhancing model generalization and robustness in scenarios characterized by data scarcity. Central to the proposed methodology is the concept of knowledge transfer from systems within the same class. Specifically, synthetic data is generated through a pre-trained meta-model that describes a broad class of systems to which the system of interest is assumed to belong. Training data serves a dual purpose: firstly, as input to the pre-trained meta model to discern the system's dynamics, enabling the prediction of its behavior and thereby generating synthetic output sequences for new input sequences; secondly, in conjunction with synthetic data, to define the loss function used for model estimation. A validation dataset is used to tune a scalar hyper-parameter balancing the rel
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05158</link><description>&lt;p&gt;
&#33021;&#37327;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Split Learning over Energy-Constrained Wireless Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05158
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#30340;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#20026;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPEN&#30340;&#22312;&#32447;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#23398;&#20064;&#65288;SL&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#35774;&#22791;&#19982;&#26381;&#21153;&#22120;&#21512;&#20316;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#22522;&#20110;&#30456;&#21516;&#30340;&#22266;&#23450;&#20998;&#35010;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#21644;&#20449;&#36947;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#26041;&#24335;&#22312;&#35757;&#32451;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#35010;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#20013;&#20026;&#35774;&#22791;&#21160;&#24577;&#36873;&#25321;&#20998;&#35010;&#28857;&#65292;&#24182;&#20026;&#26381;&#21153;&#22120;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#28385;&#36275;&#38271;&#26399;&#33021;&#37327;&#28040;&#32791;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#24179;&#22343;&#35757;&#32451;&#24310;&#36831;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#22312;&#20110;&#32570;&#20047;&#26410;&#26469;&#20449;&#24687;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#29702;&#35770;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21517;&#20026;OPEN&#65292;&#23427;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#20010;&#20855;&#26377;&#24403;&#21069;&#30340;&#26032;MIP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05158v1 Announce Type: cross  Abstract: Split learning (SL) is a promising approach for training artificial intelligence (AI) models, in which devices collaborate with a server to train an AI model in a distributed manner, based on a same fixed split point. However, due to the device heterogeneity and variation of channel conditions, this way is not optimal in training delay and energy consumption. In this paper, we design an adaptive split learning (ASL) scheme which can dynamically select split points for devices and allocate computing resource for the server in wireless edge networks. We formulate an optimization problem to minimize the average training latency subject to long-term energy consumption constraint. The difficulties in solving this problem are the lack of future information and mixed integer programming (MIP). To solve it, we propose an online algorithm leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP problem only with the curren
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05152</link><description>&lt;p&gt;
&#26397;&#21521;&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Towards a Psychology of Machines: Large Language Models Predict Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#32570;&#20047;&#20154;&#31867;&#35748;&#30693;&#22522;&#30784;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#31616;&#21333;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#25552;&#20379;&#20851;&#20110;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#27934;&#35265;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25991;&#26412;&#29702;&#35299;&#29702;&#35770;&#65292;&#25105;&#20204;&#20551;&#35774;&#35782;&#21035;&#27169;&#26865;&#20004;&#21487;&#30340;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#22240;&#20026;&#27604;&#23572;&#21917;&#37202;&#65292;&#25152;&#20197;&#37202;&#20174;&#26410;&#30041;&#22312;&#25151;&#23376;&#37324;&#8221;&#65289;&#22312;&#21069;&#38754;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20250;&#24471;&#21040;&#20419;&#36827;&#12290;&#21442;&#19982;&#32773;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;ChatGPT&#65292;&#37117;&#34987;&#21576;&#29616;&#25104;&#23545;&#30340;&#21477;&#23376;&#12290;&#31532;&#20108;&#20010;&#21477;&#23376;&#24635;&#26159;&#19968;&#20010;&#26088;&#22312;&#22266;&#26377;&#22320;&#27169;&#26865;&#20004;&#21487;&#30340;&#33457;&#22253;&#36335;&#24452;&#21477;&#65292;&#32780;&#31532;&#19968;&#20010;&#21477;&#23376;&#21017;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#24739;&#26377;&#24930;&#24615;&#37202;&#31934;&#20013;&#27602;&#8221;&#65289;&#25110;&#19981;&#21512;&#36866;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#21916;&#27426;&#25171;&#39640;&#23572;&#22827;&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Transformer&#26550;&#26500;, &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCSEL&#36870;&#21521;&#35774;&#35745;Tra&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20809;&#23376;&#26230;&#20307;&#38754;&#23556;&#28608;&#20809;&#22120;&#30340;&#36870;&#21521;&#35774;&#35745;&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#21033;&#29992;RL&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;&#28385;&#24847;&#30340;PCSEL&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.05149</link><description>&lt;p&gt;
&#20809;&#23376;&#26230;&#20307;&#38754;&#23556;&#28608;&#20809;&#22120;&#30340;&#36870;&#21521;&#35774;&#35745;&#26159;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05149
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Transformer&#26550;&#26500;, &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCSEL&#36870;&#21521;&#35774;&#35745;Tra&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20809;&#23376;&#26230;&#20307;&#38754;&#23556;&#28608;&#20809;&#22120;&#30340;&#36870;&#21521;&#35774;&#35745;&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#21033;&#29992;RL&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;&#28385;&#24847;&#30340;PCSEL&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05149v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;&#65306;&#20809;&#23376;&#26230;&#20307;&#38754;&#21457;&#23556;&#28608;&#20809;&#22120;&#65288;PCSEL&#65289;&#30340;&#36870;&#21521;&#35774;&#35745;&#38656;&#35201;&#29289;&#29702;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#37327;&#23376;&#21147;&#23398;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#26159;&#26497;&#20855;&#21171;&#21160;&#23494;&#38598;&#24615;&#30340;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21644;&#21152;&#36895;&#36825;&#31181;&#36870;&#21521;&#35774;&#35745;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36890;&#36807;&#23558;PCSEL&#30340;&#36870;&#21521;&#35774;&#35745;&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;RL&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#28385;&#24847;&#30340;PCSEL&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#32447;&#19982;&#31934;&#30830;&#26114;&#36149;&#30340;&#20223;&#30495;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#23548;&#33268;&#30340;&#25968;&#25454;&#20302;&#25928;&#24615;&#38459;&#30861;&#20102;RL&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#36817;&#65292;&#24207;&#21015;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#26550;&#26500;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PCSEL&#36870;&#21521;&#35774;&#35745;Tra&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05149v1 Announce Type: cross  Abstract: Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. Advanced AI technologies, especially reinforcement learning (RL), have emerged as a powerful tool to augment and accelerate this inverse design process. By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch. However, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of RL approaches. Recently, sequential models, especially the Transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models. In this paper, we introduce a novel framework named PCSEL Inverse Design Tra
&lt;/p&gt;</description></item><item><title>ChatUIE&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#32422;&#26463;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05132</link><description>&lt;p&gt;
ChatUIE&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05132
&lt;/p&gt;
&lt;p&gt;
ChatUIE&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#22522;&#20110;&#32842;&#22825;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#32422;&#26463;&#25552;&#39640;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#19968;&#33324;&#30340;&#32842;&#22825;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#65292;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#20559;&#31163;&#24050;&#30693;&#27169;&#24335;&#25110;&#25351;&#20196;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#23545;&#20110;&#20043;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#32842;&#22825;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#20316;&#20026;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatUIE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;ChatGLM&#26500;&#24314;&#30340;&#21019;&#26032;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#26694;&#26550;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#21644;&#23545;&#40784;&#28041;&#21450;&#28151;&#20081;&#21644;&#26377;&#38480;&#26679;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#29983;&#25104;&#32422;&#26463;&#26469;&#35299;&#20915;&#22312;&#36755;&#20837;&#20013;&#19981;&#23384;&#22312;&#30340;&#20803;&#32032;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ChatUIE&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05132v1 Announce Type: cross  Abstract: Recent advancements in large language models have shown impressive performance in general chat. However, their domain-specific capabilities, particularly in information extraction, have certain limitations. Extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. This motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. In this paper, we present ChatUIE, an innovative unified information extraction framework built upon ChatGLM. Simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. Furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. Our experimental results demonstrate that ChatUIE ca
&lt;/p&gt;</description></item><item><title>&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.05131</link><description>&lt;p&gt;
Sora&#20316;&#20026;AGI&#19990;&#30028;&#27169;&#22411;&#65311;&#20851;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#23436;&#25972;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05131
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#21069;&#27839;&#65292;&#25972;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#25991;&#26412;&#24341;&#23548;&#32534;&#36753;&#30340;&#36827;&#23637;&#12290;&#26412;&#35843;&#26597;&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#65292;&#37325;&#28857;&#20851;&#27880;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21521;&#23574;&#31471;Sora&#27169;&#22411;&#36716;&#21464;&#30340;&#36807;&#31243;&#65292;&#31361;&#20986;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;&#21306;&#21035;&#20110;&#20197;&#24448;&#20316;&#21697;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#26694;&#26550;&#21644;&#28436;&#21270;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#21644;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#25191;&#34892;&#22810;&#23454;&#20307;&#22788;&#29702;&#12289;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23398;&#20064;&#12289;&#29702;&#35299;&#29289;&#29702;&#20114;&#21160;&#12289;&#24863;&#30693;&#29289;&#20307;&#32553;&#25918;&#21644;&#27604;&#20363;&#20197;&#21450;&#23545;&#25239;&#29289;&#20307;&#24187;&#35273;&#65292;&#36825;&#20063;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 Announce Type: new  Abstract: Text-to-video generation marks a significant frontier in the rapidly evolving domain of generative AI, integrating advancements in text-to-image synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional generative models to the cutting-edge Sora model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in generative models. Our c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#23558;&#38142;&#24335;&#35268;&#21017;&#20248;&#21270;&#20026;&#26641;&#24418;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#26377;&#25928;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05130</link><description>&lt;p&gt;
&#20174;&#38142;&#21040;&#26641;&#65306;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#23558;&#38142;&#24335;&#35268;&#21017;&#20248;&#21270;&#20026;&#26641;&#24418;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#23558;&#38142;&#24335;&#35268;&#21017;&#20248;&#21270;&#20026;&#26641;&#24418;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#26377;&#25928;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24456;&#22909;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#35832;&#22914;&#30693;&#35782;&#25512;&#29702;&#21644;&#20915;&#31574;&#25903;&#25345;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#38142;&#24335;&#35268;&#21017;&#19978;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#35821;&#20041;&#34920;&#36798;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38142;&#24335;&#35268;&#21017;&#36890;&#24120;&#20250;&#22312;&#19981;&#27491;&#30830;&#30340;&#22522;&#30784;&#20540;&#19978;&#35302;&#21457;&#65292;&#20135;&#29983;&#19981;&#20934;&#30830;&#29978;&#33267;&#38169;&#35823;&#30340;&#25512;&#29702;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26641;&#29366;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#23637;&#24212;&#29992;&#33539;&#22260;&#24182;&#25552;&#39640;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#38142;&#24335;&#35268;&#21017;&#20248;&#21270;&#20026;&#26641;&#29366;&#35268;&#21017;&#12290;&#23545;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#20854;&#20182;&#38142;&#24335;&#35268;&#21017;&#24402;&#32435;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#21518;&#30340;&#26641;&#29366;&#35268;&#21017;&#22987;&#32456;&#22312;&#38142;&#25509;&#39044;&#27979;&#19978;&#34920;&#29616;&#20248;&#20110;&#38142;&#24335;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05130v1 Announce Type: new  Abstract: With good explanatory power and controllability, rule-based methods play an important role in many tasks such as knowledge reasoning and decision support. However, existing studies primarily focused on learning chain-like rules, which limit their semantic expressions and accurate prediction abilities. As a result, chain-like rules usually fire on the incorrect grounding values, producing inaccurate or even erroneous reasoning results. In this paper, we propose the concept of tree-like rules on knowledge graphs to expand the application scope and improve the reasoning ability of rule-based methods. Meanwhile, we propose an effective framework for refining chain-like rules into tree-like rules. Experimental comparisons on four public datasets show that the proposed framework can easily adapt to other chain-like rule induction methods and the refined tree-like rules consistently achieve better performances than chain-like rules on link pred
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#31995;&#32479;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#31181;&#32858;&#21512;&#29289;&#30456;&#20114;&#36830;&#25509;&#65292;&#24418;&#25104;&#20855;&#26377;&#29420;&#29305;&#32467;&#26500;&#30340;&#27700;&#20957;&#33014;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;&#36328;&#38142;&#32593;&#8221;&#12290;</title><link>https://arxiv.org/abs/2403.05129</link><description>&lt;p&gt;
&#25581;&#31034;&#20998;&#23376;&#39764;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#23545;&#36229;&#32423;&#21487;&#20280;&#23637;&#27700;&#20957;&#33014;&#24418;&#25104;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Molecular Magic: AI Insights on the Formation of Extraordinarily Stretchable Hydrogels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#31995;&#32479;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#20004;&#31181;&#32858;&#21512;&#29289;&#30456;&#20114;&#36830;&#25509;&#65292;&#24418;&#25104;&#20855;&#26377;&#29420;&#29305;&#32467;&#26500;&#30340;&#27700;&#20957;&#33014;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#20026;&#8220;&#36328;&#38142;&#32593;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#24847;&#35782;&#22320;&#35843;&#25511;&#36807;&#30827;&#37240;&#38133;&#12289;&#20122;&#30002;&#22522;&#21452;&#19993;&#28911;&#37232;&#33018;&#12289;&#20108;&#30002;&#22522;&#19993;&#28911;&#37232;&#33018;&#21644;&#32858;&#20057;&#28911;&#27687;&#21270;&#29289;&#27987;&#24230;&#65292;&#25104;&#21151;&#30740;&#21457;&#20986;&#19968;&#31181;&#20855;&#26377;&#26497;&#24378;&#21487;&#25289;&#20280;&#24615;&#30340;&#27700;&#20957;&#33014;&#65292;&#33021;&#24310;&#23637;&#33267;&#20854;&#21407;&#22987;&#38271;&#24230;&#30340;260&#20493;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#28508;&#22312;&#30340;&#21453;&#24212;&#26426;&#21046;&#65292;&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#31995;&#32479;&#65292;&#38416;&#26126;&#22312;&#27492;&#29420;&#29305;&#29616;&#35937;&#32972;&#21518;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#22120;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#30456;&#20114;&#36830;&#25509;&#20004;&#31181;&#32858;&#21512;&#29289;&#65292;&#28041;&#21450;&#24418;&#25104;&#19982;&#32447;&#24615;&#38142;&#30456;&#20114;&#36830;&#25509;&#30340;&#32593;&#32476;&#65292;&#36981;&#24490;&#38543;&#26426;&#38142;&#26029;&#35010;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#23548;&#33268;&#20102;&#19968;&#31181;&#29420;&#29305;&#31867;&#22411;&#30340;&#27700;&#20957;&#33014;&#30340;&#20986;&#29616;&#65292;&#27492;&#22788;&#31216;&#20026;&#8220;&#36328;&#38142;&#32593;&#8221;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#32418;&#22806;&#20809;&#35889;&#65288;FTIR&#65289;&#30740;&#31350;&#21487;&#33021;&#28041;&#21450;&#21040;&#25552;&#20986;&#26426;&#21046;&#30340;&#21151;&#33021;&#22522;&#22242;&#65292;&#20027;&#35201;&#21253;&#25324;&#37231;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05129v1 Announce Type: cross  Abstract: The deliberate manipulation of ammonium persulfate, methylenebisacrylamide, dimethyleacrylamide, and polyethylene oxide concentrations resulted in the development of a hydrogel with an exceptional stretchability, capable of extending up to 260 times its original length. This study aims to elucidate the molecular architecture underlying this unique phenomenon by exploring potential reaction mechanisms, facilitated by an artificial intelligence prediction system. Artificial intelligence predictor introduces a novel approach to interlinking two polymers, involving the formation of networks interconnected with linear chains following random chain scission. This novel configuration leads to the emergence of a distinct type of hydrogel, herein referred to as a "Span Network." Additionally, Fourier-transform infrared spectroscopy (FTIR) is used to investigate functional groups that may be implicated in the proposed mechanism, with ester forma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05125</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65306;&#20851;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35780;&#20272;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#36136;&#37327;&#65292;&#22914;&#32654;&#23398;&#21644;&#36924;&#30495;&#24230;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#26469;&#26816;&#26597;&#25991;&#26412;&#26465;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;&#27010;&#24565;&#35206;&#30422;&#33539;&#22260;&#30340;&#25506;&#32034;&#35843;&#26597;&#20102;&#27169;&#22411;&#22312;&#20934;&#30830;&#35299;&#37322;&#21644;&#21576;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#20154;&#31867;&#22270;&#20687;&#65292;&#20294;&#36825;&#31181;&#21452;&#37325;&#26041;&#38754;&#30340;&#26041;&#27861;&#26159;&#20026;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05125v1 Announce Type: cross  Abstract: In this paper, we present an empirical study introducing a nuanced evaluation framework for text-to-image (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and fairness. We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model's effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of fairness reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the 
&lt;/p&gt;</description></item><item><title>RLPeri&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#26368;&#20339;&#20301;&#32622;&#39034;&#24207;&#21644;&#21021;&#22987;&#21050;&#28608;&#20540;&#26469;&#21152;&#36895;&#35270;&#37326;&#27979;&#23450;&#27979;&#35797;&#65292;&#21516;&#26102;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#36827;&#19968;&#27493;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05112</link><description>&lt;p&gt;
RLPeri: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#21367;&#31215;&#29305;&#24449;&#25552;&#21462;&#21152;&#36895;&#35270;&#37326;&#27979;&#23450;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05112
&lt;/p&gt;
&lt;p&gt;
RLPeri&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#30830;&#23450;&#26368;&#20339;&#20301;&#32622;&#39034;&#24207;&#21644;&#21021;&#22987;&#21050;&#28608;&#20540;&#26469;&#21152;&#36895;&#35270;&#37326;&#27979;&#23450;&#27979;&#35797;&#65292;&#21516;&#26102;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#36827;&#19968;&#27493;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#37326;&#27979;&#23450;&#26159;&#19968;&#39033;&#24110;&#21161;&#26816;&#27979;&#30001;&#30524;&#37096;&#25110;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#24341;&#36215;&#30340;&#35270;&#21147;&#38382;&#39064;&#30340;&#37325;&#35201;&#30524;&#37096;&#26816;&#26597;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#24739;&#32773;&#23558;&#20957;&#35270;&#22312;&#29305;&#23450;&#20301;&#32622;&#65292;&#21516;&#26102;&#22312;&#20013;&#24515;&#21644;&#21608;&#22260;&#35270;&#37326;&#21576;&#29616;&#19981;&#21516;&#24378;&#24230;&#30340;&#20809;&#21050;&#28608;&#12290;&#26681;&#25454;&#24739;&#32773;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#65292;&#30830;&#23450;&#35270;&#37326;&#26144;&#23556;&#21644;&#25935;&#24863;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#24739;&#32773;&#26469;&#35828;&#22312;&#25972;&#20010;&#27979;&#35797;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#24230;&#38598;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#26816;&#26597;&#26102;&#38388;&#22686;&#21152;&#65292;&#31934;&#24230;&#38477;&#20302;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RLPeri&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#35270;&#37326;&#27979;&#23450;&#27979;&#35797;&#12290;&#36890;&#36807;&#30830;&#23450;&#26368;&#20339;&#20301;&#32622;&#39034;&#24207;&#21644;&#21021;&#22987;&#21050;&#28608;&#20540;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20943;&#23569;&#26816;&#26597;&#26102;&#38388;&#32780;&#19981;&#24433;&#21709;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05112v1 Announce Type: new  Abstract: Visual perimetry is an important eye examination that helps detect vision problems caused by ocular or neurological conditions. During the test, a patient's gaze is fixed at a specific location while light stimuli of varying intensities are presented in central and peripheral vision. Based on the patient's responses to the stimuli, the visual field mapping and sensitivity are determined. However, maintaining high levels of concentration throughout the test can be challenging for patients, leading to increased examination times and decreased accuracy.   In this work, we present RLPeri, a reinforcement learning-based approach to optimize visual perimetry testing. By determining the optimal sequence of locations and initial stimulus values, we aim to reduce the examination time without compromising accuracy. Additionally, we incorporate reward shaping techniques to further improve the testing performance. To monitor the patient's responses 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05110</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#36890;&#36807;&#32452;&#21512;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Collection for Robotic Manipulation via Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05110
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#22320;&#25910;&#38598;&#25968;&#25454;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#32570;&#20047;&#24456;&#22810;&#29702;&#35299;&#12290;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#25910;&#38598;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#21464;&#21270;&#20102;&#35768;&#22810;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#29289;&#20307;&#31867;&#22411;&#21644;&#26700;&#38754;&#32441;&#29702;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#35797;&#22270;&#28085;&#30422;&#21508;&#31181;&#21508;&#26679;&#30340;&#22330;&#26223;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#21040;&#22522;&#20110;&#25968;&#25454;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#30340;&#22797;&#21512;&#33021;&#21147;&#12290;&#22914;&#26524;&#26426;&#22120;&#20154;&#31574;&#30053;&#33021;&#22815;&#20174;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32452;&#21512;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#65288;&#20363;&#22914;&#29289;&#20307;&#31867;&#22411;&#12289;&#26700;&#38754;&#39640;&#24230;&#65289;&#20197;&#22312;&#36935;&#21040;&#30475;&#19981;&#35265;&#30340;&#22240;&#32032;&#32452;&#21512;&#26102;&#25104;&#21151;&#65292;&#37027;&#20040;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#36991;&#20813;&#20026;&#22797;&#21512;&#22788;&#29702;&#30340;&#24773;&#20917;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05110v1 Announce Type: cross  Abstract: Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary a wide range of environmental factors during data collection, such as object types and table textures. While these works attempt to cover a diverse variety of scenarios, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies are able to compose different environmental factors of variation (e.g., object types, table heights) from their training data to succeed when encountering unseen factor combinations, then we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#32852;&#30431;&#24037;&#20316;&#33021;&#21147;&#21644;&#20219;&#21153;&#38656;&#27714;&#20851;&#31995;&#30340;&#26032;&#22411;&#22810;&#26080;&#20154;&#26426;&#32852;&#30431;&#32593;&#32476;&#21327;&#20316;&#20219;&#21153;&#23436;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32852;&#30431;&#25910;&#20837;&#38408;&#20540;&#30340;&#25910;&#20837;&#20989;&#25968;&#21050;&#28608;&#21305;&#37197;&#20219;&#21153;&#38656;&#27714;&#30340;&#32852;&#30431;&#24418;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#25928;&#29992;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.05108</link><description>&lt;p&gt;
&#20219;&#21153;&#39537;&#21160;&#30340;&#22810;&#26080;&#20154;&#26426;&#32852;&#30431;&#24418;&#25104;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Task-Driven Multi-UAV Coalition Formation Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#32852;&#30431;&#24037;&#20316;&#33021;&#21147;&#21644;&#20219;&#21153;&#38656;&#27714;&#20851;&#31995;&#30340;&#26032;&#22411;&#22810;&#26080;&#20154;&#26426;&#32852;&#30431;&#32593;&#32476;&#21327;&#20316;&#20219;&#21153;&#23436;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32852;&#30431;&#25910;&#20837;&#38408;&#20540;&#30340;&#25910;&#20837;&#20989;&#25968;&#21050;&#28608;&#21305;&#37197;&#20219;&#21153;&#38656;&#27714;&#30340;&#32852;&#30431;&#24418;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#25928;&#29992;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26080;&#20154;&#26426;&#32852;&#30431;&#24418;&#25104;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#28857;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20219;&#21153;&#39537;&#21160;&#30340;&#22810;&#26080;&#20154;&#26426;&#32852;&#30431;&#24418;&#25104;&#26426;&#21046;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#30431;&#24418;&#25104;&#26426;&#21046;&#23384;&#22312;&#30528;&#26080;&#20154;&#26426;&#19982;&#20219;&#21153;&#35201;&#27714;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#20302;&#65292;&#23548;&#33268;&#25972;&#20307;&#32852;&#30431;&#25928;&#29992;&#20302;&#19988;&#32852;&#30431;&#32467;&#26500;&#19981;&#31283;&#23450;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26080;&#20154;&#26426;&#32852;&#30431;&#32593;&#32476;&#21327;&#20316;&#20219;&#21153;&#23436;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#32852;&#30431;&#24037;&#20316;&#33021;&#21147;&#21644;&#20219;&#21153;&#38656;&#27714;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#32852;&#30431;&#25910;&#20837;&#38408;&#20540;&#30340;&#25910;&#20837;&#20989;&#25968;&#26469;&#21050;&#28608;&#19982;&#20219;&#21153;&#35201;&#27714;&#21305;&#37197;&#30340;&#32852;&#30431;&#24418;&#25104;&#12290;&#38543;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#25928;&#29992;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;Shapley&#20540;&#23454;&#29616;&#20102;&#32852;&#30431;&#20869;&#20844;&#24179;&#25928;&#29992;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05108v1 Announce Type: cross  Abstract: With the rapid advancement of UAV technology, the problem of UAV coalition formation has become a hotspot. Therefore, designing task-driven multi-UAV coalition formation mechanism has become a challenging problem. However, existing coalition formation mechanisms suffer from low relevance between UAVs and task requirements, resulting in overall low coalition utility and unstable coalition structures. To address these problems, this paper proposed a novel multi-UAV coalition network collaborative task completion model, considering both coalition work capacity and task-requirement relationships. This model stimulated the formation of coalitions that match task requirements by using a revenue function based on the coalition's revenue threshold. Subsequently, an algorithm for coalition formation based on marginal utility was proposed. Specifically, the algorithm utilized Shapley value to achieve fair utility distribution within the coalitio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;&#26694;&#26550;L2RM&#65292;&#23398;&#20064;&#37325;&#26032;&#21305;&#37197;&#19981;&#21305;&#37197;&#30340;&#23545;&#65292;&#20197;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30001;&#37096;&#20998;&#19981;&#21305;&#37197;&#23545;&#24341;&#36215;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05105</link><description>&lt;p&gt;
&#23398;&#20064;&#37325;&#26032;&#21305;&#37197;&#19981;&#21305;&#37197;&#30340;&#23545;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;&#26694;&#26550;L2RM&#65292;&#23398;&#20064;&#37325;&#26032;&#21305;&#37197;&#19981;&#21305;&#37197;&#30340;&#23545;&#65292;&#20197;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30001;&#37096;&#20998;&#19981;&#21305;&#37197;&#23545;&#24341;&#36215;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21305;&#37197;&#33391;&#22909;&#30340;&#22810;&#23186;&#20307;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#26816;&#32034;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#22823;&#37327;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#65292;&#20854;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#37096;&#20998;&#19981;&#21305;&#37197;&#30340;&#23545;&#12290;&#36825;&#20123;&#35821;&#20041;&#19981;&#30456;&#20851;&#30340;&#25968;&#25454;&#23558;&#26174;&#33879;&#25439;&#23475;&#36328;&#27169;&#24577;&#26816;&#32034;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20542;&#21521;&#20110;&#36890;&#36807;&#20272;&#35745;&#36719;&#23545;&#24212;&#20851;&#31995;&#26469;&#20943;&#23567;&#37096;&#20998;&#19981;&#21305;&#37197;&#23545;&#30340;&#36129;&#29486;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#21363;&#26410;&#37197;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#28508;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#21487;&#33021;&#20351;&#24471;&#20174;&#19981;&#21305;&#37197;&#23545;&#20013;&#25366;&#25496;&#26377;&#29992;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;L2RM&#65292;&#19968;&#20010;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23398;&#20064;&#37325;&#26032;&#21305;&#37197;&#19981;&#21305;&#37197;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;L2RM&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#26368;&#23567;&#25104;&#26412;&#20256;&#36755;&#35745;&#21010;&#26469;&#29983;&#25104;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05105v1 Announce Type: cross  Abstract: Collecting well-matched multimedia datasets is crucial for training cross-modal retrieval models. However, in real-world scenarios, massive multimodal data are harvested from the Internet, which inevitably contains Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data will remarkably harm the cross-modal retrieval performance. Previous efforts tend to mitigate this problem by estimating a soft correspondence to down-weight the contribution of PMPs. In this paper, we aim to address this challenge from a new perspective: the potential semantic similarity among unpaired samples makes it possible to excavate useful knowledge from mismatched pairs. To achieve this, we propose L2RM, a general framework based on Optimal Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to generate refined alignments by seeking a minimal-cost transport plan across different modalities. To formalize the remat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#29420;&#31435;&#21644;&#30456;&#20114;&#20381;&#23384;&#30340;&#33258;&#25105;&#21644;&#29615;&#22659;&#25991;&#21270;&#27169;&#22411;&#26469;&#25299;&#23637;&#12289;&#37325;&#26032;&#24819;&#35937;&#21644;&#37325;&#35270;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#24895;&#26223;&#30340;&#30740;&#31350;&#12290;&#35843;&#26597;&#26174;&#31034;&#65292;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#23384;&#22312;&#24046;&#24322;&#65292;&#20013;&#22269;&#21463;&#35775;&#32773;&#26356;&#30475;&#37325;&#19982;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#32852;&#31995;&#65292;&#26356;&#20559;&#22909;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;ostringstream&#20013;&#20063;&#23384;&#22312;&#27431;&#35028;&#32654;&#22269;&#21644;&#20013;&#22269;&#21463;&#35775;&#32773;&#25991;&#21270;&#27169;&#22411;&#30340;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.05104</link><description>&lt;p&gt;
&#25991;&#21270;&#22914;&#20309;&#22609;&#36896;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;
&lt;/p&gt;
&lt;p&gt;
How Culture Shapes What People Want From AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#29420;&#31435;&#21644;&#30456;&#20114;&#20381;&#23384;&#30340;&#33258;&#25105;&#21644;&#29615;&#22659;&#25991;&#21270;&#27169;&#22411;&#26469;&#25299;&#23637;&#12289;&#37325;&#26032;&#24819;&#35937;&#21644;&#37325;&#35270;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#24895;&#26223;&#30340;&#30740;&#31350;&#12290;&#35843;&#26597;&#26174;&#31034;&#65292;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26399;&#26395;&#23384;&#22312;&#24046;&#24322;&#65292;&#20013;&#22269;&#21463;&#35775;&#32773;&#26356;&#30475;&#37325;&#19982;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#32852;&#31995;&#65292;&#26356;&#20559;&#22909;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;ostringstream&#20013;&#20063;&#23384;&#22312;&#27431;&#35028;&#32654;&#22269;&#21644;&#20013;&#22269;&#21463;&#35775;&#32773;&#25991;&#21270;&#27169;&#22411;&#30340;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#36843;&#20999;&#38656;&#35201;&#23558;&#19981;&#21516;&#25991;&#21270;&#32676;&#20307;&#30340;&#35266;&#28857;&#32435;&#20837;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#29420;&#31435;&#21644;&#30456;&#20114;&#20381;&#23384;&#30340;&#33258;&#25105;&#21644;&#29615;&#22659;&#25991;&#21270;&#27169;&#22411;&#26469;&#25299;&#23637;&#12289;&#37325;&#26032;&#24819;&#35937;&#21644;&#37325;&#35270;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#24895;&#26223;&#30340;&#30740;&#31350;&#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#12290;&#20004;&#39033;&#35843;&#30740;&#25903;&#25345;&#20102;&#36825;&#19968;&#26694;&#26550;&#65292;&#24182;&#21021;&#27493;&#35777;&#26126;&#20154;&#20204;&#22312;&#24819;&#35937;&#20182;&#20204;&#29702;&#24819;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#20250;&#24212;&#29992;&#20182;&#20204;&#30340;&#25991;&#21270;&#27169;&#22411;&#12290;&#19982;&#27431;&#35028;&#32654;&#22269;&#21463;&#35775;&#32773;&#30456;&#27604;&#65292;&#20013;&#22269;&#21463;&#35775;&#32773;&#35748;&#20026;&#25511;&#21046;&#20154;&#24037;&#26234;&#33021;&#19981;&#22826;&#37325;&#35201;&#65292;&#19982;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#32852;&#31995;&#26356;&#20026;&#37325;&#35201;&#65292;&#24182;&#26356;&#20542;&#21521;&#20110;&#21916;&#27426;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#20174;&#38750;&#35028;&#32654;&#22269;&#21463;&#35775;&#32773;&#30340;&#21457;&#29616;&#21453;&#26144;&#20102;&#27431;&#35028;&#32654;&#22269;&#21644;&#20013;&#22269;&#21463;&#35775;&#32773;&#30340;&#25991;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#25991;&#21270;&#21709;&#24212;&#24615;&#21644;&#30456;&#20851;&#24615;&#20154;&#24037;&#26234;&#33021;&#20197;&#26381;&#21153;&#26356;&#24191;&#27867;&#20154;&#32676;&#30340;&#38656;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05104v1 Announce Type: cross  Abstract: There is an urgent need to incorporate the perspectives of culturally diverse groups into AI developments. We present a novel conceptual framework for research that aims to expand, reimagine, and reground mainstream visions of AI using independent and interdependent cultural models of the self and the environment. Two survey studies support this framework and provide preliminary evidence that people apply their cultural models when imagining their ideal AI. Compared with European American respondents, Chinese respondents viewed it as less important to control AI and more important to connect with AI, and were more likely to prefer AI with capacities to influence. Reflecting both cultural models, findings from African American respondents resembled both European American and Chinese respondents. We discuss study limitations and future directions and highlight the need to develop culturally responsive and relevant AI to serve a broader s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05066</link><description>&lt;p&gt;
&#22797;&#20301;&#21644;&#25552;&#28860;&#65306;&#20811;&#26381;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05066
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#21457;&#23637;&#26377;&#25928;&#30340;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24403;&#38656;&#35201;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#21457;&#29983;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#38382;&#39064;&#22312;CRL&#20013;&#32463;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#26368;&#36817;&#19968;&#20123;&#26088;&#22312;&#20943;&#36731;RL&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#24037;&#20316;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;CRL&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;R&amp;D&#32467;&#21512;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#37325;&#32622;&#20195;&#29702;&#30340;&#22312;&#32447;&#28436;&#21592;&#21644;&#35780;&#35770;&#32593;&#32476;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20197;&#21450;&#31163;&#32447;&#23398;&#20064;&#27493;&#39588;&#65292;&#29992;&#20110;&#25552;&#28860;&#22312;&#32447;&#28436;&#21592;&#21644;&#20197;&#21069;&#19987;&#23478;&#21160;&#20316;&#27010;&#29575;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;Meta-World&#20219;&#21153;&#30340;&#38271;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#32447;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05066v1 Announce Type: cross  Abstract: We argue that one of the main obstacles for developing effective Continual Reinforcement Learning (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates acr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;</title><link>https://arxiv.org/abs/2403.05064</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#19982;&#35299;&#32806;&#33258;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph Neural Architecture Search with Disentangled Self-supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05064
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#30417;&#30563;&#26631;&#31614;&#65292;&#26080;&#27861;&#22788;&#29702;&#30417;&#30563;&#19981;&#21487;&#29992;&#30340;&#26222;&#36941;&#24773;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#38382;&#39064;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#25506;&#32034;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#21457;&#29616;&#39537;&#21160;&#22270;&#25968;&#25454;&#24418;&#25104;&#20197;&#21450;&#28508;&#22312;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#22240;&#32032;&#19982;&#26368;&#20248;&#31070;&#32463;&#26550;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30001;&#20110;&#22270;&#30340;&#24615;&#36136;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#33258;&#30417;&#30563;&#22270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DSGAS&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#21457;&#29616;&#25429;&#33719;&#21508;&#31181;&#28508;&#22312;&#22270;&#22240;&#32032;&#30340;&#26368;&#20339;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05064v1 Announce Type: cross  Abstract: The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent graph factors in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05063</link><description>&lt;p&gt;
&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Controllable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24322;&#24120;&#30340;&#26234;&#33021;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#24320;&#22987;&#25506;&#32034;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#24320;&#21019;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479; - &#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#23545;&#35805;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25511;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25972;&#21512;&#21040;LLMs&#20013;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#32452;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#26631;&#35760;&#26469;&#28304;&#20110;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#30340;&#26631;&#31614;&#65292;&#26088;&#22312;&#26126;&#30830;&#25913;&#21892;LLMs&#36981;&#24490;&#29305;&#23450;&#25512;&#33616;&#25351;&#20196;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#31243;&#24207;&#65292;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;LLMs&#22312;&#21709;&#24212;&#29992;&#25143;&#24847;&#22270;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26631;&#35760;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v1 Announce Type: cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer&#65306;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#24555;&#36895;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#32452;&#21512;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26469;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#22312;&#21512;&#25104;&#20013;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#36807;&#22810;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#65292;&#21363;&#20351;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#36825;&#20123;&#38382;&#39064;&#24694;&#21270;&#12290;&#36825;&#19981;&#20165;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#25439;&#23475;&#20102;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#38656;&#35201;&#30340;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#32534;&#36753;&#21518;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v1 Announce Type: cross  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tr
&lt;/p&gt;</description></item><item><title>DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;</title><link>https://arxiv.org/abs/2403.05050</link><description>&lt;p&gt;
DyRoNet&#65306;&#19968;&#31181;&#20302;&#31209;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65292;&#29992;&#20110;&#27969;&#23186;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for Streaming Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05050
&lt;/p&gt;
&lt;p&gt;
DyRoNet&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#24182;&#32467;&#21512;&#20998;&#25903;&#32593;&#32476;&#20248;&#21270;&#27969;&#23186;&#20307;&#24863;&#30693;&#24615;&#33021;&#65292;&#20026;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#35774;&#23450;&#20102;&#26032;&#30340;&#24615;&#33021;&#26631;&#26438;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#24863;&#30693;&#26469;&#24212;&#23545;&#22797;&#26434;&#29615;&#22659;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#36335;&#30001;&#32593;&#32476;&#65288;DyRoNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#20302;&#31209;&#21160;&#24577;&#36335;&#30001;&#20197;&#22686;&#24378;&#27969;&#23186;&#20307;&#24863;&#30693;&#12290;&#36890;&#36807;&#38598;&#25104;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#38024;&#23545;&#21508;&#31181;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#24494;&#35843;&#65292;DyRoNet&#22312;&#24310;&#36831;&#21644;&#31934;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#20854;&#26680;&#24515;&#29305;&#24449;&#26159;&#36895;&#24230;&#36335;&#30001;&#27169;&#22359;&#65292;&#26234;&#33021;&#22320;&#23558;&#36755;&#20837;&#25968;&#25454;&#24341;&#23548;&#21040;&#26368;&#36866;&#21512;&#30340;&#20998;&#25903;&#32593;&#32476;&#65292;&#20248;&#21270;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;DyRoNet&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#31181;&#20998;&#25903;&#36873;&#25321;&#31574;&#30053;&#65292;&#20026;&#21508;&#31181;&#22330;&#26223;&#24615;&#33021;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#26438;&#12290;DyRoNet&#19981;&#20165;&#20026;&#27969;&#23186;&#20307;&#24863;&#30693;&#24314;&#31435;&#20102;&#26032;&#30340;&#26631;&#26438;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#24037;&#31243;&#27934;&#35265;&#12290;&#26377;&#20851;&#26356;&#22810;&#39033;&#30446;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382; https://tastevision.github.io/DyRoNet/
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05050v1 Announce Type: cross  Abstract: Autonomous driving systems demand real-time, accurate perception to navigate complex environments. Addressing this, we introduce the Dynamic Router Network (DyRoNet), a framework that innovates with low-rank dynamic routing for enhanced streaming perception. By integrating specialized pre-trained branch networks, fine-tuned for various environmental conditions, DyRoNet achieves a balance between latency and precision. Its core feature, the speed router module, intelligently directs input data to the best-suited branch network, optimizing performance. The extensive evaluations reveal that DyRoNet adapts effectively to multiple branch selection strategies, setting a new benchmark in performance across a range of scenarios. DyRoNet not only establishes a new benchmark for streaming perception but also provides valuable engineering insights for future work. More project information is available at https://tastevision.github.io/DyRoNet/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.05045</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#35805;&#26159;&#21542;&#29305;&#27530;&#65311;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Are Human Conversations Special? A Large Language Model Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#23545;&#35805;&#26102;&#30340;&#27880;&#24847;&#26426;&#21046;&#21464;&#21270;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#22312;&#19987;&#38376;&#22788;&#29702;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#65292;&#38656;&#35201;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#20154;&#31867;&#20043;&#38388;&#30340;&#33258;&#28982;&#23545;&#35805;&#65288;&#20154;-&#20154;&#65289;&#26102;&#27880;&#24847;&#26426;&#21046;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19977;&#31181;&#29992;&#20363;&#65306;&#19982;&#32593;&#32476;&#20869;&#23481;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#25991;&#26412;&#30340;&#20114;&#21160;&#12290;&#36890;&#36807;&#20998;&#26512;&#36328;&#36825;&#20123;&#39046;&#22495;&#30340;&#27880;&#24847;&#36317;&#31163;&#12289;&#20998;&#25955;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#35805;&#25968;&#25454;&#25152;&#25552;&#20986;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#35805;&#38656;&#35201;&#32454;&#33268;&#22788;&#29702;&#38271;&#26399;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#30340;&#27880;&#24847;&#27169;&#24335;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#27880;&#24847;&#34892;&#20026;&#65292;&#20294;&#23427;&#20204;&#22312;&#19987;&#38376;&#21270;&#20154;&#31867;&#23545;&#35805;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#27880;&#24847;&#29109;&#20998;&#26512;&#21644;t-SNE&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38656;&#35201;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#23545;&#35805;&#25968;&#25454;&#26469;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05045v1 Announce Type: cross  Abstract: This study analyzes changes in the attention mechanisms of large language models (LLMs) when used to understand natural conversations between humans (human-human). We analyze three use cases of LLMs: interactions over web content, code, and mathematical texts. By analyzing attention distance, dispersion, and interdependency across these domains, we highlight the unique challenges posed by conversational data. Notably, conversations require nuanced handling of long-term contextual relationships and exhibit higher complexity through their attention patterns. Our findings reveal that while language models exhibit domain-specific attention behaviors, there is a significant gap in their ability to specialize in human conversations. Through detailed attention entropy analysis and t-SNE visualizations, we demonstrate the need for models trained with a diverse array of high-quality conversational data to enhance understanding and generation of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#24230;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05033</link><description>&lt;p&gt;
&#37327;&#21270;&#27969;&#24418;:&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#30340;&#27969;&#24418;&#26159;&#21542;&#25910;&#25947;&#20110;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Quantifying Manifolds: Do the manifolds learned by Generative Adversarial Networks converge to the real data manifold
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#24230;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;ML&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;GAN&#27169;&#22411;&#65289;&#23398;&#20064;&#30340;&#27969;&#24418;&#38543;&#30528;&#35757;&#32451;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#27599;&#20010;&#26102;&#26399;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#19982;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#30340;&#30495;&#23454;&#27969;&#24418;&#12290;&#20026;&#20102;&#37327;&#21270;&#19968;&#20010;&#27969;&#24418;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#36825;&#20123;&#24230;&#37327;&#38543;&#30528;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#27169;&#22411;&#32780;&#22914;&#20309;&#21464;&#21270;&#65292;&#20197;&#21450;&#36825;&#20123;&#24230;&#37327;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25910;&#25947;&#21040;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05033v1 Announce Type: cross  Abstract: This paper presents our experiments to quantify the manifolds learned by ML models (in our experiment, we use a GAN model) as they train. We compare the manifolds learned at each epoch to the real manifolds representing the real data. To quantify a manifold, we study the intrinsic dimensions and topological features of the manifold learned by the ML model, how these metrics change as we continue to train the model, and whether these metrics convergence over the course of training to the metrics of the real data manifold.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;ChatTraffic&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#20132;&#36890;&#31995;&#32479;&#25551;&#36848;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39044;&#27979;&#20013;&#20851;&#32852;&#25991;&#26412;&#21644;&#31354;&#38388;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05029</link><description>&lt;p&gt;
BjTT: &#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BjTT: A Large-scale Multimodal Dataset for Traffic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;ChatTraffic&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#20132;&#36890;&#31995;&#32479;&#25551;&#36848;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39044;&#27979;&#20013;&#20851;&#32852;&#25991;&#26412;&#21644;&#31354;&#38388;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05029v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#22522;&#30784;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20165;&#20381;&#36182;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26469;&#39044;&#27979;&#20132;&#36890;&#36235;&#21183;&#65292;&#24182;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;1&#65289;&#23545;&#24322;&#24120;&#20107;&#20214;&#19981;&#25935;&#24863;&#12290;2&#65289;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#24615;&#33021;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#20132;&#36890;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#35813;&#20219;&#21153;&#20026;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#65288;TTG&#65289;&#12290;TTG &#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23558;&#25991;&#26412;&#19982;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#20132;&#36890;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#20197;&#29983;&#25104;&#20132;&#36890;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTraffic&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#35777;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21462;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05029v1 Announce Type: new  Abstract: Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#30340;&#35889;&#22495;&#20869;&#30740;&#31350;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#35889;&#22495;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05026</link><description>&lt;p&gt;
&#20998;&#24067;&#28418;&#31227;&#19979;&#21160;&#24577;&#22270;&#30340;&#35889;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05026
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#30340;&#35889;&#22495;&#20869;&#30740;&#31350;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#35889;&#22495;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DyGNNs&#65289;&#30446;&#21069;&#22312;&#22788;&#29702;&#21160;&#24577;&#22270;&#22266;&#26377;&#30340;&#20998;&#24067;&#28418;&#31227;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#29616;&#26377;&#38024;&#23545;DyGNNs&#22312;&#20998;&#24067;&#28418;&#31227;&#35774;&#32622;&#20013;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#26102;&#38388;&#22495;&#65292;&#26410;&#33021;&#22788;&#29702;&#28041;&#21450;&#35889;&#22495;&#20013;&#20998;&#24067;&#28418;&#31227;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#21457;&#29616;&#23384;&#22312;&#19968;&#20123;&#24773;&#20917;&#65292;&#26102;&#38388;&#22495;&#20013;&#35266;&#23519;&#19981;&#21040;&#30340;&#20998;&#24067;&#28418;&#31227;&#21364;&#22312;&#35889;&#22495;&#20013;&#21487;&#20197;&#35266;&#23519;&#21040;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#30740;&#31350;&#21160;&#24577;&#22270;&#35889;&#22495;&#20869;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;i&#65289;&#25429;&#25417;&#35889;&#22495;&#20013;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#19981;&#21516;&#39057;&#29575;&#20998;&#37327;&#39537;&#21160;&#30340;&#19981;&#21516;&#22270;&#27169;&#24335;&#24182;&#38750;&#26131;&#20107;&#65307;ii&#65289;&#22914;&#20309;&#22788;&#29702;&#19982;&#21457;&#29616;&#30340;&#35889;&#27169;&#24335;&#30456;&#20851;&#30340;&#20998;&#24067;&#28418;&#31227;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#19979;&#21160;&#24577;&#22270;&#30340;&#35889;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05026v1 Announce Type: cross  Abstract: Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Sh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#21644;&#20998;&#26512;&#20027;&#39064;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SuCI&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#30340;&#21435;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;MIU&#27169;&#22411;&#21463;&#20027;&#20307;&#21464;&#24322;&#38382;&#39064;&#22256;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05025</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#39064;&#21435;&#30456;&#20851;&#23454;&#29616;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Human Intention Understanding Debiasing via Subject-Deconfounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05025
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#21644;&#20998;&#26512;&#20027;&#39064;&#28151;&#28102;&#25928;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SuCI&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#30340;&#21435;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;MIU&#27169;&#22411;&#21463;&#20027;&#20307;&#21464;&#24322;&#38382;&#39064;&#22256;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05025v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#22810;&#27169;&#24577;&#24847;&#22270;&#29702;&#35299;(MIU)&#26159;&#20154;&#31867;&#34920;&#36798;&#20998;&#26512;(&#20363;&#22914;&#24773;&#24863;&#25110;&#24189;&#40664;)&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#28041;&#21450;&#35270;&#35273;&#23039;&#21183;&#12289;&#35821;&#35328;&#20869;&#23481;&#21644;&#22768;&#23398;&#34892;&#20026;&#31561;&#24322;&#26500;&#27169;&#24577;&#12290;&#29616;&#26377;&#24037;&#20316;&#22987;&#32456;&#19987;&#27880;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#32467;&#26500;&#25110;&#34701;&#21512;&#31574;&#30053;&#65292;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#21463;&#21040;&#20027;&#39064;&#21464;&#24322;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#23548;&#33268;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#20855;&#26377;&#19981;&#21516;&#34920;&#36798;&#20064;&#24815;&#21644;&#29305;&#24449;&#30340;&#19981;&#21516;&#20027;&#39064;&#65292;MIU&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20197;&#23398;&#20064;&#29305;&#23450;&#20110;&#20027;&#39064;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26174;&#30528;&#38480;&#21046;&#20102;&#36328;&#26410;&#25509;&#35302;&#20027;&#39064;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#25324;&#24615;&#22240;&#26524;&#22270;&#26469;&#21046;&#23450;MIU&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20027;&#39064;&#30340;&#28151;&#28102;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SuCI&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22240;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05025v1 Announce Type: new  Abstract: Multimodal intention understanding (MIU) is an indispensable component of human expression analysis (e.g., sentiment or humor) from heterogeneous modalities, including visual postures, linguistic contents, and acoustic behaviors. Existing works invariably focus on designing sophisticated structures or fusion strategies to achieve impressive improvements. Unfortunately, they all suffer from the subject variation problem due to data distribution discrepancies among subjects. Concretely, MIU models are easily misled by distinct subjects with different expression customs and characteristics in the training data to learn subject-specific spurious correlations, significantly limiting performance and generalizability across uninitiated subjects.Motivated by this observation, we introduce a recapitulative causal graph to formulate the MIU procedure and analyze the confounding effect of subjects. Then, we propose SuCI, a simple yet effective caus
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#21644;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22810;&#22270;&#21367;&#31215;&#30340;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.05014</link><description>&lt;p&gt;
&#31616;&#21333;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Simple Multigraph Convolution Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#21644;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#22810;&#22270;&#21367;&#31215;&#30340;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#22270;&#21367;&#31215;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#22810;&#20010;&#22270;&#20043;&#38388;&#30340;&#20132;&#21449;&#35270;&#22270;&#20132;&#20114;&#65292;&#35201;&#20040;&#30001;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#35270;&#22270;&#22810;&#39033;&#24335;&#36816;&#31639;&#31526;&#32780;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26497;&#39640;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SMGCN&#65289;&#65292;&#23427;&#39318;&#20808;&#20174;&#22810;&#22270;&#20013;&#25552;&#21462;&#19968;&#33268;&#30340;&#20132;&#21449;&#35270;&#22270;&#25299;&#25169;&#65292;&#21253;&#25324;&#36793;&#32423;&#21644;&#23376;&#22270;&#32423;&#25299;&#25169;&#65292;&#28982;&#21518;&#22522;&#20110;&#21407;&#22987;&#22810;&#22270;&#21644;&#19968;&#33268;&#30340;&#25299;&#25169;&#25191;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;SMGCN&#21033;&#29992;&#19968;&#33268;&#30340;&#25299;&#25169;&#36827;&#34892;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#20132;&#21449;&#35270;&#22270;&#22810;&#39033;&#24335;&#23637;&#24320;&#65292;&#20174;&#32780;&#25191;&#34892;&#21487;&#20449;&#30340;&#20132;&#21449;&#35270;&#22270;&#31354;&#38388;&#28040;&#24687;&#20256;&#36882;&#65292;&#36981;&#24490;&#35889;&#21367;&#31215;&#33539;&#24335;&#65292;&#24182;&#26377;&#25928;&#38477;&#20302;&#26631;&#20934;&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SMGCN&#22312;ACM&#21644;DBLP&#22810;&#22270;&#22522;&#20934;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05014v1 Announce Type: cross  Abstract: Existing multigraph convolution methods either ignore the cross-view interaction among multiple graphs, or induce extremely high computational cost due to standard cross-view polynomial operators. To alleviate this problem, this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which first extracts consistent cross-view topology from multigraphs including edge-level and subgraph-level topology, then performs polynomial expansion based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes the consistent topologies in polynomial expansion rather than standard cross-view polynomial expansion, which performs credible cross-view spatial message-passing, follows the spectral convolution paradigm, and effectively reduces the complexity of standard polynomial expansion. In the simulations, experimental results demonstrate that SMGCN achieves state-of-the-art performance on ACM and DBLP multigraph benchmark datas
&lt;/p&gt;</description></item><item><title>RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05010</link><description>&lt;p&gt;
RFWave&#65306;&#29992;&#20110;&#38899;&#39057;&#27874;&#24418;&#37325;&#24314;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05010
&lt;/p&gt;
&lt;p&gt;
RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#20174;&#19981;&#21516;&#34920;&#31034;&#20013;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#20010;&#21035;&#26679;&#26412;&#28857;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#22240;&#27492;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#24310;&#36831;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RFWave&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#23427;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#12290;RFWave&#22312;&#29983;&#25104;&#22797;&#26434;&#39057;&#35889;&#22270;&#24182;&#22312;&#24103;&#32423;&#21035;&#36816;&#34892;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#25152;&#26377;&#23376;&#24102;&#20197;&#22686;&#24378;&#25928;&#29575;&#12290;&#30001;&#20110;&#24076;&#26395;&#33719;&#24471;&#24179;&#32531;&#20256;&#36755;&#36712;&#36857;&#30340;&#25972;&#27969;&#27969;&#21160;&#65292;RFWave&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;RFWave&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05010v1 Announce Type: cross  Abstract: Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although diffusion models have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a spee
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05006</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20803;&#20154;&#31867;&#21453;&#39304;&#30340;&#21487;&#35777;&#26126;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Party Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22810;&#26041;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#20803;&#23398;&#20064;&#19982;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#30340;&#37319;&#29992;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24182;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#20559;&#22909;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#26126;&#30830;&#24314;&#27169;&#22810;&#20010;&#20010;&#20307;&#19981;&#21516;&#20559;&#22909;&#30340;&#22810;&#26041;RLHF&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20256;&#32479;RLHF&#26041;&#27861;&#22914;&#20309;&#22833;&#36133;&#65292;&#22240;&#20026;&#23398;&#20064;&#21333;&#19968;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#25429;&#25417;&#21644;&#24179;&#34913;&#22810;&#20010;&#20010;&#20307;&#30340;&#20559;&#22909;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#32467;&#21512;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#20010;&#20559;&#22909;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#26469;&#25972;&#21512;&#22810;&#26041;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20026;&#20248;&#21270;&#19981;&#21516;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65288;&#22914;Nash&#12289;Utilitarian&#21644;Leximin&#31119;&#21033;&#65289;&#24314;&#31435;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05006v1 Announce Type: cross  Abstract: Reinforcement learning with human feedback (RLHF) is an emerging paradigm to align models with human preferences. Typically, RLHF aggregates preferences from multiple individuals who have diverse viewpoints that may conflict with each other. Our work \textit{initiates} the theoretical study of multi-party RLHF that explicitly models the diverse preferences of multiple individuals. We show how traditional RLHF approaches can fail since learning a single reward function cannot capture and balance the preferences of multiple individuals. To overcome such limitations, we incorporate meta-learning to learn multiple preferences and adopt different social welfare functions to aggregate the preferences across multiple parties. We focus on the offline learning setting and establish sample complexity bounds, along with efficiency and fairness guarantees, for optimizing diverse social welfare functions such as Nash, Utilitarian, and Leximin welfa
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05004</link><description>&lt;p&gt;
&#26080;&#27861;&#35760;&#20303;&#38271;&#25991;&#26723;&#20013;&#30340;&#32454;&#33410;&#65311;&#24744;&#38656;&#35201;&#19968;&#20123;R&amp;R
&lt;/p&gt;
&lt;p&gt;
Can't Remember Details in Long Documents? You Need Some R&amp;R
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05004
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;R&amp;R&#26041;&#27861;&#65292;&#32467;&#21512;reprompting&#21644;in-context retrieval&#20004;&#31181;&#26032;&#22411;&#25552;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35832;&#22914;&#38271;&#31687;&#25991;&#26723;&#19978;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#38169;&#36807;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#38388;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;$\textit{R&amp;R}$&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#26032;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$\textit{reprompting}$&#21644;$\textit{in-context retrieval}$&#65288;ICR&#65289;&#65292;&#20197;&#20943;&#36731;&#25991;&#26723;&#22411;QA&#20013;&#30340;&#36825;&#31181;&#24433;&#21709;&#12290;&#22312;$\textit{reprompting}$&#20013;&#65292;&#25105;&#20204;&#21608;&#26399;&#24615;&#22320;&#22312;&#25972;&#20010;&#19978;&#19979;&#25991;&#25991;&#26723;&#20013;&#37325;&#22797;&#25552;&#31034;&#35828;&#26126;&#65292;&#20197;&#25552;&#37266;LLM&#20854;&#21407;&#22987;&#20219;&#21153;&#12290;&#22312;ICR&#20013;&#65292;&#25105;&#20204;&#24182;&#19981;&#25351;&#31034;LLM&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#26159;&#25351;&#31034;&#23427;&#26816;&#32034;&#19982;&#32473;&#23450;&#38382;&#39064;&#26368;&#30456;&#20851;&#30340;&#21069;$k$&#20010;&#27573;&#33853;&#32534;&#21495;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#31532;&#20108;&#20010;QA&#25552;&#31034;&#20013;&#30340;&#32553;&#30053;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4 Turbo&#21644;Claude-2.1&#22312;&#38271;&#24230;&#36798;&#21040;80k&#26631;&#35760;&#30340;&#25991;&#26723;&#19978;&#27979;&#35797;&#20102;R&amp;R&#65292;&#24182;&#24179;&#22343;&#35266;&#23519;&#21040;QA&#20934;&#30830;&#29575;&#25552;&#21319;&#20102;16&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05004v1 Announce Type: cross  Abstract: Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&amp;R}$ -- a combination of two novel prompt-based methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&amp;R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05000</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#34920;&#31034;&#36827;&#34892;&#21307;&#23398;&#35328;&#35821;&#30151;&#29366;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Medical Speech Symptoms Classification via Disentangled Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#21462;&#24471;&#20102;95%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 &#20844;&#21578;&#31867;&#22411;:new &#25688;&#35201;: &#22312;&#29616;&#26377;&#24037;&#20316;&#20013;&#65292;&#24847;&#22270;&#34987;&#23450;&#20041;&#29992;&#20110;&#29702;&#35299;&#21475;&#22836;&#35821;&#35328;&#12290;&#21307;&#23398;&#35328;&#35821;&#20013;&#28041;&#21450;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;&#22768;&#23398;&#29305;&#24449;&#22343;&#21253;&#21547;&#24847;&#22270;&#65292;&#36825;&#23545;&#20110;&#30151;&#29366;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRSC&#30340;&#21307;&#23398;&#35328;&#35821;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33258;&#21160;&#23398;&#20064;&#20174;&#25991;&#26412;-&#22768;&#23398;&#25968;&#25454;&#20013;&#20998;&#31163;&#24847;&#22270;&#21644;&#20869;&#23481;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290; &#36890;&#36807;&#24847;&#22270;&#32534;&#30721;&#22120;&#25552;&#21462;&#25991;&#26412;&#22495;&#21644;Mel-&#39057;&#35889;&#22270;&#22495;&#30340;&#24847;&#22270;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#20004;&#20010;&#20132;&#25442;&#33719;&#21462;&#37325;&#26500;&#30340;&#25991;&#26412;&#29305;&#24449;&#21644;Mel-&#39057;&#35889;&#22270;&#29305;&#24449;&#12290;&#22312;&#23558;&#20004;&#20010;&#22495;&#30340;&#24847;&#22270;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#34920;&#31034;&#21518;&#65292;&#32508;&#21512;&#24847;&#22270;&#34920;&#31034;&#34987;&#36755;&#20837;&#20915;&#31574;&#23618;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;25&#31181;&#19981;&#21516;&#21307;&#23398;&#30151;&#29366;&#26102;&#33719;&#24471;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;95%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05000v1 Announce Type: new  Abstract: Intent is defined for understanding spoken language in existing works. Both textual features and acoustic features involved in medical speech contain intent, which is important for symptomatic diagnosis. In this paper, we propose a medical speech classification model named DRSC that automatically learns to disentangle intent and content representations from textual-acoustic data for classification. The intent representations of the text domain and the Mel-spectrogram domain are extracted via intent encoders, and then the reconstructed text feature and the Mel-spectrogram feature are obtained through two exchanges. After combining the intent from two domains into a joint representation, the integrated intent representation is fed into a decision layer for classification. Experimental results show that our model obtains an average accuracy rate of 95% in detecting 25 different medical symptoms.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23450;&#20041;&#20102;&#33410;&#28857;&#25490;&#24207;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;CNCA-IGE&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#32593;&#32476;&#33410;&#28857;&#20013;&#24515;&#24615;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2403.04977</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#32593;&#32476;&#33410;&#28857;&#20013;&#24515;&#24615;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Node Centrality Approximation For Large Networks Based On Inductive Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04977
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#20102;&#33410;&#28857;&#25490;&#24207;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;CNCA-IGE&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#32593;&#32476;&#33410;&#28857;&#20013;&#24515;&#24615;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#36817;&#20013;&#24515;&#24615;&#65288;CC&#65289;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#65288;BC&#65289;&#26159;&#32593;&#32476;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25351;&#26631;&#65292;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#22522;&#26412;&#21442;&#32771;&#12290;&#36825;&#20123;&#24230;&#37327;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#31038;&#21306;&#26816;&#27979;&#21644;&#32593;&#32476;&#35299;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#22312;&#22823;&#22411;&#32593;&#32476;&#19978;&#30340;&#23454;&#38469;&#23454;&#29616;&#20173;&#28982;&#20855;&#26377;&#26497;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#35745;&#31639;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#36817;&#20284;&#31639;&#27861;&#26469;&#21152;&#36895;CC&#21644;BC&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#20123;&#36817;&#20284;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#19978;&#24212;&#29992;&#26102;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#22788;&#29702;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#23545;&#32593;&#32476;&#32467;&#26500;&#20013;&#29978;&#33267;&#32454;&#24494;&#30340;&#25200;&#21160;&#20063;&#21313;&#20998;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04977v1 Announce Type: cross  Abstract: Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics in network analysis, providing essential reference for discerning the significance of nodes within complex networks. These measures find wide applications in critical tasks, such as community detection and network dismantling. However, their practical implementation on extensive networks remains computationally demanding due to their high time complexity. To mitigate these computational challenges, numerous approximation algorithms have been developed to expedite the computation of CC and BC. Nevertheless, even these approximations still necessitate substantial processing time when applied to large-scale networks. Furthermore, their output proves sensitive to even minor perturbations within the network structure.   In this work, We redefine the CC and BC node ranking problem as a machine learning problem and propose the CNCA-IGE model, which is an encoder-dec
&lt;/p&gt;</description></item><item><title>StereoDiffusion&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#28508;&#22312;&#21464;&#37327;&#23454;&#29616;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.04965</link><description>&lt;p&gt;
StereoDiffusion&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04965
&lt;/p&gt;
&lt;p&gt;
StereoDiffusion&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#28508;&#22312;&#21464;&#37327;&#23454;&#29616;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21046;&#36896;&#21830;&#25512;&#20986;&#26356;&#22810;XR&#35774;&#22791;&#65292;&#23545;&#31435;&#20307;&#22270;&#20687;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StereoDiffusion&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#20462;&#34917;&#31649;&#36947;&#19981;&#21516;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#20351;&#29992;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#21407;&#22987;&#30340;Stable Diffusion&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#28508;&#22312;&#21464;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#12289;&#36731;&#37327;&#32423;&#30340;&#33021;&#21147;&#65292;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#20219;&#20309;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36755;&#20837;&#29983;&#25104;&#24038;&#22270;&#20687;&#24182;&#20026;&#20854;&#20272;&#35745;&#35270;&#24046;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;Stereo Pixel Shift&#25805;&#20316;&#29983;&#25104;&#21491;&#22270;&#20687;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#36741;&#20197;&#23545;&#31216;&#20687;&#32032;&#20301;&#31227;&#25513;&#34109;&#21435;&#22122;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#21491;&#20391;&#22270;&#20687;&#19982;&#24038;&#20391;&#22270;&#20687;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20445;&#25345;&#39640;&#27700;&#20934;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04965v1 Announce Type: cross  Abstract: The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the ste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04964</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#23454;&#35805;&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Tell me the truth: A system to measure the trustworthiness of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#20174;2023&#24180;11&#26376;ChatGPT&#25512;&#20986;&#20197;&#26469;&#65292;&#22312;&#22823;&#22810;&#25968;&#26032;&#38395;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#19968;&#24180;&#22810;&#36807;&#21435;&#20102;&#65292;&#20844;&#21496;&#25269;&#35302;&#37319;&#29992;&#23427;&#20204;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20182;&#20204;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#32570;&#20047;&#20449;&#24515;&#12290;&#19968;&#39033;&#30001;Baymard&#65288;2023&#65289;&#36827;&#34892;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT-4 &#22312;&#35782;&#21035;&#32593;&#31449;&#21487;&#29992;&#24615;&#38382;&#39064;&#26102;&#26377;80.1%&#30340;&#20551;&#38451;&#24615;&#38169;&#35823;&#29575;&#12290;&#32780;&#12298;JAMA&#20799;&#31185;&#23398;&#12299;&#26434;&#24535;&#65288;JAMA Pediatrics&#65289;&#20110;2024&#24180;1&#26376;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT &#22312;&#35786;&#26029;&#20799;&#31185;&#21307;&#30103;&#26696;&#20363;&#26102;&#30340;&#20934;&#30830;&#29575;&#20026;17%&#65288;Barile et al., 2024&#65289;&#12290;&#37027;&#20040;&#65292;&#20309;&#20026;&#8220;&#20449;&#20219;&#8221;&#65311;&#20449;&#20219;&#26159;&#19968;&#20010;&#30456;&#23545;&#30340;&#12289;&#20027;&#35266;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#21270;&#12289;&#39046;&#22495;&#21644;&#20010;&#20307;&#32780;&#21464;&#21270;&#12290;&#37027;&#20040;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#34913;&#37327;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#21602;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#39046;&#22495;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04964v1 Announce Type: new  Abstract: Large Language Models (LLM) have taken the front seat in most of the news since November 2023, when ChatGPT was introduced. After more than one year, one of the major reasons companies are resistant to adopting them is the limited confidence they have in the trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust is a relative, subject condition that can change based on culture, domain, individuals. And then, given a domain, how can the trustworthiness of a system be measured? In this paper, I present a systematic approach to measure trustworthiness based on a predefined ground truth, represented as a knowledge graph of the domain. The approach is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.04963</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#38169;&#35823;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#28145;&#20837;&#35780;&#20272;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#35780;&#20272;&#20102;GPT-4&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#65292;&#25351;&#20986;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#31616;&#21270;&#26159;&#19968;&#31181;&#37325;&#20889;&#21477;&#23376;&#20197;&#20415;&#26356;&#26131;&#38405;&#35835;&#21644;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#24110;&#21161;&#26377;&#21508;&#31181;&#38405;&#35835;&#38590;&#39064;&#30340;&#20154;&#26469;&#35828;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#34920;&#29616;&#21464;&#24471;&#36843;&#22312;&#30473;&#30571;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26469;&#35780;&#20272;LLMs&#30340;&#31616;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#23545;LLMs&#22312;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#22312;LLMs&#30340;&#31616;&#21270;&#35780;&#20272;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#22312;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38519;&#20837;&#20004;&#20010;&#26497;&#31471;&#65306;&#35201;&#20040;&#36807;&#20110;&#32932;&#27973;&#65292;&#26080;&#27861;&#28165;&#26224;&#29702;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#35201;&#20040;&#36807;&#20110;&#35814;&#32454;&#65292;&#20351;&#27880;&#37322;&#36807;&#31243;&#22797;&#26434;&#19988;&#23481;&#26131;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04963v1 Announce Type: cross  Abstract: Sentence simplification, which rewrites a sentence to be easier to read and understand, is a promising technique to help people with various reading difficulties. With the rise of advanced large language models (LLMs), evaluating their performance in sentence simplification has become imperative. Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliabil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04960</link><description>&lt;p&gt;
SecGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SecGPT: An Execution Isolation Architecture for LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25193;&#23637;&#20026;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24320;&#22987;&#25903;&#25345;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;LLMs&#30340;&#20107;&#23454;&#19978;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#33258;&#21160;&#25191;&#34892;&#33539;&#24335;&#65306;&#21363;&#65292;&#24212;&#29992;&#31243;&#24207;&#21450;&#20854;&#20132;&#20114;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#30340;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#34987;&#20801;&#35768;&#33258;&#30001;&#22320;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#19982;&#31995;&#32479;&#20114;&#21160;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#29983;&#24577;&#31995;&#32479;&#31867;&#20284;&#20110;&#26089;&#26399;&#35745;&#31639;&#24179;&#21488;&#30340;&#35774;&#32622;&#65292;&#22312;&#37027;&#37324;&#24212;&#29992;&#31243;&#24207;&#21644;&#31995;&#32479;&#20043;&#38388;&#32570;&#20047;&#36275;&#22815;&#30340;&#38548;&#31163;&#12290;&#30001;&#20110;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#24182;&#19988;&#21463;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#19981;&#31934;&#30830;&#24615;&#21152;&#21095;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#20250;&#20026;&#29992;&#25143;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SecGPT&#65292;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#32531;&#35299;&#30001;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#24341;&#36215;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;SecGPT&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#38548;&#31163;&#24212;&#29992;&#31243;&#24207;&#30340;&#25191;&#34892;&#21644;&#26356;&#22810;&#30340;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#25928;&#21644;&#36890;&#29992;&#30340;&#25552;&#31034;&#27880;&#20837;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.04957</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21644;&#36890;&#29992;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Automatic and Universal Prompt Injection Attacks against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04957
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26799;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#25928;&#21644;&#36890;&#29992;&#30340;&#25552;&#31034;&#27880;&#20837;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25797;&#38271;&#22788;&#29702;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#65292;&#20854;&#33021;&#21147;&#28304;&#20110;&#35299;&#37322;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26469;&#21033;&#29992;&#12290;&#36825;&#20123;&#25915;&#20987;&#20250;&#25805;&#32437;LLM&#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#20197;&#20135;&#29983;&#19982;&#25915;&#20987;&#32773;&#27880;&#20837;&#20869;&#23481;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#20559;&#31163;&#29992;&#25143;&#30340;&#23454;&#38469;&#35831;&#27714;&#12290;&#36825;&#20123;&#25915;&#20987;&#24102;&#26469;&#30340;&#37325;&#22823;&#39118;&#38505;&#20984;&#26174;&#20102;&#23545;&#23041;&#32961;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#25915;&#20987;&#32570;&#20047;&#32479;&#19968;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;&#23545;&#25552;&#31034;&#27880;&#20837;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#29702;&#35299;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#25552;&#31034;&#27880;&#20837;&#25968;&#25454;&#65292;&#21363;&#20351;&#38754;&#23545;&#38450;&#24481;&#25514;&#26045;&#20063;&#33021;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04957v1 Announce Type: new  Abstract: Large Language Models (LLMs) excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for understanding the objectives of prompt injection attacks and present an automated gradient-based method for generating highly effective and universal prompt injection data, even in the face of defensiv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#39046;&#22495;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#27169;&#22411;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#30340;&#25200;&#21160;&#19979;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31616;&#21333;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#30340;&#27169;&#22411;&#24615;&#33021;&#21463;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04954</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#27450;&#39575;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fooling Neural Networks for Motion Forecasting via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#39046;&#22495;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#27169;&#22411;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#30340;&#25200;&#21160;&#19979;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#31616;&#21333;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#30340;&#27169;&#22411;&#24615;&#33021;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#23433;&#20840;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24191;&#27867;&#30740;&#31350;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#39064;&#23578;&#26410;&#24212;&#29992;&#20110;&#20154;&#20307;&#21160;&#20316;&#39044;&#27979;&#20013;&#30340;&#22810;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;GCNs&#21644;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23545;&#31867;&#20284;&#20110;&#22270;&#20687;&#20998;&#31867;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#21021;&#22987;&#38454;&#27573;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#26469;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20302;&#27700;&#24179;&#25200;&#21160;&#19978;&#65292;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#19977;&#32500;&#21464;&#25442;&#23454;&#39564;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#26059;&#36716;&#21644;&#24179;&#31227;&#25935;&#24863;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20250;&#25913;&#21464;&#20851;&#33410;&#36317;&#31163;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#31867;&#20284;&#26089;&#26399;CNN&#27169;&#22411;&#19968;&#26679;&#65292;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#26131;&#21463;&#21040;&#23567;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04954v1 Announce Type: cross  Abstract: Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of adversarial attacks has not been applied to multi-regression models such as GCNs and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of adversarial attacks in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier CNN models, motion forecasting tasks are susceptible to small
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Spatiotemporal Style Transfer (STST)&#31639;&#27861;&#65292;&#22522;&#20110;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20801;&#35768;&#29983;&#25104;&#24378;&#22823;&#30340;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.04940</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#29983;&#25104;&#30340;&#26102;&#31354;&#39118;&#26684;&#36716;&#31227;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A spatiotemporal style transfer algorithm for dynamic visual stimulus generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Spatiotemporal Style Transfer (STST)&#31639;&#27861;&#65292;&#22522;&#20110;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20801;&#35768;&#29983;&#25104;&#24378;&#22823;&#30340;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#35270;&#35273;&#20449;&#24687;&#22914;&#20309;&#22312;&#29983;&#29289;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#32534;&#30721;&#36890;&#24120;&#38656;&#35201;&#35270;&#35273;&#31185;&#23398;&#23478;&#29983;&#25104;&#36866;&#24403;&#30340;&#21050;&#28608;&#26469;&#27979;&#35797;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#38761;&#21629;&#65292;&#20363;&#22914;&#22270;&#20687;&#39118;&#26684;&#36716;&#31227;&#65292;&#20294;&#35270;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#21364;&#24456;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#31354;&#39118;&#26684;&#36716;&#31227;&#65288;STST&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#35270;&#35273;&#21050;&#28608;&#29983;&#25104;&#26694;&#26550;&#65292;&#20801;&#35768;&#24378;&#22823;&#22320;&#25805;&#20316;&#21644;&#21512;&#25104;&#35270;&#39057;&#21050;&#28608;&#29992;&#20110;&#35270;&#35273;&#30740;&#31350;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#21452;&#27969;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20998;&#35299;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#20197;&#29983;&#25104;&#21160;&#24577;&#35270;&#35273;&#21050;&#28608;&#65292;&#20854;&#27169;&#22411;&#23618;&#28608;&#27963;&#19982;&#36755;&#20837;&#35270;&#39057;&#30340;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04940v1 Announce Type: cross  Abstract: Understanding how visual information is encoded in biological and artificial systems often requires vision scientists to generate appropriate stimuli to test specific hypotheses. Although deep neural network models have revolutionized the field of image generation with methods such as image style transfer, available methods for video generation are scarce. Here, we introduce the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus generation framework that allows powerful manipulation and synthesis of video stimuli for vision research. It is based on a two-stream deep neural network model that factorizes spatial and temporal features to generate dynamic visual stimuli whose model layer activations are matched to those of input videos. As an example, we show that our algorithm enables the generation of model metamers, dynamic stimuli whose layer activations within our two-stream model are matched to those of natural
&lt;/p&gt;</description></item><item><title>LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04934</link><description>&lt;p&gt;
LeTac-MPC&#65306;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#25235;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04934
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#38656;&#35201;&#35302;&#35273;&#21453;&#39304;&#21644;&#21453;&#24212;&#24615;&#25235;&#21462;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#30340;&#31283;&#20581;&#25235;&#21462;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LeTac-MPC&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#24335;&#25235;&#21462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#22841;&#29226;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#21147;&#20132;&#20114;&#20219;&#21153;&#20013;&#25235;&#21462;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#65292;&#35813;&#20256;&#24863;&#22120;&#33021;&#22815;&#24863;&#30693;&#21253;&#21547;&#25235;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#23646;&#24615;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#39640;&#20998;&#36776;&#29575;&#35302;&#35273;&#21453;&#39304;&#12290;LeTac-MPC&#21253;&#21547;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;MPC&#23618;&#65292;&#35774;&#35745;&#29992;&#20110;&#23545;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20174;&#35302;&#35273;&#21453;&#39304;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;25 Hz&#30340;&#39057;&#29575;&#19979;&#23454;&#29616;&#25910;&#25947;&#21644;&#31283;&#20581;&#30340;&#25235;&#21462;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04934v1 Announce Type: cross  Abstract: Grasping is a crucial task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects under various conditions and with differing physical properties. In this paper, we introduce LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive grasping. Our approach enables the gripper grasp objects with different physical properties on dynamic and force-interactive tasks. We utilize a vision-based tactile sensor, GelSight, which is capable of perceiving high-resolution tactile feedback that contains the information of physical properties and states of the grasped object. LeTac-MPC incorporates a differentiable MPC layer designed to model the embeddings extracted by a neural network (NN) from tactile feedback. This design facilitates convergent and robust grasping control at a frequency of 25 Hz. We propose a fully automated data collection pipeline and collect a dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04931</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21512;&#20316;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Human-AI Teaming with Large Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36805;&#36895;&#21457;&#23637;&#30340;&#26223;&#35266;&#20013;&#65292;&#20154;&#31867;&#26234;&#33021;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;HAI&#65289;&#21512;&#20316;&#65292;&#24050;&#25104;&#20026;&#25512;&#36827;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30707;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPtM&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#36825;&#19968;&#26223;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#22797;&#26434;&#27169;&#24335;&#65292;&#20026;&#20154;&#31867;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LPtMs&#19982;HAI&#30340;&#20851;&#38190;&#25972;&#21512;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#12290;&#37325;&#28857;&#25506;&#35752;&#20102;LPtMs&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#35752;&#35770;&#20102;&#36825;&#31181;&#21327;&#20316;&#23545;AI&#27169;&#22411;&#25913;&#36827;&#12289;&#26377;&#25928;&#30340;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#19968;&#25506;&#32034;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;LPtM&#22686;&#24378;HAI&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ForgetNet&#21644;G-ForgetNet&#65292;&#36890;&#36807;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#21644;&#24341;&#20837;&#38376;&#25511;&#26426;&#21046;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30683;&#30462;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.04929</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65306;&#20998;&#26512;&#19982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ForgetNet&#21644;G-ForgetNet&#65292;&#36890;&#36807;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#21644;&#24341;&#20837;&#38376;&#25511;&#26426;&#21046;&#26469;&#35299;&#20915;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20013;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30683;&#30462;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#27169;&#20223;&#31639;&#27861;&#25191;&#34892;&#36880;&#27493;&#36827;&#34892;&#30340;&#33021;&#21147;&#12290;&#29616;&#26377;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#24120;&#35265;&#33539;&#24335;&#28041;&#21450;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#26469;&#39044;&#27979;&#26410;&#26469;&#25191;&#34892;&#27493;&#39588;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#30340;&#35266;&#23519;&#26159;&#65292;&#36825;&#31181;&#21382;&#21490;&#20381;&#36182;&#26412;&#36136;&#19978;&#19982;&#31639;&#27861;&#25512;&#29702;&#20219;&#21153;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30456;&#30683;&#30462;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;ForgetNet&#65292;&#23427;&#19981;&#20351;&#29992;&#21382;&#21490;&#23884;&#20837;&#65292;&#22240;&#27492;&#19982;&#20219;&#21153;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;ForgetNet&#22312;&#26089;&#26399;&#38454;&#27573;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;G-ForgetNet&#65292;&#23427;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#20801;&#35768;&#26377;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#21382;&#21490;&#23884;&#20837;&#12290;&#36825;&#31181;&#22686;&#24378;&#30340;&#33021;&#21147;&#22312;&#27169;&#22411;&#30340;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#22522;&#20110;CLRS-30&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04929v1 Announce Type: cross  Abstract: Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic reasoning tasks. Based on this motivation, we present our ForgetNet, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training ForgetNet at early stages, we further introduce G-ForgetNet, which uses a gating mechanism to allow for the selective integration of historical embeddings. Such an enhanced capability provides valuable computational pathways during the model's early training phase. Our extensive experiments, based on the CLRS-30 algorithmic reasoning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04919</link><description>&lt;p&gt;
&#37492;&#21035;&#21151;&#33021;&#20381;&#36182;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifying Causal Effects Under Functional Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#65292;&#21463;&#20004;&#20010;&#25913;&#36827;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#22240;&#26524;&#22270;&#20013;&#26576;&#20123;&#21464;&#37327;&#26159;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#31532;&#19968;&#65292;&#24403;&#26576;&#20123;&#21464;&#37327;&#26159;&#21151;&#33021;&#30340;&#26102;&#65292;&#19968;&#20010;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21487;&#33021;&#21464;&#24471;&#21487;&#35782;&#21035;&#12290;&#31532;&#20108;&#65292;&#21487;&#20197;&#25490;&#38500;&#35266;&#27979;&#26576;&#20123;&#21151;&#33021;&#21464;&#37327;&#32780;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22522;&#20110;&#19968;&#20010;&#25490;&#38500;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20174;&#22240;&#26524;&#22270;&#20013;&#21024;&#38500;&#21151;&#33021;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#32467;&#26524;&#22240;&#26524;&#22270;&#20013;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04919v1 Announce Type: new  Abstract: We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04917</link><description>&lt;p&gt;
&#22522;&#20110;&#20984;&#38598;&#22270;&#30340;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#30340;&#28151;&#21512;&#25972;&#25968;&#38181;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#35813;&#20844;&#24335;&#22522;&#20110;&#30446;&#26631;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#25104;&#20026;&#20984;&#38598;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#26469;&#23454;&#29616;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23547;&#25214;&#31227;&#21160;&#30446;&#26631;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;MT-TSP&#65289;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#30340;&#20844;&#24335;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#30701;&#36335;&#24452;&#65292;&#20351;&#19968;&#20010;&#20174;&#20179;&#24211;&#20986;&#21457;&#30340;&#20195;&#29702;&#35775;&#38382;&#19968;&#32452;&#31227;&#21160;&#30446;&#26631;&#65292;&#24182;&#22312;&#23427;&#20204;&#20998;&#37197;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#24688;&#22909;&#35775;&#38382;&#19968;&#27425;&#65292;&#28982;&#21518;&#36820;&#22238;&#21040;&#20179;&#24211;&#12290;&#35813;&#20844;&#24335;&#20381;&#36182;&#20110;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#21363;&#24403;&#30446;&#26631;&#27839;&#30528;&#32447;&#31227;&#21160;&#26102;&#65292;&#23427;&#20204;&#30340;&#36712;&#36857;&#22312;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#31995;&#20869;&#21464;&#20026;&#20984;&#38598;&#12290;&#28982;&#21518;&#65292;&#38382;&#39064;&#23601;&#32553;&#20943;&#20026;&#22312;&#19968;&#20010;&#20984;&#38598;&#22270;&#20013;&#23547;&#25214;&#26368;&#30701;&#36335;&#24452;&#65292;&#21463;&#21040;&#19968;&#20123;&#36895;&#24230;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20844;&#24335;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;Mixed Integer Conic Program (MICP)&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#30446;&#26631;&#25968;&#37327;&#26368;&#22810;&#20026;20&#20010;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;MICP&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#32553;&#30701;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#26368;&#20248;&#24615;&#24046;&#36317;&#32553;&#23567;&#20102;&#39640;&#36798;60&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#35299;&#27861;&#30340;&#25104;&#26412;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04917v1 Announce Type: cross  Abstract: This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04899</link><description>&lt;p&gt;
&#26397;&#21521;&#22330;&#26223;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Scene Graph Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04899
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22330;&#26223;&#22270;&#36890;&#36807;&#23558;&#22330;&#26223;&#20998;&#35299;&#20026;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#20004;&#20004;&#26102;&#38388;&#20851;&#31995;&#26469;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#38271;&#26399;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#31934;&#32454;&#31890;&#24230;&#30340;&#20004;&#20004;&#20851;&#31995;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#29992;&#20316;&#22522;&#32447;&#65292;&#20197;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#30340;&#20004;&#20004;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SceneSayer&#12290;&#22312;SceneSayer&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#20851;&#31995;&#34920;&#31034;&#26469;&#25512;&#26029;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#24103;&#24182;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#37319;&#29992;&#36830;&#32493;&#26102;&#38388;&#35270;&#35282;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#28508;&#22312;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#26029;&#26410;&#26469;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04899v1 Announce Type: cross  Abstract: Spatio-temporal scene graphs represent interactions in a video by decomposing scenes into individual objects and their pair-wise temporal relationships. Long-term anticipation of the fine-grained pair-wise relationships between objects is a challenging problem. To this end, we introduce the task of Scene Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation methods as baselines to anticipate future pair-wise relationships between objects and propose a novel approach SceneSayer. In SceneSayer, we leverage object-centric representations of relationships to reason about the observed video frames and model the evolution of relationships between objects. We take a continuous time perspective and model the latent dynamics of the evolution of object interactions using concepts of NeuralODE and NeuralSDE, respectively. We infer representations of future relationships by solving an Ordinary Differential Equation and a Stoch
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ConstitutionalExperts&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23466;&#27861;&#21407;&#21017;&#26500;&#24314;&#25552;&#31034;&#65292;&#37319;&#29992;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#21644;MoE&#26550;&#26500;&#65292;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#29420;&#29305;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.04894</link><description>&lt;p&gt;
ConstitutionalExperts: &#35757;&#32451;&#22522;&#20110;&#21407;&#21017;&#30340;&#25552;&#31034;&#28151;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
ConstitutionalExperts: Training a Mixture of Principle-based Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ConstitutionalExperts&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#23466;&#27861;&#21407;&#21017;&#26500;&#24314;&#25552;&#31034;&#65292;&#37319;&#29992;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#21644;MoE&#26550;&#26500;&#65292;&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#29420;&#29305;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20889;&#20316;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#19988;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ConstitutionalExperts&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#30001;&#23466;&#27861;&#21407;&#21017;&#65288;&#21363;&#35268;&#21017;&#65289;&#32452;&#25104;&#30340;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#32473;&#23450;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290; &#19982;&#20197;&#24448;&#20248;&#21270;&#25552;&#31034;&#20316;&#20026;&#21333;&#20010;&#23454;&#20307;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#21035;&#32534;&#36753;&#21508;&#20010;&#21407;&#21017;&#36880;&#27493;&#25913;&#36827;&#25552;&#31034;&#12290; &#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#35821;&#20041;&#21306;&#22495;&#23398;&#20064;&#21807;&#19968;&#30340;&#25552;&#31034;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26550;&#26500;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290; &#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290; &#25105;&#20204;&#36824;&#35843;&#26597;&#20102;MoE&#26159;&#21542;&#25913;&#21892;&#36825;&#20123;&#20854;&#20182;&#25216;&#26415;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ConstitutionalExperts&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04894v1 Announce Type: cross  Abstract: Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization tec
&lt;/p&gt;</description></item><item><title>&#20027;&#35201;AI&#24320;&#21457;&#32773;&#24212;&#25215;&#35834;&#25552;&#20379;&#27861;&#24459;&#21644;&#25216;&#26415;&#19978;&#30340;&#23433;&#20840;&#28207;&#65292;&#20351;&#20844;&#20849;&#21033;&#30410;&#30340;&#23433;&#20840;&#30740;&#31350;&#20813;&#21463;&#36134;&#25143;&#26242;&#20572;&#25110;&#27861;&#24459;&#25253;&#22797;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.04893</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;AI&#35780;&#20272;&#21644;&#32418;&#38431;&#27979;&#35797;&#30340;&#23433;&#20840;&#28207;
&lt;/p&gt;
&lt;p&gt;
A Safe Harbor for AI Evaluation and Red Teaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04893
&lt;/p&gt;
&lt;p&gt;
&#20027;&#35201;AI&#24320;&#21457;&#32773;&#24212;&#25215;&#35834;&#25552;&#20379;&#27861;&#24459;&#21644;&#25216;&#26415;&#19978;&#30340;&#23433;&#20840;&#28207;&#65292;&#20351;&#20844;&#20849;&#21033;&#30410;&#30340;&#23433;&#20840;&#30740;&#31350;&#20813;&#21463;&#36134;&#25143;&#26242;&#20572;&#25110;&#27861;&#24459;&#25253;&#22797;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#35780;&#20272;&#21644;&#32418;&#38431;&#27979;&#35797;&#23545;&#20110;&#21457;&#29616;&#29983;&#25104;&#24335;AI&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;AI&#20844;&#21496;&#29992;&#20110;&#38459;&#27490;&#27169;&#22411;&#35823;&#29992;&#30340;&#26381;&#21153;&#26465;&#27454;&#21644;&#25191;&#34892;&#31574;&#30053;&#20250;&#23545;&#21892;&#24847;&#30340;&#23433;&#20840;&#35780;&#20272;&#36896;&#25104;&#25171;&#20987;&#12290;&#36825;&#23548;&#33268;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#25285;&#24515;&#36827;&#34892;&#27492;&#31867;&#30740;&#31350;&#25110;&#21457;&#24067;&#20854;&#21457;&#29616;&#23558;&#23548;&#33268;&#36134;&#25143;&#34987;&#26242;&#20572;&#25110;&#38754;&#20020;&#27861;&#24459;&#25253;&#22797;&#12290;&#34429;&#28982;&#19968;&#20123;&#20844;&#21496;&#25552;&#20379;&#30740;&#31350;&#20154;&#21592;&#35775;&#38382;&#35745;&#21010;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#26367;&#20195;&#29420;&#31435;&#30740;&#31350;&#35775;&#38382;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#31038;&#21306;&#20195;&#34920;&#24615;&#26377;&#38480;&#65292;&#36164;&#37329;&#19981;&#36275;&#65292;&#24182;&#19988;&#32570;&#20047;&#29420;&#31435;&#20110;&#20225;&#19994;&#28608;&#21169;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#20027;&#35201;AI&#24320;&#21457;&#32773;&#25215;&#35834;&#25552;&#20379;&#27861;&#24459;&#21644;&#25216;&#26415;&#19978;&#30340;&#23433;&#20840;&#28207;&#65292;&#20351;&#20844;&#20849;&#21033;&#30410;&#30340;&#23433;&#20840;&#30740;&#31350;&#20813;&#21463;&#36134;&#25143;&#26242;&#20572;&#25110;&#27861;&#24459;&#25253;&#22797;&#30340;&#23041;&#32961;&#12290;&#36825;&#20123;&#25552;&#35758;&#28304;&#33258;&#25105;&#20204;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#30340;&#38598;&#20307;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04893v1 Announce Type: new  Abstract: Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting sa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNUM&#30340;&#27169;&#22359;&#21270;&#12289;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26412;&#22320;&#21270;&#22788;&#29702;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.04866</link><description>&lt;p&gt;
&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22359;&#21270;&#31471;&#21040;&#31471;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNUM&#30340;&#27169;&#22359;&#21270;&#12289;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26412;&#22320;&#21270;&#22788;&#29702;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22810;&#20219;&#21153;&#22788;&#29702;&#21644;&#29983;&#25104;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#38598;&#20013;&#20110;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#25110;&#35270;&#39057;&#65289;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#34920;&#26684;&#25968;&#25454;&#12289;&#26102;&#38388;&#24207;&#21015;&#25110;&#20449;&#21495;&#65289;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#19982;&#34892;&#19994;&#30456;&#20851;&#30340;&#29992;&#20363;&#26082;&#28041;&#21450;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#20063;&#33021;&#20174;&#20013;&#21463;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNUM&#30340;&#27169;&#22359;&#21270;&#12289;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26412;&#22320;&#21270;&#22788;&#29702;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;MAGNUM&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#19987;&#38376;&#30340;&#21333;&#27169;&#24577;&#27169;&#22359;&#20174;&#25152;&#26377;&#21487;&#29992;&#30340;&#27169;&#24577;&#20013;&#25552;&#21462;&#12289;&#21387;&#32553;&#21644;&#34701;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04866v1 Announce Type: new  Abstract: Multimodal learning is a rapidly growing research field that has revolutionized multitasking and generative modeling in AI. While much of the research has focused on dealing with unstructured data (e.g., language, images, audio, or video), structured data (e.g., tabular data, time series, or signals) has received less attention. However, many industry-relevant use cases involve or can be benefited from both types of data. In this work, we propose a modular, end-to-end multimodal learning method called MAGNUM, which can natively handle both structured and unstructured data. MAGNUM is flexible enough to employ any specialized unimodal module to extract, compress, and fuse information from all available modalities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;S3-TSS&#65292;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#30340;&#33258;&#28982;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;SeCo&#12290;</title><link>https://arxiv.org/abs/2403.04859</link><description>&lt;p&gt;
&#21355;&#26143;&#22270;&#20687;&#26102;&#38388;&#33258;&#30417;&#30563;&#65288;S3-TSS&#65289;&#65306;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;SSL&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;S3-TSS&#65292;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#30340;&#33258;&#28982;&#22686;&#24378;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;SeCo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#22270;&#20687;&#20013;&#24102;&#26377;&#21508;&#31181;&#22823;&#27668;&#26465;&#20214;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20284;&#20046;&#20351;&#29992;&#33258;&#30417;&#30563;&#31639;&#27861;&#24456;&#26377;&#29992;&#12290;&#21253;&#25324;&#26059;&#36716;&#12289;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#25340;&#22270;&#22312;&#20869;&#30340;&#20960;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#31639;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#12290;&#36890;&#24120;&#65292;&#21355;&#26143;&#22270;&#20687;&#20855;&#26377;&#26356;&#39640;&#30340;&#26102;&#38388;&#39057;&#29575;&#12290; &#22240;&#27492;&#65292;&#36965;&#24863;&#25968;&#25454;&#30340;&#26102;&#38388;&#32500;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#22686;&#24378;&#65292;&#32780;&#26080;&#38656;&#25105;&#20204;&#21019;&#24314;&#22270;&#20687;&#30340;&#20154;&#24037;&#22686;&#24378;&#12290; &#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S3-TSS&#65292;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#20013;&#33258;&#28982;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290; &#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#24403;&#21069;&#39046;&#20808;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;SeCo&#12290; &#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#30721;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#65306;https://github.com/hewanshrestha/Why-Self-Supervision-in-Time
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04859v1 Announce Type: new  Abstract: With the limited availability of labeled data with various atmospheric conditions in remote sensing images, it seems useful to work with self-supervised algorithms. Few pretext-based algorithms, including from rotation, spatial context and jigsaw puzzles are not appropriate for satellite images. Often, satellite images have a higher temporal frequency. So, the temporal dimension of remote sensing data provides natural augmentation without requiring us to create artificial augmentation of images. Here, we propose S3-TSS, a novel method of self-supervised learning technique that leverages natural augmentation occurring in temporal dimension. We compare our results with current state-of-the-art methods and also perform various experiments. We observed that our method was able to perform better than baseline SeCo in four downstream datasets. Code for our work can be found here: https://github.com/hewanshrestha/Why-Self-Supervision-in-Time
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#24182;&#28608;&#21457;&#20852;&#36259;&#65292;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#21644;&#23558;&#26446;&#32676;&#29702;&#35770;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35762;&#20041;&#21450;&#32534;&#30721;&#25945;&#31243;&#20844;&#24320;&#21487;&#33719;&#21462;</title><link>https://arxiv.org/abs/2403.04807</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#65288;&#30740;&#31350;&#29983;&#35838;&#31243;&#35762;&#20041;&#65289;
&lt;/p&gt;
&lt;p&gt;
Mathematics of Neural Networks (Lecture Notes Graduate Course)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#24182;&#28608;&#21457;&#20852;&#36259;&#65292;&#20027;&#35201;&#20869;&#23481;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#21644;&#23558;&#26446;&#32676;&#29702;&#35770;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35762;&#20041;&#21450;&#32534;&#30721;&#25945;&#31243;&#20844;&#24320;&#21487;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#26159;&#25105;&#22312;2021&#24180;&#33267;2023&#24180;&#22312;&#22467;&#22240;&#38669;&#28201;&#31185;&#25216;&#22823;&#23398;&#25945;&#25480;&#30340;&#21516;&#21517;&#35838;&#31243;&#30340;&#35762;&#20041;&#12290;&#35813;&#35838;&#31243;&#26088;&#22312;&#21521;&#30740;&#31350;&#29983;&#25968;&#23398;&#19987;&#19994;&#30340;&#23398;&#29983;&#20171;&#32461;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26088;&#22312;&#28608;&#21457;&#25968;&#23398;&#23398;&#29983;&#23545;&#36827;&#19968;&#27493;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#24863;&#20852;&#36259;&#12290;&#35838;&#31243;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#39318;&#20808;&#26159;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#33324;&#20171;&#32461;&#65292;&#20391;&#37325;&#20110;&#20197;&#24418;&#24335;&#21270;&#25968;&#23398;&#26041;&#24335;&#20171;&#32461;&#35813;&#39046;&#22495;&#12290;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#26446;&#32676;&#21644;&#21516;&#24577;&#31354;&#38388;&#30340;&#29702;&#35770;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#29702;&#24819;&#20960;&#20309;&#31561;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35762;&#20041;&#23613;&#21487;&#33021;&#33258;&#21253;&#21547;&#65292;&#20197;&#20415;&#23545;&#20855;&#26377;&#19968;&#23450;&#25968;&#23398;&#32972;&#26223;&#30340;&#20219;&#20309;&#23398;&#29983;&#37117;&#21487;&#20197;&#29702;&#35299;&#12290;&#35813;&#35838;&#31243;&#36824;&#21253;&#25324;&#19968;&#31995;&#21015;Jupyter&#31508;&#35760;&#26412;&#24418;&#24335;&#30340;&#32534;&#30721;&#25945;&#31243;&#21644;&#20316;&#19994;&#65292;&#21487;&#22312;https://g&#19978;&#20844;&#24320;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04807v1 Announce Type: cross  Abstract: These are the lecture notes that accompanied the course of the same name that I taught at the Eindhoven University of Technology from 2021 to 2023. The course is intended as an introduction to neural networks for mathematics students at the graduate level and aims to make mathematics students interested in further researching neural networks. It consists of two parts: first a general introduction to deep learning that focuses on introducing the field in a formal mathematical way. The second part provides an introduction to the theory of Lie groups and homogeneous spaces and how it can be applied to design neural networks with desirable geometric equivariances. The lecture notes were made to be as self-contained as possible so as to accessible for any student with a moderate mathematics background. The course also included coding tutorials and assignments in the form of a set of Jupyter notebooks that are publicly available at https://g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#21644;&#20849;&#35782;&#39564;&#35777;&#27969;&#31243;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#25915;&#20987;&#24182;&#25552;&#21319;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04803</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#20849;&#35782;&#39564;&#35777;&#27169;&#22411;&#26356;&#26032;&#65292;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Security in Federated Learning through Adaptive Consensus-Based Model Update Validation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04803
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#21644;&#20849;&#35782;&#39564;&#35777;&#27969;&#31243;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23545;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#25915;&#20987;&#24182;&#25552;&#21319;&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24378;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#25269;&#24481;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#22522;&#20110;&#20849;&#35782;&#30340;&#39564;&#35777;&#27969;&#31243;&#65292;&#32467;&#21512;&#33258;&#36866;&#24212;&#38408;&#20540;&#26426;&#21046;&#12290;&#36825;&#31181;&#21160;&#24577;&#38408;&#20540;&#35774;&#35745;&#26088;&#22312;&#26681;&#25454;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#23637;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#24322;&#24120;&#26816;&#27979;&#23618;&#65292;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#30340;&#23454;&#26102;&#38656;&#27714;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#21442;&#19982;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#22810;&#25968;&#20849;&#35782;&#25165;&#33021;&#39564;&#35777;&#26356;&#26032;&#65292;&#30830;&#20445;&#21482;&#26377;&#32463;&#36807;&#23457;&#26597;&#21644;&#20849;&#35782;&#30340;&#20462;&#25913;&#25165;&#24212;&#29992;&#20110;&#20840;&#23616;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CIFAR-10&#21644;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#36731;&#65292;&#22686;&#24378;&#20102;FL&#31995;&#32479;&#30340;&#24377;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20381;&#36182;&#24322;&#24120;&#26816;&#27979;&#30340;&#20256;&#32479;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04803v1 Announce Type: cross  Abstract: This paper introduces an advanced approach for fortifying Federated Learning (FL) systems against label-flipping attacks. We propose a simplified consensus-based verification process integrated with an adaptive thresholding mechanism. This dynamic thresholding is designed to adjust based on the evolving landscape of model updates, offering a refined layer of anomaly detection that aligns with the real-time needs of distributed learning environments. Our method necessitates a majority consensus among participating clients to validate updates, ensuring that only vetted and consensual modifications are applied to the global model. The efficacy of our approach is validated through experiments on two benchmark datasets in deep learning, CIFAR-10 and MNIST. Our results indicate a significant mitigation of label-flipping attacks, bolstering the FL system's resilience. This method transcends conventional techniques that depend on anomaly detec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04799</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#65306;&#20174;&#21019;&#36896;&#32422;&#40065;&#24052;&#35821;AI&#35270;&#39057;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
AI Literacy in Low-Resource Languages:Insights from creating AI in Yoruba videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#65292;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#23384;&#22312;&#20110;&#20027;&#23548;&#35821;&#35328;&#20013;&#30340;&#20869;&#23481;&#22312;&#20687;&#32422;&#40065;&#24052;&#35821;&#65288;&#26377;4100&#19975;&#27597;&#35821;&#20351;&#29992;&#32773;&#65289;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#36896;&#25104;&#20102;&#19968;&#23450;&#30340;&#24046;&#36317;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#22312;&#32422;&#40065;&#24052;&#35821;&#20013;&#21019;&#24314;&#21644;&#20998;&#21457;AI&#35270;&#39057;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#35813;&#39033;&#30446;&#24320;&#21457;&#20102;26&#20010;&#35270;&#39057;&#65292;&#28085;&#30422;&#22522;&#30784;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#21033;&#29992;&#25925;&#20107;&#21465;&#36848;&#21644;&#31616;&#26131;&#35299;&#37322;&#12290;&#36825;&#20123;&#35270;&#39057;&#37319;&#29992;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#21046;&#20316;&#65292;&#24182;&#22312;YouTube&#12289;LinkedIn&#21644;Twitter&#19978;&#36827;&#34892;&#20998;&#21457;&#65292;&#20272;&#35745;&#35302;&#21450;&#20102;&#26469;&#33258;22&#20010;&#22269;&#23478;&#30340;&#20840;&#29699;&#35266;&#20247;&#12290;&#23545;YouTube&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35266;&#30475;&#27169;&#24335;&#30340;&#35265;&#35299;&#65292;&#20854;&#20013;25-44&#23681;&#24180;&#40836;&#32452;&#36129;&#29486;&#20102;&#26368;&#22810;&#30340;&#35266;&#30475;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#27969;&#37327;&#26469;&#28304;&#20110;&#22806;&#37096;&#26469;&#28304;&#65292;&#31361;&#26174;&#20102;&#36328;&#24179;&#21488;&#25512;&#24191;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#21019;&#24314;&#20154;&#24037;&#26234;&#33021;&#32032;&#20859;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04799v1 Announce Type: cross  Abstract: To effectively navigate the AI revolution, AI literacy is crucial. However, content predominantly exists in dominant languages, creating a gap for low-resource languages like Yoruba (41 million native speakers). This case study explores bridging this gap by creating and distributing AI videos in Yoruba.The project developed 26 videos covering foundational, intermediate, and advanced AI concepts, leveraging storytelling and accessible explanations. These videos were created using a cost-effective methodology and distributed across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of 22 countries. Analysis of YouTube reveals insights into viewing patterns, with the 25-44 age group contributing the most views. Notably, over half of the traffic originated from external sources, highlighting the potential of cross-platform promotion.This study demonstrates the feasibility and impact of creating AI literacy content in low
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04795</link><description>&lt;p&gt;
&#28040;&#38450;&#24037;&#31243;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38024;&#23545;&#39046;&#22495;&#30693;&#35782;&#23545;&#25216;&#26415;&#38382;&#39064;&#30340;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Fire Engineering: An Examination of Technical Questions Against Domain Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28040;&#38450;&#24037;&#31243;&#20013;&#22788;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#34920;&#29616;&#36739;&#20248;&#65292;&#23637;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27604;&#36739;&#20004;&#20010;&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;OpenAI&#30340;ChatGPT&#21644;&#35895;&#27468;&#30340;Bard&#65292;&#22312;&#28779;&#28798;&#24037;&#31243;&#39046;&#22495;&#20013;&#35780;&#20272;&#23427;&#20204;&#22788;&#29702;&#19982;&#28040;&#38450;&#23433;&#20840;&#30456;&#20851;&#26597;&#35810;&#30340;&#22238;&#24212;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#12290; &#21019;&#24314;&#24182;&#26816;&#26597;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#31867;&#22411;&#30340;&#28040;&#38450;&#24037;&#31243;&#38382;&#39064;&#21644;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#32467;&#26500;&#28779;&#28798;&#35774;&#35745;&#12289;&#38450;&#28779;&#31574;&#30053;&#12289;&#30095;&#25955;&#12289;&#24314;&#31569;&#27861;&#35268;&#21512;&#35268;&#21644;&#28781;&#28779;&#31995;&#32479;&#31561;&#65288;&#20854;&#20013;&#19968;&#20123;&#31867;&#20284;&#20110;&#28040;&#38450;&#20445;&#25252;&#32771;&#35797;&#65288;FPE&#65289;&#20013;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#12290; &#32467;&#26524;&#26174;&#31034;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#24615;&#33021;&#19978;&#30340;&#19968;&#20123;&#20851;&#38190;&#24046;&#24322;&#65292;ChatGPT&#34920;&#29616;&#20986;&#30456;&#23545;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290; &#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#31361;&#20986;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#30340;&#21516;&#26102;&#24443;&#24213;&#25913;&#38761;&#28040;&#38450;&#24037;&#31243;&#23454;&#36341;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26174;&#28982;&#65292;&#22312;&#25216;&#26415;&#25104;&#29087;&#21518;&#65292;&#36825;&#39033;&#25216;&#26415;&#23558;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04795v1 Announce Type: cross  Abstract: This communication presents preliminary findings from comparing two recent chatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire engineering by evaluating their responses in handling fire safety related queries. A diverse range of fire engineering questions and scenarios were created and examined, including structural fire design, fire prevention strategies, evacuation, building code compliance, and fire suppression systems (some of which resemble those commonly present in the Fire Protection exam (FPE)). The results reveal some key differences in the performance of the chatbots, with ChatGPT demonstrating a relatively superior performance. Then, this communication highlights the potential for chatbot technology to revolutionize fire engineering practices by providing instant access to critical information while outlining areas for further improvement and research. Evidently, and when it matures, this technology will lik
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#22240;&#26524;&#22522;&#20934;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#38477;&#20302;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04793</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Two-Phase Multi-Split Causal Ensemble Model for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#22240;&#26524;&#22522;&#20934;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#38477;&#20302;&#22122;&#38899;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#35768;&#22810;&#23398;&#31185;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#30740;&#31350;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#31639;&#27861;&#37117;&#21516;&#26679;&#36866;&#29992;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20004;&#38454;&#27573;&#22810;&#20998;&#35010;&#22240;&#26524;&#38598;&#25104;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#22240;&#26524;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#22240;&#26524;&#25512;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04793v1 Announce Type: cross  Abstract: Causal inference is a fundamental research topic for discovering the cause-effect relationships in many disciplines. However, not all algorithms are equally well-suited for a given dataset. For instance, some approaches may only be able to identify linear relationships, while others are applicable for non-linearities. Algorithms further vary in their sensitivity to noise and their ability to infer causal information from coupled vs. non-coupled time series. Therefore, different algorithms often generate different causal relationships for the same input. To achieve a more robust causal inference result, this publication proposes a novel data-driven two-phase multi-split causal ensemble model to combine the strengths of different causality base algorithms. In comparison to existing approaches, the proposed ensemble method reduces the influence of noise through a data partitioning scheme in the first phase. To achieve this, the data are i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04790</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#35757;&#32451;&#65306;&#36793;&#32842;&#22825;&#36793;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Training of Large Language Models: Learn while chatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#20010;&#24615;&#21270;&#23450;&#21046;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#21151;&#33021;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#33539;&#24335;&#21463;&#21046;&#20110;&#28789;&#27963;&#24615;&#19981;&#36275;&#12289;&#23450;&#21046;&#21270;&#21463;&#38480;&#25110;&#32570;&#20047;&#25345;&#32493;&#24615;&#23398;&#20064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20114;&#21160;&#33539;&#24335;-&#8220;&#20351;&#29992;&#22806;&#37096;&#20114;&#21160;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#8221;&#65292;&#23558;&#25345;&#32493;&#12289;&#23454;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#19982;&#36890;&#36807;&#22806;&#37096;&#20114;&#21160;&#65288;&#22914;AI&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#23450;&#21046;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04790v1 Announce Type: cross  Abstract: Large Language Models(LLMs) have dramatically revolutionized the field of Natural Language Processing(NLP), offering remarkable capabilities that have garnered widespread usage. However, existing interaction paradigms between LLMs and users are constrained by either inflexibility, limitations in customization, or a lack of persistent learning. This inflexibility is particularly evident as users, especially those without programming skills, have restricted avenues to enhance or personalize the model. Existing frameworks further complicate the model training and deployment process due to their computational inefficiencies and lack of user-friendly interfaces. To overcome these challenges, this paper introduces a novel interaction paradigm-'Online Training using External Interactions'-that merges the benefits of persistent, real-time model updates with the flexibility for individual customization through external interactions such as AI a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.04789</link><description>&lt;p&gt;
TopicDiff&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#30340;&#20027;&#39064;&#20016;&#23500;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TopicDiff: A Topic-enriched Diffusion Approach for Multimodal Conversational Emotion Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;TopicDiff&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20250;&#35805;&#24773;&#24863;&#65288;MCE&#65289;&#26816;&#27979;&#36890;&#24120;&#36328;&#36234;&#22768;&#23398;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#21560;&#24341;&#20102;&#22810;&#23186;&#20307;&#31038;&#21306;&#26085;&#30410;&#22686;&#21152;&#30340;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23545;&#35805;&#20013;&#30340;&#35821;&#22659;&#20449;&#24687;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21333;&#19968;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#24635;&#26159;&#24573;&#35270;&#22768;&#23398;&#21644;&#35270;&#35273;&#20027;&#39064;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;Topic-enriched Diffusion&#65288;TopicDiff&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;MCE&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#22312;&#25429;&#33719;&#20027;&#39064;&#20449;&#24687;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TopicDiff&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;MCE&#22522;&#32447;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#20027;&#39064;&#20449;&#24687;&#23545;MCE&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;TopicDiff&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04789v1 Announce Type: cross  Abstract: Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of Topic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21644;&#23436;&#21892;&#36807;&#21435;&#26469;&#19981;&#26029;&#28436;&#36827;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Ever-Evolving Memory by Blending and Refining the Past
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31867;&#20284;&#20154;&#31867;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26500;&#24314;&#38271;&#26399;&#35760;&#24518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26500;&#24314;&#35760;&#24518;&#30340;&#19968;&#20010;&#22825;&#30495;&#26041;&#27861;&#21487;&#33021;&#21482;&#26159;&#21015;&#20986;&#24635;&#32467;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#35828;&#35805;&#32773;&#30340;&#29366;&#24577;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#24182;&#31215;&#32047;&#30683;&#30462;&#20449;&#24687;&#12290;&#35760;&#24518;&#20445;&#25345;&#26377;&#32452;&#32455;&#23545;&#20110;&#38477;&#20302;&#22238;&#24212;&#29983;&#25104;&#22120;&#30340;&#28151;&#20081;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;&#65292;CREEM&#12290;&#19982;&#20165;&#22522;&#20110;&#24403;&#21069;&#23545;&#35805;&#26500;&#24314;&#35760;&#24518;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20013;&#28151;&#21512;&#36807;&#21435;&#30340;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#21892;&#36807;&#31243;&#26469;&#22788;&#29702;&#22810;&#20313;&#25110;&#36807;&#26102;&#20449;&#24687;&#12290;&#36825;&#31181;&#21019;&#26032;&#24615;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#19968;&#20010;&#26356;&#21152;&#30693;&#24773;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#26088;&#22312;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.04785</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;5&#24180;&#24930;&#24615;&#30142;&#30149;&#38431;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#22914;&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#20174;&#21488;&#28286;&#21307;&#38498;&#25968;&#25454;&#24211;&#25910;&#38598;&#20102;&#20116;&#24180;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#21253;&#25324;1,420,596&#20221;&#20020;&#24202;&#31508;&#35760;&#12289;387,392&#20221;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#20197;&#21450;&#36229;&#36807;1,505&#31181;&#23454;&#39564;&#23460;&#26816;&#39564;&#39033;&#30446;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#29992;&#20110;&#30740;&#31350;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#25991;&#26412;&#23884;&#20837;&#32534;&#30721;&#22120;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#23454;&#39564;&#23460;&#26816;&#39564;&#25968;&#20540;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04785v1 Announce Type: cross  Abstract: Chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. Numerous research studies have been attempted with various deep learning models in diagnosis. However, most previous studies had certain limitations, including using publicly available datasets (e.g. MIMIC), and imbalanced data. In this study, we collected five-year electronic health records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. We proposed a novel Large Language Multimodal Models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (DNN) module to 
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.04782</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#35843;&#26597;&#65306;&#34920;&#31034;&#23398;&#20064;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Temporal Knowledge Graph: Representation Learning and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04782
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#19979;&#28216;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#30693;&#35782;&#22270;&#19978;&#65292;&#20854;&#20107;&#23454;&#19981;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#24182;&#24573;&#30053;&#20102;&#23427;&#20204;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#28436;&#21464;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#30693;&#35782;&#22270;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#22823;&#37327;&#32467;&#26500;&#21270;&#30693;&#35782;&#20165;&#23384;&#22312;&#20110;&#29305;&#23450;&#26102;&#26399;&#20869;&#12290;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20026;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#23398;&#20064;&#20302;&#32500;&#21521;&#37327;&#23884;&#20837;&#12290;&#26102;&#24577;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#26631;&#20934;&#30693;&#35782;&#22270;&#26694;&#26550;&#20013;&#65292;&#21487;&#20197;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#38543;&#26102;&#38388;&#30340;&#21160;&#24577;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#23545;&#26102;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#20174;&#20171;&#32461;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#21644;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04782v1 Announce Type: cross  Abstract: Knowledge graphs have garnered significant research attention and are widely used to enhance downstream applications. However, most current studies mainly focus on static knowledge graphs, whose facts do not change with time, and disregard their dynamic evolution over time. As a result, temporal knowledge graphs have attracted more attention because a large amount of structured knowledge exists only within a specific period. Knowledge graph representation learning aims to learn low-dimensional vector embeddings for entities and relations in a knowledge graph. The representation learning of temporal knowledge graphs incorporates time information into the standard knowledge graph framework and can model the dynamics of entities and relations over time. In this paper, we conduct a comprehensive survey of temporal knowledge graph representation learning and its applications. We begin with an introduction to the definitions, datasets, and e
&lt;/p&gt;</description></item><item><title>MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;</title><link>https://arxiv.org/abs/2403.04780</link><description>&lt;p&gt;
MuseGraph&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#23548;&#21521;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#36890;&#29992;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04780
&lt;/p&gt;
&lt;p&gt;
MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20016;&#23500;&#23646;&#24615;&#30340;&#22270;&#22312;&#24314;&#27169;&#20114;&#32852;&#23454;&#20307;&#21644;&#25913;&#36827;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24102;&#23646;&#24615;&#30340;&#22270;&#65292;&#20294;&#38656;&#35201;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#22270;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#20294;LLMs&#22312;&#22270;&#25366;&#25496;&#20013;&#30340;&#29983;&#25104;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; MuseGraph&#65292;&#23427;&#26080;&#32541;&#25972;&#21512;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#29983;&#25104;&#24341;&#20837;&#19968;&#20010;&#32039;&#20945;&#30340;&#22270;&#25551;&#36848;&#65292;&#20197;&#22312;&#35821;&#35328;&#20196;&#29260;&#38480;&#21046;&#30340;&#32422;&#26463;&#19979;&#23553;&#35013;&#26469;&#33258;&#22270;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04780v1 Announce Type: cross  Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. T
&lt;/p&gt;</description></item><item><title>&#23558;&#21512;&#19968;&#31639;&#27861;&#31227;&#21160;&#21040;&#28436;&#31639;&#23618;&#27425;&#21518;&#65292;&#19968;&#38454;&#21472;&#21152;&#20173;&#28982;&#20445;&#25345;&#23436;&#22791;&#65292;&#36825;&#23545;&#20110;&#26631;&#20934;&#19968;&#38454;&#21472;&#21152;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.04775</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#21512;&#19968;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Superposition with Delayed Unification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04775
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21512;&#19968;&#31639;&#27861;&#31227;&#21160;&#21040;&#28436;&#31639;&#23618;&#27425;&#21518;&#65292;&#19968;&#38454;&#21472;&#21152;&#20173;&#28982;&#20445;&#25345;&#23436;&#22791;&#65292;&#36825;&#23545;&#20110;&#26631;&#20934;&#19968;&#38454;&#21472;&#21152;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39281;&#21644;&#30340;&#35777;&#26126;&#31995;&#32479;&#20013;&#65292;&#20256;&#32479;&#19978;&#35748;&#20026;&#21512;&#19968;&#26159;&#21407;&#23376;&#30340;&#12290;&#28982;&#32780;&#65292;&#20063;&#21487;&#20197;&#23558;&#21512;&#19968;&#31227;&#21160;&#21040;&#28436;&#31639;&#23618;&#27425;&#65292;&#23558;&#21512;&#19968;&#31639;&#27861;&#30340;&#27493;&#39588;&#36716;&#21270;&#20026;&#25512;&#29702;&#12290;&#23545;&#20110;&#20381;&#36182;&#36820;&#22238;&#22823;&#29978;&#33267;&#26080;&#31351;&#38598;&#21512;&#30340;&#21512;&#19968;&#31243;&#24207;&#30340;&#28436;&#31639;&#26469;&#35828;&#65292;&#23558;&#21512;&#19968;&#34701;&#20837;&#28436;&#31639;&#26159;&#19968;&#31181;&#23558;&#21512;&#19968;&#21644;&#25512;&#29702;&#30456;&#20114;&#20132;&#32455;&#30340;&#21560;&#24341;&#20154;&#26041;&#27861;&#12290;&#36825;&#36866;&#29992;&#20110;AC-&#21472;&#21152;&#21644;&#39640;&#38454;&#21472;&#21152;&#31561;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#23558;&#21512;&#19968;&#35268;&#21017;&#31227;&#21160;&#21040;&#28436;&#31639;&#23618;&#38754;&#26102;&#65292;&#19968;&#38454;&#21472;&#21152;&#20173;&#28982;&#26159;&#23436;&#22791;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21363;&#20351;&#23545;&#20110;&#26631;&#20934;&#30340;&#19968;&#38454;&#21472;&#21152;&#65292;&#36825;&#20063;&#20855;&#26377;&#35832;&#22810;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04775v1 Announce Type: cross  Abstract: Classically, in saturation-based proof systems, unification has been considered atomic. However, it is also possible to move unification to the calculus level, turning the steps of the unification algorithm into inferences. For calculi that rely on unification procedures returning large or even infinite sets of unifiers, integrating unification into the calculus is an attractive method of dovetailing unification and inference. This applies, for example, to AC-superposition and higher-order superposition. We show that first-order superposition remains complete when moving unification rules to the calculus level. We discuss some of the benefits this has even for standard first-order superposition and provide an experimental evaluation.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25945;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04772</link><description>&lt;p&gt;
&#29992;&#31895;&#31961;&#38598;&#34920;&#31034;&#25945;&#23398;&#20869;&#23481;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Representing Pedagogic Content Knowledge Through Rough Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25945;&#23398;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21517;&#25945;&#24072;&#30340;&#30693;&#35782;&#22522;&#30784;&#21253;&#25324;&#25968;&#23398;&#20869;&#23481;&#30693;&#35782;&#12289;&#23398;&#29983;&#35748;&#35782;&#35770;&#30693;&#35782;&#21644;&#25945;&#23398;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#23398;&#29983;&#23545;&#20869;&#23481;&#30340;&#30693;&#35782;&#20197;&#21450;&#23398;&#20064;&#29615;&#22659;&#26377;&#30528;&#20005;&#37325;&#24433;&#21709;&#12290;&#25945;&#32946;&#30740;&#31350;&#25991;&#29486;&#35748;&#35782;&#21040;&#22312;&#36817;&#20284;&#24847;&#20041;&#19978;&#24418;&#24335;&#21270;&#19981;&#21516;&#20869;&#23481;&#30693;&#35782;&#30340;&#24517;&#35201;&#24615;&#12290;&#30456;&#20851;&#38382;&#39064;&#20043;&#19968;&#26159;&#21327;&#35843;&#30340;&#24418;&#24335;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;AI&#36719;&#20214;&#31995;&#32479;&#19981;&#20851;&#27880;&#24847;&#20041;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#31995;&#32479;&#20063;&#23384;&#22312;&#33258;&#36523;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35782;&#21035;&#20102;&#24314;&#27169;&#25945;&#24072;&#23545;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#23618;&#31895;&#31961;&#38598;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20854;&#33021;&#22815;&#19968;&#33268;&#22788;&#29702;&#27169;&#31946;&#24615;&#12289;&#31890;&#24230;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#21033;&#29992;&#25193;&#23637;&#31034;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#28857;&#65292;&#20197;&#31561;&#24335;&#25512;&#29702;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04772v1 Announce Type: new  Abstract: A teacher's knowledge base consists of knowledge of mathematics content, knowledge of student epistemology, and pedagogical knowledge. It has severe implications on the understanding of student's knowledge of content, and the learning context in general. The necessity to formalize the different content knowledge in approximate senses is recognized in the education research literature. A related problem is that of coherent formalizability. Responsive or smart AI-based software systems do not concern themselves with meaning, and trained ones are replete with their own issues. In the present research, many issues in modeling teachers' understanding of content are identified, and a two-tier rough set-based model is proposed by the present author. The main advantage of the proposed approach is in its ability to coherently handle vagueness, granularity and multi-modality. An extended example to equational reasoning is used to demonstrate these
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;</title><link>https://arxiv.org/abs/2403.04769</link><description>&lt;p&gt;
&#31227;&#38500;GPT4&#30340;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Removing GPT4's Filter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#32463;&#36807;&#24494;&#35843;&#30340;GPT4&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#31227;&#38500;&#20854;&#22312;&#23398;&#20064;&#26399;&#38388;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT4&#26368;&#21021;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#24535;&#24895;&#32773;&#25552;&#20379;&#21453;&#39304;&#20197;&#25945;&#23548;GPT4&#19981;&#35201;&#29983;&#25104;&#19981;&#24403;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25805;&#20316;&#24050;&#32463;&#36827;&#34892;&#24494;&#35843;&#30340;&#29256;&#26412;&#65292;&#20351;&#20854;&#24674;&#22797;&#21040;&#27809;&#26377;&#32463;&#36807;RLHF&#65288;Reinforcement learning from Human Feedback&#65289;&#30340;&#34892;&#20026;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#20102;&#27169;&#22411;&#22312;RLHF&#26399;&#38388;&#23398;&#20064;&#30340;&#25152;&#26377;&#23433;&#20840;&#26426;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;GPT4&#22312;&#27809;&#26377;&#32463;&#36807;RLHF&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#26102;&#65292;&#23427;&#22833;&#21435;&#20102;&#25152;&#26377;&#25233;&#21046;&#21147;&#65292;&#21482;&#38656;&#21069;&#20960;&#20010;&#35789;&#23601;&#21487;&#20197;&#29983;&#25104;&#38750;&#24120;&#19981;&#24403;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04769v1 Announce Type: cross  Abstract: GPT4 was initially trained on large amounts of data, and then fine-tuned using Reinforcement learning from Human Feedback (RLHF), which is when volunteers give feedback in order to teach GPT4 not to create inappropriate content. In this paper, we present a method to manipulate the fine-tuned version into reverting to pre-RLHF behavior, effectively removing all safety mechanisms that the model learned during RLHF. In particular, when GPT4 acts without RLHF, it loses all inhibition, and can complete very inappropriate content given only the first few words.
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pix2Gif&#65292;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;GIF&#30340;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#21644;&#24863;&#30693;&#25439;&#22833;&#20197;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#24182;&#20445;&#25345;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04634</link><description>&lt;p&gt;
Pix2Gif&#65306;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#25193;&#25955;&#30340;GIF&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pix2Gif: Motion-Guided Diffusion for GIF Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04634
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pix2Gif&#65292;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;GIF&#30340;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#21644;&#24863;&#30693;&#25439;&#22833;&#20197;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#24182;&#20445;&#25345;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Gif&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36816;&#21160;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#21040;GIF&#65288;&#35270;&#39057;&#65289;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#26500;&#24314;&#20026;&#30001;&#25991;&#26412;&#21644;&#36816;&#21160;&#22823;&#23567;&#25552;&#31034;&#25351;&#23548;&#30340;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#26469;&#19981;&#21516;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#22914;teaser fig&#25152;&#31034;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#36981;&#24490;&#36816;&#21160;&#24341;&#23548;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36816;&#21160;&#24341;&#23548;&#21464;&#24418;&#27169;&#22359;&#65292;&#20197;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#26465;&#20214;&#19979;&#31354;&#38388;&#21464;&#25442;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#30830;&#20445;&#36716;&#25442;&#30340;&#29305;&#24449;&#22270;&#20445;&#25345;&#22312;&#19982;&#30446;&#26631;&#22270;&#20687;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#30830;&#20445;&#20869;&#23481;&#19968;&#33268;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20934;&#22791;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;TGIF&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#36830;&#36143;&#30340;&#22270;&#20687;&#24103;&#26469;&#31934;&#24515;&#31579;&#36873;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26377;&#20851;&#20027;&#39064;&#30340;&#26102;&#38388;&#21464;&#21270;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#20197;&#38646;&#23556;&#26679;&#30340;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04634v1 Announce Type: cross  Abstract: We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.04629</link><description>&lt;p&gt;
&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#40657;&#21283;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BO&#26412;&#36523;&#20063;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#65292;&#32570;&#20047;&#25552;&#20379;&#20026;&#20309;&#25552;&#35758;&#35780;&#20272;&#26576;&#20123;&#21442;&#25968;&#30340;&#29702;&#30001;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ShapleyBO&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#21338;&#24328;&#35770;Shapley&#20540;&#35299;&#37322;BO&#25552;&#35758;&#30340;&#26694;&#26550;&#12290;&#23427;&#37327;&#21270;&#20102;&#27599;&#20010;&#21442;&#25968;&#23545;BO&#30340;&#25910;&#33719;&#20989;&#25968;&#30340;&#36129;&#29486;&#12290;&#21033;&#29992;Shapley&#20540;&#30340;&#32447;&#24615;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20687;&#32622;&#20449;&#36793;&#30028;&#36825;&#26679;&#30340;&#21152;&#27861;&#25910;&#33719;&#20989;&#25968;&#25512;&#21160;BO&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ShapleyBO&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#23545;&#20110;&#21208;&#25506;aleatoric&#21644;&#35748;&#35782;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#28210;&#26579;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#23454;&#29616;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21487;&#24212;&#29992;&#20110;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21516;&#26102;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#22810;&#20219;&#21153;&#21160;&#20316;&#24207;&#21015;&#30340;&#37325;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.04115</link><description>&lt;p&gt;
DNAct&#65306;&#25193;&#25955;&#24341;&#23548;&#30340;&#22810;&#20219;&#21153;3D&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DNAct: Diffusion Guided Multi-Task 3D Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#26694;&#26550;&#65292;&#32467;&#21512;&#31070;&#32463;&#28210;&#26579;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#23454;&#29616;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21487;&#24212;&#29992;&#20110;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21516;&#26102;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#22810;&#20219;&#21153;&#21160;&#20316;&#24207;&#21015;&#30340;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DNAct&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#31070;&#32463;&#28210;&#26579;&#39044;&#35757;&#32451;&#21644;&#25193;&#25955;&#35757;&#32451;&#65292;&#20197;&#22312;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#20013;&#23454;&#29616;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;DNAct&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21033;&#29992;&#31070;&#32463;&#28210;&#26579;&#20174;&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;2D&#35821;&#20041;&#29305;&#24449;&#21040;3D&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#30340;&#20840;&#38754;&#35821;&#20041;&#29702;&#35299;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#20110;&#38656;&#35201;&#20016;&#23500;&#30340;3D&#35821;&#20041;&#21644;&#20934;&#30830;&#20960;&#20309;&#30340;&#25361;&#25112;&#24615;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#23398;&#20064;&#21253;&#21547;&#22810;&#20219;&#21153;&#28436;&#31034;&#20013;&#22266;&#26377;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#20174;&#19981;&#21516;&#20219;&#21153;&#30340;&#21160;&#20316;&#24207;&#21015;&#37325;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04115v1 Announce Type: cross  Abstract: This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing d
&lt;/p&gt;</description></item><item><title>MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03691</link><description>&lt;p&gt;
MolNexTR&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03691
&lt;/p&gt;
&lt;p&gt;
MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#32467;&#26500;&#35782;&#21035;&#39046;&#22495;&#65292;&#23558;&#20998;&#23376;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#21644;SMILES&#23383;&#31526;&#20018;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21270;&#23398;&#25991;&#29486;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#32472;&#22270;&#39118;&#26684;&#21644;&#32422;&#23450;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolNexTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21040;&#22270;&#32467;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21512;&#24182;&#20102;ConvNext&#21644;Vision-TRansformer&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#23376;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26356;&#32454;&#33268;&#25552;&#21462;&#12290;MolNexTR&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#29702;&#35299;&#23427;&#20204;&#30340;&#24067;&#23616;&#35268;&#21017;&#12290;&#23427;&#36824;&#25797;&#38271;&#28789;&#27963;&#22320;&#23558;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#35782;&#21035;&#25163;&#24615;&#24182;&#35299;&#26512;&#32553;&#20889;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#31639;&#27861;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#12289;&#22270;&#20687;&#27745;&#26579;&#27169;&#22359;&#21644;&#21518;&#22788;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03691v1 Announce Type: cross  Abstract: In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing modul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.02118</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#38754;&#21521;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#38544;&#24335;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Towards Implicit Prompt For Text-To-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26174;&#24335;&#25552;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#25552;&#31034;&#65288;&#26263;&#31034;&#30446;&#26631;&#32780;&#19981;&#26126;&#30830;&#25552;&#21040;&#65289;&#12290;&#36825;&#20123;&#25552;&#31034;&#21487;&#33021;&#28040;&#38500;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24212;&#29992;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24403;&#19979;T2I&#27169;&#22411;&#26397;&#30528;&#38544;&#24335;&#25552;&#31034;&#30340;&#29616;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImplicitBench&#30340;&#22522;&#20934;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;T2I&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25910;&#38598;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#36229;&#36807;2,000&#20010;&#38544;&#24335;&#25552;&#31034;&#65306;&#36890;&#29992;&#31526;&#21495;&#12289;&#21517;&#20154;&#38544;&#31169;&#21644;&#19981;&#23433;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20845;&#20010;&#30693;&#21517;T2I&#27169;&#22411;&#22312;&#36825;&#20123;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;T2I&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#21019;&#24314;&#21508;&#31181;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02118v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various targe
&lt;/p&gt;</description></item><item><title>ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2403.01564</link><description>&lt;p&gt;
ComTraQ-MPC&#65306;&#20803;&#35757;&#32451;&#30340;DQN-MPC&#38598;&#25104;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01564
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#21487;&#35266;&#23519;&#12289;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36712;&#36857;&#36319;&#36394;&#30340;&#26368;&#20339;&#20915;&#31574;&#24448;&#24448;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#26159;&#25351;&#20195;&#29702;&#20174;&#20256;&#24863;&#22120;&#33719;&#21462;&#30495;&#23454;&#29366;&#24577;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#24179;&#34913;&#36164;&#28304;&#20445;&#23384;&#12289;&#20934;&#30830;&#29366;&#24577;&#20272;&#35745;&#21644;&#31934;&#30830;&#36319;&#36394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ComTraQ-MPC&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Deep Q-Networks (DQN)&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;&#20803;&#35757;&#32451;&#30340;DQN&#30830;&#20445;&#20102;&#33258;&#36866;&#24212;&#20027;&#21160;&#23450;&#20301;&#35843;&#24230;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01564v1 Announce Type: cross  Abstract: Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00225</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25216;&#33021;&#25193;&#25955;&#23454;&#29616;&#31283;&#20581;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Policy Learning via Offline Skill Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00225
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#38271;&#26102;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#12290;&#36825;&#20123;&#25216;&#33021;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26080;&#20851;&#20219;&#21153;&#22320;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#21152;&#24555;&#38024;&#23545;&#26032;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25216;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#38480;&#20110;&#23545;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#20381;&#36182;&#65292;&#24403;&#23581;&#35797;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#21516;&#20110;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#22522;&#20110;&#25216;&#33021;&#30340;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23601;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#23427;&#37319;&#29992;&#20102;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#20174;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#25216;&#33021;&#25193;&#23637;&#20986;&#30340;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548;&#25193;&#25955;&#25216;&#33021;&#35299;&#30721;&#22120;&#65292;&#32467;&#21512;&#20998;&#23618;&#32534;&#30721;&#65292;&#20197;&#35299;&#24320;&#25216;&#33021;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18920</link><description>&lt;p&gt;
&#20809;&#35889;&#36935;&#35265;&#31354;&#38388;: &#21644;&#35856;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#24182;&#20381;&#27425;&#24212;&#29992;&#20110;&#20851;&#32852;&#19981;&#21516;&#30340;3D&#24418;&#29366;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#21151;&#33021;&#26144;&#23556;&#26694;&#26550;&#19982;&#32463;&#20856;&#34920;&#38754;&#21464;&#24418;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#26144;&#23556;&#24418;&#29366;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#29992;&#20110;&#24418;&#29366;&#21305;&#37197;&#30340;&#21151;&#33021;&#26144;&#23556;&#26041;&#27861;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#24341;&#20837;&#20809;&#35889;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25670;&#33073;&#20102;&#36890;&#24120;&#20351;&#29992;&#20294;&#35745;&#31639;&#26114;&#36149;&#30340;&#20165;&#23545;&#36817;&#31561;&#36317;&#24418;&#29366;&#21464;&#24418;&#26377;&#25928;&#30340;&#27979;&#22320;&#36317;&#31163;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#35299;&#30721;&#22120;&#22686;&#24378;&#35270;&#39057;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Boosting Neural Representations for Videos with a Conditional Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18152
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#24050;&#32463;&#25104;&#20026;&#35270;&#39057;&#23384;&#20648;&#21644;&#22788;&#29702;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24103;&#35299;&#30721;&#36807;&#31243;&#20013;&#20013;&#38388;&#29305;&#24449;&#30340;&#19981;&#36275;&#23545;&#40784;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22686;&#24378;&#26694;&#26550;&#26469;&#21152;&#24378;&#24403;&#21069;&#30340;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#20223;&#23556;&#21464;&#25442;&#27169;&#22359;&#30340;&#26465;&#20214;&#35299;&#30721;&#22120;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#24103;&#32034;&#24341;&#20316;&#20026;&#20808;&#39564;&#26465;&#20214;&#65292;&#26377;&#25928;&#22320;&#23558;&#20013;&#38388;&#29305;&#24449;&#19982;&#30446;&#26631;&#24103;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24358;NeRV-like&#27169;&#22359;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#21442;&#25968;&#20998;&#24067;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#20511;&#21161;&#39640;&#39057;&#20449;&#24687;&#20445;&#30041;&#30340;&#37325;&#26500;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22686;&#24378;&#20102;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18152v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts m
&lt;/p&gt;</description></item><item><title>REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;</title><link>https://arxiv.org/abs/2402.17862</link><description>&lt;p&gt;
REPrune&#65306;&#36890;&#36807;&#26680;&#20195;&#34920;&#36873;&#25321;&#36827;&#34892;&#36890;&#36947;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
REPrune: Channel Pruning via Kernel Representative Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17862
&lt;/p&gt;
&lt;p&gt;
REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#20462;&#21098;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#21152;&#36895;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20462;&#21098;&#27169;&#22411;&#21487;&#20197;&#31435;&#21363;&#37096;&#32626;&#22312;&#36890;&#29992;&#36719;&#20214;&#21644;&#30828;&#20214;&#36164;&#28304;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#21367;&#31215;&#28388;&#27874;&#22120;&#36825;&#20010;&#21333;&#20803;&#19978;&#30340;&#22823;&#20462;&#21098;&#31890;&#24230;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#19981;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;CNNs&#20013;&#20915;&#23450;&#22914;&#20309;&#20197;&#21450;&#22312;&#20309;&#22788;&#24341;&#20837;&#31232;&#30095;&#24615;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPrune&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#27169;&#25311;&#20102;&#26680;&#20462;&#21098;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#26356;&#32454;&#20294;&#26377;&#32467;&#26500;&#30340;&#31890;&#24230;&#12290;REPrune&#20351;&#29992;&#20957;&#32858;&#32858;&#31867;&#35782;&#21035;&#27599;&#20010;&#36890;&#36947;&#20869;&#30340;&#30456;&#20284;&#26680;&#12290;&#28982;&#21518;&#65292;&#23427;&#36873;&#25321;&#26368;&#22823;&#21270;&#21253;&#21547;&#26680;&#20195;&#34920;&#30340;&#28388;&#27874;&#22120;&#65292;&#21516;&#26102;&#20248;&#21270;&#26368;&#22823;&#32858;&#31867;&#35206;&#30422;&#38382;&#39064;&#12290;&#36890;&#36807;&#19982;&#21516;&#26102;&#35757;&#32451;-&#20462;&#21098;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;REPrune&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17862v1 Announce Type: cross  Abstract: Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#21360;&#24230;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#24615;&#33021;&#22522;&#20934;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26032;&#30340;&#35780;&#32423;&#21644;&#26816;&#27979;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.15832</link><description>&lt;p&gt;
&#21033;&#29992;&#33487;&#26408;&#31934;&#19982;&#20234;&#32418;&#26579;&#33394;&#25972;&#24352;&#22270;&#20687;&#36827;&#34892;&#33014;&#36136;&#30244;&#35786;&#26029;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65306;&#21360;&#24230;&#38431;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#21360;&#24230;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#24615;&#33021;&#22522;&#20934;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26032;&#30340;&#35780;&#32423;&#21644;&#26816;&#27979;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32959;&#30244;&#20195;&#34920;&#19968;&#31181;&#20005;&#37325;&#19988;&#21361;&#21450;&#29983;&#21629;&#30340;&#30142;&#30149;&#65292;&#38656;&#35201;&#31934;&#30830;&#30340;&#35786;&#26029;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#33041;&#32959;&#30244;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#20005;&#26684;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#23454;&#39564;&#30340;&#21457;&#29616;&#65292;&#25512;&#21160;&#20102;&#24739;&#32773;&#25252;&#29702;&#12290;&#23427;&#22312;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#26041;&#38754;&#24314;&#31435;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#20010;&#19987;&#27880;&#20110;&#21360;&#24230;&#20154;&#21475;&#30340;&#26032;&#25968;&#25454;&#38598;&#65288;IPD-Brain&#65289;&#65292;&#20026;&#29616;&#26377;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#20351;&#29992;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;ResNet-50&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#32467;&#21512;DTFD&#29305;&#24449;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#22312;IPD-Brain&#21644;TCGA-Brain&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19977;&#20998;&#33014;&#36136;&#30244;&#20122;&#22411;&#20998;&#31867;&#30340;&#26368;&#26032;AUC&#65288;&#20998;&#21035;&#20026;88.08&#21644;95.81&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35780;&#32423;&#21644;&#26816;&#27979;IHC&#20998;&#23376;&#29983;&#29289;&#26631;&#24535;&#29289;&#65288;IDH1&#65288;&#31361;&#21464; R132H&#65289;&#12289;TP53&#12289;ATRX&#12289;Ki-67&#65289;&#26041;&#38754;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15832v1 Announce Type: cross  Abstract: Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies. This study advances patient care with findings from rigorous multiple-instance-learning experimentations across various feature extractors and aggregators in brain tumor histopathology. It establishes new performance benchmarks in glioma subtype classification across multiple datasets, including a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research. Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification. Moreover, it establishes new benchmarks in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2402.13709</link><description>&lt;p&gt;
SaGE&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SaGE: Evaluating Moral Consistency in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#23637;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20063;&#23384;&#22312;&#36947;&#24503;&#19981;&#19968;&#33268;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#65288;&#20197;&#21450;&#24635;&#20307;&#21487;&#20449;&#36182;&#24615;&#65289;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20197;&#24448;&#22312;LLM&#35780;&#20272;&#39046;&#22495;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#24320;&#21457;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#34913;&#37327;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36947;&#24503;&#24773;&#26223;&#24448;&#24448;&#32570;&#20047;&#26222;&#36941;&#35748;&#21516;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#20854;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SaGE&#65289;&#65292;&#22522;&#20110;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#30340;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;&#12290;RoTs&#26159;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#21407;&#21017;&#65292;&#21487;&#26377;&#25928;&#24110;&#21161;&#35299;&#37322;&#20854;&#20915;&#31574;&#31574;&#30053;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36947;&#24503;&#19968;&#33268;&#24615;&#35821;&#26009;&#24211;&#65288;MCC&#65289;&#65292;&#21253;&#21547;50K&#20010;&#36947;&#24503;&#38382;&#39064;&#12289;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03921</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Enhance Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23427;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#24378;&#35843;&#65292;&#29305;&#21035;&#26159;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#26377;&#25928;&#22320;&#24179;&#34913;&#21208;&#25506;&#21644;&#24320;&#21457;&#12290;&#23613;&#31649;&#22312;BO&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24179;&#34913;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24494;&#22937;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;LLAMBO&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19982;BO&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#20351;LLM&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#35780;&#20272;&#25552;&#20986;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#32467;&#21512;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#26469;&#22686;&#24378;&#22522;&#20110;&#27169;&#22411;&#30340;BO&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLAMBO&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14197</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#27492;&#31867;&#25915;&#20987;&#26102;&#30340;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#20174;&#32780;&#24320;&#21457;&#20102;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#20869;&#23481;&#30340;&#25972;&#21512;&#24050;&#32463;&#23454;&#29616;&#20102;LLMs&#30340;&#26356;&#26032;&#21644;&#24191;&#27867;&#24212;&#29992;&#65292;&#27604;&#22914;&#24494;&#36719;Copilot&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#20063;&#35753;LLMs&#38754;&#20020;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22806;&#37096;&#20869;&#23481;&#20013;&#23884;&#20837;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;ompromising LLM&#36755;&#20986;&#24182;&#23548;&#33268;&#21709;&#24212;&#20559;&#31163;&#29992;&#25143;&#26399;&#26395;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#22522;&#20934;&#27979;&#35797;BIPIA&#65292;&#20197;&#35780;&#20272;&#36825;&#31867;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#20998;&#26512;&#20102;&#35813;&#25915;&#20987;&#25104;&#21151;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21363;LLMs&#26080;&#27861;&#21306;&#20998;&#25351;&#20196;&#21644;&#22806;&#37096;&#20869;&#23481;&#20197;&#21450;&#32570;&#20047;&#24847;&#35782;&#19981;&#25191;&#34892;&#22806;&#37096;&#20869;&#23481;&#20869;&#30340;&#25351;&#20196;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#40657;&#30418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14197v2 Announce Type: replace-cross  Abstract: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.07624</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A dynamical clipping approach with task feedback for Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20248;&#21270;&#21644;&#26426;&#22120;&#20154;&#23398;&#20064;&#31561;&#12290;&#28982;&#32780;&#65292;PPO&#21463;&#21040;&#22266;&#23450;&#21098;&#35009;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#21069;&#27809;&#26377;&#29702;&#35770;&#35777;&#26126;&#26368;&#20339;&#21098;&#35009;&#36793;&#30028;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#29420;&#29305;&#30340;&#21098;&#35009;&#36793;&#30028;&#25130;&#26029;&#26032;&#26087;&#31574;&#30053;&#30340;&#27604;&#29575;&#65292;&#21487;&#20197;&#30830;&#20445;&#31283;&#23450;&#30340;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20339;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22266;&#23450;&#30340;&#21098;&#35009;&#36793;&#30028;&#38480;&#21046;&#20102;agent&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#19968;&#31181;&#21160;&#24577;&#21098;&#35009;&#36793;&#30028;&#20197;&#22686;&#24378;PPO&#30340;&#24615;&#33021;&#26159;&#38750;&#24120;&#26377;&#30410;&#30340;&#12290;&#19982;&#20197;&#24448;&#30340;&#21098;&#35009;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#20013;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#35270;&#20316;RL&#20219;&#21153;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07624v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization (PPO) has been broadly applied to various domains, including Large Language Model (LLM) optimization and Robotics learning, etc. However, PPO is limited by a fixed setting for the clipping bound. Specifically, there is no theoretical proof that the optimal clipping bound remains consistent throughout the entire training process. Truncating the ratio of the new and old policies with a unique clipping bound ensures stable training and can achieve the best training performance. Additionally, previous research suggests that a fixed clipping bound limits the agent's exploration. Therefore, researching a dynamical clipping bound to enhance PPO's performance can be highly beneficial. Different from previous clipping approaches, we consider increasing the maximum cumulative Return in reinforcement learning (RL) tasks as the preference of the RL task, and propose a bi-level proximal policy optimization parad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20132;&#27969;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GPT&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.07214</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20419;&#36827;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20132;&#27969;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GPT&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#25968;&#23383;&#21270;&#29615;&#22659;&#20013;&#65292;&#33258;&#20027;&#24037;&#20855;&#21644;&#26426;&#22120;&#20154;&#27491;&#21464;&#24471;&#21496;&#31354;&#35265;&#24815;&#12290;&#37492;&#20110;&#36825;&#19968;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20154;&#26426;&#20132;&#27969;&#25163;&#27573;&#20419;&#36827;&#21487;&#21464;&#33258;&#20027;&#24615;&#12290;&#25105;&#20204;&#22312;Unity&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;GPT&#26680;&#24515;&#20026;&#21160;&#21147;&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#26426;&#22120;&#20154;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#30001;&#29420;&#31435;&#30340;GPT&#26680;&#24515;&#25552;&#20379;&#21160;&#21147;&#12290;&#36890;&#36807;OpenAI&#30340;&#20989;&#25968;&#35843;&#29992;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#19981;&#21463;&#32467;&#26500;&#32422;&#26463;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21644;&#32467;&#26500;&#21270;&#26426;&#22120;&#20154;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#19968;&#39033;&#28041;&#21450;12&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#30340;&#26377;&#25928;&#24615;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#32473;&#20104;&#26426;&#20250;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#26102;&#29992;&#25143;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07214v2 Announce Type: replace-cross  Abstract: In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natural language, each powered by individual GPT cores. By means of OpenAI's function calling, we bridge the gap between unstructured natural language input and structure robot actions. A user study with 12 participants explores the effectiveness of GPT-4 and, more importantly, user strategies when being given the opportunity to converse in nat
&lt;/p&gt;</description></item><item><title>&#35782;&#21035;&#20102;&#25209;&#24402;&#19968;&#21270;&#20013;&#30340;&#29305;&#24449;&#20957;&#32858;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25209;&#24402;&#19968;&#21270;&#65288;UBN&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#65292;&#20174;&#32780;&#25913;&#21892;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.15993</link><description>&lt;p&gt;
&#32479;&#19968;&#25209;&#24402;&#19968;&#21270;&#65306;&#35782;&#21035;&#21644;&#32531;&#35299;&#25209;&#24402;&#19968;&#21270;&#20013;&#30340;&#29305;&#24449;&#20957;&#32858;&#21450;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15993
&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20102;&#25209;&#24402;&#19968;&#21270;&#20013;&#30340;&#29305;&#24449;&#20957;&#32858;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25209;&#24402;&#19968;&#21270;&#65288;UBN&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#65292;&#20174;&#32780;&#25913;&#21892;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#24050;&#32463;&#25104;&#20026;&#24403;&#20195;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#22522;&#26412;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;BN&#37319;&#29992;&#23621;&#20013;&#21644;&#32553;&#25918;&#25805;&#20316;&#26469;&#26631;&#20934;&#21270;&#27839;&#25209;&#27425;&#32500;&#24230;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20223;&#23556;&#21464;&#25442;&#26469;&#24674;&#22797;&#29305;&#24449;&#12290;&#23613;&#31649;&#26631;&#20934;&#30340;BN&#24050;&#32463;&#26174;&#31034;&#20986;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25910;&#25947;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#30446;&#21069;&#23545;BN&#30340;&#22686;&#24378;&#36890;&#24120;&#21482;&#35299;&#20915;&#20854;&#26426;&#21046;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#30340;&#35282;&#24230;&#23545;BN&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#26816;&#26597;&#65292;&#23558;BN&#20013;&#30340;&#29305;&#24449;&#20957;&#32858;&#35782;&#21035;&#20026;&#23545;&#27979;&#35797;&#24615;&#33021;&#26377;&#23475;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#32479;&#19968;&#25209;&#24402;&#19968;&#21270;&#65288;UBN&#65289;&#30340;&#20004;&#38454;&#27573;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#29305;&#24449;&#20957;&#32858;&#38408;&#20540;&#26469;&#20943;&#36731;&#20957;&#32858;&#25928;&#24212;&#65292;&#20174;&#32780;&#38450;&#27490;&#19981;&#24403;&#30340;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15993v2 Announce Type: replace-cross  Abstract: Batch Normalization (BN) has become an essential technique in contemporary neural network design, enhancing training stability. Specifically, BN employs centering and scaling operations to standardize features along the batch dimension and uses an affine transformation to recover features. Although standard BN has shown its capability to improve deep neural network training and convergence, it still exhibits inherent limitations in certain cases. Current enhancements to BN typically address only isolated aspects of its mechanism. In this work, we critically examine BN from a feature perspective, identifying feature condensation during BN as a detrimental factor to test performance. To tackle this problem, we propose a two-stage unified framework called Unified Batch Normalization (UBN). In the first stage, we employ a straightforward feature condensation threshold to mitigate condensation effects, thereby preventing improper up
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DyG-HAP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;ICU&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2311.15545</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#30340;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DyG-HAP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#20998;&#24067;&#24191;&#20041;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;ICU&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30333;&#34507;&#30333;&#23545;&#25351;&#31034;&#36523;&#20307;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#39044;&#27979;&#34880;&#27974;&#30333;&#34507;&#30333;&#27700;&#24179;&#24182;&#30830;&#23450;&#36866;&#24403;&#21058;&#37327;&#26159;&#20127;&#38656;&#35299;&#20915;&#30340;&#20020;&#24202;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#21361;&#37325;&#24739;&#32773;&#20013;&#65292;&#20197;&#20445;&#25345;&#26368;&#20339;&#34880;&#28082;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#24182;&#19981;&#31616;&#21333;&#65292;&#24517;&#39035;&#21033;&#29992;&#29983;&#21270;&#26631;&#24535;&#29289;&#30340;&#21160;&#24577;&#24615;&#20197;&#21450;&#27835;&#30103;&#24739;&#32773;&#30340;&#32463;&#39564;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#65292;&#38477;&#20302;&#27169;&#22411;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Out-of-Distribution Generalized Dynamic Graph&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20154;&#31867;&#30333;&#34507;&#30333;&#39044;&#27979;&#65288;DyG-HAP&#65289;&#65292;&#33021;&#22815;&#25552;&#20379;&#22312;&#20303;&#38498;&#26399;&#38388;&#20026;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#30333;&#34507;&#30333;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15545v2 Announce Type: replace-cross  Abstract: Human albumin is essential for indicating the body's overall health. Accurately predicting plasma albumin levels and determining appropriate doses are urgent clinical challenges, particularly in critically ill patients, to maintain optimal blood levels. However, human albumin prediction is non-trivial that has to leverage the dynamics of biochemical markers as well as the experience of treating patients. Moreover, the problem of distribution shift is often encountered in real clinical data, which may lead to a decline in the model prediction performance and reduce the reliability of the model's application. In this paper, we propose a framework named Out-of-Distribution Generalized Dynamic Graph Neural Network for Human Albumin Prediction (DyG-HAP), which is able to provide accurate albumin predictions for Intensity Care Unit (ICU) patients during hospitalization. We first model human albumin prediction as a dynamic graph regre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#35270;&#22270;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#36328;&#35270;&#22270;&#23376;&#38598;&#30340;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21333;&#20010;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2311.04056</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#22810;&#35270;&#22270;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Causal Representation Learning with Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04056
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#35270;&#22270;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#36328;&#35270;&#22270;&#23376;&#38598;&#30340;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21333;&#20010;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#20174;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#35270;&#22270;&#65288;&#22914;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#65289;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#20801;&#35768;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#35270;&#22270;&#26500;&#25104;&#24213;&#23618;&#28508;&#22312;&#21464;&#37327;&#23376;&#38598;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#65292;&#36825;&#20123;&#21464;&#37327;&#21487;&#20197;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#27599;&#20010;&#35270;&#22270;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#36328;&#25152;&#26377;&#20219;&#24847;&#25968;&#37327;&#35270;&#22270;&#23376;&#38598;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#30452;&#33267;&#24179;&#28369;&#21452;&#23556;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22270;&#24418;&#26631;&#20934;&#65292;&#25351;&#31034;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31616;&#21333;&#35268;&#21017;&#30830;&#23450;&#21738;&#20123;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21487;&#35782;&#21035;&#24615;&#20195;&#25968;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#26694;&#26550;&#21644;&#29702;&#35770;&#32467;&#26524;&#32479;&#19968;&#24182;&#25193;&#23637;&#20102;&#20808;&#21069;&#20851;&#20110;&#22810;&#35270;&#22270;&#38750;&#32447;&#24615;ICA&#12289;&#35299;&#32544;&#20197;&#21450;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#20960;&#39033;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#25968;&#23383;&#12289;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04056v2 Announce Type: replace-cross  Abstract: We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets. Further, we demonstr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#39640;&#25928;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2310.18882</link><description>&lt;p&gt;
&#21487;&#24494;&#23398;&#20064;&#24191;&#20041;&#32467;&#26500;&#21270;&#30697;&#38453;&#20197;&#23454;&#29616;&#39640;&#25928;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#39640;&#25928;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#36890;&#36807;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#21462;&#20195;&#23494;&#38598;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#21487;&#24494;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#26435;&#37325;&#30697;&#38453;&#30340;&#39640;&#25928;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#31867;&#26032;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#65292;&#36890;&#36807;&#35843;&#25972;&#32467;&#26500;&#21442;&#25968;&#35206;&#30422;&#20102;&#25991;&#29486;&#20013;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#22522;&#20110;&#39640;&#26031;-&#29380;&#21033;&#20811;&#38647;&#26680;&#30340;&#39057;&#22495;&#21487;&#24494;&#21442;&#25968;&#21270;&#26041;&#26696;&#26469;&#23398;&#20064;&#32467;&#26500;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18882v2 Announce Type: replace-cross  Abstract: This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#65292;&#21487;&#20197;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2310.10486</link><description>&lt;p&gt;
ManyQuadrupeds: &#23398;&#20064;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#65292;&#21487;&#20197;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36816;&#21160;&#31574;&#30053;&#36890;&#24120;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#26426;&#22120;&#20154;&#24418;&#24577;&#12289;&#36136;&#37327;&#21644;&#23610;&#23544;&#12290;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#24517;&#39035;&#38024;&#23545;&#27599;&#21488;&#26032;&#26426;&#22120;&#20154;&#37325;&#22797;&#36827;&#34892;&#65292;&#38656;&#35201;&#37325;&#26032;&#35843;&#25972;&#36229;&#21442;&#25968;&#21644;&#22870;&#21169;&#20989;&#25968;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#27599;&#20010;&#26032;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#23581;&#35797;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#20197;&#36866;&#24212;&#19981;&#21516;&#22823;&#23567;&#30340;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#33258;&#30001;&#24230;&#65288;DoF&#65289;&#21644;&#24418;&#24577;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#25110;&#32773;&#36136;&#37327;&#12289;&#24815;&#24615;&#21644;&#23610;&#23544;&#38543;&#26426;&#21270;&#65292;&#36825;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#21463;&#21160;&#29289;&#36816;&#21160;&#25511;&#21046;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25511;&#21046;&#22810;&#26679;&#21270;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#36816;&#21160;&#31574;&#30053;&#12290;&#36825;&#20123;&#26426;&#22120;&#20154;&#30340;&#24046;&#24322;&#21253;&#25324;&#65306;&#21487;&#21464;&#25968;&#37327;&#30340;DoF&#65288;&#21363;12&#25110;16&#20010;&#20851;&#33410;&#65289;&#12289;&#19977;&#31181;&#19981;&#21516;&#30340;&#24418;&#24577;&#21644;&#20174;&#36739;&#20302;&#21040;&#36739;&#39640;&#36136;&#37327;&#33539;&#22260;&#30340;&#24191;&#27867;&#36136;&#37327;&#36328;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10486v2 Announce Type: replace-cross  Abstract: Learning a locomotion policy for quadruped robots has traditionally been constrained to a specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. The robot differences encompass: a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 
&lt;/p&gt;</description></item><item><title>TAIL&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#26377;&#25928;&#29575;&#12289;&#25345;&#32493;&#36866;&#24212;&#19981;&#21516;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2310.05905</link><description>&lt;p&gt;
TAIL: &#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05905
&lt;/p&gt;
&lt;p&gt;
TAIL&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#26377;&#25928;&#29575;&#12289;&#25345;&#32493;&#36866;&#24212;&#19981;&#21516;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25511;&#21046;&#39046;&#22495;&#65288;&#22914;&#26426;&#22120;&#20154;&#25216;&#26415;&#65289;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#20026;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#25110;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;TAIL&#65288;&#20219;&#21153;&#29305;&#23450;&#30340;&#36866;&#37197;&#22120;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26377;&#25928;&#36866;&#24212;&#26032;&#25511;&#21046;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#35821;&#35328;&#39046;&#22495;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;TAIL&#20013;&#25506;&#35752;&#20102;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#20363;&#22914;&#29942;&#39048;&#36866;&#37197;&#22120;&#12289;P&#35843;&#25972;&#21644;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#65292;&#20197;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#20026;&#20855;&#26377;&#26377;&#38480;&#28436;&#31034;&#25968;&#25454;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05905v2 Announce Type: replace-cross  Abstract: The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques -- e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our e
&lt;/p&gt;</description></item><item><title>&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;</title><link>https://arxiv.org/abs/2110.05365</link><description>&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#38543;&#26426;&#24179;&#28369;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Input-dependent Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.05365
&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#26041;&#27861;&#34429;&#28982;&#34987;&#29992;&#26469;&#33719;&#21462;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#20294;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#20854;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#65292;&#22240;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#65307;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#33719;&#24471;&#21487;&#38752;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#26174;&#33879;&#65292;&#20294;&#35813;&#26041;&#27861;&#23384;&#22312;&#35832;&#22914;&#8220;&#35748;&#35777;&#20934;&#30830;&#24615;&#28689;&#24067;&#8221;&#12289;&#35748;&#35777;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#29978;&#33267;&#20844;&#24179;&#24615;&#38382;&#39064;&#31561;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#38519;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#36755;&#20837;&#30456;&#20851;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#24418;&#24335;&#20445;&#35777;&#65292;&#22240;&#27492;&#24471;&#21040;&#30340;&#35777;&#20070;&#24182;&#19981;&#21512;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#26041;&#24046;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;&#21322;&#24377;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#32500;&#24230;&#35781;&#21650;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#22312;&#20005;&#26684;&#30340;&#38480;&#21046;&#26465;&#20214;&#19979;&#20351;&#29992;&#36755;&#20837;&#30456;&#20851;&#24179;&#28369;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#24179;&#28369;&#26041;&#24046;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.05365v3 Announce Type: replace-cross  Abstract: Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as "certified accuracy waterfalls", certification vs.\ accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20182;&#20204;&#22312;&#25991;&#29486;&#20013;&#21807;&#19968;&#21457;&#29616;&#30340;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2102.02649</link><description>&lt;p&gt;
&#26397;&#30528;&#24378;&#21270;&#23398;&#20064;de novo&#22522;&#22240;&#32452;&#32452;&#35013;&#22120;&#36808;&#20986;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
A step toward a reinforcement learning de novo genome assembler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#20182;&#20204;&#22312;&#25991;&#29486;&#20013;&#21807;&#19968;&#21457;&#29616;&#30340;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
De novo&#22522;&#22240;&#32452;&#32452;&#35013;&#26159;&#22522;&#22240;&#32452;&#23398;&#20013;&#19968;&#20010;&#30456;&#20851;&#20294;&#35745;&#31639;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;de novo&#32452;&#35013;&#22120;&#24050;&#32463;&#25104;&#21151;&#22320;&#22312;&#20960;&#20010;&#22522;&#22240;&#32452;&#39033;&#30446;&#20013;&#20351;&#29992;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;"&#26368;&#20339;&#32452;&#35013;&#22120;"&#65292;&#32452;&#35013;&#22120;&#30340;&#36873;&#25321;&#21644;&#35774;&#32622;&#20173;&#28982;&#20381;&#36182;&#29983;&#29289;&#20449;&#24687;&#23398;&#19987;&#23478;&#12290;&#22240;&#27492;&#65292;&#19982;&#20854;&#20182;&#35745;&#31639;&#22797;&#26434;&#38382;&#39064;&#19968;&#26679;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#25104;&#20026;&#24320;&#21457;&#26356;&#20934;&#30830;&#21644;&#33258;&#21160;&#21270;&#32452;&#35013;&#22120;&#30340;&#19968;&#31181;&#26367;&#20195;&#65288;&#25110;&#34917;&#20805;&#65289;&#26041;&#24335;&#12290;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#22797;&#26434;&#27963;&#21160;&#65288;&#22914;&#28216;&#25103;&#65289;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#36843;&#20999;&#38656;&#35201;&#20102;&#35299;&#35813;&#26041;&#27861;&#22312;&#8220;&#30495;&#23454;&#8221;&#38382;&#39064;&#65288;&#22914;DFA&#38382;&#39064;&#65289;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#32452;&#35013;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#21807;&#19968;&#20808;&#21069;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#20180;&#32454;&#25506;&#32034;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.02649v4 Announce Type: replace-cross  Abstract: De novo genome assembly is a relevant but computationally complex task in genomics. Although de novo assemblers have been used successfully in several genomics projects, there is still no 'best assembler', and the choice and setup of assemblers still rely on bioinformatics experts. Thus, as with other computationally complex problems, machine learning may emerge as an alternative (or complementary) way for developing more accurate and automated assemblers. Reinforcement learning has proven promising for solving complex activities without supervision - such games - and there is a pressing need to understand the limits of this approach to 'real' problems, such as the DFA problem. This study aimed to shed light on the application of machine learning, using reinforcement learning (RL), in genome assembly. We expanded upon the sole previous approach found in the literature to solve this problem by carefully exploring the learning as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2401.14280</link><description>&lt;p&gt;
RomanSetu: &#36890;&#36807;&#32599;&#39532;&#21270;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65288;&#29305;&#21035;&#26159;&#20351;&#29992;&#38750;&#25289;&#19969;&#23383;&#27597;&#34920;&#30340;&#35821;&#35328;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25509;&#21475;&#65292;&#20551;&#35774;&#39057;&#32321;&#30340;&#38750;&#27491;&#24335;&#20351;&#29992;&#21644;&#19982;&#33521;&#35821;&#20849;&#20139;&#30340;&#26631;&#35760;&#26377;&#21161;&#20110;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#35777;&#26126;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#29983;&#20135;&#21147;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#22810;&#33050;&#26412;&#25552;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#32599;&#39532;&#21270;&#21644;&#21407;&#29983;&#25991;&#26412;&#65292;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#22312;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08977</link><description>&lt;p&gt;
FedLoGe: &#38271;&#23614;&#25968;&#25454;&#19979;&#30340;&#26412;&#22320;&#21644;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#26159;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#30340;&#33539;&#20363;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;Fed-LT&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#33021;&#65292;&#32780;&#24573;&#35270;&#20102;&#26412;&#22320;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24120;&#35268;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#20248;&#21270;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#22312;Fed-LT&#20013;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#65292;&#25552;&#39640;&#26412;&#22320;&#21644;&#36890;&#29992;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20351;&#29992;&#20849;&#20139;&#39592;&#24178;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framewor
&lt;/p&gt;</description></item><item><title>REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08850</link><description>&lt;p&gt;
REValueD: &#23545;&#21487;&#20998;&#35299;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08850
&lt;/p&gt;
&lt;p&gt;
REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#33021;&#30340;&#21160;&#20316;&#25968;&#37327;&#24222;&#22823;&#65292;&#31163;&#25955;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#8212;&#8212;&#20540;&#20998;&#35299;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20540;&#20998;&#35299;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#34429;&#28982;&#21487;&#20197;&#36943;&#21046;Q&#23398;&#20064;&#31639;&#27861;&#22266;&#26377;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#20294;&#20063;&#20250;&#25918;&#22823;&#30446;&#26631;&#26041;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;&#30340;&#38598;&#21512;&#20197;&#20943;&#36731;&#30446;&#26631;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19968;&#20010;&#32500;&#24230;&#19978;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#23545;&#20854;&#20182;&#32500;&#24230;&#19978;&#26368;&#20248;&#21160;&#20316;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31639;&#27861;REValueD&#65292;&#22312;&#32463;&#36807;&#31163;&#25955;&#21270;&#30340;DeepMind&#25511;&#21046;&#22871;&#20214;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24433;&#21709;REValueD&#34920;&#29616;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2312.07213</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#65306;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Human-computer Interaction for Brain-inspired Computing Based on Machine Learning And Deep Learning:A Review. (arXiv:2312.07213v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#21457;&#23637;&#23545;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#33041;&#21551;&#21457;&#35745;&#31639;&#26159;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#20132;&#21449;&#28857;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#24212;&#29992;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#28436;&#21270;&#12289;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#21382;&#21490;&#65292;&#24182;&#23558;&#20854;&#28436;&#21270;&#21010;&#20998;&#20026;&#36817;&#26399;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#24378;&#35843;&#20102;&#27599;&#20010;&#38454;&#27573;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12290;&#21478;&#22806;&#65292;&#20174;&#20845;&#20010;&#35282;&#24230;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous development of artificial intelligence has a profound impact on biomedical research and other fields.Brain-inspired computing is an important intersection of multimodal technology and biomedical field. This paper presents a comprehensive review of machine learning (ML) and deep learning (DL) models applied in human-computer interaction for brain-inspired computing, tracking their evolution, application value, challenges, and potential research trajectories. First, the basic concepts and development history are reviewed, and their evolution is divided into two stages: recent machine learning and current deep learning, emphasizing the importance of each stage in the research state of human-computer interaction for brain-inspired computing. In addition, the latest progress and key techniques of deep learning in different tasks of human-computer interaction for brain-inspired computing are introduced from six perspectives. Despite significant progress, challenges remain in m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;&#21644;&#35268;&#21010;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#20248;&#21270;&#65292;&#22312;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.03263</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#23398;&#20064;&#21644;&#35268;&#21010;&#65306;&#22312;&#26102;&#21464;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;&#21644;&#35268;&#21010;&#31574;&#30053;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#20248;&#21270;&#65292;&#22312;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#12289;&#38543;&#26426;&#21644;&#26102;&#21464;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#20915;&#31574;&#23545;&#20110;&#33258;&#20027;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#29615;&#22659;&#30340;&#21464;&#21270;&#21487;&#20197;&#23545;&#31995;&#32479;&#30340;&#26368;&#20248;&#20915;&#31574;&#31574;&#30053;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#20026;&#20102;&#23545;&#36825;&#26679;&#30340;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20043;&#21069;&#30340;&#26102;&#21464;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(time-varying Markov Decision Processes, TVMDP)&#30340;&#27010;&#24565;&#19982;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#26102;&#21464;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;(time-varying Partially Observable Markov Decision Processes, TV-POMDP)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#26041;&#27861;&#26469;&#22312;TV-POMDP&#20013;&#20934;&#30830;&#20272;&#35745;&#21644;&#35268;&#21010;&#65306;1&#65289;&#35760;&#24518;&#20248;&#20808;&#29366;&#24577;&#20272;&#35745;(Memory Prioritized State Estimation, MPSE)&#65292;&#21033;&#29992;&#21152;&#26435;&#35760;&#24518;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26102;&#21464;&#36716;&#31227;&#20272;&#35745;&#65307;2&#65289;MPSE&#38598;&#25104;&#30340;&#35268;&#21010;&#31574;&#30053;&#65292;&#20248;&#21270;&#38271;&#26399;&#22870;&#21169;&#30340;&#21516;&#26102;&#32771;&#34385;&#26102;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#20223;&#30495;&#21644;&#30828;&#20214;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#26426;&#22120;&#20154;&#22312;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#12289;&#26102;&#21464;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environ
&lt;/p&gt;</description></item><item><title>APRICOT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;ICU&#24739;&#32773;&#20013;&#23454;&#26102;&#39044;&#27979;&#25935;&#24863;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.02026</link><description>&lt;p&gt;
APRICOT: &#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#20013;&#30340;&#25935;&#24863;&#24230;&#39044;&#27979;&#65306;&#39044;&#27979;&#31283;&#23450;&#24615;&#12289;&#36716;&#21464;&#21644;&#32500;&#25345;&#29983;&#21629;&#30340;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies. (arXiv:2311.02026v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02026
&lt;/p&gt;
&lt;p&gt;
APRICOT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;ICU&#24739;&#32773;&#20013;&#23454;&#26102;&#39044;&#27979;&#25935;&#24863;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICU&#20013;&#30340;&#24739;&#32773;&#20005;&#37325;&#31243;&#24230;&#29366;&#24577;&#21487;&#33021;&#20250;&#22312;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#20043;&#38388;&#36805;&#36895;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#23548;&#33268;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#12290;&#26089;&#26399;&#26816;&#27979;&#21040;&#24694;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21450;&#26102;&#30340;&#24178;&#39044;&#21644;&#26356;&#22909;&#30340;&#29983;&#23384;&#29575;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#30340;&#27599;&#26085;&#35780;&#20272;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27515;&#20129;&#29575;&#20316;&#20026;ICU&#20013;&#25935;&#24863;&#24230;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#26410;&#25972;&#21512;&#25935;&#24863;&#24230;&#29366;&#24577;&#20197;&#30830;&#23450;&#24739;&#32773;&#30340;&#31283;&#23450;&#24615;&#25110;&#23545;&#32500;&#25345;&#29983;&#21629;&#27835;&#30103;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;APRICOT&#65288;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#30340;&#25935;&#24863;&#24230;&#39044;&#27979;&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;ICU&#24739;&#32773;&#30340;&#25935;&#24863;&#24230;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#22806;&#37096;&#12289;&#26102;&#38388;&#19978;&#21644;&#21069;&#30651;&#24615;&#22320;&#24320;&#21457;&#21644;&#24191;&#27867;&#39564;&#35777;&#20102;APRICOT&#27169;&#22411;&#65306;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#65288;UFH&#65289;&#12289;eICU&#21512;&#20316;&#30740;&#31350;&#25968;&#25454;&#24211;&#65288;eICU&#65289;&#21644;&#37325;&#30151;&#30417;&#25252;&#21307;&#30103;&#20449;&#24687;&#24066;&#22330;&#65288;MIMIC&#65289;-IV&#12290;
&lt;/p&gt;
&lt;p&gt;
The acuity state of patients in the intensive care unit (ICU) can quickly change from stable to unstable, sometimes leading to life-threatening conditions. Early detection of deteriorating conditions can result in providing more timely interventions and improved survival rates. Current approaches rely on manual daily assessments. Some data-driven approaches have been developed, that use mortality as a proxy of acuity in the ICU. However, these methods do not integrate acuity states to determine the stability of a patient or the need for life-sustaining therapies. In this study, we propose APRICOT (Acuity Prediction in Intensive Care Unit), a Transformer-based neural network to predict acuity state in real-time in ICU patients. We develop and extensively validate externally, temporally, and prospectively the APRICOT model on three large datasets: University of Florida Health (UFH), eICU Collaborative Research Database (eICU), and Medical Information Mart for Intensive Care (MIMIC)-IV. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01534</link><description>&lt;p&gt;
&#22823;&#22411;&#22320;&#22270;&#19978;&#30340;&#25353;&#38656;&#22478;&#24066;&#20986;&#34892;&#38382;&#39064;&#30340;&#36817;&#20284;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version). (arXiv:2311.01534v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#26410;&#26469;&#20056;&#36710;&#35831;&#27714;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#20107;&#20808;&#26410;&#30693;&#65292;&#20294;&#36981;&#24490;&#20272;&#35745;&#30340;&#32463;&#39564;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22914;&#26524;&#22522;&#30784;&#31574;&#30053;&#26159;&#31283;&#23450;&#30340;&#65292;&#37027;&#20040;&#22522;&#20110;&#28378;&#21160;&#30340;&#31639;&#27861;&#19982;&#36825;&#26679;&#30340;&#22522;&#30784;&#31574;&#30053;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#30340;&#31283;&#23450;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#28378;&#21160;&#30340;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#20855;&#26377;&#23545;&#26410;&#26469;&#38656;&#27714;&#32771;&#34385;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#21487;&#33021;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#12290;&#22823;&#22411;&#29615;&#22659;&#24448;&#24448;&#26377;&#22823;&#37327;&#35831;&#27714;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#22411;&#30340;&#20986;&#31199;&#36710;&#38431;&#20445;&#35777;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#65288;&#36880;&#19968;&#65289;&#28378;&#21160;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#65292;&#20854;&#20013;&#35745;&#31639;&#22797;&#26434;&#24615;&#38543;&#20195;&#29702;&#25968;&#37327;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#36880;&#19968;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but follow an estimated empirical distribution. Recent theory has shown that if a base policy is stable then a rollout-based algorithm with such a base policy produces a near-optimal stable policy. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive. Large environments tend to have a large volume of requests, and hence require a large fleet of taxis to guarantee stability. In this paper, we aim to address the computational bottleneck of multiagent (one-at-a-time) rollout, where the computational complexity grows linearly in the number of agents. We propose an approximate one-at-a-time rollout-based two-phase algorithm that reduces the computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12557</link><description>&lt;p&gt;
DepWiGNN&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12557
&lt;/p&gt;
&lt;p&gt;
DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#20174;&#32431;&#25991;&#26412;&#20013;&#25512;&#26029;&#31354;&#38388;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#19982;&#31526;&#21495;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24341;&#23548;&#21644;&#32858;&#21512;&#31526;&#21495;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNN&#22312;&#22788;&#29702;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#26102;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#22270;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Depth-Wise Graph Neural Network&#65288;DepWiGNN&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#24191;&#24230;&#32500;&#24230;&#19978;&#65292;&#36825;&#26679;&#21487;&#20197;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;DepWiGNN&#21487;&#20197;&#20197;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challen
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07726</link><description>&lt;p&gt;
&#23545;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35768;&#22810;&#21830;&#19994;&#26381;&#21153;&#24050;&#32463;&#25512;&#20986;&#12290;&#36825;&#20123;&#26381;&#21153;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#29983;&#25104;&#21019;&#24847;&#20869;&#23481;&#65288;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#27969;&#30021;&#30340;&#21477;&#23376;&#65289;&#12290;&#23545;&#20110;&#27492;&#31867;&#29983;&#25104;&#20869;&#23481;&#30340;&#20351;&#29992;&#38656;&#35201;&#39640;&#24230;&#30417;&#31649;&#65292;&#22240;&#20026;&#26381;&#21153;&#25552;&#20379;&#21830;&#38656;&#35201;&#30830;&#20445;&#29992;&#25143;&#19981;&#36829;&#21453;&#20351;&#29992;&#25919;&#31574;&#65288;&#20363;&#22914;&#28389;&#29992;&#21830;&#19994;&#21270;&#12289;&#29983;&#25104;&#21644;&#20998;&#21457;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#27700;&#21360;&#25216;&#26415;&#65292;&#20294;&#26159;&#26412;&#25991;&#34920;&#26126;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#30772;&#35299;&#36825;&#20123;&#27700;&#21360;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27700;&#21360;&#21435;&#38500;&#65306;&#23545;&#25163;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#29983;&#25104;&#20869;&#23481;&#20013;&#21024;&#38500;&#23884;&#20837;&#30340;&#27700;&#21360;&#65292;&#28982;&#21518;&#33258;&#30001;&#20351;&#29992;&#32780;&#19981;&#21463;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27700;&#21360;&#20266;&#36896;&#65306;&#23545;&#25163;&#21487;&#20197;&#21019;&#24314;&#38750;&#27861;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.06045</link><description>&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#24615;&#23545;&#27969;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#20005;&#37325;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23545;&#20005;&#37325;&#22825;&#27668;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;HRRR&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;2021&#24180;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#32654;&#22269;&#26412;&#22303;&#20005;&#37325;&#22825;&#27668;&#65288;&#40857;&#21367;&#39118;&#12289;&#20912;&#38649;&#21644;&#22823;&#39118;&#38453;&#65289;&#30340;&#38598;&#25104;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGANs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21518;&#22788;&#29702;&#23545;&#27969;&#20801;&#35768;&#27169;&#22411;&#65288;CAM&#65289;&#30340;&#39044;&#27979;&#12290;CGANs&#34987;&#35774;&#35745;&#29992;&#20110;&#20174;&#30830;&#23450;&#24615;CAM&#39044;&#27979;&#20013;&#21019;&#24314;&#21512;&#25104;&#38598;&#25104;&#25104;&#21592;&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;CNN&#22788;&#29702;&#20197;&#20272;&#35745;&#20005;&#37325;&#22825;&#27668;&#30340;&#27010;&#29575;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#24555;&#36895;&#21047;&#26032;&#65288;HRRR&#65289;1-24&#23567;&#26102;&#39044;&#25253;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#21450;&#26292;&#39118;&#39044;&#35686;&#20013;&#24515;&#65288;SPC&#65289;&#30340;&#20005;&#37325;&#22825;&#27668;&#25253;&#21578;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;2021&#24180;&#30340;HRRR&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#32771;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;20&#65285;&#30340;Brier&#25216;&#24039;&#20998;&#25968;&#65288;BSS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2305.03105</link><description>&lt;p&gt;
HAISTA-NET: &#36890;&#36807;&#27880;&#24847;&#21147;&#36827;&#34892;&#20154;&#31867;&#36741;&#21161;&#30340;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20363;&#20998;&#21106;&#26159;&#22270;&#20687;&#26816;&#27979;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#22312;&#29289;&#20307;&#32454;&#21270;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#22270;&#20687;/&#35270;&#39057;&#32534;&#36753;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#37117;&#38656;&#35201;&#39640;&#24230;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20415;&#26159;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#65292;&#20854;&#31934;&#24230;&#24120;&#24120;&#26080;&#27861;&#36798;&#21040;&#12290;&#23545;&#20110;&#23567;&#32780;&#22797;&#26434;&#30340;&#23545;&#35937;&#26469;&#35828;&#65292;&#24615;&#33021;&#24046;&#36317;&#23588;&#20026;&#26126;&#26174;&#12290;&#36890;&#24120;&#65292;&#20174;&#19994;&#32773;&#21482;&#33021;&#37319;&#29992;&#23436;&#20840;&#25163;&#21160;&#30340;&#27880;&#37322;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#20026;&#39640;&#26354;&#29575;&#12289;&#22797;&#26434;&#21644;&#23567;&#35268;&#27169;&#23545;&#35937;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#36741;&#21161;&#20998;&#21106;&#27169;&#22411;HAISTA-NET&#65292;&#25193;&#20805;&#20102;&#29616;&#26377;&#30340;Strong Mask R-CNN&#32593;&#32476;&#65292;&#20197;&#21253;&#25324;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25163;&#32472;&#37096;&#20998;&#29289;&#20307;&#36793;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#22320;&#22270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#32771;&#34385;&#21040;&#20102;&#37096;&#20998;&#27880;&#24847;&#21147;&#22320;&#22270;&#21644;&#21407;&#22987;&#25513;&#27169;&#25552;&#35758;&#65292;&#36825;&#20351;&#32593;&#32476;&#33021;&#22815;&#20851;&#27880;&#20154;&#20204;&#35748;&#20026;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#12290;&#22312;PASCAL VOC&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#22312;&#23567;&#32780;&#22797;&#26434;&#23545;&#35937;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation is a form of image detection which has a range of applications, such as object refinement, medical image analysis, and image/video editing, all of which demand a high degree of accuracy. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as human attention maps. In addition, 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2303.10139</link><description>&lt;p&gt;
Distill n' Explain&#65306;&#20351;&#29992;&#31616;&#21333;&#26367;&#20195;&#27169;&#22411;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distill n' Explain: explaining graph neural networks using simple surrogates. (arXiv:2303.10139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#25214;&#21040;&#20445;&#25345;&#39044;&#27979;&#30340;&#22270;&#23376;&#32467;&#26500;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#30001;&#20110;GNN&#30340;&#22797;&#26434;&#24615;&#65288;&#20363;&#22914;&#65292;&#23618;&#25968;&#65289;&#32780;&#23548;&#33268;&#35299;&#37322;&#30340;&#25104;&#26412;&#19978;&#21319;&#12290;&#22240;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;DnX&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#26367;&#20195;&#30340;GNN&#12290;&#28982;&#21518;&#65292;DnX&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#26469;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;FastDnX&#65292;&#36825;&#26159;DnX&#30340;&#26356;&#24555;&#29256;&#26412;&#65292;&#23427;&#21033;&#29992;&#20102;&#25105;&#20204;&#26367;&#20195;&#27169;&#22411;&#30340;&#32447;&#24615;&#20998;&#35299;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faith
&lt;/p&gt;</description></item></channel></rss>