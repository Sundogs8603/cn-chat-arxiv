<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>LATTE3D&#36890;&#36807;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#12289;&#21033;&#29992;3D&#25968;&#25454;&#24182;&#37319;&#29992;&#25674;&#36824;&#26041;&#27861;&#65292;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.15385</link><description>&lt;p&gt;
LATTE3D: &#22823;&#35268;&#27169;&#25674;&#36824;&#24335;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15385
&lt;/p&gt;
&lt;p&gt;
LATTE3D&#36890;&#36807;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#12289;&#21033;&#29992;3D&#25968;&#25454;&#24182;&#37319;&#29992;&#25674;&#36824;&#26041;&#27861;&#65292;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#23454;&#29616;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#22686;&#24378;3D&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;3D&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#32791;&#26102;&#30340;&#20248;&#21270;&#65292;&#27599;&#20010;&#25552;&#31034;&#21487;&#33021;&#38656;&#35201;&#38271;&#36798;&#19968;&#23567;&#26102;&#12290;&#20687;ATT3D&#36825;&#26679;&#30340;&#25674;&#36824;&#26041;&#27861;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#23454;&#29616;&#24555;&#36895;&#30340;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#21644;&#32441;&#29702;&#32454;&#33410;&#65292;&#24182;&#19988;&#24456;&#38590;&#25193;&#23637;&#21040;&#22823;&#22411;&#25552;&#31034;&#38598;&#65292;&#22240;&#27492;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATTE3D&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#22312;&#26174;&#33879;&#26356;&#22823;&#30340;&#25552;&#31034;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#12289;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110; 1)&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#26550;&#26500;&#21644; 2)&#21033;&#29992;3D&#25968;&#25454;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36890;&#36807;3D&#24863;&#30693;&#25193;&#25955;&#20808;&#39564;&#12289;&#24418;&#29366;&#27491;&#21017;&#21270;&#21644;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#21644;&#22797;&#26434;&#35757;&#32451;&#25552;&#31034;&#30340;&#31283;&#20581;&#24615;&#12290;LATTE3D&#25674;&#36824;&#20102;&#31070;&#32463;&#22330;&#21644;&#32441;&#29702;&#34920;&#38754;&#30340;&#29983;&#25104;&#65292;&#33021;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#20135;&#29983;&#39640;&#24230;&#35814;&#32454;&#30340;&#32441;&#29702;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15385v1 Announce Type: cross  Abstract: Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D gen
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.15362</link><description>&lt;p&gt;
CoLLEGe: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23884;&#20837;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CoLLEGe: Concept Embedding Generation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15362
&lt;/p&gt;
&lt;p&gt;
CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24555;&#36895;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#24120;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#24494;&#35843;&#36807;&#31243;&#25165;&#33021;&#23398;&#20064;&#24471;&#26356;&#31283;&#20581;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CoLLEGe&#65288;Concept Learning with Language Embedding Generation&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#21477;&#23376;&#25110;&#23450;&#20041;&#29983;&#25104;&#26032;&#27010;&#24565;&#30340;&#28789;&#27963;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20803;&#23398;&#20064;&#30446;&#26631;&#21482;&#26159;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#38543;&#21518;&#30340;&#21477;&#23376;&#20013;&#36827;&#34892;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#20351;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31215;&#26497;&#30446;&#26631;&#25512;&#26029;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#38646;&#23556;&#20987;&#31574;&#30053;&#36866;&#24212;&#65292;&#23545;&#26410;&#30693;&#20195;&#29702;&#20154;&#36827;&#34892;&#26368;&#20339;&#22242;&#38431;&#21512;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.15341</link><description>&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#30446;&#26631;&#28436;&#32462;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Collaborative AI Teaming in Unknown Environments via Active Goal Deduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15341
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#30446;&#26631;&#25512;&#26029;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#38646;&#23556;&#20987;&#31574;&#30053;&#36866;&#24212;&#65292;&#23545;&#26410;&#30693;&#20195;&#29702;&#20154;&#36827;&#34892;&#26368;&#20339;&#22242;&#38431;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#30475;&#21040;&#36234;&#26469;&#36234;&#22810;&#38656;&#35201;AI&#19982;&#20854;&#20182;&#20195;&#29702;&#20154;&#23494;&#20999;&#21512;&#20316;&#30340;&#24773;&#22659;&#65292;&#36825;&#20123;&#20195;&#29702;&#20154;&#30340;&#30446;&#26631;&#21644;&#31574;&#30053;&#21487;&#33021;&#20107;&#20808;&#26410;&#30693;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35757;&#32451;&#21327;&#20316;&#20195;&#29702;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#21644;&#24050;&#30693;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#24182;&#19988;&#26080;&#27861;&#35299;&#20915;&#19982;&#32463;&#24120;&#20855;&#26377;&#28508;&#22312;&#30446;&#26631;/&#22870;&#21169;&#30340;&#26410;&#30693;&#20195;&#29702;&#20154;&#36827;&#34892;&#22242;&#38431;&#21512;&#20316;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#26410;&#30693;&#20195;&#29702;&#20154;&#21512;&#20316;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#31215;&#26497;&#30446;&#26631;&#25512;&#26029;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#12289;&#20197;&#30446;&#26631;&#20026;&#26465;&#20214;&#30340;&#31574;&#30053;&#23454;&#29616;&#38646;&#23556;&#20987;&#31574;&#30053;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#26080;&#20559;&#22870;&#21169;&#20272;&#35745;&#23545;&#20110;&#19982;&#26410;&#30693;&#20195;&#29702;&#20154;&#36827;&#34892;&#26368;&#20339;&#22242;&#38431;&#21512;&#20316;&#26159;&#36275;&#22815;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#22312;&#37325;&#26032;&#35774;&#35745;&#30340;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#26143;&#38469;&#20105;&#38712;II&#24494;&#31649;&#29702;&#29615;&#22659;&#20013;&#19982;&#19981;&#21516;&#34892;&#20026;/&#22870;&#21169;&#30340;&#22810;&#26679;&#26410;&#30693;&#20195;&#29702;&#20154;&#36827;&#34892;&#22242;&#38431;&#21512;&#20316;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15341v1 Announce Type: new  Abstract: With the advancements of artificial intelligence (AI), we're seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand. However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards. In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable zero-shot policy adaptation. We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents. We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rew
&lt;/p&gt;</description></item><item><title>&#28508;&#22312;&#30340;&#23545;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24694;&#24847;&#28389;&#29992;&#23545;&#22269;&#23478;&#21644;&#22269;&#38469;&#23433;&#20840;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#20813;&#36153;&#21487;&#29992;&#30340;AI&#22914;&#20309;&#34987;&#32467;&#21512;&#25104;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#25511;&#21046;&#28857;&#21644;&#36827;&#19968;&#27493;&#25514;&#26045;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.15325</link><description>&lt;p&gt;
&#23545;&#21487;&#29992;&#20154;&#24037;&#26234;&#33021;&#28389;&#29992;&#30340;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Technological Perspective on Misuse of Available AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15325
&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#30340;&#23545;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24694;&#24847;&#28389;&#29992;&#23545;&#22269;&#23478;&#21644;&#22269;&#38469;&#23433;&#20840;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#20813;&#36153;&#21487;&#29992;&#30340;AI&#22914;&#20309;&#34987;&#32467;&#21512;&#25104;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#25511;&#21046;&#28857;&#21644;&#36827;&#19968;&#27493;&#25514;&#26045;&#20197;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#30340;&#23545;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24694;&#24847;&#28389;&#29992;&#23545;&#22269;&#23478;&#21644;&#22269;&#38469;&#23433;&#20840;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#23450;&#20041;&#33258;&#20027;&#31995;&#32479;&#65292;&#35299;&#37322;AI&#21457;&#23637;&#30340;&#29305;&#28857;&#65292;&#24182;&#23637;&#31034;&#24050;&#32463;&#23384;&#22312;&#24182;&#20844;&#24320;&#21487;&#29992;&#30340;AI&#25216;&#26415;&#22914;&#20309;&#34987;&#28389;&#29992;&#12290;&#20026;&#20102;&#24378;&#35843;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#28508;&#22312;&#34987;&#28389;&#29992;&#30340;AI&#30340;&#31034;&#20363;&#29992;&#20363;&#65292;&#36825;&#20123;&#29992;&#20363;&#23041;&#32961;&#21040;&#25919;&#27835;&#12289;&#25968;&#23383;&#21644;&#36523;&#20307;&#23433;&#20840;&#12290;&#36825;&#20123;&#29992;&#20363;&#21487;&#20197;&#22522;&#20110;&#29616;&#26377;&#30340;AI&#25216;&#26415;&#21644;&#23398;&#26415;&#30028;&#12289;&#31169;&#33829;&#37096;&#38376;&#21644;&#24320;&#21457;&#32773;&#31038;&#21306;&#30340;&#32452;&#20214;&#26500;&#24314;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20813;&#36153;&#21487;&#29992;&#30340;AI&#32452;&#21512;&#25104;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#12290;&#22522;&#20110;&#36825;&#20123;&#29992;&#20363;&#65292;&#25105;&#20204;&#25512;&#26029;&#25511;&#21046;&#28857;&#21644;&#36827;&#19968;&#27493;&#25514;&#26045;&#65292;&#20197;&#38450;&#27490;&#36890;&#36807;&#28389;&#29992;AI&#36896;&#25104;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20513;&#23548;&#22312;&#35752;&#35770;&#33258;&#20027;&#27494;&#22120;&#31995;&#32479;&#26102;&#32771;&#34385;&#23545;&#27665;&#29992;AI&#31995;&#32479;&#30340;&#24694;&#24847;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15325v1 Announce Type: cross  Abstract: Potential malicious misuse of civilian artificial intelligence (AI) poses serious threats to security on a national and international level. Besides defining autonomous systems from a technological viewpoint and explaining how AI development is characterized, we show how already existing and openly available AI technology could be misused. To underline this, we developed three exemplary use cases of potentially misused AI that threaten political, digital and physical security. The use cases can be built from existing AI technologies and components from academia, the private sector and the developer-community. This shows how freely available AI can be combined into autonomous weapon systems. Based on the use cases, we deduce points of control and further measures to prevent the potential threat through misused AI. Further, we promote the consideration of malicious misuse of civilian AI systems in the discussion on autonomous weapon syst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#24072;&#29983;&#26694;&#26550;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#65292;&#20805;&#20998;&#21033;&#29992;&#28857;&#32423;&#30417;&#30563;&#20248;&#21183;&#65292;&#20811;&#26381;&#20102;&#23558;&#24369;&#30417;&#30563;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15317</link><description>&lt;p&gt;
Point-DETR3D&#65306;&#21033;&#29992;&#31354;&#38388;&#28857;&#20808;&#39564;&#20449;&#24687;&#22686;&#24378;&#22270;&#20687;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#24072;&#29983;&#26694;&#26550;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#65292;&#20805;&#20998;&#21033;&#29992;&#28857;&#32423;&#30417;&#30563;&#20248;&#21183;&#65292;&#20811;&#26381;&#20102;&#23558;&#24369;&#30417;&#30563;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#31934;&#24230;&#30340;3D&#26816;&#27979;&#22120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26631;&#35760;3D&#27880;&#37322;&#65292;&#36825;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#28857;&#27880;&#37322;&#30340;&#24418;&#24335;&#65292;&#20026;3D&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#21069;&#26223;&#65292;&#19981;&#20165;&#26356;&#26131;&#33719;&#24471;&#19988;&#25104;&#26412;&#26356;&#20302;&#24265;&#65292;&#32780;&#19988;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;Point-DETR&#25913;&#32534;&#20026;&#20854;3D&#24418;&#24335;&#24182;&#19981;&#31616;&#21333;&#65292;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;1&#65289;&#26080;&#27861;&#23558;&#24378;&#22823;&#30340;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#65292;2&#65289;&#30001;&#20110;&#28608;&#20809;&#38647;&#36798;&#28857;&#30340;&#26497;&#24230;&#31232;&#30095;&#24615;&#65292;&#22312;&#36828;&#36317;&#31163;&#21306;&#22495;&#29983;&#25104;&#36136;&#37327;&#20302;&#19979;&#30340;&#20266;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#30340;&#24072;&#29983;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22312;&#21463;&#38480;&#30340;&#23454;&#20363;&#32423;&#27880;&#37322;&#39044;&#31639;&#20869;&#30340;&#28857;&#32423;&#30417;&#30563;&#12290;&#19982;P&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15317v1 Announce Type: cross  Abstract: Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from P
&lt;/p&gt;</description></item><item><title>CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT&#65306;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#29992;&#20110;3D&#26816;&#27979;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#36319;&#36394;&#21608;&#22260;&#29289;&#20307;&#23545;&#20110;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20809;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;LiDAR&#65289;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#39640;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#20294;&#20165;&#20351;&#29992;&#30456;&#26426;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#20854;&#25104;&#26412;&#25928;&#30410;&#12290;&#23613;&#31649;&#26080;&#32447;&#30005;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;RADAR&#65289;&#20256;&#24863;&#22120;&#22312;&#27773;&#36710;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#21407;&#22240;&#65292;&#23427;&#20204;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#38271;&#26399;&#34987;&#24573;&#35270;&#12290;&#20316;&#20026;&#19968;&#20010;&#26368;&#26032;&#30340;&#21457;&#23637;&#65292;&#30456;&#26426;&#19982;&#38647;&#36798;&#30340;&#32467;&#21512;&#27491;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Camera-RADAR 3D Detection and Tracking (CR3DT)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#29289;&#20307;&#36319;&#36394;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#12290;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#21482;&#26377;&#30456;&#26426;&#30340;BEVDet&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;CR3DT&#22312;&#26816;&#27979;&#21644;&#36319;&#36394;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#25972;&#21512;&#38647;&#36798;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v1 Announce Type: cross  Abstract: Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the sp
&lt;/p&gt;</description></item><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;</title><link>https://arxiv.org/abs/2403.15301</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#30784;&#36827;&#34892;&#35268;&#21010;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15301
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#35268;&#33539;&#30340;&#24773;&#26223;&#20013;&#23398;&#20064;&#33021;&#22815;&#21487;&#38752;&#27867;&#21270;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#26469;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#24471;&#20854;&#20013;&#30340;&#27599;&#19968;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#28041;&#21450;&#30456;&#21516;&#19968;&#32452;&#23376;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#32452;&#21512;&#21487;&#20197;&#34987;&#29992;&#26469;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#36890;&#36807;&#35268;&#21010;&#32452;&#21512;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28176;&#36817;&#19978;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;</title><link>https://arxiv.org/abs/2403.15297</link><description>&lt;p&gt;
&#29992;&#20110;&#29702;&#24615;&#25512;&#29702;&#30340;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sphere Neural-Networks for Rational Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#26469;&#36827;&#34892;&#29702;&#24615;&#25512;&#29702;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;SphNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#25104;&#21151;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#20854;&#31867;&#20154;&#38382;&#39064;&#22238;&#31572;&#30340;&#33021;&#21147;&#20197;&#21450;&#19981;&#26029;&#25552;&#21319;&#30340;&#25512;&#29702;&#24615;&#33021;&#37117;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#21542;&#20250;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22312;&#23450;&#24615;&#19978;&#25193;&#23637;&#20197;&#36229;&#36234;&#32479;&#35745;&#33539;&#24335;&#24182;&#23454;&#29616;&#39640;&#32423;&#35748;&#30693;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#26500;&#24314;&#22359;&#20174;&#21521;&#37327;&#25512;&#24191;&#21040;&#29699;&#20307;&#30340;&#26041;&#24335;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31616;&#30340;&#23450;&#24615;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29699;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;SphNNs&#65289;&#29992;&#20110;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#21644;&#26816;&#26597;&#36827;&#34892;&#31867;&#20154;&#25512;&#29702;&#65292;&#24182;&#20026;&#19977;&#27573;&#35770;&#25512;&#29702;&#24320;&#21457;&#20102;SphNN&#65292;&#36825;&#26159;&#20154;&#31867;&#29702;&#24615;&#30340;&#32553;&#24433;&#12290;SphNN&#19981;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#37051;&#22495;&#31354;&#38388;&#20851;&#31995;&#30340;&#31070;&#32463;&#31526;&#21495;&#36716;&#25442;&#26144;&#23556;&#26469;&#25351;&#23548;&#20174;&#24403;&#21069;&#29699;&#24418;&#37197;&#32622;&#21521;&#30446;&#26631;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15297v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model th
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#24212;&#29992;ChatGPT&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#20854;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15274</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#65306;&#31532;&#19968;&#24180;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#24212;&#29992;ChatGPT&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#20854;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#26631;&#24535;&#30528;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#25506;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#28085;&#30422;&#32452;&#23398;&#12289;&#36951;&#20256;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#12289;&#33647;&#29289;&#21457;&#29616;&#12289;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29702;&#35299;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#32534;&#31243;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#25945;&#32946;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25551;&#32472;&#20102;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15274v1 Announce Type: cross  Abstract: The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#31038;&#20132;&#32593;&#32476;&#32423;&#32852;&#39044;&#27979;&#30340;&#20998;&#23618;&#20449;&#24687;&#22686;&#24378;&#32593;&#32476;&#65288;HIENet&#65289;&#65292;&#23558;&#22522;&#26412;&#32423;&#32852;&#24207;&#21015;&#12289;&#29992;&#25143;&#31038;&#20132;&#22270;&#21644;&#23376;&#32423;&#32852;&#22270;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#22320;&#25366;&#25496;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#23618;&#32423;&#35821;&#20041;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.15257</link><description>&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#32593;&#32476;&#32423;&#32852;&#39044;&#27979;&#30340;&#20998;&#23618;&#20449;&#24687;&#22686;&#24378;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#31038;&#20132;&#32593;&#32476;&#32423;&#32852;&#39044;&#27979;&#30340;&#20998;&#23618;&#20449;&#24687;&#22686;&#24378;&#32593;&#32476;&#65288;HIENet&#65289;&#65292;&#23558;&#22522;&#26412;&#32423;&#32852;&#24207;&#21015;&#12289;&#29992;&#25143;&#31038;&#20132;&#22270;&#21644;&#23376;&#32423;&#32852;&#22270;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#22320;&#25366;&#25496;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#23618;&#32423;&#35821;&#20041;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#29702;&#35299;&#20449;&#24687;&#32423;&#32852;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#20449;&#24687;&#22686;&#24378;&#32593;&#32476;&#65288;HIENet&#65289;&#29992;&#20110;&#32423;&#32852;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22522;&#26412;&#32423;&#32852;&#24207;&#21015;&#12289;&#29992;&#25143;&#31038;&#20132;&#22270;&#21644;&#23376;&#32423;&#32852;&#22270;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HIENet&#21033;&#29992;DeepWalk&#23558;&#32423;&#32852;&#20449;&#24687;&#37319;&#26679;&#20026;&#19968;&#31995;&#21015;&#24207;&#21015;&#65292;&#28982;&#21518;&#25910;&#38598;&#29992;&#25143;&#20043;&#38388;&#30340;&#36335;&#24452;&#20449;&#24687;&#20197;&#25552;&#21462;&#20256;&#25773;&#32773;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26102;&#38388;&#26631;&#35760;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#32858;&#21512;&#23376;&#32423;&#32852;&#22270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15257v1 Announce Type: cross  Abstract: Understanding information cascades in networks is a fundamental issue in numerous applications. Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation. However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance. In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction. Our approach integrates fundamental cascade sequence, user social graphs, and sub-cascade graph into a unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences. It then gathers path information between users to extract the social relationships of propagators. Additionally, we employ a time-stamped graph convolutional network to aggregate sub-cascade graph information effectively. Ultimately, 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20855;&#26377;&#26465;&#20214;&#25928;&#26524;&#30340;&#38750;&#24179;&#20961;&#23433;&#20840;&#34892;&#21160;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAM Learning of Conditional Effects (Conditional-SAM)&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15251</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#25928;&#26524;&#30340;PDDL&#39046;&#22495;&#30340;&#23433;&#20840;&#23398;&#20064;--&#25193;&#23637;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
Safe Learning of PDDL Domains with Conditional Effects -- Extended Version
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15251
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#26465;&#20214;&#25928;&#26524;&#30340;&#38750;&#24179;&#20961;&#23433;&#20840;&#34892;&#21160;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAM Learning of Conditional Effects (Conditional-SAM)&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#39046;&#22495;&#29420;&#31435;&#35268;&#21010;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#36825;&#20123;&#35268;&#21010;&#22120;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#34892;&#21160;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#26576;&#31181;&#35268;&#21010;&#39046;&#22495;&#25551;&#36848;&#35821;&#35328;&#32473;&#20986;&#12290;&#25163;&#21160;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#34892;&#21160;&#27169;&#22411;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#20174;&#35266;&#27979;&#20013;&#33258;&#21160;&#23398;&#20064;&#34892;&#21160;&#27169;&#22411;&#12290;&#22914;&#26524;&#20351;&#29992;&#36825;&#26679;&#30340;&#34892;&#21160;&#27169;&#22411;&#21019;&#24314;&#30340;&#27599;&#20010;&#35745;&#21010;&#37117;&#19982;&#30495;&#23454;&#26410;&#30693;&#30340;&#34892;&#21160;&#27169;&#22411;&#20445;&#25345;&#19968;&#33268;&#65292;&#21017;&#36825;&#26679;&#30340;&#34892;&#21160;&#27169;&#22411;&#34987;&#31216;&#20026;&#23433;&#20840;&#30340;&#12290;&#23384;&#22312;&#29992;&#20110;&#23398;&#20064;&#36825;&#31181;&#23433;&#20840;&#34892;&#21160;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#20855;&#26377;&#26465;&#20214;&#25110;&#36890;&#29992;&#25928;&#26524;&#30340;&#39046;&#22495;&#65292;&#32780;&#36825;&#20123;&#26159;&#35768;&#22810;&#35268;&#21010;&#38382;&#39064;&#20013;&#24120;&#35265;&#30340;&#26500;&#36896;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#20855;&#26377;&#26465;&#20214;&#25928;&#26524;&#30340;&#38750;&#24179;&#20961;&#23433;&#20840;&#34892;&#21160;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21512;&#29702;&#30340;&#20551;&#35774;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#36825;&#26679;&#30340;&#23398;&#20064;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#23398;&#20064;&#26465;&#20214;&#25928;&#26524;&#30340;SAM Learning&#65288;Conditional-SAM&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;al
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15251v1 Announce Type: new  Abstract: Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent's actions, given in some planning domain description language. Manually designing such an action model is a notoriously challenging task. An alternative is to automatically learn action models from observation. Such an action model is called safe if every plan created with it is consistent with the real, unknown action model. Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems. We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples. Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first al
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.15250</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#32467;&#26524;&#22312;LLM&#20013;&#30340;&#20840;&#38754;&#37325;&#26032;&#35780;&#20272;&#65306;&#19968;&#31181;&#22810;&#26041;&#20301;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15250
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#35780;&#20272;&#22312;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#21069;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#32553;&#25918;&#12289;&#35757;&#32451;&#31867;&#22411;&#12289;&#26550;&#26500;&#31561;&#22240;&#32032;&#28145;&#21051;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#35780;&#20998;&#30340;&#24433;&#21709;&#31243;&#24230;&#21644;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35780;&#20272;&#23616;&#38480;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#32479;&#35745;&#35270;&#35282;&#26356;&#26377;&#25928;&#22320;&#28548;&#28165;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#24471;&#20998;&#30340;&#24433;&#21709;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#37325;&#26032;&#26816;&#26597;&#65292;&#38024;&#23545;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#32479;&#35745;&#26041;&#27861;&#35770;&#12290;&#20854;&#20013;&#21253;&#25324;ANOVA&#12289;Tukey HSD&#26816;&#39564;&#12289;GAMM&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15249</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#36816;&#21160;&#36716;&#31227;&#30340;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Spectral Motion Alignment for Video Motion Transfer using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#35270;&#39057;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65288;VDMs&#65289;&#26174;&#33879;&#20419;&#36827;&#20102;&#23558;&#36755;&#20837;&#35270;&#39057;&#23450;&#21046;&#20026;&#30446;&#26631;&#22806;&#35266;&#12289;&#36816;&#21160;&#31561;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#25552;&#21462;&#35270;&#39057;&#24103;&#30340;&#36816;&#21160;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#36830;&#32493;&#24103;&#27531;&#24046;&#20316;&#20026;&#30446;&#26631;&#36816;&#21160;&#21521;&#37327;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#32570;&#20047;&#20840;&#23616;&#36816;&#21160;&#32972;&#26223;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#36880;&#24103;&#22833;&#30495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;SMA&#36890;&#36807;&#25972;&#21512;&#39057;&#22495;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20419;&#36827;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;SMA&#22312;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15249v1 Announce Type: cross  Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while main
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;SimCLR&#23545;&#20892;&#19994;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20892;&#19994;&#35270;&#35273;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.15248</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39592;&#24178;&#26694;&#26550;&#29992;&#20110;&#22810;&#26679;&#21270;&#20892;&#19994;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#30740;&#31350;&#21033;&#29992;SimCLR&#23545;&#20892;&#19994;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#20892;&#19994;&#35270;&#35273;&#20219;&#21153;&#65292;&#28040;&#38500;&#20102;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#20013;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20855;&#26377;&#25913;&#21464;&#28216;&#25103;&#35268;&#21017;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#20892;&#19994;&#36716;&#21464;&#20026;&#25968;&#25454;&#39537;&#21160;&#12289;&#31934;&#30830;&#21644;&#21487;&#25345;&#32493;&#30340;&#34892;&#19994;&#12290;&#28145;&#24230;&#23398;&#20064;&#36171;&#20104;&#20102;&#20892;&#19994;&#35270;&#35273;&#20998;&#26512;&#22823;&#37327;&#22797;&#26434;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20294;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#29942;&#39048;&#65292;&#22240;&#20026;&#25163;&#21160;&#26631;&#27880;&#23481;&#26131;&#20986;&#38169;&#12289;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#32570;&#20047;&#26377;&#25928;&#30340;&#26631;&#27880;&#26041;&#27861;&#28608;&#21457;&#20102;&#25105;&#20204;&#32771;&#34385;&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#20174;&#21407;&#22987;&#20892;&#19994;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#22914;&#20309;&#36890;&#36807;&#28040;&#38500;&#23545;&#22823;&#35268;&#27169;&#24102;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#35299;&#38145;&#20102;&#23545;&#22810;&#26679;&#21270;&#20892;&#19994;&#35270;&#35273;&#20219;&#21153;&#30340;&#28508;&#22312;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;SimCLR&#65292;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#22823;&#37327;&#26410;&#26631;&#27880;&#30340;&#30495;&#23454;&#19990;&#30028;&#20892;&#19994;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;ResNet-50&#39592;&#24178;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15248v1 Announce Type: cross  Abstract: Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider self-supervised learning as a paradigm shift, learning meaningful feature representations from raw agricultural image data. In this work, we explore how self-supervised representation learning unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a contrastive learning approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agricult
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15245</link><description>&lt;p&gt;
&#35270;&#39057;&#30340;&#22686;&#24378;&#25512;&#29702;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasoning-Enhanced Object-Centric Learning for Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15245
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#29289;&#20307;&#34920;&#31034;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#35270;&#39057;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#26377;&#25928;&#25512;&#29702;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;STATM&#65289;&#30340;&#26032;&#22411;&#25512;&#29702;&#27169;&#22359;&#12290;&#35760;&#24518;&#32531;&#20914;&#21306;&#20027;&#35201;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#19978;&#28216;&#27169;&#22359;&#30340;&#27133;&#20301;&#20449;&#24687;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#36890;&#36807;&#27133;&#20301;&#20026;&#22522;&#30784;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#35760;&#24518;&#22686;&#24378;&#32593;&#32476;&#65288;MMEN&#65289;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#65292;&#36890;&#36807;&#22810;&#35282;&#24230;&#25366;&#25496;&#20851;&#38190;&#33410;&#28857;&#24182;&#21033;&#29992;&#35760;&#24518;&#32593;&#32476;&#23384;&#20648;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26410;&#30693;&#24773;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15235</link><description>&lt;p&gt;
&#22810;&#35282;&#24230;&#35760;&#24518;&#22686;&#24378;&#32593;&#32476;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#35760;&#24518;&#22686;&#24378;&#32593;&#32476;&#65288;MMEN&#65289;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#65292;&#36890;&#36807;&#22810;&#35282;&#24230;&#25366;&#25496;&#20851;&#38190;&#33410;&#28857;&#24182;&#21033;&#29992;&#35760;&#24518;&#32593;&#32476;&#23384;&#20648;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26410;&#30693;&#24773;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#22312;&#21450;&#26102;&#38459;&#27490;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#20851;&#38190;&#33410;&#28857;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#20165;&#20174;&#20256;&#25773;&#32467;&#26500;&#30340;&#35282;&#24230;&#32771;&#34385;&#33410;&#28857;&#24433;&#21709;&#65292;&#24182;&#19988;&#23545;&#26410;&#30693;&#24773;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35282;&#24230;&#35760;&#24518;&#22686;&#24378;&#32593;&#32476;&#65288;MMEN&#65289;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#65292;&#35813;&#32593;&#32476;&#20174;&#22810;&#20010;&#35282;&#24230;&#25366;&#25496;&#20851;&#38190;&#33410;&#28857;&#24182;&#21033;&#29992;&#35760;&#24518;&#32593;&#32476;&#23384;&#20648;&#21382;&#21490;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MMEN&#39318;&#20808;&#20174;&#29992;&#25143;&#23646;&#24615;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#35282;&#24230;&#26500;&#24314;&#20004;&#20010;&#20256;&#25773;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#35760;&#24518;&#32593;&#32476;&#29992;&#20110;&#23384;&#20648;&#31867;&#20284;&#23376;&#22270;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;&#26410;&#30693;&#24773;&#22659;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;MMEN&#24212;&#29992;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#32467;&#21512;&#33410;&#28857;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15235v1 Announce Type: cross  Abstract: Identifying key nodes in social networks plays a crucial role in timely blocking false information. Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios. In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information. Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using graph attention networks. Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model's generalization performance in unknown scenarios. Finally, MMEN applies adaptive weights to combine the node influ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#20174;&#38750;&#19987;&#23478;&#22788;&#31574;&#21010;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.15218</link><description>&lt;p&gt;
&#26080;&#35770;&#20309;&#26102;&#12289;&#20309;&#22320;&#12289;&#35841;&#20154;&#65306;&#30740;&#31350;&#29992;&#20110;&#20247;&#21253;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#20174;&#38750;&#19987;&#23478;&#22788;&#31574;&#21010;&#21307;&#23398;&#22270;&#20687;&#26631;&#27880;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27880;&#37322;&#30340;&#31574;&#21010;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23548;&#33268;"&#29421;&#31364;"&#19987;&#27880;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20855;&#26377;&#26377;&#38480;&#30340;&#36716;&#21270;&#25928;&#29992;&#12290;&#26368;&#36817;&#65292;&#20687;&#37096;&#20998;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#20041;&#20998;&#21106;&#65292;&#36328;&#21508;&#20010;&#39046;&#22495;&#21253;&#25324;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#31616;&#21270;&#27880;&#37322;&#36807;&#31243;&#26377;&#24456;&#22823;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;SAM&#23578;&#26410;&#22312;&#20247;&#21253;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#31574;&#21010;&#27880;&#37322;&#26469;&#35757;&#32451;3D DL&#20998;&#21106;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SAM&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20154;&#21592;&#20013;&#20247;&#21253;"&#31232;&#30095;"&#27880;&#37322;&#65292;&#20135;&#29983;&#29992;&#20110;&#35757;&#32451;3D nnU-Net&#27169;&#22411;&#65288;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;DL&#20998;&#21106;&#27169;&#22411;&#65289;&#30340;"&#23494;&#38598;"&#20998;&#21106;&#25513;&#27169;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22320;&#38754;&#30495;&#23454;&#24230;&#30456;&#27604;&#65292;SAM&#29983;&#25104;&#30340;&#27880;&#37322;&#23637;&#29616;&#20986;&#39640;&#24179;&#22343;Dice&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15218v1 Announce Type: cross  Abstract: Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in "narrowly" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing "sparse" annotations from non-experts to generate "dense" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#35774;&#35745;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#30740;&#31350;&#20102;AI&#20107;&#29289;&#35774;&#35745;&#20013;&#39764;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19971;&#22823;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#21407;&#21017;&#23545;&#39749;&#21147;&#21644;&#21916;&#24551;&#26080;&#24120;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.15216</link><description>&lt;p&gt;
AI&#39764;&#26415;&#30340;&#21046;&#36896;&#19982;&#35299;&#26500;&#65306;&#19968;&#20010;&#35774;&#35745;&#20998;&#31867;&#23398;
&lt;/p&gt;
&lt;p&gt;
(Un)making AI Magic: a Design Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#35774;&#35745;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#30740;&#31350;&#20102;AI&#20107;&#29289;&#35774;&#35745;&#20013;&#39764;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19971;&#22823;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#21407;&#21017;&#23545;&#39749;&#21147;&#21644;&#21916;&#24551;&#26080;&#24120;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35774;&#35745;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#30740;&#31350;&#20102;&#39764;&#26415;&#22312;AI&#20107;&#29289;&#35774;&#35745;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#35774;&#35745;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#25110;&#20943;&#23569;&#39764;&#27861;&#21644;&#39749;&#21147;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#20174;&#22260;&#32469;AI&#25216;&#26415;&#36817;&#26399;&#21457;&#23637;&#30340;&#35774;&#35745;&#35805;&#35821;&#24320;&#22987;&#65292;&#24378;&#35843;&#29305;&#23450;&#30340;&#20132;&#20114;&#29305;&#24615;&#65292;&#27604;&#22914;&#31639;&#27861;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#65292;&#24182;&#38416;&#26126;&#19982;&#39764;&#27861;&#21644;&#36229;&#33258;&#28982;&#24605;&#32500;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#21453;&#24605;&#26469;&#33258;&#20004;&#23626;&#35774;&#35745;&#19982;AI&#30805;&#22763;&#35838;&#31243;&#20013;52&#21517;&#23398;&#29983;&#30340;&#35774;&#35745;&#39033;&#30446;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19971;&#22823;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#35299;&#24320;&#20102;&#27599;&#20010;&#21407;&#21017;&#22312;&#39749;&#21147;&#21644;&#21916;&#24551;&#26080;&#24120;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#35774;&#35745;/&#20154;&#26426;&#20132;&#20114;&#20174;&#19994;&#32773;&#21487;&#20197;&#22914;&#20309;&#25509;&#32435;&#21644;&#21033;&#29992;&#36825;&#31181;&#20998;&#31867;&#23398;&#65292;&#29305;&#21035;&#26159;&#20197;&#25903;&#25345;&#25506;&#32034;&#19982;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15216v1 Announce Type: cross  Abstract: This paper examines the role that enchantment plays in the design of AI things by constructing a taxonomy of design approaches that increase or decrease the perception of magic and enchantment. We start from the design discourse surrounding recent developments in AI technologies, highlighting specific interaction qualities such as algorithmic uncertainties and errors and articulating relations to the rhetoric of magic and supernatural thinking. Through analyzing and reflecting upon 52 students' design projects from two editions of a Master course in design and AI, we identify seven design principles and unpack the effects of each in terms of enchantment and disenchantment. We conclude by articulating ways in which this taxonomy can be approached and appropriated by design/HCI practitioners, especially to support exploration and reflexivity.
&lt;/p&gt;</description></item><item><title>FSD-Inference&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#20840;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15195</link><description>&lt;p&gt;
FSD-Inference: &#20855;&#26377;&#21487;&#25193;&#23637;&#20113;&#36890;&#20449;&#30340;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#20998;&#24067;&#24335;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15195
&lt;/p&gt;
&lt;p&gt;
FSD-Inference&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#20840;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15195v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25277;&#35937;: &#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#25552;&#20379;&#20102;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#21487;&#20280;&#32553;&#24615;&#12289;&#24377;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#23545;&#20869;&#23384;&#12289;CPU&#21644;&#20989;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#20854;&#22312;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#8220;&#20840;&#26381;&#21153;&#22120;&#8221;&#24179;&#21488;&#36890;&#36807;&#24555;&#36895;&#32593;&#32476;&#21644;&#24050;&#24314;&#31435;&#33391;&#22909;&#30340;&#36827;&#31243;&#38388;&#36890;&#20449;&#65288;IPC&#65289;&#26426;&#21046;&#65288;&#22914;MPI&#21644;&#20849;&#20139;&#20869;&#23384;&#65289;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#22312;&#26080;&#26381;&#21153;&#22120;&#39046;&#22495;&#32570;&#20047;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#37325;&#35201;IPC&#35201;&#27714;&#30340;&#24182;&#34892;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;FSD-Inference&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#26381;&#21153;&#22120;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;ML&#25512;&#26029;&#31995;&#32479;&#12290;&#25105;&#20204;&#25506;&#35752;&#28508;&#22312;&#30340;&#36890;&#20449;&#28192;&#36947;&#65292;&#19982;&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#20026;&#26080;&#26381;&#21153;&#22120;&#25968;&#25454;&#23494;&#38598;&#22411;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;ML&#35774;&#35745;&#20102;&#19968;&#27969;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;ML&#25512;&#26029;&#30340;&#20840;&#26032;&#26080;&#26381;&#21153;&#22120;&#36890;&#20449;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15195v1 Announce Type: cross  Abstract: Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML infe
&lt;/p&gt;</description></item><item><title>SFOD&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#33033;&#20914;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39318;&#27425;&#22312;&#20107;&#20214;&#30456;&#26426;&#20013;&#23545;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.15192</link><description>&lt;p&gt;
SFOD&#65306;&#33033;&#20914;&#34701;&#21512;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SFOD: Spiking Fusion Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15192
&lt;/p&gt;
&lt;p&gt;
SFOD&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#33033;&#20914;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39318;&#27425;&#22312;&#20107;&#20214;&#30456;&#26426;&#20013;&#23545;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#39640;&#21160;&#24577;&#33539;&#22260;&#12289;&#20302;&#21151;&#32791;&#21644;&#39640;&#20687;&#32032;&#24102;&#23485;&#20026;&#29305;&#24449;&#65292;&#20026;&#29305;&#23450;&#32972;&#26223;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20107;&#20214;&#25968;&#25454;&#22266;&#26377;&#30340;&#31232;&#30095;&#24615;&#21644;&#24322;&#27493;&#24615;&#32473;&#29616;&#26377;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#24102;&#26469;&#25361;&#25112;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21463;&#20154;&#33041;&#32534;&#30721;&#21644;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#21551;&#21457;&#65292;&#20026;&#36825;&#20123;&#22256;&#38590;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#20107;&#20214;&#30456;&#26426;&#20351;&#29992;SNN&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33033;&#20914;&#34701;&#21512;&#30446;&#26631;&#26816;&#27979;&#22120;&#65288;SFOD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#22522;&#20110;SNN&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33033;&#20914;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39318;&#27425;&#22312;&#24212;&#29992;&#20110;&#20107;&#20214;&#30456;&#26426;&#30340;SNN&#20013;&#20174;&#19981;&#21516;&#23610;&#24230;&#34701;&#21512;&#29305;&#24449;&#22270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25972;&#21512;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15192v1 Announce Type: cross  Abstract: Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for object detection in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in object detection using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to SNN-based object detection. Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.15176</link><description>&lt;p&gt;
&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#25913;&#21892;&#20102;&#31070;&#32463;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Brain-grounding of semantic vectors improves neural decoding of visual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20934;&#30830;&#20840;&#38754;&#30340;&#31639;&#27861;&#26469;&#35299;&#30721;&#22823;&#33041;&#20869;&#23481;&#26159;&#31070;&#32463;&#31185;&#23398;&#21644;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#22823;&#33041;&#27963;&#21160;&#27169;&#24335;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#20041;&#21521;&#37327;&#34920;&#31034;&#30340;&#31070;&#32463;&#35299;&#30721;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#20041;&#21521;&#37327;&#30340;&#33041;&#25509;&#22320;&#65292;&#23427;&#23545;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15176v1 Announce Type: cross  Abstract: Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26816;&#27979;&#20027;&#35201;&#25233;&#37057;&#30151;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26102;&#65292;&#22312;&#20132;&#20114;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#19978;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.15170</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#24515;&#29702;&#30142;&#30149;&#30340;&#24773;&#22659;&#20013;&#25506;&#32034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26816;&#27979;&#20027;&#35201;&#25233;&#37057;&#30151;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26102;&#65292;&#22312;&#20132;&#20114;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#19978;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23578;&#26410;&#26377;&#20154;&#25506;&#32034;&#29992;&#20110;&#26816;&#27979;&#22810;&#31181;&#24515;&#29702;&#38556;&#30861;&#30340;&#36825;&#31181;&#26041;&#27861;&#12290;&#23384;&#22312;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#30340;&#29702;&#30001;&#22312;&#20110;&#22810;&#31181;&#24515;&#29702;&#38556;&#30861;&#20043;&#38388;&#30340;&#30151;&#29366;&#37325;&#21472;&#12290;&#22240;&#27492;&#65292;&#25910;&#38598;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#30340;&#34892;&#20026;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#19982;&#22810;&#31181;&#38556;&#30861;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;SSL&#25512;&#23548;&#20986;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#65292;&#29992;&#20110;&#20351;&#29992;&#22312;&#20114;&#21160;&#20250;&#35805;&#26399;&#38388;&#25910;&#38598;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#26816;&#27979;&#37325;&#24230;&#25233;&#37057;&#38556;&#30861;&#65288;MDD&#65289;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#24773;&#22659;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#36890;&#36807;&#39044;&#27979;&#22810;&#20010;&#22266;&#23450;&#30446;&#26631;&#25110;&#25513;&#33180;&#24103;&#35757;&#32451;&#30340;SSL&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22266;&#23450;&#30446;&#26631;&#65292;&#20197;&#20351;&#29983;&#25104;&#30340;&#34920;&#31034;&#23545;MDD&#30340;&#26816;&#27979;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15170v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD 
&lt;/p&gt;</description></item><item><title>&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#20851;&#38190;&#22312;&#20110;&#36716;&#25442;&#22270;&#30340;&#23646;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#29702;&#24819;&#30340;&#36716;&#25442;&#22270;&#32467;&#26500;&#26159;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#12290;</title><link>https://arxiv.org/abs/2403.15167</link><description>&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#36716;&#25442;&#22270;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transition Graph Properties of Target Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15167
&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#30340;&#20851;&#38190;&#22312;&#20110;&#36716;&#25442;&#22270;&#30340;&#23646;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#29702;&#24819;&#30340;&#36716;&#25442;&#22270;&#32467;&#26500;&#26159;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#31867;&#20998;&#31867;&#26159;&#19968;&#20010;&#28151;&#21512;&#20998;&#31867;&#21644;&#36716;&#25442;&#27169;&#22411;&#65292;&#20854;&#32508;&#21512;&#30446;&#26631;&#26159;&#23558;&#23545;&#35937;&#20998;&#37197;&#21040;&#26576;&#20010;&#25152;&#35859;&#30340;&#30446;&#26631;&#25110;&#27491;&#24120;&#31867;&#12290;&#20998;&#31867;&#36807;&#31243;&#26159;&#36845;&#20195;&#30340;&#65292;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#26576;&#20010;&#31867;&#20013;&#30340;&#23545;&#35937;&#32463;&#21382;&#19982;&#35813;&#31867;&#30456;&#23545;&#24212;&#30340;&#21160;&#20316;&#65292;&#24341;&#21457;&#23545;&#35937;&#21521;&#20854;&#20013;&#19968;&#20010;&#31867;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#31867;&#36716;&#25442;&#30340;&#36716;&#25442;&#24207;&#21015;&#24517;&#39035;&#35774;&#35745;&#24471;&#33021;&#20026;&#23545;&#35937;&#26368;&#32456;&#20998;&#37197;&#21040;&#30446;&#26631;&#31867;&#25552;&#20379;&#25903;&#25345;&#12290;&#36716;&#25442;&#36807;&#31243;&#21487;&#20197;&#29992;&#26377;&#21521;&#22270;&#30340;&#24418;&#24335;&#25551;&#36848;&#65292;&#26368;&#32456;&#20998;&#31867;&#30340;&#25104;&#21151;&#20027;&#35201;&#21462;&#20915;&#20110;&#36825;&#20010;&#22270;&#30340;&#23646;&#24615;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#36716;&#25442;&#22270;&#30340;&#29702;&#24819;&#32467;&#26500;&#26159;&#19968;&#20010;&#26397;&#26681;&#39030;&#28857;&#26041;&#21521;&#21462;&#21521;&#30340;&#26377;&#26681;&#26641;&#65292;&#35813;&#39030;&#28857;&#23545;&#24212;&#20110;&#27491;&#24120;&#31867;&#12290;&#24456;&#26126;&#26174;&#65292;&#20219;&#24847;&#31639;&#27861;&#65288;&#31574;&#30053;&#65289;&#30340;&#36716;&#25442;&#22270;&#21487;&#33021;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15167v1 Announce Type: cross  Abstract: Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed graph, and the success of the final classification is mainly due to the properties of this graph. In our previous research we showed that the desirable structure of the transition graph is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition graph of an arbitrary algorithm (policy) may not have 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MedDeepCyleAL&#65292;&#19968;&#20010;&#23454;&#29616;&#23436;&#25972;&#20027;&#21160;&#23398;&#20064;&#21608;&#26399;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#28789;&#27963;&#36873;&#25321;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15143</link><description>&lt;p&gt;
&#22270;&#20687;&#26631;&#27880;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65306;&#30524;&#31185;-AI&#39033;&#30446;&#30340;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MedDeepCyleAL&#65292;&#19968;&#20010;&#23454;&#29616;&#23436;&#25972;&#20027;&#21160;&#23398;&#20064;&#21608;&#26399;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#28789;&#27963;&#36873;&#25321;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26631;&#27880;&#26159;&#21307;&#23398;&#25104;&#20687;&#21644;&#30142;&#30149;&#35786;&#26029;&#39046;&#22495;&#20013;&#30830;&#20445;&#24739;&#32773;&#33719;&#24471;&#36866;&#24403;&#27835;&#30103;&#24182;&#36861;&#36394;&#27835;&#30103;&#36807;&#31243;&#20013;&#36827;&#23637;&#30340;&#26368;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#26631;&#27880;&#22823;&#37327;&#30340;2D&#21644;3D&#25104;&#20687;&#25968;&#25454;&#21487;&#33021;&#26497;&#20026;&#32321;&#29712;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#21106;&#31639;&#27861;&#23436;&#20840;&#25913;&#21464;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20351;&#24471;&#33258;&#21160;&#21270;&#22270;&#20687;&#20998;&#21106;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#20934;&#30830;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#38656;&#35201;&#25163;&#21160;&#26631;&#27880;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#34701;&#21512;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#20998;&#21106;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#26631;&#27880;&#25968;&#25454;&#22522;&#30784;&#19978;&#21457;&#25381;&#26356;&#39640;&#30340;&#25928;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MedDeepCycleAL&#65292;&#19968;&#20010;&#23454;&#29616;&#23436;&#25972;AL&#24490;&#29615;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#23427;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36873;&#25321;&#20182;&#20204;&#24076;&#26395;&#37319;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#21253;&#25324;&#20102;&#19968;&#20010;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15143v1 Announce Type: cross  Abstract: Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating Active Learning (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annot
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65292;&#37319;&#29992;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;&#24320;&#25918;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15137</link><description>&lt;p&gt;
CACA Agent&#65306;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;AI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CACA Agent: Capability Collaboration based AI Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65292;&#37319;&#29992;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;&#24320;&#25918;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#20195;&#29702;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#20309;&#24555;&#36895;&#37096;&#32626;AI&#20195;&#29702;&#20197;&#21450;&#22914;&#20309;&#26041;&#20415;&#22320;&#25193;&#23637;AI&#20195;&#29702;&#30340;&#24212;&#29992;&#22330;&#26223;&#24050;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65288;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;AI&#20195;&#29702;&#65289;&#65292;&#37319;&#29992;&#20102;&#21463;&#26381;&#21153;&#35745;&#31639;&#21551;&#21457;&#30340;&#24320;&#25918;&#26550;&#26500;&#12290;CACA&#20195;&#29702;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#19981;&#20165;&#20943;&#23569;&#20102;&#23545;&#21333;&#20010;LLM&#30340;&#20381;&#36182;&#65292;&#36824;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#29992;&#24037;&#20855;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28436;&#31034;&#26469;&#35828;&#26126;&#25805;&#20316;&#21644;&#24212;&#29992;&#22330;&#26223;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15137v1 Announce Type: new  Abstract: As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario exten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15115</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20154;&#26426;&#20132;&#20114;&#30340;&#20250;&#35805;&#26368;&#22823;&#21270;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Language Models in Dialogue: Conversational Maxims for Human-AI Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22797;&#26434;&#65292;&#20294;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#35748;&#20026;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#32570;&#38519;&#21487;&#20197;&#24402;&#22240;&#20110;&#36829;&#21453;&#19968;&#20010;&#25110;&#22810;&#20010;&#23545;&#35805;&#21407;&#21017;&#12290;&#36890;&#36807;&#20511;&#37492;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017; - &#21253;&#25324;&#25968;&#37327;&#12289;&#36136;&#37327;&#12289;&#30456;&#20851;&#24615;&#12289;&#26041;&#24335;&#12289;&#20161;&#24904;&#20197;&#21450;&#36879;&#26126;&#24230; - &#26469;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#20154;&#26426;&#20114;&#21160;&#32972;&#26223;&#19979; Grice &#30340;&#21069;&#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20004;&#20010;&#26032;&#30340;&#20934;&#21017;&#65292;&#20161;&#24904;&#65288;&#28041;&#21450;&#29983;&#25104;&#21644;&#21442;&#19982;&#26377;&#23475;&#20869;&#23481;&#65289;&#21644;&#36879;&#26126;&#24230;&#65288;&#28041;&#21450;&#35782;&#21035;&#33258;&#24049;&#30340;&#30693;&#35782;&#36793;&#30028;&#12289;&#25805;&#20316;&#32422;&#26463;&#21644;&#24847;&#22270;&#65289;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#29420;&#29305;&#34892;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#25552;&#20986;&#30340;&#20934;&#21017;&#20026;&#22914;&#20309;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15115v1 Announce Type: cross  Abstract: Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.15114</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120; Q4RPD&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#21253;&#35065;&#25237;&#36882;&#36335;&#24452;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#38382;&#39064;&#31616;&#21270;&#21644;&#25216;&#26415;&#25463;&#24452;&#65292;&#38024;&#23545;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30340;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#20851;&#20110;&#37327;&#23376;&#35745;&#31639;&#21644;&#36335;&#24452;&#38382;&#39064;&#20043;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#22810;&#20135;&#12290;&#22823;&#37096;&#20998;&#20316;&#21697;&#22260;&#32469;&#30528;&#32463;&#20856;&#38382;&#39064;&#65292;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#25110;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#23613;&#31649;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#19981;&#21487;&#21542;&#35748;&#30340;&#26159;&#23427;&#20204;&#20197;&#23398;&#26415;&#20026;&#23548;&#21521;&#30340;&#29305;&#28857;&#26080;&#27861;&#28385;&#36275;&#29616;&#23454;&#19990;&#30028;&#30340;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#38382;&#39064;&#31616;&#21270;&#25110;&#25216;&#26415;&#25463;&#24452;&#12290;&#30456;&#21453;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27714;&#35299;&#22120;&#65292;&#21629;&#21517;&#20026;Q4RPD&#65292;&#32771;&#34385;&#21040;&#19968;&#31995;&#21015;&#30495;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#24322;&#26500;&#36710;&#38431;&#12289;&#20248;&#20808;&#25237;&#36882;&#21644;&#26681;&#25454;&#21253;&#35065;&#37325;&#37327;&#21644;&#23610;&#23544;&#30830;&#23450;&#30340;&#23481;&#37327;&#12290;Q4RPD&#37319;&#29992;&#20102;D-Wave&#30340;Leap&#21463;&#38480;&#20108;&#27425;&#27169;&#22411;&#28151;&#21512;&#27714;&#35299;&#22120;&#12290;&#20026;&#20102;&#35777;&#26126;Q4RPD&#30340;&#24212;&#29992;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#39564;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15114v1 Announce Type: cross  Abstract: Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation compo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23376;&#31561;&#21464;&#21407;&#29702;&#30340;CoordiGraph&#26694;&#26550;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22686;&#24378;&#21327;&#35843;&#36816;&#21160;&#25511;&#21046;&#65292;&#25913;&#21892;&#20102;&#20851;&#33410;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15100</link><description>&lt;p&gt;
&#29992;&#20110;&#21327;&#35843;&#36816;&#21160;&#25511;&#21046;&#30340;&#23376;&#31561;&#21464;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Subequivariant Reinforcement Learning Framework for Coordinated Motion Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15100
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23376;&#31561;&#21464;&#21407;&#29702;&#30340;CoordiGraph&#26694;&#26550;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22686;&#24378;&#21327;&#35843;&#36816;&#21160;&#25511;&#21046;&#65292;&#25913;&#21892;&#20102;&#20851;&#33410;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#21327;&#35843;&#23545;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36816;&#21160;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#20195;&#29702;&#20307;&#21644;&#23427;&#20204;&#30340;&#36816;&#21160;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#32771;&#34385;&#21040;&#20851;&#33410;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CoordiGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#23398;&#20013;&#30340;&#23376;&#31561;&#21464;&#21407;&#29702;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#21327;&#35843;&#36816;&#21160;&#25511;&#21046;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#23558;&#31561;&#21464;&#24615;&#21407;&#29702;&#23884;&#20837;&#21040;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20197;&#22312;&#37325;&#21147;&#24433;&#21709;&#19979;&#27169;&#25311;&#20851;&#33410;&#20043;&#38388;&#37325;&#35201;&#30340;&#24494;&#22937;&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#36816;&#21160;&#25511;&#21046;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23545;&#22797;&#26434;&#20195;&#29702;&#20307;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#30456;&#27604;&#65292;CoordiGraph&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15100v1 Announce Type: cross  Abstract: Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15097</link><description>&lt;p&gt;
&#35770;&#25454;&#24863;&#30693;&#20107;&#20214;&#38142;&#25509;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Argument-Aware Approach To Event Linking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15097
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#20107;&#20214;&#38142;&#25509;&#23558;&#25991;&#26412;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#20013;&#30456;&#20851;&#33410;&#28857;&#36830;&#25509;&#36215;&#26469;&#12290;&#20808;&#21069;&#22312;&#20107;&#20214;&#38142;&#25509;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#20511;&#37492;&#20102;&#23454;&#20307;&#38142;&#25509;&#30340;&#26041;&#27861;&#65292;&#24573;&#30053;&#20102;&#20107;&#20214;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#19982;&#24191;&#27867;&#25506;&#35752;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30456;&#27604;&#65292;&#20107;&#20214;&#20855;&#26377;&#26356;&#21152;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20854;&#20851;&#32852;&#30340;&#35770;&#25454;&#26356;&#26377;&#25928;&#22320;&#21152;&#20197;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#20107;&#20214;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#23548;&#33268;&#20107;&#20214;&#30693;&#35782;&#24211;&#30340;&#31232;&#32570;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#38656;&#35201;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#19968;&#39046;&#22495;&#21463;&#21040;&#20102;&#26377;&#38480;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#35760;&#20107;&#20214;&#35770;&#25454;&#20449;&#24687;&#26469;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26377;&#20851;&#20107;&#20214;&#25552;&#21450;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;&#65292;&#35299;&#20915;DRL&#20248;&#21270;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20943;&#23569;&#22797;&#21512;&#35823;&#24046;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.15091</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15091
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#38271;&#30701;&#26399;&#35760;&#24518;&#27745;&#27700;&#22788;&#29702;&#27169;&#25311;&#22120;&#65292;&#35299;&#20915;DRL&#20248;&#21270;&#24037;&#19994;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20943;&#23569;&#22797;&#21512;&#35823;&#24046;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#28216;&#25103;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#65292;&#20294;&#22312;&#27745;&#27700;&#22788;&#29702;&#31561;&#24037;&#19994;&#36807;&#31243;&#30340;&#20248;&#21270;&#20013;&#23454;&#26045;DRL&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#33021;&#22815;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#20195;&#34920;&#23454;&#38469;&#24037;&#21378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#35757;&#32451;DRL&#31574;&#30053;&#12290;&#27745;&#27700;&#22788;&#29702;&#25968;&#25454;&#30340;&#38543;&#26426;&#24615;&#21644;&#38750;&#32447;&#24615;&#23548;&#33268;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#20135;&#29983;&#19981;&#31283;&#23450;&#21644;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#25913;&#36827;&#30340;&#27169;&#22411;&#20013;&#21487;&#33021;&#20986;&#29616;&#27169;&#25311;&#34892;&#20026;&#19981;&#27491;&#30830;&#30340;&#19968;&#20010;&#21487;&#33021;&#21407;&#22240;&#26159;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#21363;&#22312;&#27169;&#25311;&#36807;&#31243;&#20013;&#35823;&#24046;&#30340;&#32047;&#31215;&#12290;&#22797;&#21512;&#35823;&#24046;&#26159;&#22240;&#20026;&#27169;&#22411;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20013;&#20351;&#29992;&#20854;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#38469;&#25968;&#25454;&#19982;&#39044;&#27979;&#20043;&#38388;&#30340;&#35823;&#24046;&#38543;&#30528;&#27169;&#25311;&#30340;&#36827;&#34892;&#32780;&#32047;&#31215;&#12290;&#25105;&#20204;&#23454;&#26045;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#27745;&#27700;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15091v1 Announce Type: cross  Abstract: Even though Deep Reinforcement Learning (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a simulation environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over long time horizons. One possible reason for the models' incorrect simulation behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the simulation. The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the simulation continues. We implemented two methods to improve the trained models for wastew
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#33026;&#36136;&#32452;&#23398;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#12289;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#21644;&#33026;&#36136;&#26631;&#27880;&#65292;&#20197;&#21450;&#22312;&#27668;&#27873;&#30005;&#21943;&#38654;&#30005;&#31163;-&#22810;&#21453;&#24212;&#30417;&#27979;&#26041;&#27861;&#20013;&#35782;&#21035;&#19981;&#39281;&#21644;&#33026;&#36136;&#30899;&#30899;&#21452;&#38190;&#20301;&#32622;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15076</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#33026;&#36136;&#32452;&#23398;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Lipidomic Automation Workflow using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#33026;&#36136;&#32452;&#23398;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#12289;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#21644;&#33026;&#36136;&#26631;&#27880;&#65292;&#20197;&#21450;&#22312;&#27668;&#27873;&#30005;&#21943;&#38654;&#30005;&#31163;-&#22810;&#21453;&#24212;&#30417;&#27979;&#26041;&#27861;&#20013;&#35782;&#21035;&#19981;&#39281;&#21644;&#33026;&#36136;&#30899;&#30899;&#21452;&#38190;&#20301;&#32622;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33026;&#36136;&#32452;&#23398;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#27880;&#21644;&#35299;&#37322;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#33026;&#36136;&#30340;&#21270;&#23398;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#20197;&#21450;&#32467;&#26500;&#24322;&#26500;&#20307;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#26631;&#27880;&#36807;&#31243;&#12290;&#23613;&#31649;&#23384;&#22312;&#22810;&#31181;&#29992;&#20110;&#30446;&#26631;&#33026;&#36136;&#37492;&#23450;&#30340;&#21830;&#19994;&#21644;&#24320;&#28304;&#36719;&#20214;&#65292;&#20294;&#32570;&#20047;&#33258;&#21160;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#19982;&#32479;&#35745;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#24037;&#20855;&#38598;&#25104;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20840;&#38754;&#33026;&#36136;&#32452;&#23398;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#65288;CLAW&#65289;&#24179;&#21488;&#65292;&#24102;&#26377;&#22522;&#20110;&#23450;&#21046;&#22810;&#21453;&#24212;&#30417;&#27979;&#65288;MRM&#65289;&#21069;&#20307;&#21644;&#20135;&#29289;&#31163;&#23376;&#23545;&#36716;&#25442;&#30340;&#35299;&#26512;&#12289;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#21644;&#33026;&#36136;&#26631;&#27880;&#38598;&#25104;&#24037;&#20316;&#27969;&#31243;&#12290;CLAW&#21253;&#21547;&#22810;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#22312;&#19982;&#33261;&#27687;&#30005;&#21943;&#38654;&#30005;&#31163;&#65288;OzESI&#65289;-MRM&#26041;&#27861;&#30456;&#32467;&#21512;&#26102;&#35782;&#21035;&#19981;&#39281;&#21644;&#33026;&#36136;&#30340;&#30899;&#30899;&#21452;&#38190;&#20301;&#32622;&#12290;&#20026;&#23637;&#31034;CLAW&#20013;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#65292;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#33026;&#36136;&#32452;&#23398;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15076v1 Announce Type: cross  Abstract: Lipidomics generates large data that makes manual annotation and interpretation challenging. Lipid chemical and structural diversity with structural isomers further complicates annotation. Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools. We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions. CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated workflow in CLAW, large-scale lipidomics data was collec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.15075</link><description>&lt;p&gt;
&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15075
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#22312;&#22270;&#32467;&#26500;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#25968;&#25454;&#20013;&#36827;&#34892;&#21327;&#21516;&#36807;&#28388;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#20851;&#31995;&#23494;&#24230;&#23548;&#33268;&#22810;&#36339;&#22270;&#20132;&#20114;&#35745;&#31639;&#21518;&#21452;&#21521;&#33410;&#28857;&#30340;&#22270;&#36866;&#24212;&#24615;&#19981;&#21516;&#65292;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#27169;&#22411;&#23454;&#29616;&#29702;&#24819;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25512;&#33616;&#20219;&#21153;&#26694;&#26550;&#65292;&#31216;&#20026;&#21452;&#20391;&#19981;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;BusGCL&#65289;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;-&#39033;&#30446;&#33410;&#28857;&#20851;&#31995;&#23494;&#24230;&#30340;&#21452;&#20391;&#19981;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#21452;&#20391;&#20999;&#29255;&#23545;&#27604;&#35757;&#32451;&#26356;&#22909;&#22320;&#25512;&#29702;&#29992;&#25143;&#21644;&#39033;&#30446;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#32771;&#34385;&#22522;&#20110;&#36229;&#22270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#25366;&#25496;&#38544;&#24335;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#32858;&#21512;&#33021;&#21147;&#26356;&#36866;&#21512;&#29992;&#25143;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15075v1 Announce Type: cross  Abstract: Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training. Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, emb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MM-Diff&#30340;&#32479;&#19968;&#19988;&#20813;&#35843;&#20248;&#30340;&#22270;&#20687;&#20010;&#24615;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#21333;&#20010;&#21644;&#22810;&#20010;&#20027;&#20307;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#21516;&#26102;&#22686;&#24378;&#25991;&#26412;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15059</link><description>&lt;p&gt;
MM-Diff&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#26465;&#20214;&#34701;&#21512;&#23454;&#29616;&#39640;&#20445;&#30495;&#22270;&#20687;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MM-Diff&#30340;&#32479;&#19968;&#19988;&#20813;&#35843;&#20248;&#30340;&#22270;&#20687;&#20010;&#24615;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#21333;&#20010;&#21644;&#22810;&#20010;&#20027;&#20307;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#21516;&#26102;&#22686;&#24378;&#25991;&#26412;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20813;&#35843;&#20248;&#20010;&#24615;&#21270;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25552;&#39640;&#20027;&#39064;&#20445;&#30495;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#37325;&#26032;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#35201;&#20040;&#23558;&#20854;&#34701;&#20837;&#23494;&#38598;&#30340;&#35270;&#35273;&#23884;&#20837;&#65292;&#36825;&#20004;&#32773;&#37117;&#23384;&#22312;&#27867;&#21270;&#21644;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26080;&#32422;&#26463;&#30340;&#36328;&#27880;&#24847;&#26426;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20027;&#20307;&#22270;&#20687;&#29983;&#25104;&#20013;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Diff&#65292;&#19968;&#20010;&#32479;&#19968;&#19988;&#20813;&#35843;&#20248;&#30340;&#22270;&#20687;&#20010;&#24615;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#21333;&#20010;&#21644;&#22810;&#20010;&#20027;&#39064;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#21516;&#26102;&#22686;&#24378;&#25991;&#26412;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#20445;&#30495;&#24230;&#65292;MM-Diff&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;CLS&#21644;&#34917;&#19969;&#23884;&#20837;&#12290;CLS&#23884;&#20837;&#19968;&#26041;&#38754;&#29992;&#20110;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#65292;&#21478;&#19968;&#26041;&#38754;&#19982;&#34917;&#19969;&#23884;&#20837;&#19968;&#36215;&#24471;&#20986;&#23569;&#37327;&#23500;&#21547;&#32454;&#33410;&#30340;&#20027;&#39064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15059v1 Announce Type: cross  Abstract: Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.15049</link><description>&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#23548;&#33322;&#21040;&#30446;&#30340;&#22320;&#12290;&#29616;&#26377;&#30340;VLN&#20195;&#29702;&#35757;&#32451;&#26041;&#27861;&#39044;&#35774;&#22266;&#23450;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65306;&#24341;&#20837;&#26032;&#29615;&#22659;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#20445;&#30041;&#24050;&#32463;&#36935;&#21040;&#30340;&#29615;&#22659;&#30340;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;VLN&#20195;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#35780;&#20272;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15049v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based meth
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#29305;&#24449;&#65292;&#22312;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#20013;&#21462;&#24471;&#20986;&#33394;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.15044</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15044
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#29305;&#24449;&#65292;&#22312;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#20013;&#21462;&#24471;&#20986;&#33394;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#26159;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25968;&#37327;&#30340;&#28608;&#22686;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#29305;&#24449;&#21487;&#20197;&#22312;&#35768;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#20248;&#21183;&#26469;&#35299;&#20915;&#34920;&#36798;&#65288;Expr&#65289;&#35782;&#21035;&#21644;&#20215;&#20301;-&#21796;&#37266;&#65288;VA&#65289;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#35780;&#20272;Aff-Wild2&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#25552;&#21462;&#27169;&#22411;&#30340;&#26368;&#32456;&#38544;&#34255;&#23618;&#20316;&#20026;&#29305;&#24449;&#12290;&#22312;&#36827;&#34892;&#39044;&#22788;&#29702;&#12289;&#25554;&#20540;&#25110;&#21367;&#31215;&#20197;&#23545;&#40784;&#25552;&#21462;&#30340;&#29305;&#24449;&#21518;&#65292;&#37319;&#29992;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#27169;&#24577;&#34701;&#21512;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;- FulgenceWen/ABAW6th&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15044v1 Announce Type: cross  Abstract: Multimodal fusion is a significant method for most multimodal tasks. With the recent surge in the number of large pre-trained models, combining both multimodal fusion methods and pre-trained model features can achieve outstanding performance in many multimodal tasks. In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features. Following preprocessing and interpolation or convolution to align the extracted features, different models are employed for modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15027</link><description>&lt;p&gt;
&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Grey-informed neural network for time-series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#34987;&#35270;&#20026;&#40657;&#30418;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24314;&#35758;&#23454;&#26045;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#12290;GINN &#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#28784;&#33394;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#20351;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#34987;&#35266;&#23519;&#21040;&#33021;&#22815;&#25581;&#31034;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#32463;&#39564;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#22411;&#26631;&#20934;&#21270;&#26041;&#27861;&#24182;&#20351;&#29992;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#30340;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;</title><link>https://arxiv.org/abs/2403.14999</link><description>&lt;p&gt;
&#39764;&#27861;&#19982;&#37327;&#23376;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Magic for the Age of Quantized DNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14999
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#22411;&#26631;&#20934;&#21270;&#26041;&#27861;&#24182;&#20351;&#29992;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#30340;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25152;&#31034;&#65292;&#20351;&#24471;&#22312;&#23567;&#35268;&#27169;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#23545;&#20135;&#21697;&#25972;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#21270;&#26041;&#27861;&#65288;&#23618;&#25209;&#26631;&#20934;&#21270;&#65289;&#65292;&#20854;&#29420;&#31435;&#20110;&#23567;&#25209;&#37327;&#22823;&#23567;&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#26399;&#38388;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24102;&#26435;&#26631;&#20934;&#21270;&#30340;&#32553;&#25918;&#37327;&#21270;&#21152;&#26435;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30456;&#21516;&#30340;&#20989;&#25968;&#37327;&#21270;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#20195;&#29702;&#26799;&#24230;&#26469;&#35757;&#32451;&#26082;&#20855;&#26377;&#37327;&#21270;&#26435;&#37325;&#21448;&#20855;&#26377;&#37327;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#39764;&#27861;&#19982;&#37327;&#23376;&#21270;DNN&#26102;&#20195;&#65288;MaQD&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#37327;&#21270;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#31934;&#24230;&#38477;&#32423;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14999v1 Announce Type: cross  Abstract: Recently, the number of parameters in DNNs has explosively increased, as exemplified by LLMs (Large Language Models), making inference on small-scale computers more difficult. Model compression technology is, therefore, essential for integration into products. In this paper, we propose a method of quantization-aware training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we quantize the weights by the scaled round-clip function with the weight standardization. We also quantize activation functions using the same function and apply surrogate gradients to train the model with both quantized weights and the quantized activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our quantization method can be achieved with minimal accuracy degradation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14977</link><description>&lt;p&gt;
&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Piecewise-Linear Manifolds for Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26080;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;UDML&#65289;&#33268;&#21147;&#20110;&#20165;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#36924;&#36817;&#27169;&#22411;&#39640;&#32500;&#25968;&#25454;&#27969;&#24418;&#65292;&#20854;&#20013;&#27599;&#20010;&#20302;&#32500;&#32447;&#24615;&#29255;&#27573;&#36817;&#20284;&#20110;&#28857;&#30340;&#23567;&#37051;&#22495;&#20869;&#30340;&#25968;&#25454;&#27969;&#24418;&#12290;&#36825;&#20123;&#37051;&#22495;&#29992;&#20110;&#20272;&#35745;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#36825;&#31181;&#30456;&#20284;&#24615;&#20272;&#35745;&#19982;&#22522;&#20934;&#30456;&#27604;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#22320;&#38754;&#30495;&#30456;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#20351;&#29992;&#22312;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20195;&#29702;&#26469;&#27169;&#25311;&#20998;&#27573;&#32447;&#24615;&#27969;&#24418;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14977v1 Announce Type: cross  Abstract: Unsupervised deep metric learning (UDML) focuses on learning a semantic representation space using only unlabeled data. This challenging problem requires accurately estimating the similarity between data points, which is used to supervise a deep network. For this purpose, we propose to model the high-dimensional data manifold using a piecewise-linear approximation, with each low-dimensional linear piece approximating the data manifold in a small neighborhood of a point. These neighborhoods are used to estimate similarity between data points. We empirically show that this similarity estimate correlates better with the ground truth than the similarity estimates of current state-of-the-art techniques. We also show that proxies, commonly used in supervised metric learning, can be used to model the piecewise-linear manifold in an unsupervised setting, helping improve performance. Our method outperforms existing unsupervised metric learning 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
&#19968;&#22270;&#32988;&#21315;&#35328;&#65306;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22270;&#35889;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#23558;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#24341;&#20837;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#35797;&#28857;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30001;&#20110;&#36807;&#24230;&#24635;&#32467;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#65292;&#20197;&#21450;&#30001;&#20110;&#22270;&#20687;&#24341;&#20837;&#36716;&#31227;&#24615;&#27010;&#24565;&#32780;&#23548;&#33268;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#29616;&#26377;&#36777;&#35770;&#26041;&#26696;&#30340;&#24402;&#32435;&#65288;&#33258;&#19979;&#32780;&#19978;&#65289;&#24615;&#36136;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#65288;&#33258;&#19978;&#32780;&#19979;&#65289;&#30340;&#36777;&#35770;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#35889;&#36777;&#35770;&#65288;BDoG&#65289;&#12290;&#22312;BDoG&#20013;&#65292;&#36777;&#35770;&#20165;&#38480;&#20110;&#34013;&#22270;&#22270;&#20013;&#65292;&#20197;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#25688;&#35201;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#30340;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#65292;BDoG&#32531;&#35299;&#20102;&#39057;&#32321;&#20294;&#26080;&#20851;&#30340;&#27010;&#24565;&#24102;&#26469;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#29616;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BDoG&#65292;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;BDD&#39564;&#25910;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;BDD&#23454;&#36341;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14965</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#34892;&#20026;&#39537;&#21160;&#24320;&#21457;&#39564;&#25910;&#27979;&#35797;&#29983;&#25104;&#20013;&#30340;&#20840;&#38754;&#35780;&#20272;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#29983;&#25104;BDD&#39564;&#25910;&#27979;&#35797;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;BDD&#23454;&#36341;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#39537;&#21160;&#24320;&#21457;&#65288;BDD&#65289;&#26159;&#19968;&#31181;&#20419;&#36827;&#24320;&#21457;&#20154;&#21592;&#12289;QA&#20998;&#26512;&#21592;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#21512;&#20316;&#30340;&#25935;&#25463;&#27979;&#35797;&#26041;&#27861;&#35770;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;BDD&#23454;&#36341;&#65292;&#20174;&#32780;&#33258;&#21160;&#21270;&#39564;&#25910;&#27979;&#35797;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#38646;&#21644;&#23569;&#37327;&#25552;&#31034;&#26469;&#35780;&#20272;&#35832;&#22914;GPT-3.5&#12289;GPT-4&#12289;Llama-2-13B&#21644;PaLM-2&#31561;LLMs&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#25552;&#31034;&#25216;&#26415;&#12289;LLMs&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#35814;&#32454;&#26041;&#27861;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#26080;&#38169;&#35823;&#30340;BDD&#39564;&#25910;&#27979;&#35797;&#65292;&#34920;&#29616;&#26356;&#20339;&#12290;&#23569;&#37327;&#25552;&#31034;&#25216;&#26415;&#31361;&#26174;&#20102;&#36890;&#36807;&#32435;&#20837;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25552;&#20379;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#26816;&#26597;&#20102;&#35821;&#27861;&#38169;&#35823;&#12289;&#39564;&#35777;&#20934;&#30830;&#24615;&#20197;&#21450;LLMs&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22686;&#24378;BDD&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14965v1 Announce Type: cross  Abstract: Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;(RARG)&#65292;&#36890;&#36807;&#25910;&#38598;&#31185;&#23398;&#26469;&#28304;&#30340;&#35777;&#25454;&#26469;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#36991;&#20813;&#36807;&#24230;&#37325;&#22797;&#12290;</title><link>https://arxiv.org/abs/2403.14952</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#39537;&#21160;&#30340;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;(RARG)&#65292;&#36890;&#36807;&#25910;&#38598;&#31185;&#23398;&#26469;&#28304;&#30340;&#35777;&#25454;&#26469;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#36991;&#20813;&#36807;&#24230;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32440;&#65306;arXiv:2403.14952v1   &#31867;&#22411;&#65306;&#20132;&#21449;   &#25688;&#35201;&#65306;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#27867;&#28389;&#23545;&#20844;&#20849;&#21033;&#30410;&#26500;&#25104;&#20102;&#37325;&#35201;&#23041;&#32961;&#12290;&#34429;&#28982;&#35768;&#22810;&#22312;&#32447;&#29992;&#25143;&#31215;&#26497;&#21442;&#19982;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#21709;&#24212;&#32570;&#20047;&#31036;&#35980;&#21644;&#25903;&#25345;&#20107;&#23454;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#27809;&#26377;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23548;&#33268;&#25991;&#26412;&#36136;&#37327;&#19981;&#20339;&#21644;&#21709;&#24212;&#36807;&#20110;&#37325;&#22797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#32034;&#22686;&#24378;&#21709;&#24212;&#29983;&#25104;&#65288;RARG&#65289;&#65292;&#35813;&#26041;&#27861;&#20174;&#31185;&#23398;&#26469;&#28304;&#25910;&#38598;&#25903;&#25345;&#35777;&#25454;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35777;&#25454;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;RARG&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#35777;&#25454;&#25910;&#38598;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#26469;&#26816;&#32034;&#21644;&#37325;&#26032;&#25490;&#21015;&#35777;&#25454;&#25991;&#26723;&#65292;&#20351;&#29992;&#19968;&#20010;dat
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14952v1 Announce Type: cross  Abstract: The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a dat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14951</link><description>&lt;p&gt;
&#31616;&#21333;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Simple Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#19978;&#32321;&#37325;&#30340;&#35757;&#32451;&#25104;&#26412;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#22270;&#21387;&#32553;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#28041;&#21450;&#35843;&#25972;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23567;&#23610;&#24230;&#21387;&#32553;&#22270;&#19978;&#30340;&#35757;&#32451;&#20197;&#22312;&#22823;&#35268;&#27169;&#21407;&#22987;&#22270;&#19978;&#20351;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#21387;&#32553;&#22270;&#21644;&#21407;&#22987;&#22270;&#20043;&#38388;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#22914;&#26799;&#24230;&#12289;GNNs&#30340;&#20998;&#24067;&#21644;&#36712;&#36857;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#25351;&#26631;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#21487;&#33021;&#20250;&#24178;&#25200;&#21387;&#32553;&#22270;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#21387;&#32553;&#36807;&#31243;&#38750;&#24120;&#32321;&#37325;&#21644;&#19981;&#31283;&#23450;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#31616;&#21270;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#20013;&#30340;&#25351;&#26631;&#23545;&#20934;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#20174;GNNs&#32487;&#25215;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#22806;&#37096;&#21442;&#25968;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14951v1 Announce Type: cross  Abstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, distribution and trajectory of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from GNNs. In our approach, we eliminate external parameters and exclusively retain the target conden
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;</title><link>https://arxiv.org/abs/2403.14946</link><description>&lt;p&gt;
&#19968;&#20010;&#32447;&#24615;&#23618;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#20302;&#31209;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
A Single Linear Layer Yields Task-Adapted Low-Rank Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#30001;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#32452;&#25104;&#30340;&#22686;&#37327;&#30697;&#38453;$ \Delta W $&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;&#30697;&#38453;$ W_0 $&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;$ W_0 $&#21644;$ \Delta W $&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;$ W_0 $&#19982;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;LoRA&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#20013;&#34164;&#21547;&#20102;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36716;&#25442;&#30697;&#38453;&#22312;&#27599;&#19968;&#23618;&#20043;&#38388;&#26159;&#30456;&#20284;&#30340;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#65292;&#23558;&#27599;&#19968;&#23618;&#30340;$ W_0 $&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#26465;&#20214;&#21442;&#25968;&#21270;&#30340;LoRA (CondLoRA) &#26041;&#27861;&#65292;&#26469;&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14941</link><description>&lt;p&gt;
&#20174;&#22270;&#32467;&#26500;&#35282;&#24230;&#32479;&#19968;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#65306;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;GraphMLP&#65292;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#65292;&#22312;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24110;&#21161;&#31361;&#30772;&#20102;&#29616;&#26377;&#35780;&#20272;&#26631;&#20934;&#21644;&#25968;&#25454;&#20844;&#24320;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#28966;&#28857;&#21644;&#20851;&#38190;&#39046;&#22495;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#26082;&#35265;&#35777;&#20102;&#20174;&#22478;&#24066;&#32423;&#21040;&#36947;&#36335;&#32423;&#39044;&#27979;&#21462;&#24471;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#38543;&#30528;&#36710;&#36742;&#23545;&#19968;&#20999;&#65288;V2X&#65289;&#25216;&#26415;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#20132;&#36890;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#36947;&#36335;&#32423;&#20132;&#36890;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#21463;&#21040;&#20102;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#26631;&#20934;&#30340;&#32570;&#20047;&#20197;&#21450;&#26377;&#38480;&#30340;&#20844;&#24320;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#23545;&#36710;&#36947;&#32423;&#20132;&#36890;&#39044;&#27979;&#20013;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#31354;&#38388;&#25299;&#25169;&#32467;&#26500;&#21644;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#32467;&#26500;&#21644;MLP&#32593;&#32476;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;GraphMLP&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#19981;&#20844;&#24320;&#30340;&#20195;&#30721;&#65292;&#24182;&#22522;&#20110;&#27492;&#20805;&#20998;&#32780;&#20844;&#27491;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14941v1 Announce Type: cross  Abstract: Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on graph structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14932</link><description>&lt;p&gt;
&#19987;&#27880;&#39537;&#21160;&#30340;&#25512;&#29702;:&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Reasoning: Unlocking the Potential of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#26469;&#22686;&#24378;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#38750;&#35821;&#20041;&#26631;&#35760;&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#20302;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#37325;&#26032;&#24179;&#34913;&#20559;&#26012;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25277;&#35937;&#26356;&#21152;&#24494;&#22937;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25512;&#29702;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36825;&#20123;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#23618;&#22320;&#33258;&#36866;&#24212;&#36339;&#36807;&#35299;&#30721;&#23618;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#21644;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.14919</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#30340;&#20998;&#23618;&#36339;&#36291;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Skip Decoding for Efficient Autoregressive Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#23618;&#22320;&#33258;&#36866;&#24212;&#36339;&#36807;&#35299;&#30721;&#23618;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#21644;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#31574;&#30053;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#25552;&#21069;&#32467;&#26463;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21152;&#36895;&#25512;&#26029;&#38454;&#27573;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#12290;&#19982;&#38656;&#35201;&#39069;&#22806;&#21487;&#35757;&#32451;&#32452;&#20214;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;HSD&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#26681;&#25454;&#24403;&#21069;&#24207;&#21015;&#38271;&#24230;&#20197;&#20998;&#23618;&#30340;&#26041;&#24335;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#35299;&#30721;&#23618;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#24182;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20116;&#20010;&#24102;&#26377;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#26174;&#31034;&#65292;HSD&#22312;&#24179;&#34913;&#25928;&#29575;&#21644;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#20960;&#20046;&#36339;&#36807;&#19968;&#21322;&#30340;&#23618;&#65292;HSD&#21487;&#20197;&#19982;&#21407;&#22987;&#33258;&#22238;&#24402;d&#27169;&#22411;&#30456;&#27604;&#20445;&#25345;90%&#30340;&#25991;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d
&lt;/p&gt;</description></item><item><title>Stance Reasoner&#26159;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#38646;-shot&#31038;&#20132;&#23186;&#20307;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#27169;&#22411;&#65292;&#24182;&#33021;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14895</link><description>&lt;p&gt;
&#26041;&#20301;&#25512;&#29702;&#22120;&#65306;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#38646;-shot&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14895
&lt;/p&gt;
&lt;p&gt;
Stance Reasoner&#26159;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#38646;-shot&#31038;&#20132;&#23186;&#20307;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#27169;&#22411;&#65292;&#24182;&#33021;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#20016;&#23500;&#30340;&#35266;&#28857;&#20869;&#23481;&#26469;&#28304;&#12290;&#31435;&#22330;&#26816;&#27979;&#20801;&#35768;&#33258;&#21160;&#20174;&#36825;&#20123;&#20869;&#23481;&#20013;&#25552;&#21462;&#29992;&#25143;&#23545;&#21508;&#31181;&#35805;&#39064;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#20851;&#27880;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#65292;&#21363;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#65288;a&#65289;&#23545;&#30446;&#26631;&#35805;&#39064;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#65288;b&#65289;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26032;&#35805;&#39064;&#30340;&#36890;&#29992;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Stance Reasoner&#65292;&#19968;&#31181;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#19978;&#30340;&#26174;&#24335;&#25512;&#29702;&#26469;&#24341;&#23548;&#27169;&#22411;&#25512;&#26029;&#26377;&#20851;&#25991;&#26723;&#22312;&#30446;&#26631;&#19978;&#30340;&#31435;&#22330;&#30340;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;Stance Reasoner&#22312;3&#20010;Twitter&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14895v1 Announce Type: cross  Abstract: Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users' opinions on various topics from such content. We focus on zero-shot stance detection, where the model's success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model's inference about the document's stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while a
&lt;/p&gt;</description></item><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#20013;&#23454;&#29616;&#25805;&#32437;&#25915;&#20987;&#65292;&#36827;&#32780;&#36873;&#25321;&#39046;&#23548;&#32773;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#20998;&#26512;&#23637;&#31034;&#20102;PC&#30697;&#38453;&#22823;&#23567;&#12289;&#19981;&#19968;&#33268;&#31243;&#24230;&#21644;&#25805;&#32437;&#36731;&#26494;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.14885</link><description>&lt;p&gt;
&#22312;&#19968;&#31181;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#20013;&#24314;&#31435;&#39046;&#23548;&#32773;
&lt;/p&gt;
&lt;p&gt;
Establishing a leader in a pairwise comparisons method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#20013;&#23454;&#29616;&#25805;&#32437;&#25915;&#20987;&#65292;&#36827;&#32780;&#36873;&#25321;&#39046;&#23548;&#32773;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#20998;&#26512;&#23637;&#31034;&#20102;PC&#30697;&#38453;&#22823;&#23567;&#12289;&#19981;&#19968;&#33268;&#31243;&#24230;&#21644;&#25805;&#32437;&#36731;&#26494;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#36873;&#20030;&#21046;&#24230;&#19968;&#26679;&#65292;&#20915;&#31574;&#26041;&#27861;&#20063;&#23481;&#26131;&#21463;&#21040;&#20915;&#31574;&#32773;&#30340;&#25805;&#32437;&#12290;&#26377;&#25928;&#38450;&#24481;&#36825;&#20123;&#23041;&#32961;&#30340;&#33021;&#21147;&#21482;&#33021;&#26469;&#33258;&#23545;&#25805;&#32437;&#26426;&#21046;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#21487;&#20197;&#29992;&#26469;&#21457;&#21160;&#25805;&#32437;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#20013;&#20351;&#20004;&#20010;&#36873;&#25321;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#26435;&#37325;&#30456;&#31561;&#65292;&#24182;&#22240;&#27492;&#36873;&#25321;&#19968;&#20010;&#39046;&#23548;&#32773;&#12290;&#29702;&#35770;&#32771;&#34385;&#19982;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#30456;&#20276;&#38543;&#65292;&#23637;&#31034;&#20102;PC&#30697;&#38453;&#30340;&#22823;&#23567;&#12289;&#19981;&#19968;&#33268;&#31243;&#24230;&#21644;&#25805;&#32437;&#30340;&#36731;&#26494;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#25105;&#20204;&#20043;&#21069;&#30740;&#31350;&#30340;&#24310;&#32493;&#65292;&#21457;&#34920;&#22312;(Szybowski et al., 2023)&#30340;&#35770;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14885v1 Announce Type: new  Abstract: Abstract Like electoral systems, decision-making methods are also vulnerable to manipulation by decision-makers. The ability to effectively defend against such threats can only come from thoroughly understanding the manipulation mechanisms. In the presented article, we show two algorithms that can be used to launch a manipulation attack. They allow for equating the weights of two selected alternatives in the pairwise comparison method and, consequently, choosing a leader. The theoretical considerations are accompanied by a Monte Carlo simulation showing the relationship between the size of the PC matrix, the degree of inconsistency, and the ease of manipulation. This work is a continuation of our previous research published in the paper (Szybowski et al., 2023)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14859</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27604;&#36739;&#21487;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14859
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#30340;LLM&#21487;&#20197;&#21709;&#24212;&#26126;&#30830;&#21046;&#23450;&#20026;&#25552;&#31034;&#30340;&#26597;&#35810;&#65292;&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#21033;&#29992;LLM&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;LLM&#20013;&#35821;&#20041;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#26126;&#30830;&#25552;&#31034;&#21644;&#65288;b&#65289;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#20998;&#37197;&#32473;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#30340;&#38544;&#24335;&#20272;&#35745;&#65292;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#27604;&#36739;&#20102;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#36328;&#27169;&#22411;&#26550;&#26500;&#21644;&#21487;&#20449;&#24230;&#25968;&#25454;&#38598;&#65292;&#65288;i&#65289;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#21477;&#23376;&#21487;&#20449;&#24230;&#26368;&#21487;&#38752;&#30340;&#25351;&#26631;&#65292;&#38646;&#29031;&#23556;&#25552;&#31034;&#20135;&#29983;&#19981;&#19968;&#33268;&#19988;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#30340;&#32467;&#26524;&#65307;&#65288;ii&#65289;&#22522;&#20110;LL&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#65307;&#65288;iii&#65289;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;</title><link>https://arxiv.org/abs/2403.14843</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#38750;&#39640;&#26031;&#24490;&#29615;&#27169;&#22411;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Local Causal Discovery with Linear non-Gaussian Cyclic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14843
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#65292;&#22240;&#20026;&#32463;&#24120;&#20250;&#20986;&#29616;&#21457;&#29616;&#20840;&#23616;&#22240;&#26524;&#32467;&#26500;&#24182;&#38750;&#24517;&#35201;&#30340;&#24773;&#20917;&#65292;&#20852;&#36259;&#20165;&#20165;&#22312;&#20110;&#21333;&#20010;&#30446;&#26631;&#21464;&#37327;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#32479;&#19968;&#30340;&#23616;&#37096;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#38750;&#39640;&#26031;&#27169;&#22411;&#65292;&#26080;&#35770;&#20854;&#26159;&#21542;&#26159;&#24490;&#29615;&#30340;&#25110;&#38750;&#24490;&#29615;&#30340;&#12290;&#25105;&#20204;&#23558;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30340;&#24212;&#29992;&#20174;&#20840;&#23616;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#29420;&#31435;&#23376;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#30446;&#26631;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20013;&#31934;&#30830;&#35782;&#21035;&#31561;&#25928;&#30340;&#23616;&#37096;&#26377;&#21521;&#32467;&#26500;&#21644;&#22240;&#26524;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14843v1 Announce Type: cross  Abstract: Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acycl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20247;&#21253;&#22810;&#35821;&#35328;&#35821;&#38899;&#21487;&#25026;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#20844;&#24320;&#21457;&#24067;&#22810;&#35821;&#35328;&#35821;&#38899;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23454;&#39564;&#23460;&#27979;&#37327;&#26114;&#36149;&#19988;&#19981;&#26131;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14817</link><description>&lt;p&gt;
&#20247;&#21253;&#22810;&#35821;&#35328;&#35821;&#38899;&#21487;&#25026;&#24230;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Crowdsourced Multilingual Speech Intelligibility Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20247;&#21253;&#22810;&#35821;&#35328;&#35821;&#38899;&#21487;&#25026;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#20844;&#24320;&#21457;&#24067;&#22810;&#35821;&#35328;&#35821;&#38899;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#23454;&#39564;&#23460;&#27979;&#37327;&#26114;&#36149;&#19988;&#19981;&#26131;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#38899;&#39057;&#29305;&#24615;&#30340;&#20986;&#29616;&#65292;&#23545;&#20854;&#23545;&#35821;&#38899;&#21487;&#25026;&#24230;&#30340;&#24433;&#21709;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#38500;&#20102;&#29616;&#26377;&#30340;&#26114;&#36149;&#19988;&#19981;&#26131;&#25193;&#23637;&#30340;&#23454;&#39564;&#23460;&#27979;&#37327;&#22806;&#65292;&#23545;&#20110;&#20247;&#21253;&#35780;&#20272;&#35821;&#35328;&#21487;&#25026;&#24230;&#30340;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#30456;&#20851;&#26631;&#20934;&#21644;&#24314;&#35758;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#65292;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#27979;&#35797;&#26448;&#26009;&#20063;&#23578;&#26410;&#20805;&#36275;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20247;&#21253;&#21487;&#25026;&#24230;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#27979;&#35797;&#35774;&#35745;&#12289;&#22810;&#35821;&#35328;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#20844;&#24320;&#21457;&#24067;&#65292;&#20197;&#21450;&#25105;&#20204;&#26089;&#26399;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14817v1 Announce Type: cross  Abstract: With the advent of generative audio features, there is an increasing need for rapid evaluation of their impact on speech intelligibility. Beyond the existing laboratory measures, which are expensive and do not scale well, there has been comparatively little work on crowdsourced assessment of intelligibility. Standards and recommendations are yet to be defined, and publicly available multilingual test materials are lacking. In response to this challenge, we propose an approach for a crowdsourced intelligibility assessment. We detail the test design, the collection and public release of the multilingual speech data, and the results of our early experiments.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#31561;&#22240;&#32032;&#23545;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25299;&#23637;&#20102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14800</link><description>&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning: A Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14800
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#21457;&#29616;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#31561;&#22240;&#32032;&#23545;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25299;&#23637;&#20102;&#22312;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#26041;&#27861;&#33021;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#29109;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#29978;&#33267;&#26377;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#38543;&#26426;&#25277;&#26679;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#22914;&#36215;&#22987;&#39044;&#31639;&#12289;&#39044;&#31639;&#27493;&#38271;&#21644;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21462;&#24471;&#21331;&#36234;&#32467;&#26524;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#25193;&#23637;&#21040;&#20854;&#20182;&#20219;&#21153;&#65292;&#25506;&#32034;&#20027;&#21160;&#23398;&#20064;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#30446;&#26631;&#26816;&#27979;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#20855;&#20307;&#24314;&#35758;&#65292;&#20026;&#26410;&#26469;&#30340;&#20027;&#21160;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#36890;&#36807;&#25581;&#31034;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20102;&#35299;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#26088;&#22312;&#28608;&#21457;&#22312;&#20855;&#26377;&#26377;&#38480;&#27880;&#37322;&#39044;&#31639;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#25512;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14800v1 Announce Type: cross  Abstract: We conduct a comprehensive evaluation of state-of-the-art deep active learning methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based active learning, and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining's impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the active learning effectiveness in combination with semi-supervised learning, and object detection. Our experiments provide valuable insights and concrete recommendations for future active learning studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26032;&#38382;&#39064;&#35774;&#32622;&#65292;&#20801;&#35768;&#22312;&#35268;&#21010;&#32456;&#27490;&#20043;&#21069;&#27966;&#21457;&#21160;&#20316;&#65292;&#36866;&#29992;&#20110;&#19968;&#20123;&#26102;&#38388;&#21387;&#21147;&#36739;&#22823;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.14796</link><description>&lt;p&gt;
&#22312;&#26102;&#38047;&#28404;&#31572;&#22768;&#20013;&#35268;&#21010;&#21644;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Planning and Acting While the Clock Ticks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14796
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26032;&#38382;&#39064;&#35774;&#32622;&#65292;&#20801;&#35768;&#22312;&#35268;&#21010;&#32456;&#27490;&#20043;&#21069;&#27966;&#21457;&#21160;&#20316;&#65292;&#36866;&#29992;&#20110;&#19968;&#20123;&#26102;&#38388;&#21387;&#21147;&#36739;&#22823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#26102;&#38388;&#35268;&#21010;&#20551;&#35774;&#35268;&#21010;&#26159;&#31163;&#32447;&#36827;&#34892;&#30340;&#65292;&#28982;&#21518;&#25191;&#34892;&#20174;&#26102;&#38388;0&#24320;&#22987;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#24773;&#22659;&#21270;&#26102;&#38388;&#35268;&#21010;&#65292;&#20854;&#20013;&#35268;&#21010;&#20174;&#26102;&#38388;0&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#35268;&#21010;&#32456;&#27490;&#21518;&#25191;&#34892;&#12290;&#24773;&#22659;&#21270;&#26102;&#38388;&#35268;&#21010;&#21453;&#26144;&#20102;&#26356;&#21152;&#30495;&#23454;&#30340;&#24773;&#26223;&#65292;&#21363;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#26102;&#38388;&#26159;&#27969;&#36893;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24773;&#22659;&#21270;&#26102;&#38388;&#35268;&#21010;&#20013;&#65292;&#24517;&#39035;&#22312;&#25191;&#34892;&#20219;&#20309;&#21160;&#20316;&#20043;&#21069;&#29983;&#25104;&#23436;&#25972;&#30340;&#35745;&#21010;&#12290;&#22312;&#19968;&#20123;&#26102;&#38388;&#21387;&#21147;&#36739;&#22823;&#30340;&#38382;&#39064;&#20013;&#65292;&#26102;&#38388;&#22826;&#32039;&#36843;&#65292;&#26080;&#27861;&#22312;&#24517;&#39035;&#25191;&#34892;&#31532;&#19968;&#20010;&#21160;&#20316;&#20043;&#21069;&#23436;&#25104;&#35268;&#21010;&#12290;&#20363;&#22914;&#65292;&#19968;&#36742;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19968;&#36742;&#21345;&#36710;&#26397;&#23427;&#20498;&#36710;&#26102;&#65292;&#21487;&#33021;&#29616;&#22312;&#24212;&#35813;&#36530;&#24320;&#65292;&#28982;&#21518;&#20877;&#35745;&#21010;&#22914;&#20309;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65306;&#24182;&#21457;&#35268;&#21010;&#21644;&#25191;&#34892;&#65292;&#21363;&#22312;&#35268;&#21010;&#32456;&#27490;&#20043;&#21069;&#21487;&#20197;&#27966;&#21457;&#65288;&#25191;&#34892;&#65289;&#21160;&#20316;&#12290;&#19982;&#20197;&#21069;&#20851;&#20110;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#24517;&#39035;&#22788;&#29702;&#23454;&#26102;&#26102;&#38047;&#30340;&#25130;&#27490;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14796v1 Announce Type: new  Abstract: Standard temporal planning assumes that planning takes place offline and then execution starts at time 0. Recently, situated temporal planning was introduced, where planning starts at time 0 and execution occurs after planning terminates. Situated temporal planning reflects a more realistic scenario where time passes during planning. However, in situated temporal planning a complete plan must be generated before any action is executed. In some problems with time pressure, timing is too tight to complete planning before the first action must be executed. For example, an autonomous car that has a truck backing towards it should probably move out of the way now and plan how to get to its destination later. In this paper, we propose a new problem setting: concurrent planning and execution, in which actions can be dispatched (executed) before planning terminates. Unlike previous work on planning and execution, we must handle wall clock deadli
&lt;/p&gt;</description></item><item><title>Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14791</link><description>&lt;p&gt;
Particip-AI: &#19968;&#31181;&#27665;&#20027;&#35843;&#26597;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14791
&lt;/p&gt;
&lt;p&gt;
Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;ChatGPT&#65292;&#20284;&#20046;&#38477;&#20302;&#20102;&#20844;&#20247;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21450;&#21033;&#29992;&#20854;&#21147;&#37327;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#21644;&#21457;&#23637;&#20173;&#25484;&#25569;&#22312;&#23569;&#25968;&#20154;&#25163;&#20013;&#65292;&#21457;&#23637;&#36895;&#24230;&#21152;&#24555;&#19988;&#32570;&#20047;&#39118;&#38505;&#35780;&#20272;&#12290;&#20316;&#20026;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#27835;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Particip-AI&#65292;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#23558;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#21361;&#23475;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#25910;&#38598;&#20351;&#29992;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#21644;&#35814;&#32454;&#22320;&#30740;&#31350;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#22312;&#22791;&#36873;&#26041;&#26696;&#19979;&#65288;&#21363;&#24320;&#21457;&#21644;&#19981;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#24773;&#20917;&#65289;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#21361;&#23475;&#65292;&#24182;&#36890;&#36807;&#20570;&#20986;&#23545;&#20854;&#21457;&#23637;&#30340;&#32467;&#35770;&#24615;&#36873;&#25321;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#25351;&#23548;&#27665;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;295&#20010;&#20154;&#21475;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#20803;&#32032;&#20256;&#36798;&#30456;&#21516;&#24847;&#20041;&#65292;&#20294;&#20351;&#24471;&#37325;&#26032;&#35782;&#21035;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.14790</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23646;&#24615;&#20445;&#30041;&#22270;&#20687;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models for Attribute-Preserving Image Anonymization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14790
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#20803;&#32032;&#20256;&#36798;&#30456;&#21516;&#24847;&#20041;&#65292;&#20294;&#20351;&#24471;&#37325;&#26032;&#35782;&#21035;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21311;&#21517;&#21270;&#30340;&#29983;&#25104;&#25216;&#26415;&#23545;&#20110;&#29983;&#25104;&#33021;&#22815;&#20445;&#25252;&#22270;&#20687;&#20013;&#34987;&#25551;&#36848;&#20010;&#20307;&#38544;&#31169;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#25968;&#25454;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#30041;&#38754;&#37096;&#23646;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26410;&#33021;&#37319;&#32435;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#32771;&#34385;&#22312;&#21311;&#21517;&#21270;&#36807;&#31243;&#20013;&#23558;&#22330;&#26223;&#21644;&#32972;&#26223;&#32435;&#20837;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#22270;&#20687;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#22330;&#26223;&#30340;&#27599;&#19968;&#20010;&#20803;&#32032;&#37117;&#34987;&#20445;&#30041;&#20197;&#20256;&#36798;&#30456;&#21516;&#30340;&#24847;&#20041;&#65292;&#20294;&#20197;&#19968;&#31181;&#20351;&#37325;&#26032;&#35782;&#21035;&#21464;&#24471;&#22256;&#38590;&#30340;&#26041;&#24335;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#38024;&#23545;&#27492;&#30446;&#30340;&#25552;&#20986;&#20102;&#20004;&#31181;LDMs&#65306;CAMOUFLaGE-Base&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;ControlNets&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#26088;&#22312;&#22686;&#21152;&#23454;&#38469;&#22270;&#20687;&#21644;&#21311;&#21517;&#21270;&#22270;&#20687;&#20043;&#38388;&#36317;&#31163;&#30340;&#26032;&#25511;&#21046;&#26426;&#21046;&#12290;CAMOFULaGE-Light&#22522;&#20110;Adapter&#25216;&#26415;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#31181;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14790v1 Announce Type: cross  Abstract: Generative techniques for image anonymization have great potential to generate datasets that protect the privacy of those depicted in the images, while achieving high data fidelity and utility. Existing methods have focused extensively on preserving facial attributes, but failed to embrace a more comprehensive perspective that considers the scene and background into the anonymization process. This paper presents, to the best of our knowledge, the first approach to image anonymization based on Latent Diffusion Models (LDMs). Every element of a scene is maintained to convey the same meaning, yet manipulated in a way that makes re-identification difficult. We propose two LDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained ControlNets, and a new controlling mechanism designed to increase the distance between the real and anonymized images. CAMOFULaGE-Light is based on the Adapter technique, coupled with an encoding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.14783</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;VQA&#65306;&#25506;&#32034;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#26234;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#20316;&#20026;&#24037;&#20855;&#65292;&#20197;&#20811;&#26381;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#19981;&#23545;&#20854;&#36827;&#34892;&#29305;&#23450;VQA&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#25552;&#20986;&#20102;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#31361;&#20986;&#20102;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14783v1 Announce Type: cross  Abstract: This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.
&lt;/p&gt;</description></item><item><title>StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.14773</link><description>&lt;p&gt;
StreamingT2V: &#19968;&#31181;&#19968;&#33268;&#12289;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#38271;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14773
&lt;/p&gt;
&lt;p&gt;
StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#20351;&#24471;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20010;&#24615;&#21270;&#20869;&#23481;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#35270;&#39057;&#65288;&#36890;&#24120;&#20026;16&#25110;24&#24103;&#65289;&#65292;&#24403;&#22825;&#30495;&#22320;&#25193;&#23637;&#21040;&#38271;&#35270;&#39057;&#21512;&#25104;&#30340;&#24773;&#20917;&#26102;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#30828;&#35009;&#21098;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StreamingT2V&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;80&#12289;240&#12289;600&#12289;1200&#25110;&#26356;&#22810;&#24103;&#30340;&#38271;&#35270;&#39057;&#65292;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CAM&#65289;&#30340;&#30701;&#26399;&#35760;&#24518;&#22359;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23558;&#24403;&#21069;&#29983;&#25104;&#26465;&#20214;&#35774;&#32622;&#20026;&#20808;&#21069;&#22359;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#22359;&#36807;&#28193;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21517;&#20026;&#22806;&#35266;&#20445;&#23384;&#27169;&#22359;&#30340;&#38271;&#26399;&#35760;&#24518;&#22359;&#65292;&#20174;&#31532;&#19968;&#20010;&#35270;&#39057;&#22359;&#20013;&#25552;&#21462;&#39640;&#32423;&#22330;&#26223;&#21644;&#23545;&#35937;&#29305;&#24449;&#65292;&#20197;&#38450;&#27490;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14772</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#39640;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#31639;&#27861;&#20801;&#35768;&#23545;&#25163;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#24182;&#26816;&#26597;&#20854;&#36755;&#20986;&#26469;&#37325;&#24314;&#32593;&#32476;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23618;&#26469;&#33719;&#24471;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#12290; &#19977;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#22312;&#22270;&#20687;&#21435;&#22122;&#65292;&#30446;&#26631;&#35782;&#21035;&#21644;&#23545;&#25239;&#24615;&#35823;&#20998;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#28431;&#27934;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#25163;&#27573;&#26469;&#25269;&#24481;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#30721;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#30340;&#26080;&#20851;&#31169;&#20154;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#32780;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#20247;&#25152;&#21608;&#30693;&#21482;&#26377;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14736</link><description>&lt;p&gt;
NaNa&#21644;MiGu&#65306;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#34507;&#30333;&#36136;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#26159;&#21160;&#24577;&#21464;&#21270;&#30340;&#65292;&#36825;&#23558;&#20915;&#23450;&#34507;&#30333;&#36136;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;ProNet&#65292;&#20165;&#35775;&#38382;&#26377;&#38480;&#30340;&#26500;&#35937;&#29305;&#24449;&#21644;&#34507;&#30333;&#36136;&#20391;&#38142;&#29305;&#24449;&#65292;&#23548;&#33268;&#39044;&#27979;&#20013;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#19981;&#20999;&#23454;&#38469;&#21644;&#34507;&#30333;&#36136;&#31867;&#21035;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;NaNa&#21644;MiGu&#65292;&#23558;&#34507;&#30333;&#36136;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#32435;&#20837;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#21644;&#20849;&#23884;&#27531;&#24046;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#34507;&#30333;&#36136;&#30340;&#20998;&#23376;&#29983;&#29289;&#29289;&#29702;&#12289;&#20108;&#32423;&#32467;&#26500;&#12289;&#21270;&#23398;&#38190;&#21644;&#31163;&#23376;&#29305;&#24449;&#26469;&#20419;&#36827;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.14733</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
Open Knowledge Base Canonicalization with Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#30340;&#26500;&#24314;&#23545;&#20110;&#35832;&#22810;&#22522;&#20110;&#30693;&#35782;&#30340;&#32593;&#32476;&#24212;&#29992;&#22914;&#32593;&#32476;&#25628;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;OKB&#20013;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#20851;&#31995;&#30701;&#35821;&#24448;&#24448;&#23384;&#22312;&#20887;&#20313;&#21644;&#27495;&#20041;&#65292;&#36825;&#38656;&#35201;&#23545;OKB&#36827;&#34892;&#35268;&#33539;&#21270;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#36827;&#19968;&#27493;&#20419;&#36827;&#35268;&#33539;&#21270;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#32858;&#31867;&#21644;KGE&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#19988;&#20026;&#36825;&#20123;&#23376;&#20219;&#21153;&#35774;&#35745;&#30340;&#26041;&#27861;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MulCanon&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;OKB&#30340;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#65292;&#24102;&#26469;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14733v1 Announce Type: new  Abstract: The construction of large open knowledge bases (OKBs) is integral to many knowledge-driven applications on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between clustering and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, diffusion model is used in the soft clustering process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCano
&lt;/p&gt;</description></item><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#21160;&#21270;&#33521;&#22269;&#25919;&#24220;1.43&#20159;&#31508;&#20132;&#26131;&#20013;&#23637;&#29616;&#24040;&#22823;&#28508;&#21147;&#65292;&#33410;&#30465;&#27599;&#31508;&#22797;&#26434;&#20132;&#26131;&#19968;&#20998;&#38047;&#30340;&#26102;&#38388;&#21363;&#21487;&#33410;&#32422;&#22823;&#37327;&#24037;&#20316;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.14712</link><description>&lt;p&gt;
&#29992;&#20110;&#23448;&#20698;&#29983;&#20135;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#34913;&#37327;AI&#24110;&#21161;&#33258;&#21160;&#21270;&#33521;&#22269;&#25919;&#24220;1.43&#20159;&#31508;&#20132;&#26131;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14712
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#21160;&#21270;&#33521;&#22269;&#25919;&#24220;1.43&#20159;&#31508;&#20132;&#26131;&#20013;&#23637;&#29616;&#24040;&#22823;&#28508;&#21147;&#65292;&#33410;&#30465;&#27599;&#31508;&#22797;&#26434;&#20132;&#26131;&#19968;&#20998;&#38047;&#30340;&#26102;&#38388;&#21363;&#21487;&#33410;&#32422;&#22823;&#37327;&#24037;&#20316;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25919;&#24220;&#23545;&#20154;&#24037;&#26234;&#33021;&#28508;&#21147;&#22312;&#25913;&#21892;&#20844;&#20849;&#26381;&#21153;&#29983;&#20135;&#21147;&#26041;&#38754;&#20805;&#28385;&#26399;&#24453;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#22797;&#26434;&#20294;&#37325;&#22797;&#30340;&#23448;&#20698;&#20219;&#21153;&#65292;&#20174;&#32780;&#37322;&#25918;&#29087;&#32451;&#21592;&#24037;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#33521;&#22269;&#20013;&#22830;&#25919;&#24220;&#38754;&#21521;&#20844;&#27665;&#30340;&#23448;&#20698;&#20915;&#31574;&#27969;&#31243;&#36827;&#34892;&#35268;&#27169;&#21270;&#25506;&#32034;&#65292;&#34913;&#37327;&#20854;AI&#39537;&#21160;&#33258;&#21160;&#21270;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20272;&#35745;&#33521;&#22269;&#20013;&#22830;&#25919;&#24220;&#27599;&#24180;&#36827;&#34892;&#22823;&#32422;10&#20159;&#31508;&#38754;&#21521;&#20844;&#20247;&#30340;&#20132;&#26131;&#65292;&#25552;&#20379;&#32422;400&#31181;&#26381;&#21153;&#65292;&#20854;&#20013;&#22823;&#32422;&#26377;1.43&#20159;&#31508;&#26159;&#22797;&#26434;&#37325;&#22797;&#20132;&#26131;&#12290;&#25105;&#20204;&#20272;&#35745;&#36825;&#20123;&#22797;&#26434;&#20132;&#26131;&#20013;&#26377;84%&#26159;&#39640;&#24230;&#21487;&#33258;&#21160;&#21270;&#30340;&#65292;&#20195;&#34920;&#30528;&#19968;&#20010;&#24040;&#22823;&#30340;&#28508;&#22312;&#26426;&#36935;&#65306;&#20165;&#33410;&#30465;&#19968;&#20010;&#22797;&#26434;&#20132;&#26131;&#30340;&#19968;&#20998;&#38047;&#24179;&#22343;&#26102;&#38388;&#65292;&#23601;&#30456;&#24403;&#20110;&#33410;&#30465;&#32422;1200&#20154;&#24180;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14712v1 Announce Type: cross  Abstract: There is currently considerable excitement within government about the potential of artificial intelligence to improve public service productivity through the automation of complex but repetitive bureaucratic tasks, freeing up the time of skilled staff. Here, we explore the size of this opportunity, by mapping out the scale of citizen-facing bureaucratic decision-making procedures within UK central government, and measuring their potential for AI-driven automation. We estimate that UK central government conducts approximately one billion citizen-facing transactions per year in the provision of around 400 services, of which approximately 143 million are complex repetitive transactions. We estimate that 84% of these complex transactions are highly automatable, representing a huge potential opportunity: saving even an average of just one minute per complex transaction would save the equivalent of approximately 1,200 person-years of work e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#12289;&#35780;&#20272;&#26041;&#27861;&#21450;&#31526;&#21512;&#36127;&#36131;&#20219;AI&#26631;&#20934;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.14711</link><description>&lt;p&gt;
&#20154;&#22312;&#29615;AI&#29992;&#20110;&#20316;&#24330;&#29615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop AI for Cheating Ring Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#12289;&#35780;&#20272;&#26041;&#27861;&#21450;&#31526;&#21512;&#36127;&#36131;&#20219;AI&#26631;&#20934;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#32447;&#32771;&#35797;&#30340;&#26131;&#33719;&#21462;&#24615;&#65292;&#22312;&#32447;&#32771;&#35797;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#32771;&#35797;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#19987;&#19994;&#20316;&#24330;&#26381;&#21153;&#24110;&#21161;&#24694;&#24847;&#32771;&#29983;&#36890;&#36807;&#32771;&#35797;&#30340;&#32972;&#26223;&#19979;&#65292;&#24418;&#25104;&#20102;&#25152;&#35859;&#30340;&#8220;&#20316;&#24330;&#29615;&#8221;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#29615;AI&#20316;&#24330;&#29615;&#26816;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#38459;&#27490;&#36825;&#20123;&#20316;&#24330;&#29615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#31181;&#20154;&#22312;&#29615;AI&#31995;&#32479;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#25506;&#35752;&#20102;&#20854;&#35774;&#35745;&#21407;&#21017;&#65292;&#26088;&#22312;&#23454;&#29616;&#26816;&#27979;&#20316;&#24330;&#32773;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#29992;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#19982;AI&#31995;&#32479;&#30456;&#20851;&#30340;&#24847;&#22806;&#39118;&#38505;&#12290;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#36981;&#24490;&#20102;&#36127;&#36131;&#20219;&#30340;AI&#65288;RAI&#65289;&#26631;&#20934;&#65292;&#30830;&#20445;&#22312;&#25972;&#20010;&#24320;&#21457;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14711v1 Announce Type: cross  Abstract: Online exams have become popular in recent years due to their accessibility. However, some concerns have been raised about the security of the online exams, particularly in the context of professional cheating services aiding malicious test takers in passing exams, forming so-called "cheating rings". In this paper, we introduce a human-in-the-loop AI cheating ring detection system designed to detect and deter these cheating rings. We outline the underlying logic of this human-in-the-loop AI system, exploring its design principles tailored to achieve its objectives of detecting cheaters. Moreover, we illustrate the methodologies used to evaluate its performance and fairness, aiming to mitigate the unintended risks associated with the AI system. The design and development of the system adhere to Responsible AI (RAI) standards, ensuring that ethical considerations are integrated throughout the entire development process.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;AI&#25512;&#33616;&#27169;&#22411;&#20026;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#25552;&#20379;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;&#25903;&#25345;&#24037;&#20855;&#65292;&#20197;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#29992;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.14710</link><description>&lt;p&gt;
&#20351;&#29992;&#25512;&#33616;&#27169;&#22411;&#20026;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Use of recommendation models to provide support to dyslexic students
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;AI&#25512;&#33616;&#27169;&#22411;&#20026;&#35829;&#35835;&#38556;&#30861;&#23398;&#29983;&#25552;&#20379;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;&#25903;&#25345;&#24037;&#20855;&#65292;&#20197;&#25552;&#20379;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#29992;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#38556;&#30861;&#26159;&#26368;&#26222;&#36941;&#30340;&#29305;&#23450;&#23398;&#20064;&#38556;&#30861;&#65292;&#20005;&#37325;&#24433;&#21709;&#19981;&#21516;&#35748;&#30693;&#39046;&#22495;&#12290;&#36825;&#21453;&#36807;&#26469;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20005;&#37325;&#24433;&#21709;&#36776;&#35835;&#38556;&#30861;&#23398;&#29983;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23398;&#29983;&#24517;&#39035;&#24471;&#21040;&#29305;&#23450;&#30340;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#25903;&#25345;&#24517;&#39035;&#39640;&#24230;&#20010;&#24615;&#21270;&#65292;&#22240;&#20026;&#38556;&#30861;&#20135;&#29983;&#30340;&#38382;&#39064;&#21487;&#33021;&#22240;&#20154;&#32780;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20026;&#36776;&#35835;&#38556;&#30861;&#23398;&#29983;&#24314;&#35758;&#26368;&#21512;&#36866;&#30340;&#25903;&#25345;&#24037;&#20855;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#20379;&#19968;&#31181;&#30495;&#27491;&#26377;&#29992;&#30340;&#26377;&#38024;&#23545;&#24615;&#24110;&#21161;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#25512;&#33616;&#31639;&#27861;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26088;&#22312;&#26816;&#27979;&#20010;&#20154;&#20559;&#22909;&#24182;&#25552;&#20379;&#26368;&#21512;&#36866;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#23454;&#26045;&#21644;&#35757;&#32451;&#20102;&#19977;&#20010;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#39033;&#30446;&#30340;&#27169;&#22411;&#12289;&#22522;&#20110;&#29992;&#25143;&#30340;&#27169;&#22411;&#21644;&#21152;&#26435;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#19968;&#20010;&#36739;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14710v1 Announce Type: cross  Abstract: Dyslexia is the most widespread specific learning disorder and significantly impair different cognitive domains. This, in turn, negatively affects dyslexic students during their learning path. Therefore, specific support must be given to these students. In addition, such a support must be highly personalized, since the problems generated by the disorder can be very different from one to another. In this work, we explored the possibility of using AI to suggest the most suitable supporting tools for dyslexic students, so as to provide a targeted help that can be of real utility. To do this, we relied on recommendation algorithms, which are a branch of machine learning, that aim to detect personal preferences and provide the most suitable suggestions. We hence implemented and trained three collaborative-filtering recommendation models, namely an item-based, a user-based and a weighted-hybrid model, and studied their performance on a large
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#25104;&#33021;&#21147;&#24102;&#26469;&#20102;&#23545;&#33829;&#38144;&#30740;&#31350;&#30340;&#23041;&#32961;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#22312;&#35823;&#23548;&#24615;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#37327;&#21270;&#20102;&#20854;&#23545;&#33829;&#38144;&#30740;&#31350;&#30340;&#25200;&#20081;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#32423;&#26816;&#27979;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.14706</link><description>&lt;p&gt;
&#20445;&#25252;&#33829;&#38144;&#30740;&#31350;&#65306;&#20154;&#24037;&#26234;&#33021;&#21046;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#30340;&#29983;&#25104;&#12289;&#35782;&#21035;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Safeguarding Marketing Research: The Generation, Identification, and Mitigation of AI-Fabricated Disinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14706
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#25104;&#33021;&#21147;&#24102;&#26469;&#20102;&#23545;&#33829;&#38144;&#30740;&#31350;&#30340;&#23041;&#32961;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#22312;&#35823;&#23548;&#24615;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#37327;&#21270;&#20102;&#20854;&#23545;&#33829;&#38144;&#30740;&#31350;&#30340;&#25200;&#20081;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#32423;&#26816;&#27979;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#29983;&#25104;&#20869;&#23481;&#20960;&#20046;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#36129;&#29486;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#23041;&#32961;&#65306;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#21487;&#20197;&#29992;&#26469;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#21644;&#25197;&#26354;&#35748;&#30693;&#65292;&#23548;&#33268;&#23545;&#25968;&#23383;&#24179;&#21488;&#30340;&#20449;&#20219;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#22312;&#33829;&#38144;&#25991;&#29486;&#21644;&#23454;&#36341;&#26041;&#38754;&#20570;&#20986;&#20102;&#19977;&#26041;&#38754;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#23427;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21046;&#36896;&#27169;&#20223;&#30495;&#23454;&#20869;&#23481;&#24418;&#24335;&#30340;&#35823;&#23548;&#24615;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65288;UGC&#65289;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#20854;&#27425;&#65292;&#23427;&#37327;&#21270;&#20102;&#36825;&#31181;UGC&#23545;&#33829;&#38144;&#30740;&#31350;&#30340;&#25200;&#20081;&#24433;&#21709;&#65292;&#31361;&#26174;&#20102;&#21363;&#20351;&#26159;&#26368;&#20302;&#27700;&#24179;&#30340;&#34394;&#20551;&#20449;&#24687;&#20063;&#20250;&#20351;&#20998;&#26512;&#26694;&#26550;&#21463;&#21040;&#24433;&#21709;&#30340;&#33030;&#24369;&#24615;&#12290;&#31532;&#19977;&#65292;&#23427;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#39640;&#32423;&#26816;&#27979;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#26631;&#20934;&#25216;&#26415;&#26080;&#27861;&#36807;&#28388;&#25481;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#21462;&#32508;&#21512;&#24615;&#26041;&#27861;&#26469;&#20445;&#25252;&#33829;&#38144;&#30740;&#31350;&#65292;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14706v1 Announce Type: cross  Abstract: Generative AI has ushered in the ability to generate content that closely mimics human contributions, introducing an unprecedented threat: Deployed en masse, these models can be used to manipulate public opinion and distort perceptions, resulting in a decline in trust towards digital platforms. This study contributes to marketing literature and practice in three ways. First, it demonstrates the proficiency of AI in fabricating disinformative user-generated content (UGC) that mimics the form of authentic content. Second, it quantifies the disruptive impact of such UGC on marketing research, highlighting the susceptibility of analytics frameworks to even minimal levels of disinformation. Third, it proposes and evaluates advanced detection frameworks, revealing that standard techniques are insufficient for filtering out AI-generated disinformation. We advocate for a comprehensive approach to safeguarding marketing research that integrates
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040; emerged words &#19982; natural language concepts &#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.14705</link><description>&lt;p&gt;
&#27010;&#24565;&#26368;&#20339;&#21305;&#37197;&#65306;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#20013;&#30340;&#32452;&#21512;&#24615;
&lt;/p&gt;
&lt;p&gt;
Concept-Best-Matching: Evaluating Compositionality in Emergent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040; emerged words &#19982; natural language concepts &#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27807;&#36890;&#20197;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33719;&#21462;&#30340;&#36890;&#20449;&#21327;&#35758;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22823;&#37327;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#65292;&#20854;&#20013;\emph{&#32452;&#21512;&#24615;}&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#26399;&#26395;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31243;&#24207;&#24182;&#26410;&#30452;&#25509;&#26292;&#38706;&#26032;&#20852;&#36890;&#20449;&#30340;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#21363;&#25214;&#21040;&#26032;&#20852;&#35789;&#27719;&#19982;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#12290;&#26368;&#20339;&#21305;&#37197;&#31639;&#27861;&#25552;&#20379;&#20102;&#20840;&#23616;&#20998;&#25968;&#21644;&#20174;&#26032;&#20852;&#35789;&#27719;&#21040;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#30340;&#32763;&#35793;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#25552;&#20379;&#26032;&#20852;&#35789;&#27719;&#19982;&#20154;&#31867;&#27010;&#24565;&#20043;&#38388;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14705v1 Announce Type: new  Abstract: Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with \emph{compositionality} featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts. The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;&#65292;&#19981;&#20855;&#22791;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#24615;&#12289;&#24207;&#21015;&#24615;&#21644;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#20854;&#23436;&#22791;&#24615;&#24182;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;</title><link>https://arxiv.org/abs/2403.14704</link><description>&lt;p&gt;
&#19968;&#20010;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A minimal coalition logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14704
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#26368;&#23567;&#32852;&#30431;&#36923;&#36753;&#65292;&#19981;&#20855;&#22791;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#24615;&#12289;&#24207;&#21015;&#24615;&#21644;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#20854;&#23436;&#22791;&#24615;&#24182;&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#30431;&#36923;&#36753;&#26159;&#25112;&#30053;&#25512;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#36923;&#36753;&#12290;&#26412;&#25991;&#39318;&#20808;&#25351;&#20986;&#32852;&#30431;&#36923;&#36753;&#27169;&#22411;&#65292;&#21363;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#65292;&#23384;&#22312;&#19977;&#20010;&#36807;&#20110;&#24378;&#30340;&#20551;&#35774;&#12290;&#20854;&#19968;&#26159;&#20195;&#29702;&#30340;&#29420;&#31435;&#24615;&#65307;&#21363;&#65292;&#20004;&#20010;&#19981;&#21516;&#32852;&#30431;&#30340;&#20004;&#20010;&#21487;&#29992;&#32852;&#21512;&#21160;&#20316;&#30340;&#21512;&#24182;&#24635;&#26159;&#21487;&#20197;&#21512;&#24182;&#25104;&#20004;&#20010;&#32852;&#30431;&#30340;&#32852;&#30431;&#12290;&#20854;&#20108;&#26159;&#24207;&#21015;&#24615;&#65307;&#21363;&#65292;&#32852;&#30431;&#24635;&#26159;&#26377;&#21487;&#29992;&#30340;&#32852;&#21512;&#21160;&#20316;&#12290;&#20854;&#19977;&#26159;&#30830;&#23450;&#24615;&#65292;&#21363;&#65292;&#22823;&#32852;&#30431;&#30340;&#21512;&#20316;&#21160;&#20316;&#24635;&#26159;&#26377;&#21807;&#19968;&#32467;&#26524;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19968;&#33324;&#24182;&#21457;&#21338;&#24328;&#27169;&#22411;&#30340;&#32852;&#30431;&#36923;&#36753;&#65292;&#35813;&#27169;&#22411;&#19981;&#20855;&#22791;&#36825;&#19977;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36923;&#36753;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#19982;&#32852;&#30431;&#36923;&#36753;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#12290;&#22312;&#25112;&#30053;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#19968;&#36923;&#36753;&#20284;&#20046;&#26159;&#26368;&#23567;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14704v1 Announce Type: cross  Abstract: Coalition logic is a central logic in strategic reasoning studies. In this paper, we first argue that Coalition Logic models, concurrent game models, have three too-strong assumptions. The first one is the independence of agents; that is, the merge of two available joint actions of two disjoint coalitions is always available for the union of the two coalitions. The second one is seriality; that is, coalitions always have available joint actions. The third one is determinism, that is, the grand coalition's joint actions always have a unique outcome. Second, we present a coalition logic based on general concurrent game models, which do not have the three assumptions. We show the completeness of this logic and compare it with Coalition Logic in detail. This logic seems minimal in the context of strategic reasoning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24314;&#31569;&#24072;&#22312;&#35774;&#35745;&#21487;&#38752;&#31995;&#32479;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#19979;&#31995;&#32479;&#30340;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14697</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24320;&#25918;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#19981;&#21487;&#39044;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An AIC-based approach for articulating unpredictable problems in open complex environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24314;&#31569;&#24072;&#22312;&#35774;&#35745;&#21487;&#38752;&#31995;&#32479;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#19979;&#31995;&#32479;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#24314;&#31569;&#24072;&#22312;&#35774;&#35745;&#21644;&#20445;&#35777;&#31995;&#32479;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#31995;&#32479;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#21892;&#24314;&#31569;&#24072;&#22312;&#35774;&#35745;&#21487;&#38752;&#31995;&#32479;&#65288;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#65289;&#26102;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#21033;&#29992;&#33322;&#31354;&#33322;&#22825;&#26696;&#20363;&#30740;&#31350;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;&#35782;&#21035;&#20102;&#24433;&#21709;&#39134;&#26426;&#26816;&#27979;&#30340;&#22810;&#20010;&#22240;&#32032;&#65288;&#25361;&#25112;&#65289;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22797;&#26434;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#22686;&#24378;&#24314;&#31569;&#24072;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14697v1 Announce Type: cross  Abstract: This research paper presents an approach to enhancing the predictive capability of architects in the design and assurance of systems, focusing on systems operating in dynamic and unpredictable environments. By adopting a systems approach, we aim to improve architects' predictive capabilities in designing dependable systems (for example, ML-based systems). An aerospace case study is used to illustrate the approach. Multiple factors (challenges) influencing aircraft detection are identified, demonstrating the effectiveness of our approach in a complex operational setting. Our approach primarily aimed to enhance the architect's predictive capability.
&lt;/p&gt;</description></item><item><title>GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#65292;&#36824;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#26159;&#22312;&#22269;&#38469;&#21270;&#26041;&#38754;&#38656;&#27880;&#24847;&#36991;&#20813;&#20854;&#35823;&#29992;&#23548;&#33268;&#30340;&#20840;&#29699;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14694</link><description>&lt;p&gt;
GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of GPT Language Models for Innovation in Activities in University Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14694
&lt;/p&gt;
&lt;p&gt;
GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#65292;&#36824;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#26159;&#22312;&#22269;&#38469;&#21270;&#26041;&#38754;&#38656;&#27880;&#24847;&#36991;&#20813;&#20854;&#35823;&#29992;&#23548;&#33268;&#30340;&#20840;&#29699;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#65288;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65289;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#21160;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#23558;GPT&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22823;&#23398;&#25945;&#23398;&#30340;&#21508;&#20010;&#32500;&#24230;&#20013;&#23384;&#22312;&#30528;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#20174;&#23398;&#29983;&#21644;&#25945;&#24072;&#27963;&#21160;&#21019;&#26032;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#20197;&#21450;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#20174;&#22269;&#38469;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35823;&#29992;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#30528;&#19968;&#20010;&#38656;&#35201;&#37319;&#21462;&#19968;&#31995;&#21015;&#20849;&#21516;&#25514;&#26045;&#30340;&#20840;&#29699;&#38382;&#39064;&#65292;&#36825;&#20123;&#25514;&#26045;&#38656;&#35201;&#21508;&#22320;&#21306;&#30340;&#22823;&#23398;&#20849;&#21516;&#21442;&#19982;&#12290;&#22312;&#19968;&#20123;&#22269;&#23478;&#65292;&#24050;&#32463;&#23545;&#35780;&#20272;&#24037;&#20855;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#20197;&#30830;&#20445;&#24037;&#20316;&#26159;&#30001;&#23398;&#29983;&#23436;&#25104;&#30340;&#65292;&#32780;&#19981;&#26159;&#30001;&#20154;&#24037;&#26234;&#33021;&#23436;&#25104;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#36825;&#31181;&#20195;&#34920;&#24615;&#23398;&#31185;&#30340;&#19968;&#20010;&#20855;&#20307;&#35838;&#39064;&#65292;&#27604;&#22914;&#36719;&#20214;&#24037;&#31243;&#20013;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14694v1 Announce Type: cross  Abstract: The GPT (Generative Pre-trained Transformer) language models are an artificial intelligence and natural language processing technology that enables automatic text generation. There is a growing interest in applying GPT language models to university teaching in various dimensions. From the perspective of innovation in student and teacher activities, they can provide support in understanding and generating content, problem-solving, as well as personalization and test correction, among others. From the dimension of internationalization, the misuse of these models represents a global problem that requires taking a series of common measures in universities from different geographical areas. In several countries, there has been a review of assessment tools to ensure that work is done by students and not by AI. To this end, we have conducted a detailed experiment in a representative subject of Computer Science such as Software Engineering, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#12289;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;A2CI&#65292;&#26088;&#22312;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#65292;&#33021;&#26377;&#25928;&#24212;&#23545;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#22823;&#37327;&#22320;&#29699;&#31185;&#23398;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14693</link><description>&lt;p&gt;
A2CI&#65306;&#22522;&#20110;&#20113;&#30340;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;&#65292;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#12289;&#38754;&#21521;&#26381;&#21153;&#30340;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;A2CI&#65292;&#26088;&#22312;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#65292;&#33021;&#26377;&#25928;&#24212;&#23545;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#22823;&#37327;&#22320;&#29699;&#31185;&#23398;&#25968;&#25454;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22320;&#31185;&#23398;&#25968;&#25454;&#20026;&#31185;&#23398;&#30028;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#26426;&#36935;&#12290;&#21033;&#29992;&#36965;&#24863;&#21355;&#26143;&#12289;&#22320;&#38754;&#20256;&#24863;&#22120;&#32593;&#32476;&#29978;&#33267;&#31038;&#20132;&#23186;&#20307;&#36755;&#20837;&#25910;&#38598;&#21040;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#29616;&#22312;&#21487;&#20197;&#36827;&#34892;&#26356;&#22810;&#22823;&#35268;&#27169;&#12289;&#38271;&#26399;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;NASA&#21644;&#20854;&#20182;&#25919;&#24220;&#26426;&#26500;&#27599;&#23567;&#26102;&#25910;&#38598;&#21644;&#25972;&#29702;&#30340;&#25968;&#30334;TB&#20449;&#24687;&#23545;&#20110;&#24076;&#26395;&#25913;&#21892;&#23545;&#22320;&#29699;&#22823;&#27668;&#31995;&#32479;&#30340;&#29702;&#35299;&#30340;&#22823;&#27668;&#31185;&#23398;&#23478;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#22823;&#37327;&#25968;&#25454;&#30340;&#26377;&#25928;&#21457;&#29616;&#12289;&#32452;&#32455;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#30001;NSF&#36164;&#21161;&#30340;&#39033;&#30446;&#30340;&#25104;&#26524;&#65292;&#35813;&#39033;&#30446;&#24320;&#21457;&#20102;&#19968;&#20010;&#22320;&#29702;&#31354;&#38388;&#19979;&#23618;&#32467;&#26500;&#8212;&#8212;A2CI&#65288;&#22823;&#27668;&#20998;&#26512;&#19979;&#23618;&#32467;&#26500;&#65289;&#65292;&#20197;&#25903;&#25345;&#22823;&#27668;&#30740;&#31350;&#12290;&#39318;&#20808;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#26381;&#21153;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#28982;&#21518;&#35814;&#32454;&#25551;&#36848;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14693v1 Announce Type: cross  Abstract: Big earth science data offers the scientific community great opportunities. Many more studies at large-scales, over long-terms and at high resolution can now be conducted using the rich information collected by remote sensing satellites, ground-based sensor networks, and even social media input. However, the hundreds of terabytes of information collected and compiled on an hourly basis by NASA and other government agencies present a significant challenge for atmospheric scientists seeking to improve the understanding of the Earth atmospheric system. These challenges include effective discovery, organization, analysis and visualization of large amounts of data. This paper reports the outcomes of an NSF-funded project that developed a geospatial cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) -- to support atmospheric research. We first introduce the service-oriented system framework then describe in detail the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.14692</link><description>&lt;p&gt;
AI &#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#65306;GenAI &#25903;&#25345;&#35780;&#20272;&#30340;&#35797;&#28857;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#24555;&#36895;&#37319;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#24341;&#21457;&#20102;&#23545;&#23398;&#26415;&#35802;&#20449;&#12289;&#35780;&#20272;&#23454;&#36341;&#21644;&#23398;&#29983;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#31105;&#27490;&#25110;&#38459;&#27490;GenAI&#24037;&#20855;&#24050;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#65292;&#24809;&#32602;&#24615;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#33521;&#22269;&#36234;&#21335;&#22823;&#23398;&#65288;BUV&#65289;&#36827;&#34892;&#30340;&#35797;&#28857;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#26045;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;GenAI&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#12290;AIAS&#30001;&#20116;&#20010;&#32423;&#21035;&#32452;&#25104;&#65292;&#20174;&#8220;&#26080;AI&#8221;&#21040;&#8220;&#23436;&#20840;AI&#8221;&#65292;&#20351;&#25945;&#32946;&#24037;&#20316;&#32773;&#33021;&#22815;&#35774;&#35745;&#20391;&#37325;&#20110;&#38656;&#35201;&#20154;&#31867;&#36755;&#20837;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#35780;&#20272;&#12290;&#22312;&#23454;&#26045;AIAS&#21518;&#65292;&#35797;&#28857;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#26174;&#30528;&#20943;&#23569;&#65292;&#23398;&#29983;&#25104;&#32489;&#25552;&#39640;&#20102;5.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14692v1 Announce Type: cross  Abstract: The rapid adoption of Generative Artificial Intelligence (GenAI) technologies in higher education has raised concerns about academic integrity, assessment practices, and student learning. Banning or blocking GenAI tools has proven ineffective, and punitive approaches ignore the potential benefits of these technologies. This paper presents the findings of a pilot study conducted at British University Vietnam (BUV) exploring the implementation of the Artificial Intelligence Assessment Scale (AIAS), a flexible framework for incorporating GenAI into educational assessments. The AIAS consists of five levels, ranging from 'No AI' to 'Full AI', enabling educators to design assessments that focus on areas requiring human input and critical thinking.   Following the implementation of the AIAS, the pilot study results indicate a significant reduction in academic misconduct cases related to GenAI, a 5.9% increase in student attainment across the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#21487;&#33021;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14691</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29992;&#25143;&#20449;&#20219;&#65306;&#20197;&#21307;&#30103;&#20026;&#37325;&#28857;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and User Trust: Focus on Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#21487;&#33021;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#38543;&#30528;LLMs&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#20381;&#36182;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#36755;&#20986;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#23548;&#33268;&#20020;&#24202;&#21307;&#24072;&#25216;&#33021;&#20943;&#23569;&#65292;&#22240;&#20026;&#20182;&#20204;&#19982;&#22522;&#26412;&#35786;&#26029;&#36807;&#31243;&#30340;&#21442;&#19982;&#20943;&#23569;&#12290;&#23613;&#31649;&#30446;&#21069;&#36824;&#22788;&#20110;&#29702;&#35770;&#38454;&#27573;&#65292;&#20294;&#36825;&#31181;&#21453;&#39304;&#24490;&#29615;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#31215;&#26497;&#23545;&#35805;&#21644;&#25112;&#30053;&#25514;&#26045;&#65292;&#20197;&#30830;&#20445;LLM&#25216;&#26415;&#30340;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#33258;&#25105;&#21442;&#32771;&#23398;&#20064;&#24490;&#29615;&#20197;&#21450;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#25216;&#33021;&#19979;&#38477;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14691v1 Announce Type: cross  Abstract: This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;</title><link>https://arxiv.org/abs/2403.14690</link><description>&lt;p&gt;
&#23558;&#22270;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#33258;&#21160;&#27714;&#35299;&#20960;&#20309;&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#34987;&#35748;&#20026;&#26159;&#36808;&#21521;&#36890;&#29992;&#25968;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#65292;&#20854;&#20381;&#25176;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20256;&#32479;&#36923;&#36753;&#25512;&#29702;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#36890;&#36807;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#22914;&#32447;&#26465;&#25110;&#28857;&#26469;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#38656;&#35201;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#26102;&#36873;&#25321;&#21512;&#36866;&#30340;&#36741;&#21161;&#32452;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#33258;&#21160;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#24615;&#33021;&#26159;&#36890;&#36807;&#20174;&#31867;&#21035;&#24211;&#20013;&#31351;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#21487;&#33021;&#24615;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#25928;&#29575;&#26041;&#38754;&#20570;&#20986;&#22949;&#21327;&#65292;&#24517;&#39035;&#37319;&#29992;&#24191;&#27867;&#30340;&#31574;&#30053;&#25628;&#32034;&#12290;&#20026;&#20102;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14690v1 Announce Type: cross  Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We first
&lt;/p&gt;</description></item><item><title>&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14689</link><description>&lt;p&gt;
&#21457;&#23637;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#20135;&#19994;&#26631;&#20934;&#65306;&#25361;&#25112;&#12289;&#31574;&#30053;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14689
&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#33258;&#21160;&#21270;&#34892;&#25919;&#21644;&#25945;&#23398;&#20219;&#21153;&#20197;&#21450;&#38477;&#20302;&#20869;&#23481;&#21019;&#24314;&#25104;&#26412;&#26469;&#38761;&#26032;&#25945;&#32946;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#32570;&#20047;&#26631;&#20934;&#21270;&#23454;&#36341;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#20998;&#25955;&#65292;&#32473;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#25552;&#20379;&#23545;&#24403;&#21069;&#23616;&#21183;&#12289;&#25361;&#25112;&#21644;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31574;&#30053;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#22987;&#36890;&#36807;&#30740;&#31350;AIED&#22312;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#31995;&#32479;&#20114;&#25805;&#20316;&#24615;&#12289;&#26412;&#20307;&#26144;&#23556;&#12289;&#25968;&#25454;&#38598;&#25104;&#12289;&#35780;&#20272;&#21644;&#36947;&#24503;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;...</title><link>https://arxiv.org/abs/2403.14687</link><description>&lt;p&gt;
&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#32570;&#22833;&#20540;&#22635;&#34917;&#25216;&#26415;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Imputation Techniques for Missing Values on Healthcare Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#22312;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#20540;&#26159;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#19968;&#31181;&#24120;&#35265;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#22312;&#20581;&#24247;&#25968;&#25454;&#20013;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#19971;&#31181;&#22635;&#34917;&#25216;&#26415;&#65288;&#22343;&#20540;&#22635;&#34917;&#12289;&#20013;&#20301;&#25968;&#22635;&#34917;&#12289;&#26368;&#36817;&#35266;&#23519;&#20540;&#22635;&#34917;&#12289;K-&#26368;&#36817;&#37051;&#22635;&#34917;&#12289;&#25554;&#20540;&#22635;&#34917;&#12289;Missforest&#22635;&#34917;&#21644;&#38142;&#24335;&#26041;&#31243;&#22810;&#37325;&#22635;&#34917;&#65289;&#22312;&#19977;&#20010;&#20581;&#24247;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#19981;&#21516;&#30334;&#20998;&#27604;&#30340;&#32570;&#22833;&#20540;&#65288;10\%&#12289;15\%&#12289;20\%&#21644;25\%&#65289;&#65292;&#24182;&#20351;&#29992;&#22635;&#34917;&#25216;&#26415;&#23545;&#36825;&#20123;&#32570;&#22833;&#20540;&#36827;&#34892;&#22635;&#34917;&#12290;&#36890;&#36807;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;Mi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14687v1 Announce Type: cross  Abstract: Missing values or data is one popular characteristic of real-world datasets, especially healthcare data. This could be frustrating when using machine learning algorithms on such datasets, simply because most machine learning models perform poorly in the presence of missing values. The aim of this study is to compare the performance of seven imputation techniques, namely Mean imputation, Median Imputation, Last Observation carried Forward (LOCF) imputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation, Missforest imputation, and Multiple imputation by Chained Equations (MICE), on three healthcare datasets. Some percentage of missing values - 10\%, 15\%, 20\% and 25\% - were introduced into the dataset, and the imputation techniques were employed to impute these missing values. The comparison of their performance was evaluated by using root mean squared error (RMSE) and mean absolute error (MAE). The results show that Mi
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14683</link><description>&lt;p&gt;
&#19968;&#39033;&#36947;&#20041;&#20351;&#21629;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25345;&#32493;&#36229;&#23545;&#40784;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
A Moral Imperative: The Need for Continual Superalignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14683
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#12290;&#36229;&#23545;&#40784;&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#20445;&#36229;&#26234;&#33021;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#12290;&#23613;&#31649;&#20854;&#23637;&#26395;&#20196;&#20154;&#25391;&#22859;&#65292;&#25105;&#20204;&#35748;&#20026;&#23454;&#29616;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;LLM&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#20154;&#31867;&#36947;&#24503;&#30340;&#21160;&#24577;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21078;&#26512;&#20102;&#23558;&#19981;&#26029;&#21464;&#21270;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#35889;&#31995;&#32534;&#30721;&#21040;LLMs&#20013;&#30340;&#25361;&#25112;&#65292;&#31361;&#20986;&#20102;&#38745;&#24577;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20154;&#31867;&#31038;&#20250;&#21160;&#24577;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65306;&#19968;&#20010;&#23637;&#31034;&#20102;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23450;&#24615;&#36716;&#21464;&#65292;&#21478;&#19968;&#20010;&#21576;&#29616;&#20102;&#21487;&#37327;&#21270;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14683v1 Announce Type: cross  Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illus
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;CVAE-USM&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#21644;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.14682</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#30340;&#28145;&#24230;&#29983;&#25104;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#29992;&#20110;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;CVAE-USM&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#21644;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#65292;&#26377;&#25928;&#22320;&#22312;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#36827;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#20551;&#35774;&#36890;&#24120;&#22833;&#36133;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#29992;&#25143;&#22330;&#26223;&#20013;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#36890;&#29992;&#24207;&#21015;&#26144;&#23556;&#65288;CVAE-USM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25918;&#26494; i.i.d. &#20551;&#35774;&#24182;&#21033;&#29992;&#26102;&#38388;&#20851;&#31995;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;HAR&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14682v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the assumption that training and testing data are independent and identically distributed (i.i.d.) often fails, particularly in cross-user scenarios where data distributions vary significantly. This discrepancy highlights the limitations of conventional domain adaptation methods in HAR, which typically overlook the inherent temporal relations in time-series data. To bridge this gap, our study introduces a Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) approach, which addresses the unique challenges of time-series domain adaptation in HAR by relaxing the i.i.d. assumption and leveraging temporal relations to align data distributions effectively across different users. This method combines the strengths of Variational Autoencoder (VAE) and Universal Sequence Mapping (USM) to capture and utilize common temporal patterns between users for improved activity recognition. Ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;AI&#20262;&#29702;&#23398;&#25991;&#29486;&#36827;&#34892;&#20840;&#38754;&#35745;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;AI&#20262;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#19977;&#37096;&#26354;&#65292;&#24182;&#25552;&#20986;&#20102;&#19971;&#20010;&#20851;&#38190;AI&#20262;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20851;&#20110;&#22823;&#20262;&#29702;&#27169;&#22411;&#65288;LEM&#65289;&#21644;AI&#35782;&#21035;&#19982;&#25193;&#23637;&#30340;&#20004;&#20010;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.14681</link><description>&lt;p&gt;
AI&#20262;&#29702;&#23398;&#65306;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12289;&#20851;&#38190;&#38382;&#39064;&#21644;&#20027;&#35201;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;
AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;AI&#20262;&#29702;&#23398;&#25991;&#29486;&#36827;&#34892;&#20840;&#38754;&#35745;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;AI&#20262;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#19977;&#37096;&#26354;&#65292;&#24182;&#25552;&#20986;&#20102;&#19971;&#20010;&#20851;&#38190;AI&#20262;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20851;&#20110;&#22823;&#20262;&#29702;&#27169;&#22411;&#65288;LEM&#65289;&#21644;AI&#35782;&#21035;&#19982;&#25193;&#23637;&#30340;&#20004;&#20010;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20262;&#29702;&#23398;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#23398;&#26415;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#23545;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;AI&#20262;&#29702;&#23398;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35745;&#37327;&#20998;&#26512;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#19977;&#37096;&#26354;&#21457;&#23637;&#36807;&#31243;&#65292;&#21253;&#25324;&#19968;&#20010;&#23413;&#21270;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#19987;&#27880;&#20110;&#36171;&#20104;AI&#20154;&#31867;&#23646;&#24615;&#30340;&#38454;&#27573;&#65292;&#26368;&#32456;&#26159;&#24378;&#35843;&#21457;&#23637;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;AI&#31995;&#32479;&#30340;&#31532;&#19977;&#38454;&#27573;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19971;&#20010;&#20851;&#38190;AI&#20262;&#29702;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;Collingridge&#22256;&#22659;&#12289;AI&#22320;&#20301;&#36777;&#35770;&#12289;&#19982;AI&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#12289;&#38544;&#31169;&#20445;&#25252;&#22797;&#26434;&#24615;&#12289;&#23545;&#20844;&#27491;&#21644;&#20844;&#24179;&#30340;&#32771;&#34385;&#12289;&#23545;&#31639;&#27861;&#32479;&#27835;&#21644;&#20154;&#31867;&#34928;&#24369;&#30340;&#25285;&#24551;&#65292;&#20197;&#21450;&#36229;&#26234;&#33021;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#20182;&#20204;&#30830;&#23450;&#20102;AI&#20262;&#29702;&#23398;&#20013;&#20004;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#21363;&#22823;&#20262;&#29702;&#27169;&#22411;&#65288;LEM&#65289;&#21644;AI&#35782;&#21035;&#19982;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14681v1 Announce Type: cross  Abstract: Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal area of scholarly research. This study conducts a comprehensive bibliometric analysis of the AI ethics literature over the past two decades. The analysis reveals a discernible tripartite progression, characterized by an incubation phase, followed by a subsequent phase focused on imbuing AI with human-like attributes, culminating in a third phase emphasizing the development of human-centric AI systems. After that, they present seven key AI ethics issues, encompassing the Collingridge dilemma, the AI status debate, challenges associated with AI transparency and explainability, privacy protection complications, considerations of justice and fairness, concerns about algocracy and human enfeeblement, and the issue of superintelligence. Finally, they identify two notable research gaps in AI ethics regarding the large ethics model (LEM) and AI identification and exten
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14680</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;: &#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Trust in AI: Progress, Challenges, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14680
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#12289;&#26381;&#21153;&#21644;&#20135;&#21697;&#65292;&#35828;&#26126;&#20102;&#26469;&#33258;&#29992;&#25143;&#35282;&#24230;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#19981;&#20165;&#20316;&#20026;&#19968;&#20123;&#26377;&#30410;&#24037;&#20855;&#24191;&#27867;&#28183;&#36879;&#21040;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#65292;&#32780;&#19988;&#36824;&#20250;&#25104;&#20026;&#20195;&#34920;&#25105;&#20204;&#30340;&#26367;&#20195;&#24615;&#20195;&#29702;&#20154;&#65292;&#25110;&#32773;&#20250;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;&#12289;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;&#25805;&#32437;&#24615;&#24515;&#26234;&#12290;&#36817;&#26469;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#21450;&#20854;&#30456;&#20851;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#31687;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#22312;&#23545;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#25991;&#29486;&#20013;&#23545;&#20449;&#20219;&#30340;&#27010;&#24565;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14680v1 Announce Type: cross  Abstract: The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#23398;&#26415;&#31354;&#30333;</title><link>https://arxiv.org/abs/2403.14676</link><description>&lt;p&gt;
&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#29992;&#20110;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Uncertainty Estimation for Cognitive Diagnosis Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#23398;&#26415;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#25945;&#32946;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#23545;&#30693;&#35782;&#27010;&#24565;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#22522;&#20110;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#20010;&#24615;&#21270;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#32852;&#31995;&#34180;&#24369;&#65292;&#27979;&#37327;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#65292;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20063;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35748;&#30693;&#35786;&#26029;&#20013;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#30740;&#31350;&#33853;&#21518;&#20110;&#23545;&#35748;&#30693;&#35786;&#26029;&#39640;&#32423;&#27169;&#22411;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#25928;&#29575;&#26377;&#38480;&#65292;&#24182;&#20026;&#20855;&#26377;&#20132;&#20114;&#21151;&#33021;&#21442;&#25968;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65289;&#30340;&#31934;&#23494;&#27169;&#22411;&#30041;&#19979;&#20102;&#19968;&#20010;&#23398;&#26415;&#31354;&#30333;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24191;&#27867;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#32479;&#19968;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#20272;&#35745;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#31181;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14676v1 Announce Type: cross  Abstract: Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a u
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14668</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#65306;&#25104;&#20154;&#35782;&#23383;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting Learning Performance with Large Language Models: A Study in Adult Literacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 &#20844;&#21578;&#31867;&#21035;&#65306;&#36328;&#39046;&#22495; &#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#26174;&#33879;&#22686;&#24378;&#20102;&#25104;&#20154;&#35782;&#23383;&#22521;&#35757;&#65292;&#36825;&#26159;&#31038;&#20250;&#21442;&#19982;&#12289;&#23601;&#19994;&#26426;&#20250;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#32423;AI&#27169;&#22411;&#65288;&#21253;&#25324;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20102;LLMs&#22522;&#20110;&#20854;&#20869;&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#33021;&#21147;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#30340;&#28508;&#21147;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;ITS AutoTutor&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#35780;&#20272;&#20102;GPT-4&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#23637;&#29616;&#20986;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#30693;&#35782;&#36319;&#36394;&#12289;&#34920;&#29616;&#22240;&#32032;&#20998;&#26512;&#12289;&#31232;&#30095;&#22240;&#32032;&#20998;&#26512;&#65289;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 Announce Type: cross  Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Fact
&lt;/p&gt;</description></item><item><title>&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#27809;&#26377;&#23436;&#20840;&#34987;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#25429;&#25417;&#21040;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#38750;&#27954;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2403.14662</link><description>&lt;p&gt;
&#38750;&#27954;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#21457;&#23637;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies of AI Policy Development in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14662
&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#27809;&#26377;&#23436;&#20840;&#34987;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#25429;&#25417;&#21040;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#38750;&#27954;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14662v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38656;&#35201;&#26032;&#30340;&#35780;&#20272;&#26041;&#24335;&#65292;&#20197;&#35780;&#20272;&#38750;&#27954;&#22269;&#23478;&#22312;&#22269;&#23478;&#25216;&#26415;&#20351;&#29992;&#21644;&#25112;&#30053;&#26041;&#38754;&#30340;&#20934;&#22791;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#38024;&#23545;&#19968;&#33324;&#25968;&#23383;&#37319;&#32435;&#21644;&#29305;&#23450;AI&#25919;&#31574;&#30340;&#8220;&#20934;&#22791;&#24773;&#20917;&#8221;&#35780;&#20272;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#20840;&#29699;&#20934;&#22791;&#24773;&#20917;&#35780;&#20272;&#24182;&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;&#38750;&#27954;&#22269;&#23478;&#22312;AI&#20934;&#22791;&#26041;&#38754;&#30340;&#36827;&#27493;&#65292;&#24182;&#20026;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35780;&#20272;&#26469;&#36866;&#24212;&#38750;&#27954;&#32972;&#26223;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#32771;&#34385;&#36825;&#20123;&#25351;&#26631;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#19982;&#38750;&#27954;&#32972;&#26223;&#30456;&#21563;&#21512;&#65292;&#20197;&#21450;&#36825;&#20123;&#25351;&#26631;&#22312;&#25429;&#25417;&#38750;&#27954;&#22269;&#23478;&#22312;&#23454;&#29616;AI&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#38469;&#24037;&#20316;&#26041;&#38754;&#25152;&#36951;&#28431;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#23545;&#38750;&#27954;&#22235;&#20010;&#22320;&#29702;&#21644;&#32463;&#27982;&#23610;&#24230;&#19981;&#21516;&#30340;&#22269;&#23478;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#29699;&#35780;&#20272;&#36951;&#28431;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#25913;&#21892;&#22269;&#23478;&#30340;AI&#20934;&#22791;&#26631;&#20934;&#20197;&#21450;&#22914;&#20309;&#20351;&#31038;&#20250;&#33021;&#22815;&#33719;&#30410;&#20110;AI&#30340;&#39640;&#23618;&#25919;&#31574;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14662v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) requires new ways of evaluating national technology use and strategy for African nations. We conduct a survey of existing 'readiness' assessments both for general digital adoption and for AI policy in particular. We conclude that existing global readiness assessments do not fully capture African states' progress in AI readiness and lay the groundwork for how assessments can be better used for the African context. We consider the extent to which these indicators map to the African context and what these indicators miss in capturing African states' on-the-ground work in meeting AI capability. Through case studies of four African nations of diverse geographic and economic dimensions, we identify nuances missed by global assessments and offer high-level policy considerations for how states can best improve their AI readiness standards and prepare their societies to capture the benefits of AI.
&lt;/p&gt;</description></item><item><title>&#22312;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#24335;&#8212;&#8212;&#32463;&#27982;&#26426;&#22120;&#20154;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#36164;&#28304;&#20248;&#21270;&#12289;&#20449;&#24687;&#20132;&#25442;&#21644;&#20132;&#20114;&#21327;&#35758;&#31561;&#26041;&#38754;&#65292;&#23454;&#29616;prosumers&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#39640;&#25928;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#26368;&#20248;&#30340;&#33021;&#28304;&#20998;&#20139;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.14660</link><description>&lt;p&gt;
&#32463;&#27982;&#26426;&#22120;&#20154;&#65306;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#30340;&#26032;&#20852;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14660
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#20114;&#32852;&#32593;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#24335;&#8212;&#8212;&#32463;&#27982;&#26426;&#22120;&#20154;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#36164;&#28304;&#20248;&#21270;&#12289;&#20449;&#24687;&#20132;&#25442;&#21644;&#20132;&#20114;&#21327;&#35758;&#31561;&#26041;&#38754;&#65292;&#23454;&#29616;prosumers&#30340;&#32463;&#27982;&#21512;&#29702;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#39640;&#25928;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#26368;&#20248;&#30340;&#33021;&#28304;&#20998;&#20139;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#20114;&#32852;&#32593;&#65288;EI&#65289;&#20316;&#20026;&#26032;&#20852;&#30340;&#20849;&#20139;&#32463;&#27982;&#24179;&#21488;&#65292;&#21033;&#29992;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20026;&#26234;&#33021;&#22478;&#24066;&#25552;&#20379;&#28789;&#27963;&#30340;&#26412;&#22320;&#33021;&#28304;&#20379;&#24212;&#12290;EI&#26088;&#22312;&#35299;&#38145;&#22312;&#26234;&#33021;&#22478;&#24066;&#20013;&#22522;&#20110;&#23627;&#39030;&#20809;&#20239;&#26495;&#12289;&#36710;&#32852;&#32593;&#25216;&#26415;&#12289;&#20998;&#32452;&#33021;&#28304;&#31649;&#29702;&#31561;&#30340;&#26412;&#22320;&#33021;&#28304;&#24066;&#22330;&#20013;&#65292;prosumers&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#21644;&#20998;&#20139;&#65292;prosumers&#21487;&#20197;&#22312;&#27492;&#20043;&#38388;&#28789;&#27963;&#20999;&#25442;&#25552;&#20379;&#32773;&#21644;&#28040;&#36153;&#32773;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;prosumers&#25972;&#21512;&#21040;EI&#20013;&#23558;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#36164;&#28304;&#20248;&#21270;&#12289;&#20449;&#24687;&#20132;&#25442;&#21644;&#20132;&#20114;&#21327;&#35758;&#31561;&#26041;&#38754;&#25552;&#20379;&#22522;&#20110;AI/IoT&#30340;&#39640;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#39640;&#25928;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#26368;&#20248;&#30340;&#33021;&#28304;&#20998;&#20139;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14660v1 Announce Type: cross  Abstract: Energy Internet (EI) is emerging as new share economy platform for flexible local energy supplies in smart cities. Empowered by the Internet-of-Things (IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy trading and sharing among prosumers, who can adeptly switch roles between providers and consumers in localized energy markets with rooftop photovoltaic panels, vehicle-to-everything technologies, packetized energy management, etc. The integration of prosumers in EI, however, will encounter many challenges in modelling, analyzing, and designing an efficient, economic, and social-optimal platform for energy sharing, calling for advanced AI/IoT-based solutions to resource optimization, information exchange, and interaction protocols in the context of the share economy. In this study, we aim to introduce a recently emerged paradigm, Machina Economicus, to investigate the economic rationality in modelling, analysis,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Social AI Data Infrastructure&#30340;&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#24037;&#20316;&#20197;&#21450;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.14659</link><description>&lt;p&gt;
&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65306;&#26500;&#24314;&#29616;&#29366;&#21644;&#24341;&#39046;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Social AI Data Infrastructure&#30340;&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#24037;&#20316;&#20197;&#21450;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#20154;&#31867;&#31038;&#20250;&#29983;&#27963;&#20013;&#65292;&#36825;&#20123;&#25216;&#26415;&#23558;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#31038;&#20250;&#26234;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#34913;&#37327;&#31038;&#20250;&#26234;&#33021;&#30340;&#23396;&#31435;&#32500;&#24230;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#20219;&#20309;&#20316;&#21697;&#26469;&#23558;&#36825;&#20123;&#32447;&#32034;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#30740;&#31350;&#32773;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#30740;&#31350;&#31354;&#30333;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20957;&#32858;&#23376;&#39046;&#22495;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31038;&#20132;AI&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;&#21253;&#21547;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#24037;&#20316;&#65292;&#21516;&#26102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#22312;&#24110;&#21161;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#23545;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#28508;&#22312;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14659v1 Announce Type: cross  Abstract: As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#20856;&#22411;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#22914;&#20309;&#24341;&#23548;&#21019;&#36896;&#20986;&#31181;&#26063;&#21270;&#25216;&#26415;&#65292;&#20197;&#21450;&#22914;&#20309;&#26356;&#22909;&#29702;&#35299;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24378;&#21270;&#23545;&#40657;&#20154;&#30340;&#25490;&#26021;&#12290;</title><link>https://arxiv.org/abs/2403.14658</link><description>&lt;p&gt;
&#35782;&#21035;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20154;&#30340;&#28508;&#22312;&#36827;&#36335;
&lt;/p&gt;
&lt;p&gt;
Identifying Potential Inlets of Man in the Artificial Intelligence Development Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#20856;&#22411;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#22914;&#20309;&#24341;&#23548;&#21019;&#36896;&#20986;&#31181;&#26063;&#21270;&#25216;&#26415;&#65292;&#20197;&#21450;&#22914;&#20309;&#26356;&#22909;&#29702;&#35299;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24378;&#21270;&#23545;&#40657;&#20154;&#30340;&#25490;&#26021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#23450;&#20856;&#22411;&#25110;&#26631;&#20934;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#22914;&#20309;&#40723;&#21169;&#25110;&#20419;&#25104;&#31181;&#26063;&#21270;&#25216;&#26415;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#39318;&#20808;&#35201;&#29702;&#35299;&#35199;&#23572;&#32500;&#23045;&#183;&#28201;&#29305;&#65288;Sylvia Wynter&#65289;&#23545;&#29983;&#29289;&#20013;&#24515;&#20154;&#31867;&#31867;&#22411;&#21450;&#20854;&#23558;&#40657;&#20154;&#25490;&#38500;&#22312;&#20154;&#31867;&#20043;&#22806;&#30340;&#23450;&#20041;&#12290;&#28982;&#21518;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#35748;&#20026;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#30340;&#20856;&#22411;&#27493;&#39588;&#65292;&#23558;&#20854;&#20998;&#20026;6&#20010;&#38454;&#27573;&#65306;&#30830;&#23450;&#38382;&#39064;&#12289;&#24320;&#21457;&#36807;&#31243;&#21644;&#31649;&#29702;&#24037;&#20855;&#36873;&#25321;&#12289;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#39118;&#38505;&#35780;&#20272;&#12289;&#20197;&#21450;&#38598;&#25104;&#21644;&#30417;&#25511;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#26356;&#22909;&#22320;&#29702;&#35299;&#28201;&#29305;&#30340;&#29983;&#29289;&#20013;&#24515;&#20154;&#26159;&#22914;&#20309;&#34987;&#25105;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#21629;&#21608;&#26399;&#20013;&#29983;&#20135;&#30340;&#25216;&#26415;&#21450;&#29983;&#21629;&#21608;&#26399;&#26412;&#36523;&#25152;&#20195;&#34920;&#21644;&#24378;&#21270;&#30340;&#65307;&#25105;&#20204;&#24076;&#26395;&#30830;&#23450;&#36890;&#36807;&#21306;&#20998;&#40657;&#20154;&#19982;&#8220;&#29702;&#24819;&#8221;&#20154;&#31867;&#23548;&#33268;&#27704;&#32493;&#24615;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14658v1 Announce Type: cross  Abstract: In this paper we hope to identify how the typical or standard artificial intelligence development process encourages or facilitates the creation of racialized technologies. We begin by understanding Sylvia Wynter's definition of the biocentric Man genre and its exclusion of Blackness from humanness. We follow this with outlining what we consider to be the typical steps for developing an AI-based technology, which we have broken down into 6 stages: identifying a problem, development process and management tool selection, dataset development and data processing, model development, deployment and risk assessment, and integration and monitoring. The goal of this paper is to better understand how Wynter's biocentric Man is being represented and reinforced by the technologies we are producing in the AI lifecycle and by the lifecycle itself; we hope to identify ways in which the distinction of Blackness from the "ideal" human leads to perpetu
&lt;/p&gt;</description></item><item><title>MemeCraft&#26159;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#65292;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#65292;&#24102;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14652</link><description>&lt;p&gt;
MemeCraft&#65306;&#24773;&#22659;&#21644;&#31435;&#22330;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#27169;&#22240;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14652
&lt;/p&gt;
&lt;p&gt;
MemeCraft&#26159;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#65292;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#65292;&#24102;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14652v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#32447;&#27169;&#22240;&#22312;&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20316;&#20026;&#24378;&#22823;&#30340;&#25968;&#23383;&#25991;&#21270;&#20135;&#29289;&#23853;&#38706;&#22836;&#35282;&#65292;&#23427;&#20204;&#19981;&#20165;&#25552;&#20379;&#20102;&#24189;&#40664;&#65292;&#36824;&#20026;&#25919;&#27835;&#35805;&#35821;&#12289;&#31038;&#20250;&#25209;&#21028;&#21644;&#20449;&#24687;&#20256;&#25773;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;&#23427;&#20204;&#22312;&#22609;&#36896;&#22312;&#32447;&#31038;&#21306;&#24773;&#32490;&#26041;&#38754;&#30340;&#24191;&#27867;&#24433;&#21709;&#21147;&#20351;&#20854;&#25104;&#20026;&#31454;&#36873;&#21644;&#25512;&#21160;&#24847;&#35782;&#24418;&#24577;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#27169;&#22240;&#29983;&#25104;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#65292;&#20197;&#21450;&#22312;&#26377;&#25928;&#20256;&#36798;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MemeCraft&#65292;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#12290;MemeCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#23558;&#29992;&#25143;&#25552;&#31034;&#36716;&#21270;&#20026;&#24341;&#20154;&#20837;&#32988;&#30340;&#22810;&#27169;&#24577;&#27169;&#22240;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#22522;&#20110;&#23545;&#21019;&#36896;&#26377;&#20105;&#35758;&#20869;&#23481;&#30340;&#28508;&#22312;&#28389;&#29992;&#30340;&#35748;&#35782;&#65292;&#20855;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14652v1 Announce Type: cross  Abstract: Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism
&lt;/p&gt;</description></item><item><title>AI-SPRINT&#39033;&#30446;&#19987;&#27880;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#27178;&#36328;&#35745;&#31639;&#36830;&#32493;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#32500;&#25252;&#21644;&#26816;&#39564;&#20197;&#21450;&#20892;&#19994;4.0&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#31185;&#23398;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14650</link><description>&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#32500;&#25252;&#21644;&#26816;&#39564;&#20197;&#21450;&#20892;&#19994;4.0&#39046;&#22495;&#21033;&#29992;&#35745;&#31639;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14650
&lt;/p&gt;
&lt;p&gt;
AI-SPRINT&#39033;&#30446;&#19987;&#27880;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#27178;&#36328;&#35745;&#31639;&#36830;&#32493;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#32500;&#25252;&#21644;&#26816;&#39564;&#20197;&#21450;&#20892;&#19994;4.0&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#31185;&#23398;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14650v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: AI-SPRINT&#39033;&#30446;&#20110;2021&#24180;&#21551;&#21160;&#65292;&#30001;&#27431;&#27954;&#22996;&#21592;&#20250;&#36164;&#21161;&#65292;&#19987;&#27880;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#27178;&#36328;&#35745;&#31639;&#36830;&#32493;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#36825;&#31181;&#36830;&#32493;&#24615;&#30830;&#20445;&#20102;&#26469;&#33258;&#38598;&#20013;&#24335;&#25968;&#25454;&#20013;&#24515;&#21040;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26381;&#21153;&#30340;&#21327;&#21516;&#38598;&#25104;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#21644;&#36866;&#24212;&#24615;&#30340;&#35745;&#31639;&#21644;&#24212;&#29992;&#20132;&#20184;&#12290;AI-SPRINT&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#31185;&#23398;&#36827;&#23637;&#65292;&#21253;&#25324;&#27969;&#31243;&#31616;&#21270;&#12289;&#25928;&#29575;&#25552;&#39640;&#20197;&#21450;&#22312;&#23454;&#26102;&#25805;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27491;&#22914;&#19977;&#20010;&#23454;&#38469;&#29992;&#20363;&#25152;&#35777;&#26126;&#30340;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#32500;&#25252;&#21644;&#26816;&#39564;&#20197;&#21450;&#20892;&#19994;4.0&#36825;&#20123;&#24212;&#29992;&#65292;&#31361;&#20986;&#23427;&#20204;&#30340;&#23454;&#38469;&#23454;&#26045;&#21644;&#36890;&#36807;&#38598;&#25104;AI-SPRINT&#25216;&#26415;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#24037;&#20855;&#38142;&#22914;&#20309;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#24182;&#20248;&#21270;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14650v1 Announce Type: cross  Abstract: The AI-SPRINT project, launched in 2021 and funded by the European Commission, focuses on the development and implementation of AI applications across the computing continuum. This continuum ensures the coherent integration of computational resources and services from centralized data centers to edge devices, facilitating efficient and adaptive computation and application delivery. AI-SPRINT has achieved significant scientific advances, including streamlined processes, improved efficiency, and the ability to operate in real time, as evidenced by three practical use cases. This paper provides an in-depth examination of these applications -- Personalized Healthcare, Maintenance and Inspection, and Farming 4.0 -- highlighting their practical implementation and the objectives achieved with the integration of AI-SPRINT technologies. We analyze how the proposed toolchain effectively addresses a range of challenges and refines processes, disc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Empsing&#35774;&#35745;&#30340;&#22810;&#27493;&#39588;&#34892;&#21160;&#27169;&#22411;(MSAM)&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#37319;&#29992;AI&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20840;&#38754;&#25506;&#35752;&#35774;&#35745;&#21407;&#21017;&#12289;&#26550;&#26500;&#21644;&#26410;&#26469;&#21457;&#23637;&#65292;&#20197;&#21450;&#35780;&#20272;&#20854;&#24615;&#33021;&#24182;&#23637;&#26395;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14645</link><description>&lt;p&gt;
&#20026;&#20225;&#19994;AI&#37319;&#29992;&#35774;&#35745;&#22810;&#27493;&#39588;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Designing Multi-Step Action Models for Enterprise AI Adoption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Empsing&#35774;&#35745;&#30340;&#22810;&#27493;&#39588;&#34892;&#21160;&#27169;&#22411;(MSAM)&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#37319;&#29992;AI&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20840;&#38754;&#25506;&#35752;&#35774;&#35745;&#21407;&#21017;&#12289;&#26550;&#26500;&#21644;&#26410;&#26469;&#21457;&#23637;&#65292;&#20197;&#21450;&#35780;&#20272;&#20854;&#24615;&#33021;&#24182;&#23637;&#26395;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Empsing&#35774;&#35745;&#30340;&#38381;&#28304;AI&#27169;&#22411;Multi-Step Action Model (MSAM)&#65292;&#26088;&#22312;&#35299;&#20915;&#38459;&#30861;&#20225;&#19994;&#37319;&#29992;AI&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;MSAM&#30340;&#22522;&#26412;&#21407;&#21017;&#12289;&#35774;&#35745;&#26550;&#26500;&#21644;&#26410;&#26469;&#21457;&#23637;&#36712;&#36857;&#12290;&#23427;&#36890;&#36807;&#20005;&#26684;&#30340;&#27979;&#35797;&#26041;&#27861;&#35780;&#20272;&#20102;MSAM&#30340;&#24615;&#33021;&#65292;&#24182;&#35774;&#24819;&#20102;&#20854;&#23545;&#25512;&#21160;&#32452;&#32455;&#20869;AI&#37319;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14645v1 Announce Type: cross  Abstract: This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI model designed by Empsing to address challenges hindering AI adoption in enterprises. Through a holistic examination, this paper explores MSAM's foundational principles, design architecture, and future trajectory. It evaluates MSAM's performance via rigorous testing methodologies and envisions its potential impact on advancing AI adoption within organizations.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14643</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT and its Impact on Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14643
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#23384;&#22312;&#19968;&#27573;&#26102;&#38388;&#20102;&#65292;&#20294;&#31361;&#28982;&#38388;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#24863;&#35874;&#35895;&#27468;&#12289;&#24494;&#36719;&#12289;&#20803;&#23431;&#23449;&#31561;&#31185;&#25216;&#30028;&#20027;&#35201;&#21697;&#29260;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;OpenAI&#36890;&#36807;&#20854;&#24320;&#21019;&#24615;&#21457;&#26126;ChatGPT&#35302;&#21457;&#20102;&#25353;&#38062;&#12290;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#22312;&#23545;&#35805;&#32972;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#22797;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#12290;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#19978;&#19979;&#25991;&#29983;&#25104;&#21644;&#38754;&#21521;&#24320;&#25918;&#22495;&#30340;&#35757;&#32451;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#24212;&#29992;&#20110;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#23458;&#25143;&#26381;&#21153;&#20877;&#21040;&#35821;&#35328;&#32763;&#35793;&#31561;&#24191;&#27867;&#39046;&#22495;&#12290;&#23427;&#20855;&#26377;&#24443;&#24213;&#25913;&#21464;&#21508;&#34892;&#19994;&#24182;&#36716;&#21464;&#25105;&#20204;&#19982;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;ChatGPT&#20063;&#24341;&#21457;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#21253;&#25324;&#36947;&#24503;&#26041;&#38754;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#29983;&#25104;&#24335;AI&#33021;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#20351;&#29992;AI&#21161;&#25163;Syntea&#22312;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#20013;&#24179;&#22343;&#20943;&#23569;&#20102;27%&#30340;&#23398;&#20064;&#26102;&#38388;&#65292;&#34920;&#26126;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#36890;&#36807;&#20010;&#24615;&#21270;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.14642</link><description>&lt;p&gt;
&#38761;&#26032;&#36828;&#31243;&#23398;&#20064;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#23548;&#30340;&#23398;&#20064;&#36827;&#23637;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#29983;&#25104;&#24335;AI&#33021;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#20351;&#29992;AI&#21161;&#25163;Syntea&#22312;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#20013;&#24179;&#22343;&#20943;&#23569;&#20102;27%&#30340;&#23398;&#20064;&#26102;&#38388;&#65292;&#34920;&#26126;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#36890;&#36807;&#20010;&#24615;&#21270;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Generative AI&#39044;&#35745;&#23558;&#23545;&#25945;&#32946;&#20135;&#29983;&#24040;&#22823;&#31215;&#26497;&#24433;&#21709;; &#20294;&#26159;&#65292;&#30446;&#21069;&#23578;&#26410;&#22312;&#22823;&#23398;&#23618;&#38754;&#23637;&#31034;&#36825;&#31181;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;AI&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#26159;&#21542;&#20351;&#29992;AI&#21160;&#21147;&#23398;&#36741;&#23548;&#21161;&#25163;Syntea&#24433;&#21709;&#20102;IU&#22269;&#38469;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;40&#22810;&#20010;&#35838;&#31243;&#20013;&#25968;&#30334;&#21517;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Syntea&#22312;Syntea&#21457;&#24067;&#21518;&#31532;&#19977;&#20010;&#26376;&#26174;&#33879;&#20943;&#23569;&#20102;&#20182;&#20204;&#30340;&#23398;&#20064;&#26102;&#38388;--&#24179;&#22343;&#32422;&#20943;&#23569;&#20102;27\%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#25928;&#24212;&#30340;&#24133;&#24230;&#21644;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#35777;&#26126;&#20102;&#29983;&#25104;&#24335;AI&#20316;&#20026;&#26174;&#33879;&#25913;&#36827;&#21644;&#21152;&#24555;&#23398;&#20064;&#30340;&#20851;&#38190;&#26464;&#26438;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14642v1 Announce Type: cross  Abstract: Generative AI is expected to have a vast, positive impact on education; however, at present, this potential has not yet been demonstrated at scale at university level. In this study, we present first evidence that generative AI can increase the speed of learning substantially in university students. We tested whether using the AI-powered teaching assistant Syntea affected the speed of learning of hundreds of distance learning students across more than 40 courses at the IU International University of Applied Sciences. Our analysis suggests that using Syntea reduced their study time substantially--by about 27\% on average--in the third month after the release of Syntea. Taken together, the magnitude of the effect and the scalability of the approach implicate generative AI as a key lever to significantly improve and accelerate learning by personalisation.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#12289;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35780;&#20272;&#39044;&#27979;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2403.14641</link><description>&lt;p&gt;
&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#32593;&#32476;&#23433;&#20840;&#12289;&#36879;&#26126;&#24230;&#12289;&#31283;&#20581;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;&#25506;&#35752;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14641
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#12289;&#20915;&#31574;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35780;&#20272;&#39044;&#27979;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25972;&#21512;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#20013;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;AI&#32452;&#20214;&#24341;&#20837;&#30340;&#25361;&#25112;&#20197;&#21450;&#23545;&#27979;&#35797;&#31243;&#24207;&#30340;&#24433;&#21709;&#65292;&#30528;&#37325;&#20851;&#27880;&#21487;&#20449;&#36182;AI&#30340;&#19968;&#20123;&#22522;&#26412;&#35201;&#27714;&#12290;&#35752;&#35770;&#30340;&#20027;&#39064;&#21253;&#25324;AI&#22312;AVs&#30340;&#21508;&#20010;&#25805;&#20316;&#23618;&#20013;&#30340;&#20316;&#29992;&#12289;&#27431;&#30431;AI&#27861;&#26696;&#23545;AVs&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23545;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#26032;&#27979;&#35797;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#36824;&#23601;&#32593;&#32476;&#23433;&#20840;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#12289;AI&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38656;&#27714;&#20197;&#21450;&#35780;&#20272;AVs&#20013;&#39044;&#27979;&#31995;&#32479;&#31283;&#20581;&#24615;&#21644;&#36947;&#24503;&#34892;&#20026;&#30340;&#21327;&#35758;&#25552;&#20379;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#35813;&#35770;&#25991;&#25351;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;AI&#22312;AV&#25216;&#26415;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#26410;&#26469;&#26041;&#21521;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#20102;&#36328;&#23398;&#31185;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14641v1 Announce Type: cross  Abstract: This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#23581;&#35797;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#8220;&#22949;&#21327;&#8221;&#29256;&#26412;&#65292;&#24182;&#25552;&#20986;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.14639</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20041;&#26234;&#24935;&#22478;&#24066;
&lt;/p&gt;
&lt;p&gt;
On Defining Smart Cities using Transformer Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14639
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#23581;&#35797;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#8220;&#22949;&#21327;&#8221;&#29256;&#26412;&#65292;&#24182;&#25552;&#20986;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#22478;&#24066;&#27491;&#22312;&#36805;&#36895;&#37319;&#29992;&#26234;&#33021;&#25216;&#26415;&#65292;&#25913;&#21464;&#22478;&#24066;&#29983;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#19968;&#36235;&#21183;&#65292;&#20294;&#26377;&#20851;&#8220;&#26234;&#24935;&#22478;&#24066;&#8221;&#30340;&#26222;&#36941;&#25509;&#21463;&#30340;&#23450;&#20041;&#20173;&#28982;&#38590;&#20197;&#30028;&#23450;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#24212;&#19982;&#20808;&#21069;&#21442;&#19982;&#23450;&#20041;&#36825;&#19968;&#27010;&#24565;&#30340;&#22823;&#22810;&#25968;&#19987;&#23478; resonating &#30340;&#26032;&#8220;&#22949;&#21327;&#8221;&#23450;&#20041;&#65292;&#24182;&#26088;&#22312;&#39564;&#35777;&#29616;&#26377;&#23450;&#20041;&#20043;&#19968;&#12290;&#25105;&#20204;&#20174;&#34892;&#19994;&#12289;&#23398;&#26415;&#30028;&#21644;&#21508;&#31181;&#30456;&#20851;&#32452;&#32455;&#20013;&#23457;&#26597;&#20102;60&#20010;&#26234;&#24935;&#22478;&#24066;&#30340;&#23450;&#20041;&#65292;&#37319;&#29992;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;AI&#21644;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#20197;&#36798;&#25104;&#36825;&#19968;&#22949;&#21327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#35780;&#20272;&#25216;&#26415;&#65292;&#36890;&#24120;&#21487;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#26234;&#24935;&#22478;&#24066;&#23450;&#20041;&#65292;&#35780;&#20272;&#20854;&#29420;&#29305;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;AI&#26469;&#20998;&#26512;&#21508;&#31181;&#29616;&#26377;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14639v1 Announce Type: cross  Abstract: Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#21644;&#20197;&#31038;&#20250;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;AI&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#26356;&#22909;&#22320;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#31649;&#29702;&#19981;&#20844;&#24179;&#20559;&#35265;&#21644;&#27495;&#35270;&#21487;&#33021;&#22312;&#25972;&#20010;AI&#39033;&#30446;&#24037;&#20316;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#22810;&#31181;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14636</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;AI&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI Fairness in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14636
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#21644;&#20197;&#31038;&#20250;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;AI&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#26356;&#22909;&#22320;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#31649;&#29702;&#19981;&#20844;&#24179;&#20559;&#35265;&#21644;&#27495;&#35270;&#21487;&#33021;&#22312;&#25972;&#20010;AI&#39033;&#30446;&#24037;&#20316;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#22810;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36798;&#25104;&#23545;&#20110;AI&#20844;&#24179;&#24615;&#26222;&#36941;&#25509;&#21463;&#30340;&#23450;&#20041;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#27835;&#29702;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#31038;&#20250;&#20013;&#65292;&#20851;&#20110;&#20844;&#24179;&#27010;&#24565;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#26368;&#22909;&#23454;&#36341;&#30340;&#35266;&#28857;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25506;&#32034;&#22522;&#20110;&#19978;&#19979;&#25991;&#21644;&#20197;&#31038;&#20250;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;AI&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#26356;&#22909;&#22320;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#31649;&#29702;&#19981;&#20844;&#24179;&#20559;&#35265;&#21644;&#27495;&#35270;&#21487;&#33021;&#22312;&#25972;&#20010;AI&#39033;&#30446;&#24037;&#20316;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#22810;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14636v1 Announce Type: cross  Abstract: Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow.   We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#21487;&#25345;&#32493;&#24615;&#27010;&#24565;&#21450;&#24037;&#20855;&#65292;&#26088;&#22312;&#25351;&#23548;AI&#39033;&#30446;&#22242;&#38431;&#35780;&#20272;&#31038;&#20250;&#24433;&#21709;&#21644;&#20262;&#29702;&#21487;&#23481;&#24525;&#24615;&#65292;&#25552;&#20986;&#20102;&#20419;&#36827;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#30340;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14635</link><description>&lt;p&gt;
AI&#21487;&#25345;&#32493;&#24615;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992; &#31532;&#19968;&#37096;&#20998;&#65306;&#21487;&#25345;&#32493;AI&#39033;&#30446;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#21487;&#25345;&#32493;&#24615;&#27010;&#24565;&#21450;&#24037;&#20855;&#65292;&#26088;&#22312;&#25351;&#23548;AI&#39033;&#30446;&#22242;&#38431;&#35780;&#20272;&#31038;&#20250;&#24433;&#21709;&#21644;&#20262;&#29702;&#21487;&#23481;&#24525;&#24615;&#65292;&#25552;&#20986;&#20102;&#20419;&#36827;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25345;&#32493;&#30340;AI&#39033;&#30446;&#25345;&#32493;&#20851;&#27880;&#35774;&#35745;&#12289;&#24320;&#21457;&#21644;&#37096;&#32626;AI&#25216;&#26415;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#20135;&#29983;&#30340;&#36716;&#21464;&#25928;&#24212;&#20197;&#21450;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#24433;&#21709;&#12290;&#30528;&#37325;&#20110;AI&#21487;&#25345;&#32493;&#24615;&#30340;&#39033;&#30446;&#30830;&#20445;&#20215;&#20540;&#23548;&#21521;&#12289;&#21327;&#20316;&#21644;&#39044;&#35265;&#24615;&#21453;&#24605;&#24341;&#23548;&#35780;&#20272;&#28508;&#22312;&#31038;&#20250;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#24182;&#24341;&#23548;&#36127;&#36131;&#20219;&#30340;&#21019;&#26032;&#23454;&#36341;&#12290;&#35813;&#24037;&#20316;&#25163;&#20876;&#26159;&#25104;&#23545;&#25552;&#20379;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#23558;AI&#21487;&#25345;&#32493;&#24615;&#20184;&#35832;&#23454;&#36341;&#25152;&#38656;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#12290;&#23427;&#20171;&#32461;&#20102;SUM&#20215;&#20540;&#65292;&#24110;&#21161;AI&#39033;&#30446;&#22242;&#38431;&#35780;&#20272;&#20854;&#39033;&#30446;&#30340;&#28508;&#22312;&#31038;&#20250;&#24433;&#21709;&#21644;&#20262;&#29702;&#21487;&#23481;&#24525;&#24615;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#27969;&#31243;(SEP)&#65292;&#25552;&#20379;&#24037;&#20855;&#20419;&#36827;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#27604;&#20363;&#21442;&#19982;&#21644;&#24847;&#35265;&#65292;&#24378;&#35843;&#24179;&#31561;&#21644;&#26377;&#24847;&#20041;&#30340;&#21442;&#19982;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14635v1 Announce Type: cross  Abstract: Sustainable AI projects are continuously responsive to the transformative effects as well as short-, medium-, and long-term impacts on individuals and society that the design, development, and deployment of AI technologies may have. Projects, which centre AI Sustainability, ensure that values-led, collaborative, and anticipatory reflection both guides the assessment of potential social and ethical impacts and steers responsible innovation practices.   This workbook is the first part of a pair that provides the concepts and tools needed to put AI Sustainability into practice. It introduces the SUM Values, which help AI project teams to assess the potential societal impacts and ethical permissibility of their projects. It then presents a Stakeholder Engagement Process (SEP), which provides tools to facilitate proportionate engagement of and input from stakeholders with an emphasis on equitable and meaningful participation and positionali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14617</link><description>&lt;p&gt;
Videoshop&#65306;&#20855;&#26377;&#22122;&#22768;&#22806;&#25512;&#25193;&#25955;&#21453;&#28436;&#30340;&#26412;&#22320;&#21270;&#35821;&#20041;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14617
&lt;/p&gt;
&lt;p&gt;
Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Videoshop&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#29992;&#20110;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#12290;Videoshop&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20219;&#20309;&#32534;&#36753;&#36719;&#20214;&#65292;&#21253;&#25324;Photoshop&#21644;&#29983;&#25104;&#22635;&#20805;&#65292;&#20462;&#25913;&#31532;&#19968;&#24103;&#65307;&#23427;&#20250;&#33258;&#21160;&#23558;&#36825;&#20123;&#26356;&#25913;&#20256;&#25773;&#21040;&#20854;&#20313;&#24103;&#65292;&#20445;&#25345;&#35821;&#20041;&#12289;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#32534;&#36753;&#19981;&#21516;&#65292;Videoshop&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#25110;&#21024;&#38500;&#23545;&#35937;&#65292;&#35821;&#20041;&#19978;&#26356;&#25913;&#23545;&#35937;&#65292;&#23558;&#32032;&#26448;&#29031;&#29255;&#25554;&#20837;&#35270;&#39057;&#31561;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#22806;&#35266;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#20540;&#36827;&#34892;&#22122;&#22768;&#22806;&#25512;&#21453;&#28436;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#39057;&#32534;&#36753;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20174;&#20013;&#25105;&#20204;&#29983;&#25104;&#26681;&#25454;&#32534;&#36753;&#22270;&#20687;&#35843;&#25972;&#30340;&#35270;&#39057;&#12290;Videoshop&#22312;2&#20010;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;10&#20010;&#35780;&#20272;&#25351;&#26631;&#23545;6&#20010;&#22522;&#32447;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
&lt;/p&gt;</description></item><item><title>AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.14468</link><description>&lt;p&gt;
AnyV2V&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14468
&lt;/p&gt;
&lt;p&gt;
AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#28041;&#21450;&#32534;&#36753;&#28304;&#35270;&#39057;&#20197;&#21450;&#39069;&#22806;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#25991;&#26412;&#25552;&#31034;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#65289;&#65292;&#20197;&#29983;&#25104;&#19982;&#28304;&#35270;&#39057;&#21644;&#25552;&#20379;&#30340;&#25511;&#21046;&#30456;&#21305;&#37197;&#30340;&#26032;&#35270;&#39057;&#12290;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#28385;&#36275;&#24191;&#27867;&#29992;&#25143;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AnyV2V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20813;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#35270;&#39057;&#32534;&#36753;&#31616;&#21270;&#20026;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65288;&#20363;&#22914;InstructPix2Pix&#12289;InstantID&#31561;&#65289;&#20462;&#25913;&#31532;&#19968;&#24103;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;I2VGen-XL&#65289;&#36827;&#34892;DDIM&#36870;&#36716;&#21644;&#29305;&#24449;&#27880;&#20837;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;AnyV2V&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#26377;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;AnyV2V&#36824;&#21487;&#20197;&#25903;&#25345;&#26032;&#39062;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#21253;&#25324;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;&#19982;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#38024;&#23545;&#25193;&#25955;&#20998;&#21106;&#30340;&#26550;&#26500;&#25913;&#36827;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.14440</link><description>&lt;p&gt;
&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Analysing Diffusion Segmentation for Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#25193;&#25955;&#20998;&#21106;&#19982;&#25193;&#25955;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#38024;&#23545;&#25193;&#25955;&#20998;&#21106;&#30340;&#26550;&#26500;&#25913;&#36827;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20854;&#33021;&#22815;&#25552;&#20379;&#27010;&#29575;&#24314;&#27169;&#21644;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#24615;&#21551;&#21457;&#20102;&#23427;&#20204;&#34987;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#65292;&#27169;&#22411;&#30340;&#22810;&#27425;&#39044;&#27979;&#21487;&#20197;&#20135;&#29983;&#20998;&#21106;&#32467;&#26524;&#65292;&#19981;&#20165;&#36136;&#37327;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#27169;&#22411;&#26412;&#36136;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#20998;&#21106;&#24615;&#33021;&#30340;&#24378;&#22823;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23545;&#25193;&#25955;&#20998;&#21106;&#21644;&#22270;&#20687;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#32570;&#20047;&#20998;&#26512;&#21644;&#35752;&#35770;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#36825;&#20123;&#26550;&#26500;&#25552;&#20379;&#30340;&#25913;&#36827;&#22312;&#20998;&#21106;&#39046;&#22495;&#19982;&#22312;&#29305;&#23450;&#20110;&#25193;&#25955;&#20998;&#21106;&#30340;&#30410;&#22788;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14440v1 Announce Type: cross  Abstract: Denoising Diffusion Probabilistic models have become increasingly popular due to their ability to offer probabilistic modeling and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a partic
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#21644;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.13969</link><description>&lt;p&gt;
&#36825;&#24182;&#38750;&#19968;&#20010;&#25968;&#25454;&#38382;&#39064;&#65306;&#31639;&#27861;&#19982;&#26435;&#21147;&#22312;&#21152;&#25343;&#22823;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
"This is not a data problem": Algorithms and Power in Public Higher Education in Canada
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13969
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#21644;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20915;&#31574;&#22312;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#26085;&#30410;&#34987;&#37319;&#29992;&#12290;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#25968;&#25454;&#39537;&#21160;&#23454;&#36341;&#25193;&#23637;&#19982;&#26032;&#20844;&#20849;&#31649;&#29702;&#26041;&#27861;&#22312;&#26032;&#33258;&#30001;&#20027;&#20041;&#25919;&#24220;&#30340;&#25512;&#21160;&#19979;&#21516;&#27493;&#36827;&#34892;&#12290;&#26412;&#30740;&#31350;&#23545;&#21152;&#25343;&#22823;&#23433;&#22823;&#30053;&#30465;&#19968;&#20010;&#20844;&#31435;&#23398;&#38498;&#25968;&#25454;&#21644;&#31639;&#27861;&#30340;&#28145;&#24230;&#27665;&#26063;&#24535;&#26696;&#20363;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23398;&#38498;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23398;&#38498;&#30340;&#27969;&#31243;&#21644;&#20851;&#31995;&#22914;&#20309;&#25903;&#25345;&#36825;&#20123;&#32467;&#26524;&#65292;&#20197;&#21450;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#23398;&#38498;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#30340;&#30475;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26085;&#30410;&#20381;&#36182;&#31639;&#27861;&#20915;&#31574;&#23548;&#33268;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#65292;&#29616;&#26377;&#19981;&#24179;&#31561;&#21152;&#21095;&#65292;&#24182;&#23548;&#33268;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#31639;&#27861;&#20915;&#31574;&#24310;&#32493;&#30340;&#22686;&#21152;&#21046;&#24230;&#26435;&#21147;&#30340;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13969v1 Announce Type: cross  Abstract: Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13869</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#31232;&#26377;&#20107;&#20214;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#23545;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#26500;&#25104;&#20102;&#37325;&#22823;&#28508;&#22312;&#23041;&#32961;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#20174;&#24403;&#21069;&#29366;&#24577;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#19968;&#20010;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#37325;&#35201;&#24615;&#8221;&#30340;&#25351;&#26631;&#12290;&#39044;&#27979;&#37325;&#35201;&#24615;&#30340;&#22797;&#26434;&#24615;&#28304;&#33258;&#20110;&#26497;&#31471;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#36825;&#26159;&#30001;&#39640;&#32500;&#21464;&#37327;&#20013;&#19982;&#32597;&#35265;&#20107;&#20214;&#30456;&#20851;&#32852;&#24341;&#36215;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32597;&#35265;&#24615;&#35781;&#21650;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#20040;&#36807;&#20110;&#20445;&#23432;&#65292;&#35201;&#20040;&#23481;&#26131;&#24573;&#35270;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#65292;&#22240;&#27492;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#37325;&#35201;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>S2DM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Sector-Shaped Diffusion Model&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#35821;&#20041;&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#19968;&#32452;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#29305;&#24449;&#19978;&#21464;&#21270;&#65292;&#22312;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.13408</link><description>&lt;p&gt;
S2DM&#65306;&#38754;&#21521;&#35270;&#39057;&#29983;&#25104;&#30340;&#25159;&#24418;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S2DM: Sector-Shaped Diffusion Models for Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13408
&lt;/p&gt;
&lt;p&gt;
S2DM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Sector-Shaped Diffusion Model&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#35821;&#20041;&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#19968;&#32452;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#29305;&#24449;&#19978;&#21464;&#21270;&#65292;&#22312;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#19968;&#24605;&#24819;&#24212;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#26102;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20445;&#25345;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21644;&#36830;&#32493;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#23558;&#35270;&#39057;&#24103;&#19982;&#26399;&#26395;&#30340;&#26102;&#38388;&#29305;&#24449;&#23545;&#40784;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#38543;&#26426;&#29305;&#24449;&#25152;&#33268;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Sector-Shaped Diffusion Model&#65288;S2DM&#65289;&#65292;&#20854;&#25159;&#24418;&#25193;&#25955;&#21306;&#22495;&#30001;&#19968;&#32452;&#20197;&#30456;&#21516;&#22122;&#22768;&#28857;&#20026;&#36215;&#28857;&#30340;&#23556;&#32447;&#29366;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#24418;&#25104;&#12290;S2DM&#33021;&#22815;&#29983;&#25104;&#19968;&#32452;&#22312;&#35821;&#20041;&#21644;&#38543;&#26426;&#29305;&#24449;&#19978;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#20869;&#22312;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#36866;&#24403;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#22312;&#26102;&#38388;&#29305;&#24449;&#19978;&#21464;&#21270;&#12290;&#25105;&#20204;&#23558;S2DM&#24212;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#20809;&#27969;&#20316;&#20026;&#26102;&#38388;&#26465;&#20214;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;S2DM&#20248;&#20110;&#35768;&#22810;&#24050;&#23384;&#22312;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13408v2 Announce Type: replace-cross  Abstract: Diffusion models have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many exis
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13362</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#36134;&#25143;&#28608;&#21169;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#28040;&#36153;
&lt;/p&gt;
&lt;p&gt;
Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#12289;&#20449;&#20219;&#19979;&#38477;&#20197;&#21450;&#23545;&#27665;&#20027;&#35268;&#33539;&#25903;&#25345;&#21160;&#25671;&#26159;&#32654;&#22269;&#27665;&#20027;&#38754;&#20020;&#30340;&#32039;&#36843;&#23041;&#32961;&#12290;&#25509;&#35302;&#39564;&#35777;&#21644;&#20248;&#36136;&#26032;&#38395;&#21487;&#33021;&#38477;&#20302;&#20010;&#20154;&#23545;&#36825;&#20123;&#23041;&#32961;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#20351;&#20844;&#27665;&#26356;&#20855;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#12289;&#27665;&#31929;&#20027;&#20041;&#21644;&#26497;&#31471;&#20826;&#27966;&#35328;&#35770;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#29615;&#22659;&#20013;&#22686;&#24378;&#29992;&#25143;&#25509;&#35302;&#21644;&#21442;&#19982;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#23545; 28,457 &#20010; Twitter &#29992;&#25143;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20026;&#26399;&#20004;&#21608;&#30340;&#30000;&#37326;&#23454;&#39564;&#65288;&#20174; 2023 &#24180; 1 &#26376; 19 &#26085;&#21040; 2 &#26376; 3 &#26085;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102; 28 &#20010;&#21033;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#65292;&#22312;&#29992;&#25143;&#21457;&#34920;&#26377;&#20851;&#20307;&#32946;&#12289;&#23089;&#20048;&#25110;&#29983;&#27963;&#26041;&#24335;&#30340;&#25512;&#25991;&#26102;&#22238;&#22797;&#19968;&#20010;&#20869;&#23481;&#30456;&#20851;&#30340;&#22238;&#22797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#30828;&#20195;&#30721;&#20803;&#32032;&#65306;&#19968;&#20010;&#25351;&#21521;&#20248;&#36136;&#26032;&#38395;&#26426;&#26500;&#30456;&#20851;&#20027;&#39064;&#37096;&#20998;&#30340; URL &#21644;&#40723;&#21169;&#20851;&#27880;&#20854; Twitter &#36134;&#25143;&#12290;&#20026;&#36827;&#19968;&#27493;&#27979;&#35797;&#26426;&#22120;&#20154;&#23545;&#24615;&#21035;&#30340;&#24046;&#24322;&#24433;&#21709;&#65292;&#34987;&#35797;&#29992;&#25143;&#34987;&#38543;&#26426;&#20998;&#37197;&#20197;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.12211</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#32570;&#22833;&#39044;&#27979;&#30340;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#35760;&#24405;&#36890;&#24120;&#30001;&#19981;&#21516;&#30340;&#27169;&#24577;&#32452;&#25104;&#65292;&#22914;&#22270;&#29255;&#12289;&#25991;&#26412;&#21644;&#34920;&#26684;&#20449;&#24687;&#12290;&#25972;&#21512;&#25152;&#26377;&#27169;&#24577;&#21487;&#20197;&#25552;&#20379;&#24739;&#32773;&#29366;&#20917;&#30340;&#20840;&#38754;&#35270;&#22270;&#65292;&#32780;&#32437;&#21521;&#20998;&#26512;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#30142;&#30149;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#32437;&#21521;&#22810;&#27169;&#24577;&#22810;&#35270;&#22270;&#65288;MMMV&#65289;&#25968;&#25454;&#32570;&#22833;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#25968;&#37327;&#30340;&#26102;&#38388;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#26088;&#22312;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#65292;&#26080;&#35770;&#20854;&#26159;&#21542;&#23436;&#25972;&#12290;&#25105;&#20204;&#22312;&#39592;&#20851;&#33410;&#28814;&#20513;&#35758;&#65288;OAI&#65289;&#30340;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;&#30140;&#30171;&#21644;Kellgren-Lawrence&#20998;&#32423;&#65288;KLG&#65289;&#22312;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#39044;&#27979;&#30340;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12211v1 Announce Type: cross  Abstract: Medical records often consist of different modalities, such as images, text, and tabular information. Integrating all modalities offers a holistic view of a patient's condition, while analyzing them longitudinally provides a better understanding of disease progression. However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period. In this work, we introduce a unified model for longitudinal multi-modal multi-view (MMMV) prediction with missingness. Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability. We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint. We d
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.11879</link><description>&lt;p&gt;
&#21333;&#27169;&#24577;&#22810;&#20219;&#21153;&#34701;&#21512;&#29992;&#20110;&#24773;&#24863;&#27169;&#20223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31532;&#20845;&#23626;&#25143;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#36827;&#34892;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Wav2Vec 2.0&#26694;&#26550;&#65292;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#25773;&#23458;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#28085;&#30422;&#35821;&#35328;&#21644;&#35821;&#22806;&#20803;&#32032;&#30340;&#24191;&#27867;&#38899;&#39057;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#34701;&#21512;&#25216;&#26415;&#22686;&#24378;&#20102;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#25216;&#26415;&#23558;&#20010;&#20307;&#29305;&#24449;&#19982;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21040;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;Wav2Vec 2.0&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;valence-arousal-dominance&#65288;VAD&#65289;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#37319;&#29992;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#23545;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#26102;&#38388;&#20998;&#26512;&#12290;&#20165;&#21033;&#29992;&#25152;&#25552;&#20379;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11220</link><description>&lt;p&gt;
CPA-Enhancer&#65306;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#19979;&#30446;&#26631;&#26816;&#27979;&#30340;&#38142;&#24335;&#24605;&#32500;&#39537;&#21160;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#26377;&#25928;&#25552;&#21319;&#21463;&#25439;&#22270;&#20687;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#21333;&#19968;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#30830;&#23450;&#36864;&#21270;&#31867;&#22411;&#65292;&#24182;&#20026;&#27599;&#31181;&#31867;&#22411;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#22686;&#24378;&#22120;CPA-Enhancer&#65292;&#29992;&#20110;&#26410;&#30693;&#36864;&#21270;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CPA-Enhancer&#22312;CoT&#25552;&#31034;&#30340;&#36880;&#27493;&#25351;&#23548;&#19979;&#36880;&#27493;&#35843;&#25972;&#20854;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#25552;&#31034;&#32534;&#30721;&#20102;&#19982;&#36864;&#21270;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#21033;&#29992;CoT&#25552;&#31034;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CPA-Enhancer&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#22686;&#24378;&#27169;&#22411;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#36890;&#29992;&#26816;&#27979;&#22120;&#20013;&#65292;&#22312;&#19981;&#20107;&#20808;&#30693;&#36947;&#36864;&#21270;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21463;&#25439;&#22270;&#20687;&#19978;&#23454;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPA-E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#22312;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#39033;&#24320;&#21019;&#24615;&#21457;&#23637;&#65292;&#33021;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.10482</link><description>&lt;p&gt;
GPT4&#21160;&#21147;AI&#20195;&#29702;&#33021;&#25104;&#20026;&#36275;&#22815;&#20248;&#31168;&#30340;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#24072;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10482
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#22312;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#39033;&#24320;&#21019;&#24615;&#21457;&#23637;&#65292;&#33021;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#34987;&#23450;&#20041;&#20026;&#35299;&#37322;&#25237;&#36164;&#32452;&#21512;&#30456;&#23545;&#20110;&#22522;&#20934;&#30340;&#36229;&#39069;&#32489;&#25928;&#39537;&#21160;&#22240;&#32032;&#30340;&#36807;&#31243;&#65292;&#22312;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#65292;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#37329;&#31649;&#29702;&#34892;&#19994;&#12290;&#26681;&#26893;&#20110;&#29282;&#22266;&#30340;&#37329;&#34701;&#21644;&#25968;&#23398;&#26694;&#26550;&#20013;&#65292;&#36825;&#31181;&#20998;&#26512;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#21644;&#26041;&#27861;&#23398;&#24050;&#22312;&#20247;&#22810;&#23398;&#26415;&#30740;&#31350;&#35770;&#25991;&#21644;&#33879;&#20316;&#20013;&#24471;&#21040;&#24191;&#27867;&#35760;&#24405;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;AI&#20195;&#29702;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#12290;&#36825;&#20123;&#20195;&#29702;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#35745;&#31639;&#21644;&#20998;&#26512;&#25237;&#36164;&#32452;&#21512;&#34920;&#29616;&#19982;&#22522;&#20934;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#32489;&#25928;&#24402;&#22240;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;AI&#20195;&#29702;&#24212;&#29992;&#20110;&#21508;&#31181;&#37325;&#35201;&#30340;&#32489;&#25928;&#24402;&#22240;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10482v1 Announce Type: cross  Abstract: Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks. In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09920</link><description>&lt;p&gt;
&#39044;&#27979;AI&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Generalization of AI Colonoscopy Models to Unseen Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09920
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#35782;&#21035;&#26032;&#29616;&#35937;&#65292;&#24182;&#39044;&#27979;&#32467;&#32928;&#38236;&#27169;&#22411;&#23545;&#26410;&#30693;&#25216;&#26415;&#21644;&#19981;&#21516;&#22269;&#23478;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#21644;&#30446;&#26631; AI&#32467;&#32928;&#38236;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#25216;&#26415;&#38656;&#35201;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;Masked Siamese Network&#8221;&#65288;MSN&#65289;&#22312;&#26410;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#26032;&#29616;&#35937;&#24182;&#39044;&#27979;&#24687;&#32905;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;MSN&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#24687;&#32905;&#22270;&#20687;&#20013;&#34987;&#23631;&#34109;&#30340;&#21306;&#22495;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;MSN&#20165;&#22312;&#20197;&#33394;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#26085;&#26412;&#32467;&#32928;&#38236;&#65288;354&#20010;&#35270;&#39057;&#65292;128&#23567;&#26102;&#65289;&#19978;&#26816;&#27979;&#26410;&#30693;&#25216;&#26415;&#65306;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#21644;&#33394;&#32454;&#32990;&#20869;&#38236;&#65288;CE&#65289;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;MSN&#39044;&#27979;&#36328;&#22269;&#32467;&#32928;&#38236;&#35270;&#39057;&#19978;&#30340;&#24687;&#32905;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;&#65288;CADe&#65289;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;MSN&#26410;&#25509;&#21463;&#36807;&#26469;&#33258;&#26085;&#26412;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09920v1 Announce Type: cross  Abstract: Background and aims Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels.   Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan.   Results MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-t
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09530</link><description>&lt;p&gt;
VisionGPT-3D:&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21521;&#35270;&#35273;&#32452;&#20214;&#30340;&#28436;&#36827;&#20419;&#36827;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#65292;&#20363;&#22914;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#24182;&#35782;&#21035;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#20803;&#32032;&#12290;&#20197;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#19987;&#27880;&#20110;&#22522;&#20110;&#26126;&#30830;&#23450;&#20041;&#23545;&#35937;&#30340;&#22270;&#20687;&#26816;&#27979;&#12289;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35270;&#35273;&#23545;&#35937;&#65292;&#20026;&#25991;&#26412;&#32972;&#26223;&#25552;&#20379;&#20102;&#35270;&#35273;&#24067;&#23616;&#12290;OpenAI GPT-4&#24050;&#25104;&#20026;LLMs&#30340;&#39030;&#23792;&#65292;&#32780;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#23558;2D&#22270;&#20687;&#36716;&#25442;&#20026;&#23427;&#20204;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#19982;&#38382;&#39064;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#32467;&#26524;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292; conslidate&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.07869</link><description>&lt;p&gt;
TeleMoMa&#65306;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07869
&lt;/p&gt;
&lt;p&gt;
TeleMoMa &#26159;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#25805;&#20316;&#30340;&#27169;&#22359;&#21270;&#22810;&#21151;&#33021;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#20154;&#26426;&#25509;&#21475;&#12289;&#38477;&#20302;&#38376;&#27099;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#20026;&#31227;&#21160;&#25805;&#20316;&#22120;&#25552;&#20379;&#20102;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#38480;&#21046;&#27169;&#20223;&#23398;&#20064;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#31227;&#21160;&#25805;&#20316;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#19982;&#38745;&#27490;&#25805;&#20316;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36828;&#31243;&#25805;&#20316;&#30028;&#38754;&#65292;&#25910;&#38598;&#28436;&#31034;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TeleMoMa&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#36523;&#36828;&#31243;&#25805;&#20316;&#31227;&#21160;&#25805;&#20316;&#22120;&#30340;&#36890;&#29992;&#21644;&#27169;&#22359;&#21270;&#30028;&#38754;&#12290;TeleMoMa&#23558;&#21253;&#25324;RGB&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#12289;&#34394;&#25311;&#29616;&#23454;&#25511;&#21046;&#22120;&#12289;&#38190;&#30424;&#12289;&#25805;&#32437;&#26438;&#31561;&#22810;&#20010;&#20154;&#26426;&#25509;&#21475;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#21450;&#36825;&#20123;&#25509;&#21475;&#30340;&#20219;&#20309;&#32452;&#21512;&#12290;&#22312;&#20854;&#26356;&#26131;&#35775;&#38382;&#30340;&#29256;&#26412;&#20013;&#65292; TeleMoMa&#21487;&#20197;&#20165;&#20351;&#29992;&#35270;&#35273;&#65288;&#22914;RGB-D&#30456;&#26426;&#65289;&#21363;&#21487;&#24037;&#20316;&#65292;&#38477;&#20302;&#20102;&#20154;&#31867;&#25552;&#20379;&#31227;&#21160;&#25805;&#20316;&#28436;&#31034;&#30340;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#36828;&#31243;&#25805;&#20316;&#20960;&#20010;&#29616;&#26377;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#8212;&#8212;PAL Tiago++, Toyota HSR&#21644;Fetch&#26469;&#23637;&#29616;TeleMoMa&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.05752</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;KG-TOSA&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#30340;&#36807;&#22810;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26159;&#19968;&#31181;&#21253;&#21547;&#21508;&#31181;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#24322;&#26500;&#22270;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#22312;KG&#19978;&#35757;&#32451;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;HGNN&#26041;&#27861;&#21463;KG&#30340;&#22823;&#23567;&#12289;&#23494;&#24230;&#20197;&#21450;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#36807;&#22810;&#30340;&#22797;&#26434;&#24615;&#12290;AI&#20174;&#19994;&#32773;&#25163;&#24037;&#35774;&#35745;&#20986;&#19968;&#20010;&#19982;&#29305;&#23450;&#20219;&#21153;&#30456;&#20851;&#30340;KG G&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23376;&#22270;&#65288;TOSG&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;G&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#23376;&#38598;&#12290;&#20351;&#29992;TOSG&#32780;&#19981;&#26159;G&#26469;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#20943;&#36731;&#23545;&#22823;&#22411;KG&#25152;&#38656;&#30340;&#36807;&#22810;&#35745;&#31639;&#12290;&#35774;&#35745;TOSG&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;KG&#30340;&#32467;&#26500;&#21644;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KG-TOSA&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;TOSG&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;KG&#19978;&#36827;&#34892;&#38754;&#21521;&#20219;&#21153;&#30340;HGNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05574</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#36827;&#34892;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#29087;&#32451;&#22788;&#29702;&#35748;&#30693;&#37325;&#26500;&#31561;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#32670;&#32827;&#12289;&#19981;&#20449;&#20219;&#12289;&#27835;&#30103;&#24072;&#25216;&#33021;&#24046;&#24322;&#21644;&#36164;&#28304;&#31232;&#32570;&#31561;&#25361;&#25112;&#12290;&#22312;&#20808;&#21069;&#30340;&#35748;&#30693;&#37325;&#26500;&#20013;&#65292;&#20027;&#35201;&#23558;&#36127;&#38754;&#24773;&#32490;&#36716;&#21270;&#20026;&#31215;&#26497;&#30340;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#26377;&#38480;&#65292;&#32463;&#24120;&#19981;&#33021;&#20419;&#36827;&#23458;&#25143;&#33258;&#25105;&#21457;&#29616;&#26367;&#20195;&#35270;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24110;&#21161;&#21644;&#36171;&#33021;&#36890;&#36807;&#33258;&#36866;&#24212;&#35821;&#35328;&#22312;&#24515;&#29702;&#22686;&#24378;&#65288;HealMe&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#37325;&#26500;&#30103;&#27861;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24819;&#27861;&#65292;&#24182;&#20419;&#36827;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35270;&#35282;&#12290;HealMe&#19982;&#20256;&#32479;LLM&#26041;&#27861;&#19981;&#21516;&#65292;&#37319;&#29992;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#23427;&#36890;&#36807;&#31995;&#32479;&#25351;&#23548;&#23458;&#25143;&#21306;&#20998;&#24773;&#22659;&#21644;&#24863;&#21463;&#65292;&#38598;&#24605;&#24191;&#30410;&#23547;&#25214;&#26367;&#20195;&#35270;&#35282;&#65292;&#24182;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02893</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65288;ECI&#65289;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21477;&#23376;&#32423;ECI&#65292;&#32780;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25991;&#26723;&#32423;ECI&#65288;DECI&#65289;&#21364;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#65288;GIMC&#65289;&#30340;&#24322;&#26500;&#22270;&#20132;&#20114;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;ECI&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#20132;&#20114;&#32593;&#32476;&#26469;&#24314;&#27169;&#25991;&#26723;&#20013;&#20998;&#25955;&#20107;&#20214;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#20174;&#28304;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#27169;&#22359;&#65292;&#20197;&#35843;&#25972;&#36328;&#35821;&#35328;&#38388;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24179;&#22343;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#32422;9.4%&#21644;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14428</link><description>&lt;p&gt;
KoCoSa: &#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoSa: Korean Context-aware Sarcasm Detection Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#19968;&#31181;&#35328;&#35821;&#35773;&#21050;&#30340;&#26041;&#24335;&#65292;&#25351;&#30340;&#26159;&#26377;&#20154;&#35828;&#20102;&#21644;&#20182;&#20204;&#30340;&#26412;&#24847;&#30456;&#21453;&#30340;&#35805;&#65292;&#36890;&#24120;&#26159;&#20026;&#20102;&#22066;&#31505;&#19968;&#20010;&#20154;&#12289;&#24773;&#20917;&#25110;&#24819;&#27861;&#12290;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#26816;&#27979;&#35773;&#21050;&#24212;&#35813;&#21453;&#26144;&#19978;&#19979;&#25991;&#65288;&#21363;&#23545;&#35805;&#21382;&#21490;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;KoCoSa&#65288;&#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#65289;&#65292;&#21253;&#25324;12.8K&#20010;&#26085;&#24120;&#38889;&#25991;&#23545;&#35805;&#20197;&#21450;&#35813;&#20219;&#21153;&#22312;&#26368;&#21518;&#19968;&#27425;&#22238;&#22797;&#19978;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26500;&#24314;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65306;1&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#28304;&#23545;&#35805;&#20013;&#29983;&#25104;&#26032;&#30340;&#35773;&#21050;&#23545;&#35805;&#65292;2&#65289;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#28388;&#24322;&#24120;&#21644;&#26377;&#27602;&#23545;&#35805;&#65292;3&#65289;&#20026;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#38024;&#23545;&#38889;&#25991;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#35813;&#22522;&#32447;&#26159;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00248</link><description>&lt;p&gt;
&#23558;&#8220;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#8221;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00248
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#20854;&#20998;&#21106;&#33945;&#29256;&#32570;&#20047;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;SAM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#23454;&#29616;&#39640;&#24230;&#31934;&#30830;&#30340;&#23545;&#35937;&#20998;&#21106;&#65288;&#21363;&#31216;&#20026;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;DIS&#65289;&#25265;&#26377;&#24456;&#39640;&#26399;&#26395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIS-SAM&#65292;&#23558;SAM&#25512;&#36827;&#33267;DIS&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#31934;&#30830;&#32454;&#33410;&#12290;DIS-SAM&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#39640;&#24230;&#20934;&#30830;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;SAM&#30340;&#21487;&#20419;&#36827;&#35774;&#35745;&#12290;DIS-SAM&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;SAM&#19982;&#19987;&#38376;&#29992;&#20110;DIS&#30340;&#20462;&#25913;&#21518;&#30340;IS-Net&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;DIS-SAM&#30456;&#27604;SAM&#21644;HQ-SA&#34920;&#29616;&#20986;&#26174;&#30528;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#27493;&#39588;&#65292;&#24182;&#20998;&#20139;&#20102;&#35813;&#36890;&#29992;&#20998;&#31867;&#22120;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;</title><link>https://arxiv.org/abs/2312.17543</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26500;&#24314;&#39640;&#25928;&#30340;&#36890;&#29992;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Building Efficient Universal Classifiers with Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#27493;&#39588;&#65292;&#24182;&#20998;&#20139;&#20102;&#35813;&#36890;&#29992;&#20998;&#31867;&#22120;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17543v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20027;&#27969;&#36873;&#25321;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#25143;&#22312;&#21482;&#24819;&#33258;&#21160;&#21270;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#24182;&#19981;&#38656;&#35201;&#29983;&#25104;&#22411;LLMs&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#36739;&#23567;&#30340;&#31867;&#20284;BERT&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#20219;&#21153;&#65292;&#36825;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#65288;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#20309;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#25110;&#32773;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#23569;&#26679;&#26412;&#65289;&#65292;&#21516;&#26102;&#27604;&#29983;&#25104;&#22411;LLMs&#39640;&#25928;&#24471;&#22810;&#12290;&#26412;&#25991;(1) &#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#21407;&#29702;&#31867;&#20284;&#20110;&#29983;&#25104;&#22411;LLMs&#30340;&#25351;&#23548;&#24494;&#35843;&#65292;(2) &#25552;&#20379;&#20102;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#21487;&#37325;&#29992;Jupyter&#31508;&#35760;&#26412;&#30340;&#36880;&#27493;&#25351;&#21335;&#65292;(3) &#20849;&#20139;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.16427</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning to Embed Time Series Patches Independently
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16427
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#23884;&#20837;&#26102;&#38388;&#24207;&#21015;&#29255;&#27573;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#21644;&#29420;&#31435;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;MLP&#27169;&#22411;&#20197;&#21450;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25513;&#30721;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21551;&#21457;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#39318;&#20808;&#23558;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#22788;&#29702;&#24182;&#37096;&#20998;&#25513;&#30422;&#65292;&#28982;&#21518;&#35757;&#32451;Transformer&#27169;&#22411;&#36890;&#36807;&#20174;&#26410;&#25513;&#30422;&#30340;&#22359;&#39044;&#27979;&#34987;&#25513;&#30422;&#22359;&#26469;&#25429;&#25417;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#25429;&#25417;&#36825;&#31181;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21487;&#33021;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#31574;&#30053;&#65307;&#30456;&#21453;&#65292;&#29420;&#31435;&#23398;&#20064;&#23884;&#20837;&#29255;&#27573;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;1&#65289;&#31616;&#21333;&#30340;&#22359;&#37325;&#26500;&#20219;&#21153;&#65292;&#33258;&#21160;&#23558;&#27599;&#20010;&#22359;&#36827;&#34892;&#32534;&#30721;&#32780;&#19981;&#26597;&#30475;&#20854;&#20182;&#22359;&#65292;&#20197;&#21450;2&#65289;&#29420;&#33258;&#23884;&#20837;&#27599;&#20010;&#22359;&#30340;&#31616;&#21333;&#22359;&#24335;MLP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20114;&#34917;&#23545;&#27604;&#23398;&#20064;&#26469;&#26377;&#25928;&#22320;&#20998;&#23618;&#25429;&#33719;&#30456;&#37051;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.16424</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#36719;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Soft Contrastive Learning for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCLT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#36719;&#23545;&#27604;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#24573;&#30053;&#22266;&#26377;&#30456;&#20851;&#24615;&#25152;&#23548;&#33268;&#30340;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23545;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#34920;&#31034;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30456;&#20284;&#30340;&#23454;&#20363;&#25110;&#30456;&#37051;&#26102;&#38388;&#25139;&#30340;&#20540;&#36827;&#34892;&#23545;&#27604;&#20250;&#24573;&#30053;&#23427;&#20204;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftCLT&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#36719;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#20174;&#38646;&#21040;&#19968;&#30340;&#36719;&#36171;&#20540;&#30340;&#23454;&#20363;&#32423;&#21644;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;1)&#22522;&#20110;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#23450;&#20041;&#20102;&#23454;&#20363;&#32423;&#23545;&#27604;&#25439;&#22833;&#30340;&#36719;&#36171;&#20540;&#65292;&#24182;&#20026;2)&#22522;&#20110;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#20102;&#26102;&#38388;&#32423;&#23545;&#27604;&#25439;&#22833;&#12290;SoftCLT&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#27809;&#26377;&#36807;&#22810;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
&lt;/p&gt;</description></item><item><title>VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.14125</link><description>&lt;p&gt;
VideoPoet&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VideoPoet: A Large Language Model for Zero-Shot Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14125
&lt;/p&gt;
&lt;p&gt;
VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VideoPoet&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#21508;&#31181;&#19981;&#21516;&#30340;&#26465;&#20214;&#20449;&#21495;&#20013;&#21512;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;VideoPoet&#37319;&#29992;&#35299;&#30721;&#22120;-&#20165;Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#12290;&#35757;&#32451;&#21327;&#35758;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;VideoPoet&#22312;&#33258;&#22238;&#24402;Transformer&#26694;&#26550;&#20013;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#29983;&#25104;&#30446;&#26631;&#30340;&#28151;&#21512;&#12290;&#39044;&#35757;&#32451;&#30340;LLM&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#33021;&#21147;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;VideoPoet&#29983;&#25104;&#39640;&#20445;&#30495;&#36816;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14125v2 Announce Type: replace-cross  Abstract: We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/
&lt;/p&gt;</description></item><item><title>PIA&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#21160;&#20316;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2312.13964</link><description>&lt;p&gt;
PIA:&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13964
&lt;/p&gt;
&lt;p&gt;
PIA&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#21160;&#20316;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#21019;&#20316;&#65292;&#20351;&#38750;&#19987;&#23478;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29420;&#29305;&#39118;&#26684;&#30340;&#24778;&#20154;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25991;&#26412;&#20026;&#36825;&#20123;&#20010;&#24615;&#21270;&#22270;&#20687;&#22686;&#21152;&#36924;&#30495;&#30340;&#21160;&#20316;&#22312;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#36890;&#36807;&#25991;&#26412;&#23454;&#29616;&#21160;&#20316;&#21487;&#25511;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PIA&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#29983;&#25104;&#22120;&#65292;&#20854;&#22312;&#19982;&#26465;&#20214;&#22270;&#20687;&#23545;&#40784;&#12289;&#36890;&#36807;&#25991;&#26412;&#23454;&#29616;&#21160;&#20316;&#21487;&#25511;&#24615;&#20197;&#21450;&#19982;&#21508;&#31181;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#29305;&#23450;&#35843;&#25972;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;PIA&#22312;&#22522;&#30784;T2I&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#26102;&#38388;&#23545;&#40784;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#20309;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#21521;&#22270;&#20687;&#21160;&#30011;&#27169;&#22411;&#30340;&#26080;&#32541;&#36716;&#25442;&#12290;PIA&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#24341;&#20837;&#26465;&#20214;&#27169;&#22359;&#65292;&#21033;&#29992;&#26465;&#20214;&#24103;&#21644;&#24103;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13964v2 Announce Type: replace-cross  Abstract: Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame af
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>Hulk&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;</title><link>https://arxiv.org/abs/2312.01697</link><description>&lt;p&gt;
Hulk: &#19968;&#31181;&#38754;&#21521;&#20154;&#31867;&#20013;&#24515;&#20219;&#21153;&#30340;&#36890;&#29992;&#30693;&#35782;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hulk: A Universal Knowledge Translator for Human-Centric Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01697
&lt;/p&gt;
&lt;p&gt;
Hulk&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#65292;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#24863;&#30693;&#20219;&#21153;&#65292;&#20363;&#22914;&#34892;&#20154;&#26816;&#27979;&#12289;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#21644;&#23039;&#24577;&#20272;&#35745;&#65292;&#22312;&#35832;&#22914;&#20803;&#23431;&#23449;&#21644;&#20307;&#32946;&#20998;&#26512;&#31561;&#24191;&#27867;&#30340;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#26469;&#65292;&#20986;&#29616;&#20102;&#21457;&#23637;&#26088;&#22312;&#21463;&#30410;&#20110;&#24191;&#27867;&#20154;&#31867;&#20013;&#24515;&#24863;&#30693;&#20219;&#21153;&#30340;&#20154;&#31867;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#34429;&#28982;&#35768;&#22810;&#20154;&#31867;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#25506;&#32034;&#29992;&#20110;&#20154;&#31867;&#20013;&#24515;&#21450;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#30340;3D&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22810;&#19979;&#28216;&#20219;&#21153;&#21644;&#24773;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hulk&#65292;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;2D&#35270;&#35273;&#12289;3D&#35270;&#35273;&#12289;&#22522;&#20110;&#39592;&#26550;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#20154;&#31867;&#20013;&#24515;&#36890;&#29992;&#27169;&#22411;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#21508;&#31181;&#20219;&#21153;&#29305;&#23450;&#22836;&#37096;&#21387;&#32553;&#25104;&#20004;&#20010;&#36890;&#29992;&#22836;&#37096;&#65292;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#34920;&#31034;&#65292;&#22914;&#35821;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01697v4 Announce Type: replace-cross  Abstract: Human-centric perception tasks, e.g., pedestrian detection, skeleton-based action recognition, and pose estimation, have wide industrial applications, such as metaverse and sports analysis. There is a recent surge to develop human-centric foundation models that can benefit a broad range of human-centric perception tasks. While many human-centric foundation models have achieved success, they did not explore 3D and vision-language tasks for human-centric and required task-specific finetuning. These limitations restrict their application to more downstream tasks and situations. To tackle these problems, we present Hulk, the first multimodal human-centric generalist model, capable of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks without task-specific finetuning. The key to achieving this is condensing various task-specific heads into two general heads, one for discrete representations, e.g., languages, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#20854;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20316;&#20026;&#26234;&#33021;&#20915;&#31574;&#32773;&#26469;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.00812</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#33258;&#21160;&#39550;&#39542;&#65306;&#19968;&#20010;&#23433;&#20840;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Empowering Autonomous Driving with Large Language Models: A Safety Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#20854;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20316;&#20026;&#26234;&#33021;&#20915;&#31574;&#32773;&#26469;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#22312;&#38271;&#23614;&#26410;&#30693;&#39550;&#39542;&#22330;&#26223;&#20013;&#36935;&#21040;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38556;&#30861;&#65292;&#20027;&#35201;&#28304;&#33258;AD&#31995;&#32479;&#20869;&#37096;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#24067;&#22806;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#21040;AD&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23427;&#20204;&#24378;&#22823;&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;LLM&#29992;&#20316;&#34892;&#20026;&#35268;&#21010;&#20013;&#30340;&#26234;&#33021;&#20915;&#31574;&#32773;&#65292;&#37197;&#22791;&#19968;&#20010;&#23433;&#20840;&#39564;&#35777;&#22120;&#25252;&#30462;&#36827;&#34892;&#19978;&#19979;&#25991;&#23433;&#20840;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#39550;&#39542;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#30740;&#31350;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;LLM&#35843;&#33410;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#21644;&#19968;&#31181;&#24102;&#26377;&#29366;&#24577;&#26426;&#30340;LLM&#21551;&#29992;&#20132;&#20114;&#24335;&#34892;&#20026;&#35268;&#21010;&#26041;&#26696;&#12290;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00812v4 Announce Type: replace  Abstract: Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.00094</link><description>&lt;p&gt;
&#22312;&#22823;&#32422;5&#20010;&#27493;&#39588;&#20013;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#22522;&#20110;ODE&#30340;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast ODE-based Sampling for Diffusion Models in Around 5 Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#25277;&#26679;&#21487;&#20197;&#34987;&#35270;&#20026;&#35299;&#20915;&#30456;&#24212;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFE&#65289;&#33719;&#24471;&#20934;&#30830;&#35299;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21033;&#29992;&#39640;&#38454;ODE&#27714;&#35299;&#22120;&#30340;&#21508;&#31181;&#24555;&#36895;&#25277;&#26679;&#22120;&#65292;&#24182;&#19988;&#27604;&#26368;&#21021;&#30340;&#19968;&#38454;&#27714;&#35299;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#20540;&#26041;&#27861;&#22266;&#26377;&#22320;&#23548;&#33268;&#26576;&#20123;&#36817;&#20284;&#35823;&#24046;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#20855;&#26377;&#26497;&#23567;NFE&#65288;&#20363;&#22914;&#65292;&#32422;&#20026;5&#65289;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#65292;&#27599;&#20010;&#25277;&#26679;&#36712;&#36857;&#20960;&#20046;&#20301;&#20110;&#23884;&#20837;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#20108;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#30340;AME&#36817;&#20284;&#22343;&#26041;&#21521;&#27714;&#35299;&#22120;&#65288;AMED-Solver&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#25554;&#20214;&#20351;&#29992;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#26377;&#30340;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00094v2 Announce Type: replace-cross  Abstract: Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-base
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26159;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#24182;&#19982;&#25163;&#24037;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2311.18598</link><description>&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisable Agents for Neural Network Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18598
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26159;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#24182;&#19982;&#25163;&#24037;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#22312;&#20110;&#22797;&#26434;&#30340;&#35757;&#32451;&#21160;&#24577;&#12289;&#39640;&#35745;&#31639;&#35201;&#27714;&#21644;&#38271;&#26102;&#38388;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#26234;&#33021;&#20307;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#65288;GANNO&#65289;&#30340;&#26694;&#26550;--&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#21709;&#24212;&#24335;&#22320;&#35843;&#24230;&#36229;&#21442;&#25968;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;GANNO&#21033;&#29992;&#27599;&#23618;&#19968;&#20010;&#26234;&#33021;&#20307;&#35266;&#23519;&#23616;&#37096;&#21270;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#26469;&#35843;&#25972;&#36825;&#20123;&#21160;&#24577;&#65292;&#20174;&#32780;&#22312;&#23618;&#32423;&#19978;&#38598;&#20307;&#25913;&#21892;&#20840;&#23616;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GANNO&#26469;&#25511;&#21046;&#23618;&#32423;&#23398;&#20064;&#29575;&#65292;&#24182;&#23637;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#21709;&#24212;&#28789;&#27963;&#30340;&#35843;&#24230;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#26174;&#31034;GANNO&#22312;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#21021;&#22987;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25104;&#21151;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18598v2 Announce Type: replace-cross  Abstract: Optimising deep neural networks is a challenging task due to complex training dynamics, high computational requirements, and long training times. To address this difficulty, we propose the framework of Generalisable Agents for Neural Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL) approach that learns to improve neural network optimisation by dynamically and responsively scheduling hyperparameters during training. GANNO utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance. In this paper, we use GANNO to control the layerwise learning rate and show that the framework can yield useful and responsive schedules that are competitive with handcrafted heuristics. Furthermore, GANNO is shown to perform robustly across a wide variety of unseen initial conditions, and can succe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#35782;&#21035;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#35782;&#21035;&#26368;&#20855;&#25361;&#25112;&#24615;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.15243</link><description>&lt;p&gt;
ID&#26679;&#24335;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CLIP&#35782;&#21035;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#35782;&#21035;&#26368;&#20855;&#25361;&#25112;&#24615;OOD&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#36741;&#21161;&#24322;&#24120;&#20540;&#26469;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#24322;&#24120;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#20174;&#36741;&#21161;&#24322;&#24120;&#20540;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24322;&#24120;&#20540;&#20197;&#25913;&#21892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20173;&#38754;&#20020;&#26377;&#25928;&#21306;&#20998;&#19982;ID&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#26679;&#26412;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;ID&#26679;&#24335;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;OOD&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;CLIP&#20174;ID&#26679;&#26412;&#30340;&#37051;&#36817;&#31354;&#38388;&#20013;&#21457;&#29616;ID&#26679;&#24335;&#30340;&#24322;&#24120;&#20540;&#65292;&#20174;&#32780;&#24110;&#21161;&#35782;&#21035;&#36825;&#20123;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;OOD&#26679;&#26412;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35782;&#21035;&#30340;ID&#26679;&#24335;&#24322;&#24120;&#20540;&#36827;&#19968;&#27493;&#21033;&#29992;CLIP&#30340;&#33021;&#21147;&#36827;&#34892;OOD&#26816;&#27979;&#12290;&#21463;&#30410;&#20110;&#24378;&#22823;&#30340;CLIP&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23569;&#37327;ID&#26679;&#26412;&#21363;&#21487;&#23398;&#20064;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#32780;&#26080;&#38656;&#26292;&#38706;&#20854;&#20182;&#36741;&#21161;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15243v3 Announce Type: replace-cross  Abstract: Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., \idlike samples. To this end, we propose a novel OOD detection framework that discovers \idlike outliers using CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified \idlike outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#20809;&#23398;&#22270;&#20687;&#19982;&#23454;&#38469;&#26426;&#26800;&#24615;&#33021;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.10278</link><description>&lt;p&gt;
&#29289;&#29702;&#22686;&#24378;&#30340;&#20809;&#23398;&#34920;&#38754;&#21360;&#36857;&#22810;&#20445;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#20809;&#23398;&#22270;&#20687;&#19982;&#23454;&#38469;&#26426;&#26800;&#24615;&#33021;&#20043;&#38388;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25351;&#32441;&#20316;&#20026;&#27599;&#20010;&#20154;&#30340;&#29420;&#29305;&#32780;&#24378;&#22823;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#24110;&#21161;&#35686;&#23519;&#35782;&#21035;&#36523;&#20221;&#12290;&#31867;&#20284;&#22320;&#65292;&#35768;&#22810;&#33258;&#28982;&#20307;&#21644;&#20869;&#22312;&#26426;&#26800;&#29305;&#24615;&#20063;&#21487;&#20197;&#36890;&#36807;&#34920;&#38754;&#29305;&#24449;&#24471;&#21040;&#21807;&#19968;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20445;&#30495;&#31070;&#32463;&#32593;&#32476;(MFNN)&#35299;&#20915;&#36825;&#20010;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10278v2 Announce Type: replace-cross  Abstract: Human fingerprints serve as one unique and powerful characteristic for each person, from which policemen can recognize the identity. Similar to humans, many natural bodies and intrinsic mechanical qualities can also be uniquely identified from surface characteristics. To measure the elasto-plastic properties of one material, one formally sharp indenter is pushed into the measured body under constant force and retracted, leaving a unique residual imprint of the minute size from several micrometers to nanometers. However, one great challenge is how to map the optical image of this residual imprint into the real wanted mechanical properties, \ie, the tensile force curve. In this paper, we propose a novel method to use multi-fidelity neural networks (MFNN) to solve this inverse problem. We first build up the NN model via pure simulation data, and then bridge the sim-to-real gap via transfer learning. Considering the difficulty of c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.15929</link><description>&lt;p&gt;
E-Sparse: &#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340; N:M &#31232;&#30095;&#24615;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15929
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21098;&#26525;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#35757;&#32451;&#36807;&#31243;&#26114;&#36149;&#65292;&#35745;&#31639;&#38656;&#27714;&#22823;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#38544;&#34255;&#29366;&#24577;&#29305;&#24449;&#30340;&#20449;&#24687;&#29109;&#24341;&#20837;&#21040;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#20013;&#65292;&#21363; E-Sparse&#65292;&#20197;&#25552;&#39640;LLM&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;E-Sparse&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#24615;&#26469;&#25552;&#21319;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#32467;&#21512;&#20960;&#31181;&#26032;&#39062;&#25216;&#26415;&#26469;&#23454;&#29616;&#65306;(1)&#24341;&#20837;&#20449;&#24687;&#29109;&#26469;&#22686;&#24378;&#21442;&#25968;&#26435;&#37325;&#21644;&#36755;&#20837;&#29305;&#24449;&#33539;&#25968;&#30340;&#37325;&#35201;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#26525;&#24230;&#37327;&#65292;&#24182;&#22312;&#19981;&#20462;&#25913;&#21097;&#20313;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;N:M&#31232;&#30095;&#24615;&#12290;(2)&#35774;&#35745;&#20840;&#23616;&#26420;&#32032;&#27927;&#29260;&#21644;&#23616;&#37096;&#22359;&#27927;&#29260;&#65292;&#24555;&#36895;&#20248;&#21270;&#20449;&#24687;&#20998;&#24067;&#65292;&#20805;&#20998;&#24212;&#23545; N:M &#31232;&#30095;&#24615;&#23545;LLMs&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;E-Sparse &#34987;&#23454;&#29616;&#20026;&#19968;&#31181; Spars
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20026;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2310.14098</link><description>&lt;p&gt;
&#31283;&#23450;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#65306;&#29992;&#20110;&#20248;&#21270;&#25152;&#26377;&#31283;&#23450;&#34892;&#20026;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14098
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20026;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#26469;&#23450;&#20041;&#25628;&#32034;&#22495;&#12290;&#26368;&#36817;&#22312;&#34892;&#20026;&#31995;&#32479;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#37096;&#27169;&#22411;&#65307;&#36825;&#20351;&#24471;&#22312;&#23436;&#20840;&#22522;&#20110;&#36755;&#20837;-&#36755;&#20986;&#25506;&#32034;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;Youla-Kucera&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#24615;&#23454;&#29616;&#25104;&#20026;&#21487;&#33021;&#12290;&#25110;&#35768;&#26356;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21046;&#23450;&#21644;&#20998;&#26512;&#20102;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;Youla-Kucera&#26041;&#27861;&#23545;&#20110;&#25511;&#21046;&#22120;&#35774;&#35745;&#38656;&#35201;&#19968;&#20010;&#31283;&#23450;&#30340;&#8220;&#21442;&#25968;&#8221;&#12290;&#20026;&#20102;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#25152;&#26377;&#31283;&#23450;&#32447;&#24615;&#36816;&#31639;&#31526;&#30340;&#38598;&#21512;&#36890;&#36807;&#30697;&#38453;&#22240;&#23376;&#21270;&#26041;&#27861;&#34987;&#26126;&#30830;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#38750;&#32447;&#24615;&#25193;&#23637;&#65292;&#20197;&#34920;&#36798;&#21442;&#25968;&#21270;&#30340;&#19968;&#32452;&#31283;&#23450;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14098v2 Announce Type: replace-cross  Abstract: We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Perhaps of independent interest, we formulate and analyze the stability of such data-driven models in the presence of noise. The Youla-Kucera approach requires a stable "parameter" for controller design. For the training of reinforcement learning agents, the set of all stable linear operators is given explicitly through a matrix factorization approach. Moreover, a nonlinear extension is given using a neural network to express a parameterized set of stab
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;&#20013;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#40831;&#40840;&#40843;&#40831;&#26816;&#27979;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2310.00354</link><description>&lt;p&gt;
AI-Dentify: &#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#36817;&#37051;&#29273;&#40840;&#40843;&#40831;&#22312;&#20840;&#26223;X&#20809;&#19978;&#30340;&#26816;&#27979;--HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00354
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;HUNT4&#21475;&#33108;&#20581;&#24247;&#30740;&#31350;&#20013;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#40831;&#40840;&#40843;&#40831;&#26816;&#27979;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#29273;&#40831;&#40843;&#22351;&#30340;&#35786;&#26029;&#38656;&#35201;&#23545;&#24739;&#32773;&#30340;&#35786;&#26029;&#24615;&#20840;&#26223;X&#20809;&#22270;&#20687;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#65292;&#28982;&#21518;&#36890;&#36807;&#35270;&#35273;&#26816;&#26597;&#21644;&#35302;&#35786;&#35782;&#21035;&#20855;&#26377;&#28508;&#22312;&#30149;&#21464;&#30340;&#29273;&#40831;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65292;&#26377;&#26395;&#36890;&#36807;&#25552;&#20379;&#24555;&#36895;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20840;&#26223;X&#20809;&#22270;&#20687;&#20998;&#26512;&#26469;&#24110;&#21161;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00354v2 Announce Type: replace-cross  Abstract: Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.   Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.   Results: he trained models show an increase in average precision
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#26512;&#31867;&#25991;&#26412;&#30340;Unix shell&#25915;&#20987;&#26085;&#24535;&#65292;&#25552;&#20986;&#20102;LogPr\'ecis&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#25915;&#20987;&#32773;&#31574;&#30053;&#24182;&#25581;&#31034;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#39034;&#24207;</title><link>https://arxiv.org/abs/2307.08309</link><description>&lt;p&gt;
LogPr\'ecis&#65306;&#37322;&#25918;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#24694;&#24847;&#26085;&#24535;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LogPr\'ecis: Unleashing Language Models for Automated Malicious Log Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.08309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#26512;&#31867;&#25991;&#26412;&#30340;Unix shell&#25915;&#20987;&#26085;&#24535;&#65292;&#25552;&#20986;&#20102;LogPr\'ecis&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#25915;&#20987;&#32773;&#31574;&#30053;&#24182;&#25581;&#31034;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2307.08309v3 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#23433;&#20840;&#30456;&#20851;&#26085;&#24535;&#30340;&#25910;&#38598;&#26159;&#29702;&#35299;&#25915;&#20987;&#34892;&#20026;&#21644;&#35786;&#26029;&#28431;&#27934;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#26080;&#19982;&#20262;&#27604;&#30340;&#28508;&#21147;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;LMs&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23545;&#23433;&#20840;&#19987;&#23478;&#26377;&#29992;&#65292;&#22240;&#20026;&#20182;&#20204;&#30340;&#26085;&#24535;&#21253;&#21547;&#20869;&#22312;&#28151;&#20081;&#21644;&#28151;&#28102;&#20449;&#24687;&#12290;&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;LM&#30340;&#26368;&#26032;&#25216;&#26415;&#20013;&#21463;&#30410;&#65292;&#20197;&#33258;&#21160;&#20998;&#26512;&#31867;&#25991;&#26412;&#30340;Unix shell&#25915;&#20987;&#26085;&#24535;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24443;&#24213;&#30340;&#35774;&#35745;&#26041;&#27861;&#35770;&#65292;&#23548;&#33268;LogPr\'ecis&#12290;&#23427;&#20197;&#21407;&#22987;&#30340;shell&#20250;&#35805;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#35782;&#21035;&#21644;&#20998;&#37197;&#25915;&#20987;&#32773;&#31574;&#30053;&#32473;&#20250;&#35805;&#30340;&#27599;&#20010;&#37096;&#20998;&#65292;&#21363;&#25581;&#31034;&#25915;&#20987;&#32773;&#30446;&#26631;&#30340;&#39034;&#24207;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LogPr\'ecis&#25903;&#25345;&#20998;&#26512;&#20004;&#20010;&#22823;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.08309v3 Announce Type: replace-cross  Abstract: The collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. Still, their analysis remains a daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises whether and how LMs could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. In this paper, we systematically study how to benefit from the state-of-the-art in LM to automatically analyze text-like Unix shell attack logs. We present a thorough design methodology that leads to LogPr\'ecis. It receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. We demonstrate LogPr\'ecis capability to support the analysis of two large datasets conta
&lt;/p&gt;</description></item><item><title>FunQA&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;HumorQA&#12289;CreativeQA&#21644;MagicQA&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#24778;&#21916;&#35270;&#39057;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2306.14899</link><description>&lt;p&gt;
FunQA&#65306;&#36808;&#21521;&#20196;&#20154;&#24778;&#35766;&#30340;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
FunQA: Towards Surprising Video Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14899
&lt;/p&gt;
&lt;p&gt;
FunQA&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;HumorQA&#12289;CreativeQA&#21644;MagicQA&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#24778;&#21916;&#35270;&#39057;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#35270;&#39057;&#65292;&#27604;&#22914;&#26377;&#36259;&#30340;&#29255;&#27573;&#12289;&#21019;&#24847;&#28436;&#20986;&#25110;&#35270;&#35273;&#24187;&#35937;&#65292;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#23545;&#36825;&#20123;&#35270;&#39057;&#30340;&#27427;&#36175;&#19981;&#20165;&#20165;&#26159;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#21453;&#24212;&#65307;&#30456;&#21453;&#65292;&#23427;&#21462;&#20915;&#20110;&#20154;&#31867;&#29702;&#35299;&#65288;&#20197;&#21450;&#27427;&#36175;&#65289;&#36825;&#20123;&#35270;&#39057;&#20013;&#25152;&#25551;&#32472;&#30340;&#24120;&#35782;&#36829;&#21453;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FunQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#12290;&#19982;&#22823;&#22810;&#25968;&#20391;&#37325;&#20110;&#19981;&#22826;&#24778;&#35766;&#30340;&#32972;&#26223;&#65288;&#20363;&#22914;&#28921;&#39274;&#25110;&#35828;&#26126;&#35270;&#39057;&#65289;&#30340;&#35270;&#39057;QA&#22522;&#20934;&#19981;&#21516;&#65292;FunQA&#28085;&#30422;&#20102;&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#31867;&#22411;&#30340;&#24778;&#21916;&#35270;&#39057;&#65306;1&#65289;HumorQA&#65292;2&#65289;CreativeQA&#21644;3&#65289;MagicQA&#12290;&#23545;&#20110;&#27599;&#20010;&#23376;&#38598;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;QA&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#21453;&#30452;&#35273;&#26102;&#38388;&#25139;&#23450;&#20301;&#12289;&#35814;&#32454;&#35270;&#39057;&#25551;&#36848;&#20197;&#21450;&#22260;&#32469;&#21453;&#30452;&#35273;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14899v2 Announce Type: replace-cross  Abstract: Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose hi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#20026;&#20102;&#35780;&#20272;&#21644;&#25490;&#24207;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#27668;&#20505;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2303.00652</link><description>&lt;p&gt;
&#30830;&#23450;&#27491;&#30830;&#30340;XAI&#26041;&#27861;--&#27668;&#20505;&#31185;&#23398;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#35780;&#20272;&#21644;&#25490;&#24207;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#20026;&#20102;&#35780;&#20272;&#21644;&#25490;&#24207;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#27668;&#20505;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#12290;&#23384;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#24212;&#29992;&#20110;&#27668;&#20505;&#31185;&#23398;&#20013;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#32570;&#23569;&#22320;&#38754;&#30495;&#23454;&#35299;&#37322;&#20351;&#20182;&#20204;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#21464;&#24471;&#22797;&#26434;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27668;&#20505;&#32972;&#26223;&#19979;&#30340;XAI&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#30340;&#26399;&#26395;&#35299;&#37322;&#29305;&#24615;&#65292;&#21363;&#31283;&#20581;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#23450;&#20301;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20197;&#26576;&#19968;&#26696;&#20363;&#30740;&#31350;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#39044;&#27979;&#20102;&#21313;&#24180;&#30340;&#24180;&#22343;&#28201;&#24230;&#22270;&#12290;&#22312;&#35757;&#32451;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20043;&#21518;&#65292;&#24212;&#29992;&#22810;&#31181;XAI&#26041;&#27861;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#22312;&#27599;&#20010;&#23646;&#24615;&#19978;&#19982;&#38543;&#26426;&#22343;&#21248;&#35299;&#37322;&#30340;&#25216;&#33021;&#20998;&#25968;&#12290;&#29420;&#31435;&#20110;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;XAI m
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00652v2 Announce Type: replace-cross  Abstract: Explainable artificial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multi-layer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we find that XAI m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2211.11940</link><description>&lt;p&gt;
&#29992;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with Speculative Opponent Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#23616;&#37096;&#20449;&#24687;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#21463;&#25511;&#20195;&#29702;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25163;&#24314;&#27169;&#36890;&#36807;&#26500;&#24314;&#20854;&#20182;&#20195;&#29702;&#30340;&#27169;&#22411;&#65292;&#20351;&#21463;&#25511;&#20195;&#29702;&#30340;&#20915;&#31574;&#21463;&#30410;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#23545;&#25163;&#30340;&#35266;&#23519;&#21644;&#34892;&#20026;&#65292;&#20294;&#24403;&#23545;&#25163;&#30340;&#34892;&#20026;&#19981;&#21487;&#35266;&#23519;&#25110;&#38590;&#20197;&#33719;&#24471;&#26102;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#32431;&#31929;&#30340;&#23616;&#37096;&#20449;&#24687;&#65288;&#21363;&#21463;&#25511;&#20195;&#29702;&#30340;&#35266;&#23519;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#65289;&#23454;&#29616;&#25512;&#27979;&#23545;&#25163;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28436;&#21592;&#32500;&#25345;&#23545;&#23545;&#25163;&#30340;&#25512;&#27979;&#20449;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#23616;&#37096;&#35266;&#23519;&#26469;&#39044;&#27979;&#23545;&#25163;&#30340;&#21160;&#20316;&#65292;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#35780;&#35770;&#23478;&#27169;&#22411;&#25919;&#31574;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#23427;&#21453;&#26144;&#20102;&#28436;&#21592;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25351;&#23548;&#28436;&#21592;&#25152;&#20381;&#36182;&#30340;&#25512;&#27979;&#23545;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24230;&#37327;&#24182;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2203.05222</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65306;&#38024;&#23545;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Similarity-based Label Inference Attack against Training and Inference of Split Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.05222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24230;&#37327;&#24182;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split learning&#26159;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#12290;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#34987;&#20999;&#21106;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21482;&#20132;&#25442;&#20999;&#21106;&#23618;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#20102;&#35299;Split learning&#30340;&#23433;&#20840;&#24615;&#33021;&#23545;&#35768;&#22810;&#38544;&#31169;&#25935;&#24863;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;Split learning&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20132;&#25442;&#30340;&#20013;&#38388;&#32467;&#26524;&#65292;&#21253;&#25324;&#30772;&#30862;&#30340;&#25968;&#25454;&#65288;&#21363;&#20174;&#21407;&#22987;&#25968;&#25454;&#25552;&#21462;&#30340;&#29305;&#24449;&#65289;&#21644;&#26799;&#24230;&#65292;&#24050;&#32463;&#21487;&#20197;&#36879;&#38706;&#20986;&#31169;&#23494;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#30340;&#26631;&#31614;&#27844;&#28431;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#38024;&#23545;&#26799;&#24230;&#21644;&#30772;&#30862;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#27431;&#27663;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#31181;&#30456;&#20284;&#24230;&#27979;&#37327;&#26174;&#31034;&#21487;&#20197;&#22312;&#27431;&#27663;&#31354;&#38388;&#20013;&#32479;&#19968;&#12290;&#22522;&#20110;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#24674;&#22797;&#31169;&#23494;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.05222v2 Announce Type: replace-cross  Abstract: Split learning is a promising paradigm for privacy-preserving distributed learning. The learning model can be cut into multiple portions to be collaboratively trained at the participants by exchanging only the intermediate results at the cut layer. Understanding the security performance of split learning is critical for many privacy-sensitive applications. This paper shows that the exchanged intermediate results, including the smashed data (i.e., extracted features from the raw data) and gradients during training and inference of split learning, can already reveal the private labels. We mathematically analyze the potential label leakages and propose the cosine and Euclidean similarity measurements for gradients and smashed data, respectively. Then, the two similarity measurements are shown to be unified in Euclidean space. Based on the similarity metric, we design three label inference attacks to efficiently recover the private
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08699</link><description>&lt;p&gt;
&#20851;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#26088;&#22312;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#30149;&#29702;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#35013;&#26377;&#25668;&#20687;&#22836;&#30340;&#26174;&#24494;&#38236;&#25110;&#20840;&#25195;&#25551;&#20202;&#33719;&#21462;&#12290;&#21033;&#29992;&#30456;&#20284;&#24615;&#35745;&#31639;&#22522;&#20110;&#36825;&#20123;&#22270;&#20687;&#21305;&#37197;&#24739;&#32773;&#65292;&#22312;&#30740;&#31350;&#21644;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#26368;&#36817;&#25628;&#32034;&#25216;&#26415;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#21508;&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;&#32454;&#32990;&#32467;&#26500;&#36827;&#34892;&#24494;&#22937;&#30340;&#37327;&#21270;&#65292;&#20419;&#36827;&#27604;&#36739;&#65292;&#24182;&#22312;&#19982;&#35786;&#26029;&#21644;&#27835;&#30103;&#36807;&#30340;&#30149;&#20363;&#25968;&#25454;&#24211;&#36827;&#34892;&#27604;&#36739;&#26102;&#23454;&#29616;&#20851;&#20110;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#26032;&#24739;&#32773;&#39044;&#27979;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#27010;&#36848;&#65292;&#20197;&#23547;&#27714;&#26377;&#25928;&#12289;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.08731</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23454;&#29616;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#19990;&#30028;&#26426;&#21046;&#25110;&#23646;&#24615;&#21457;&#29983;&#31361;&#28982;&#21464;&#21270;&#26102;&#65292;&#20195;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#23646;&#24615;&#25110;&#29366;&#24577;&#36716;&#25442;&#30340;&#31361;&#21464;&#31216;&#20026;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#22312;&#29983;&#25104;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#20013;&#23454;&#26045;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20445;&#25252;&#37096;&#32626;&#26102;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#24207;&#21015;&#20915;&#31574;&#30456;&#20851;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26412;&#20307;&#35770;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20195;&#29702;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#36716;&#25442;&#20998;&#24067;&#20013;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04025</link><description>&lt;p&gt;
MSAC&#65306;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;(SER)&#25552;&#20986;&#20102;MSAC&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#39062;&#30340;CNN-based SER&#27169;&#22411;&#21644;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#30340;&#26356;&#31934;&#32454;&#25511;&#21046;&#21644;&#25429;&#25417;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;SER&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#24773;&#24863;&#23646;&#24615;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#65292;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32780;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;SER&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21508;&#31181;&#35821;&#38899;&#23646;&#24615;&#30340;&#25968;&#25454;&#20998;&#24067;&#26469;&#24314;&#27169;&#35821;&#38899;&#24773;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN&#30340;SER&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#21152;&#24615;&#36793;&#30028;&#26368;&#22823;&#21270;&#36719;&#20214;&#26368;&#22823;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#25193;&#22823;&#20102;&#19981;&#21516;&#31867;&#21035;&#29305;&#24449;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35821;&#38899;&#23646;&#24615;&#25511;&#21046;&#26041;&#27861;MSAC&#65292;&#20197;&#26126;&#30830;&#25511;&#21046;&#35821;&#38899;&#23646;&#24615;&#65292;&#20351;&#27169;&#22411;&#21463;&#24773;&#24863;&#26080;&#20851;&#23646;&#24615;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#24182;&#25429;&#25417;&#21040;&#26356;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#27979;&#35797;&#21644;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;SER&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.05080</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#22320;&#29699;&#31227;&#21160;&#32773;: &#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;(OT)&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;,&#39318;&#27425;&#20986;&#29616;&#20110;18&#19990;&#32426;,&#24182;&#24341;&#21457;&#20986;&#22823;&#37327;&#26041;&#27861;&#26469;&#22238;&#31572;&#35768;&#22810;&#29702;&#35770;&#21644;&#24212;&#29992;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#21313;&#24180;&#35265;&#35777;&#20102;&#36825;&#20010;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#30528;&#36129;&#29486;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#21450;&#20854;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#22312;&#19987;&#39064;&#19982;&#32972;&#26223;&#30340;&#20801;&#35768;&#19979;,&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#38754;&#35843;&#26597;,&#24182;&#30830;&#20445;&#20854;&#21576;&#29616;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#39318;&#20808;,&#25105;&#20204;&#35299;&#37322;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#32972;&#26223;,&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#31867;&#22411;&#12289;&#29305;&#24615;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;,&#25105;&#20204;&#30528;&#37325;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26368;&#20248;&#36755;&#36816;&#25193;&#23637;&#20197;&#24212;&#23545;&#24403;&#21069;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25193;&#23637;OT&#30340;&#25991;&#29486;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;,&#24182;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#21576;&#29616;&#32467;&#26524;&#20197;&#20419;&#36827;&#29702;&#35299;&#12290;&#26368;&#21518;,&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#25193;&#23637;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.00759</link><description>&lt;p&gt;
Merlin-Arthur&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#8220;Merlin-Arthur&#8221;&#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#22312;&#19981;&#20551;&#35774;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30456;&#23545;&#24378;&#24230;&#21644;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#26159;&#20687;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#22797;&#26434;&#26234;&#33021;&#20307;&#20063;&#33021;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#12290;&#36825;&#20123;&#20445;&#35777;&#21253;&#25324;&#23545;&#27492;&#20998;&#31867;&#22120;&#36873;&#25321;&#30340;&#29305;&#24449;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#19978;&#19979;&#30028;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21463;&#20132;&#20114;&#24335;&#35777;&#26126;&#31995;&#32479;&#20013; Merlin-Arthur &#21327;&#35758;&#30340;&#21551;&#21457;&#65292;&#24182;&#20197;&#21487;&#27979;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;&#22768;&#38899;&#21644;&#23436;&#25972;&#24615;&#65289;&#34920;&#36798;&#20102;&#36825;&#20123;&#32422;&#26463;&#12290;&#19982;&#29616;&#26377;&#30340;&#20132;&#20114;&#24335;&#35774;&#32622;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#26368;&#20248;&#26234;&#33021;&#20307;&#25110;&#29305;&#24449;&#29420;&#31435;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#30456;&#23545;&#24378;&#24230;&#20197;&#21450;&#26032;&#30340;&#8220;&#38750;&#23545;&#31216;&#29305;&#24449;&#30456;&#20851;&#24615;&#8221;&#27010;&#24565;&#26469;&#25429;&#25417;&#20351;&#21487;&#35299;&#37322;&#24615;&#20445;&#35777;&#22256;&#38590;&#30340;&#31934;&#30830;&#30456;&#20851;&#24615;&#31867;&#22411;&#12290; &#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#23454;&#39564;&#21487;&#39564;&#35777;&#39640;&#20114;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
&lt;/p&gt;</description></item></channel></rss>