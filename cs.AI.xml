<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01276</link><description>&lt;p&gt;
&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;: &#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: a Perspective of Stability and Fairness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#25351;&#26631;&#26469;&#35780;&#20272;FU&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#39564;&#35777;&#12289;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#27934;&#23519;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#36825;&#20123;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#24773;&#20917;&#19979;&#32852;&#37030;&#21462;&#28040;&#23398;&#20064;&#65288;FU&#65289;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FU&#35780;&#20272;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#39564;&#35777;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#23616;&#37096;&#20844;&#24179;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#23545;&#20855;&#26377;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#21462;&#28040;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#23545;FU&#20013;&#26435;&#34913;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;FU&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#26435;&#34913;&#30340;FU&#26426;&#21046;&#65292;&#20026;FU&#26426;&#21046;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FU&#26426;&#21046;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#26435;&#34913;&#65292;&#30830;&#35748;&#20102;&#20174;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01620</link><description>&lt;p&gt;
Voice EHR:&#24341;&#20837;&#22810;&#27169;&#24335;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Voice EHR: Introducing Multimodal Audio Data for Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24555;&#36895;&#20998;&#31867;&#24739;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#65292;&#24182;&#21487;&#33021;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#25913;&#21892;&#32467;&#26524;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#39640;&#25910;&#20837;&#12289;&#33521;&#35821;&#22269;&#23478;&#20351;&#29992;&#26114;&#36149;&#35760;&#24405;&#35774;&#22791;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#25216;&#26415;&#38754;&#20020;&#36164;&#28304;&#21463;&#38480;&#12289;&#39640;&#25910;&#20837;&#22330;&#25152;&#30340;&#37096;&#32626;&#25361;&#25112;&#65292;&#38899;&#39057;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#25910;&#38598;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20165;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;/&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#23427;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#20256;&#32479;&#35821;&#38899;/&#21628;&#21560;&#29305;&#24449;&#12289;&#35821;&#38899;&#27169;&#24335;&#21644;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#35821;&#35328;&#30340;&#22797;&#26434;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34917;&#20607;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#20249;&#20276;&#36130;&#22242;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01620v1 Announce Type: cross  Abstract: Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partner
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16687</link><description>&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16687
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21069;&#26223;&#12290; LLM&#20855;&#26377;&#35299;&#37322;&#30693;&#35782;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20026;&#23398;&#29983;&#25552;&#20379;&#23545;&#35805;&#24335;&#25945;&#23398;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#26816;&#39564;LLM&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#25945;&#23398;&#22330;&#26223;&#20013;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25945;&#32946;&#32773;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290; &#26412;&#30740;&#31350;&#25307;&#21215;&#20102;34&#21517;&#26412;&#31185;&#29983;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#38543;&#26426;&#20998;&#20026;&#20004;&#32452;&#12290; &#23454;&#39564;&#32452;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#24335;&#25945;&#23398;&#65292;&#32780;&#25511;&#21046;&#32452;&#19982;&#20154;&#31867;&#25945;&#24072;&#20114;&#21160;&#12290; &#20004;&#32452;&#37117;&#23398;&#20064;&#20102;&#20449;&#24687;&#30456;&#20851;&#35838;&#31243;&#8220;&#25968;&#23383;&#22270;&#20687;&#8221;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;</title><link>https://arxiv.org/abs/2403.15301</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#31574;&#30053;&#22522;&#30784;&#36827;&#34892;&#35268;&#21010;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15301
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#23398;&#20064;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#27599;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#23376;&#38382;&#39064;&#65292;&#22312;FSA&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#65292;&#32452;&#21512;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#21487;&#29992;&#20110;&#26080;&#38656;&#39069;&#22806;&#23398;&#20064;&#29983;&#25104;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#26041;&#27861;&#33021;&#22815;&#28176;&#36817;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#35268;&#33539;&#30340;&#24773;&#26223;&#20013;&#23398;&#20064;&#33021;&#22815;&#21487;&#38752;&#27867;&#21270;&#20110;&#22810;&#20010;&#20219;&#21153;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32487;&#25215;&#29305;&#24449;&#26469;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#22522;&#30784;&#65292;&#20351;&#24471;&#20854;&#20013;&#30340;&#27599;&#19968;&#20010;&#65288;&#23376;&#65289;&#31574;&#30053;&#35299;&#20915;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#30001;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#25551;&#36848;&#30340;&#20219;&#21153;&#20013;&#28041;&#21450;&#30456;&#21516;&#19968;&#32452;&#23376;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#32452;&#21512;&#21487;&#20197;&#34987;&#29992;&#26469;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#36890;&#36807;&#35268;&#21010;&#32452;&#21512;&#65288;&#23376;&#65289;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28176;&#36817;&#19978;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.12588</link><description>&lt;p&gt;
&#20027;&#35201;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning of the Prime Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12588
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#32032;&#25968;&#23398;&#20064;&#24615;&#36136;&#30340;&#29702;&#35770;&#35770;&#35777;&#65292;&#21457;&#29616;Erd\H{o}s-Kac&#23450;&#24459;&#19981;&#22826;&#21487;&#33021;&#34987;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#65292;&#24182;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#29109;&#26041;&#27861;&#25512;&#23548;&#20102;&#27010;&#29575;&#25968;&#35770;&#20013;&#30340;&#20960;&#20010;&#23450;&#29702;&#65292;&#21253;&#25324;&#21704;&#20195;-&#25289;&#39532;&#21162;&#37329;&#23450;&#29702;&#30340;&#19968;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#35770;&#35777;&#65292;&#35299;&#37322;&#20102;Y.-H. He&#20851;&#20110;&#32032;&#25968;&#21487;&#23398;&#24615;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#24182;&#20551;&#35774;Erd\H{o}s-Kac&#23450;&#24459;&#26497;&#19981;&#21487;&#33021;&#34987;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21457;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12588v1 Announce Type: cross  Abstract: In the present work we use maximum entropy methods to derive several theorems in probabilistic number theory, including a version of the Hardy-Ramanujan Theorem. We also provide a theoretical argument explaining the experimental observations of Y.-H. He about the learnability of primes, and posit that the Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning techniques. Numerical experiments that we perform corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.12451</link><description>&lt;p&gt;
INSIGHT: &#24102;&#26377;&#35821;&#35328;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#31526;&#21495;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#65288;NS-RL&#65289;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#20915;&#31574;&#21046;&#23450;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20854;&#29305;&#28857;&#26159;&#31526;&#21495;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#35270;&#35273;&#35266;&#27979;&#30340;&#20219;&#21153;&#65292;NS-RL&#28041;&#21450;&#23545;&#29366;&#24577;&#36827;&#34892;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#26080;&#27861;&#21033;&#29992;&#22870;&#21169;&#20449;&#21495;&#26469;&#32454;&#21270;&#32467;&#26500;&#21270;&#29366;&#24577;&#12290;&#21487;&#35775;&#38382;&#24615;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#37322;&#24403;&#21069;&#30340;&#31526;&#21495;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#25919;&#31574;&#21644;&#20915;&#31574;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;&#22312;&#20061;&#20010;Atari&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12451v1 Announce Type: new  Abstract: Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module. Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrat
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.05732</link><description>&lt;p&gt;
&#20445;&#23432;DDPG - &#26080;&#38598;&#25104;&#30340;&#24754;&#35266;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conservative DDPG -- Pessimistic RL without Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#23432;DDPG&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;$Q$-&#30446;&#26631;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#24809;&#32602;&#26469;&#35299;&#20915;DDPG&#20013;&#30340;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#23454;&#29616;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DDPG&#21463;&#21040;&#39640;&#20272;&#20559;&#24046;&#38382;&#39064;&#30340;&#38459;&#30861;&#65292;&#20854;&#20013;&#20854;$Q$-&#20272;&#35745;&#20542;&#21521;&#20110;&#22840;&#22823;&#23454;&#38469;$Q$&#20540;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#19968;&#20559;&#35265;&#30340;&#26041;&#27861;&#28041;&#21450;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25110;&#32773;&#22522;&#20110;&#22797;&#26434;&#23545;&#25968;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#38590;&#20197;&#29702;&#35299;&#21644;&#23454;&#26045;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;$Q$-&#30446;&#26631;&#24182;&#32467;&#21512;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#25439;&#22833;&#24809;&#32602;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#29992;&#36739;&#23569;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#20445;&#23432;DDPG&#22312;&#21508;&#31181;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#20248;&#20110;DDPG&#12290;&#25105;&#20204;&#22987;&#32456;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#22312;&#19982;TD3&#21644;TD7&#30456;&#27604;&#24615;&#33021;&#26356;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#20248;&#36234;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26159;&#20197;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#35201;&#27714;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05732v1 Announce Type: new  Abstract: DDPG is hindered by the overestimation bias problem, wherein its $Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to this bias involve ensemble-based methods, which require significant computational resources, or complex log-policy-based approaches, which are difficult to understand and implement. In contrast, we propose a straightforward solution using a $Q$-target and incorporating a behavioral cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can be easily implemented with minimal code and without the need for an ensemble. Our empirical findings strongly support the superiority of Conservative DDPG over DDPG across various MuJoCo and Bullet tasks. We consistently observe better performance in all evaluated tasks and even competitive or superior performance compared to TD3 and TD7, all achieved with significantly reduced computational requirements.
&lt;/p&gt;</description></item><item><title>RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.05010</link><description>&lt;p&gt;
RFWave&#65306;&#29992;&#20110;&#38899;&#39057;&#27874;&#24418;&#37325;&#24314;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05010
&lt;/p&gt;
&lt;p&gt;
RFWave&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#65292;&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#20986;&#33394;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#20174;&#19981;&#21516;&#34920;&#31034;&#20013;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#37325;&#24314;&#38899;&#39057;&#27874;&#24418;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#20010;&#21035;&#26679;&#26412;&#28857;&#32423;&#21035;&#36827;&#34892;&#25805;&#20316;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#22240;&#27492;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#24310;&#36831;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RFWave&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#39057;&#24102;&#25972;&#27969;&#27969;&#21160;&#26041;&#27861;&#65292;&#23427;&#20174;Mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#39640;&#20445;&#30495;&#24230;&#38899;&#39057;&#27874;&#24418;&#12290;RFWave&#22312;&#29983;&#25104;&#22797;&#26434;&#39057;&#35889;&#22270;&#24182;&#22312;&#24103;&#32423;&#21035;&#36816;&#34892;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#25152;&#26377;&#23376;&#24102;&#20197;&#22686;&#24378;&#25928;&#29575;&#12290;&#30001;&#20110;&#24076;&#26395;&#33719;&#24471;&#24179;&#32531;&#20256;&#36755;&#36712;&#36857;&#30340;&#25972;&#27969;&#27969;&#21160;&#65292;RFWave&#20165;&#38656;10&#20010;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;RFWave&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#37325;&#24314;&#36136;&#37327;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05010v1 Announce Type: cross  Abstract: Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although diffusion models have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a spee
&lt;/p&gt;</description></item><item><title>StereoDiffusion&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#28508;&#22312;&#21464;&#37327;&#23454;&#29616;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.04965</link><description>&lt;p&gt;
StereoDiffusion&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26080;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04965
&lt;/p&gt;
&lt;p&gt;
StereoDiffusion&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31435;&#20307;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#28508;&#22312;&#21464;&#37327;&#23454;&#29616;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21046;&#36896;&#21830;&#25512;&#20986;&#26356;&#22810;XR&#35774;&#22791;&#65292;&#23545;&#31435;&#20307;&#22270;&#20687;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StereoDiffusion&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#20462;&#34917;&#31649;&#36947;&#19981;&#21516;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#20351;&#29992;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#21407;&#22987;&#30340;Stable Diffusion&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20462;&#25913;&#20102;&#28508;&#22312;&#21464;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#12289;&#36731;&#37327;&#32423;&#30340;&#33021;&#21147;&#65292;&#24555;&#36895;&#29983;&#25104;&#31435;&#20307;&#22270;&#20687;&#23545;&#65292;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#25110;&#20219;&#20309;&#22270;&#20687;&#21518;&#22788;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36755;&#20837;&#29983;&#25104;&#24038;&#22270;&#20687;&#24182;&#20026;&#20854;&#20272;&#35745;&#35270;&#24046;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;Stereo Pixel Shift&#25805;&#20316;&#29983;&#25104;&#21491;&#22270;&#20687;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#36741;&#20197;&#23545;&#31216;&#20687;&#32032;&#20301;&#31227;&#25513;&#34109;&#21435;&#22122;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#21491;&#20391;&#22270;&#20687;&#19982;&#24038;&#20391;&#22270;&#20687;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20445;&#25345;&#39640;&#27700;&#20934;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04965v1 Announce Type: cross  Abstract: The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the ste
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.03020</link><description>&lt;p&gt;
SplAgger&#65306;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#21106;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SplAgger: Split Aggregation for Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#36825;&#20123;&#26234;&#33021;&#20307;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19968;&#31867;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34987;&#31216;&#20026;&#40657;&#30418;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#29616;&#25104;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20043;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#21478;&#19968;&#31867;&#26041;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#25512;&#26029;&#20986;&#26410;&#30693;&#20219;&#21153;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#24207;&#21015;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#20219;&#21153;&#25512;&#26029;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;&#20219;&#21153;&#25512;&#26029;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03020v1 Announce Type: cross  Abstract: A core ambition of reinforcement learning (RL) is the creation of agents capable of rapid learning in novel tasks. Meta-RL aims to achieve this by directly learning such agents. One category of meta-RL methods, called black box methods, does so by training off-the-shelf sequence models end-to-end. In contrast, another category of methods have been developed that explicitly infer a posterior distribution over the unknown task. These methods generally have distinct objectives and sequence models designed to enable task inference, and so are known as task inference methods. However, recent evidence suggests that task inference objectives are unnecessary in practice. Nonetheless, it remains unclear whether task inference sequence models are beneficial even when task inference objectives are not. In this paper, we present strong evidence that task inference sequence models are still beneficial. In particular, we investigate sequence models 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;</title><link>https://arxiv.org/abs/2403.02688</link><description>&lt;p&gt;
DOCTOR: &#38024;&#23545;&#26102;&#38388;&#28418;&#31227;&#28909;&#21464;&#21270;&#30340;&#21160;&#24577;&#33455;&#29255;&#30699;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal Variations Toward Self-Corrected Photonic Tensor Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02688
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;DOCTOR&#65292;&#38024;&#23545;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#20013;&#30340;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#12289;&#21407;&#20301;&#20934;&#30830;&#24230;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photonic computing&#20316;&#20026;&#21152;&#36895;&#35745;&#31639;&#23494;&#38598;&#22411;&#20154;&#24037;&#26234;&#33021;(AI)&#24037;&#20316;&#36127;&#36733;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#12289;&#24310;&#36831;&#25935;&#24863;&#30340;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#36895;&#24230;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20809;&#23376;&#24352;&#37327;&#21152;&#36895;&#22120;&#30340;&#37096;&#32626;&#36935;&#21040;&#20102;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#30001;&#20110;&#30828;&#20214;&#22122;&#22768;&#21644;&#29615;&#22659;&#21464;&#21270;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#33073;&#26426;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#21644;&#29255;&#19978;&#35757;&#32451;&#26469;&#22686;&#24378;&#23545;&#20855;&#26377;&#36866;&#24230;&#12289;&#38745;&#24577;&#22122;&#22768;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#21464;&#21270;&#23481;&#24525;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#26102;&#38388;&#28418;&#31227;&#21464;&#21270;&#23548;&#33268;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#38656;&#35201;&#23454;&#26102;&#12289;&#21407;&#20301;&#26657;&#20934;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21160;&#24577;&#29255;&#19978;&#30699;&#27491;&#26694;&#26550;&#65292;&#31216;&#20026;DOCTOR&#65292;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;&#12289;&#21407;&#20301;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#38024;&#23545;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02688v1 Announce Type: cross  Abstract: Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads, offering unparalleled speed and energy efficiency, especially in resource-limited, latency-sensitive edge computing environments. However, the deployment of analog photonic tensor accelerators encounters reliability challenges due to hardware noises and environmental variations. While off-chip noise-aware training and on-chip training have been proposed to enhance the variation tolerance of optical neural accelerators with moderate, static noises, we observe a notable performance degradation over time due to temporally drifting variations, which requires a real-time, in-situ calibration mechanism. To tackle this challenging reliability issues, for the first time, we propose a lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing adaptive, in-situ accuracy recovery against temporally
&lt;/p&gt;</description></item><item><title>&#24847;&#22823;&#21033;&#23545;ChatGPT&#23454;&#26045;&#31105;&#20196;&#21518;&#65292;&#19981;&#21516;&#32463;&#39564;&#30340;&#29992;&#25143;&#29983;&#20135;&#21147;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#22312;&#30701;&#26399;&#20869;&#20135;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#22343;&#26377;&#25552;&#21319;&#65292;&#32780;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#29983;&#20135;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.01964</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24322;&#36136;&#29983;&#20135;&#21147;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
The Heterogeneous Productivity Effects of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01964
&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#23545;ChatGPT&#23454;&#26045;&#31105;&#20196;&#21518;&#65292;&#19981;&#21516;&#32463;&#39564;&#30340;&#29992;&#25143;&#29983;&#20135;&#21147;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#22312;&#30701;&#26399;&#20869;&#20135;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#22343;&#26377;&#25552;&#21319;&#65292;&#32780;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#29983;&#20135;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#24847;&#22823;&#21033;&#23545;ChatGPT&#65288;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#30340;&#31105;&#20196;&#23545;&#20010;&#20154;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#24847;&#22823;&#21033;&#21450;&#20854;&#20182;&#27431;&#27954;&#22269;&#23478;&#36229;&#36807;36,000&#21517;GitHub&#29992;&#25143;&#30340;&#27599;&#26085;&#32534;&#30721;&#36755;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23558;&#36825;&#20123;&#25968;&#25454;&#19982;&#35813;&#31105;&#20196;&#30340;&#31361;&#28982;&#23459;&#24067;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24046;&#24322;&#24615;&#24046;&#24322;&#26694;&#26550;&#12290;&#22312;&#21463;&#24433;&#21709;&#30340;&#24847;&#22823;&#21033;&#29992;&#25143;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#65292;&#36755;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#30701;&#26399;&#20869;&#22686;&#21152;&#65292;&#32780;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#29983;&#20135;&#21147;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01964v1 Announce Type: cross  Abstract: We analyse the individual productivity effects of Italy's ban on ChatGPT, a generative pretrained transformer chatbot. We compile data on the daily coding output quantity and quality of over 36,000 GitHub users in Italy and other European countries and combine these data with the sudden announcement of the ban in a difference-in-differences framework. Among the affected users in Italy, we find a short-term increase in output quantity and quality for less experienced users and a decrease in productivity on more routine tasks for experienced users.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#24182;&#23558;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20419;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.01823</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#30340;RT-H&#21160;&#20316;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
RT-H: Action Hierarchies Using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01823
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#24182;&#23558;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20419;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20026;&#23558;&#22797;&#26434;&#27010;&#24565;&#20998;&#35299;&#20026;&#21487;&#28040;&#21270;&#30340;&#37096;&#20998;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#24335;&#12290;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24037;&#20316;&#20351;&#29992;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#31574;&#30053;&#65292;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#21644;&#35821;&#35328;&#20013;&#25351;&#23450;&#30340;&#39640;&#32423;&#20219;&#21153;&#26469;&#39044;&#27979;&#21160;&#20316;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#32467;&#26500;&#22312;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#20013;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#8220;&#25343;&#21487;&#20048;&#32592;&#8221;&#21644;&#8220;&#25688;&#33529;&#26524;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#22312;&#35821;&#20041;&#19978;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65288;&#20363;&#22914;&#65292;&#8220;&#25343;&#21487;&#20048;&#32592;&#8221;&#21644;&#8220;&#20498;&#26479;&#23376;&#8221;&#65289;&#65292;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#23398;&#20064;&#23558;&#39640;&#32423;&#20219;&#21153;&#26144;&#23556;&#21040;&#21160;&#20316;&#38656;&#35201;&#26356;&#22810;&#30340;&#28436;&#31034;&#25968;&#25454;&#12290;&#20026;&#20102;&#26550;&#36215;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25105;&#20204;&#30340;idea&#26159;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#29992;&#26356;&#31934;&#32454;&#30340;&#30701;&#35821;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#20363;&#22914;&#8220;&#21521;&#21069;&#31227;&#21160;&#25163;&#33218;&#8221;&#12290;&#23558;&#36825;&#20123;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#36843;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01823v1 Announce Type: cross  Abstract: Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., "pick coke can" and "pick an apple") in multi-task datasets. However, as tasks become more semantically diverse (e.g., "pick coke can" and "pour cup"), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like "move arm forward". Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.01773</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#25913;&#21892;&#22270;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving out-of-distribution generalization in graphs via hierarchical semantic environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20998;&#24067;&#36716;&#31227;&#21644;&#32570;&#20047;&#29615;&#22659;&#32972;&#26223;&#65292;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#29983;&#25104;&#24179;&#38754;&#29615;&#22659;&#26469;&#22686;&#24378;&#22270;&#30340;OOD&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24179;&#38754;&#29615;&#22659;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#21253;&#21547;&#21508;&#31181;&#35757;&#32451;&#29615;&#22659;&#65288;&#22914;&#39592;&#26550;&#12289;&#22823;&#23567;&#31561;&#65289;&#30340;DrugOOD&#25968;&#25454;&#38598;&#65292;&#24179;&#38754;&#29615;&#22659;&#26080;&#27861;&#20805;&#20998;&#35299;&#20915;&#20854;&#39640;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#29983;&#25104;&#26356;&#20855;&#35821;&#20041;&#20016;&#23500;&#30340;&#29615;&#22659;&#65292;&#20197;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#22270;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#36755;&#20837;&#22270;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#25552;&#21462;&#36755;&#20837;&#22270;&#20013;&#30340;&#21464;&#20307;&#23376;&#22270;&#65292;&#20197;&#22312;&#26412;&#22320;&#29615;&#22659;&#19978;&#29983;&#25104;&#20195;&#29702;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#38543;&#26426;&#27880;&#24847;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.15255</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Structure Learning Under Missing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15255
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24471;&#20998;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#23398;&#20064;&#35270;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#26469;&#25214;&#21040;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#20250;&#24341;&#20837;&#40481;&#29983;&#34507;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#26631;&#26159;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#20294;&#40065;&#26834;&#30340;&#25554;&#34917;&#38656;&#35201;&#32771;&#34385;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#25110;&#26356;&#22909;&#22320;&#22240;&#26524;&#20851;&#31995;&#12290;&#20165;&#20165;&#29992;&#29616;&#26377;&#30340;&#25554;&#34917;&#26041;&#27861;&#22635;&#20805;&#32570;&#22833;&#20540;&#65292;&#28982;&#21518;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#26500;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32570;&#22833;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#36825;&#31181;&#26368;&#20248;&#36755;&#36816;&#30340;&#35266;&#28857;&#19981;&#21516;&#20110;&#29616;&#26377;&#22522;&#20110;EM&#30340;&#22522;&#20110;&#24471;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#23398;&#20064;&#25237;&#24433;&#20026;&#23494;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#24341;&#36215;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#21644;&#35266;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#27779;&#23572;&#20177;&#26031;&#22374;&#36317;&#31163;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15255v1 Announce Type: cross  Abstract: Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive simulations and real-data experiments, our framework 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;</title><link>https://arxiv.org/abs/2402.15043</link><description>&lt;p&gt;
KIEval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;KIEval&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;LLM-powered "interactor"&#35282;&#33394;&#23454;&#29616;&#21160;&#24577;&#30340;&#25239;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#34987;&#22840;&#22823;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#26088;&#22312;&#26816;&#27979;&#21463;&#27745;&#26579;&#30340;&#25991;&#26412;&#65292;&#20294;&#20391;&#37325;&#20110;&#37327;&#21270;&#27745;&#26579;&#31243;&#24230;&#32780;&#38750;&#20934;&#30830;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KIEval&#65292;&#36825;&#26159;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#24335;&#20132;&#20114;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;LLM&#39537;&#21160;&#30340;&#8220;&#20132;&#20114;&#32773;&#8221;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25239;&#27745;&#26579;&#35780;&#20272;&#12290;&#20174;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#24120;&#35268;LLM&#22522;&#20934;&#38382;&#39064;&#24320;&#22987;&#65292;KIEval&#21033;&#29992;&#21160;&#24577;&#29983;&#25104;&#30340;&#12289;&#22810;&#36718;&#12289;&#20197;&#30693;&#35782;&#20026;&#37325;&#28857;&#30340;&#23545;&#35805;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#30340;&#21709;&#24212;&#26159;&#21542;&#20165;&#26159;&#22522;&#20934;&#31572;&#26696;&#30340;&#22238;&#24518;&#65292;&#36824;&#26159;&#34920;&#26126;&#20102;&#28145;&#20837;&#29702;&#35299;&#24182;&#33021;&#22312;&#26356;&#22797;&#26434;&#30340;&#23545;&#35805;&#20013;&#24212;&#29992;&#30693;&#35782;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19971;&#20010;&#39046;&#20808;&#30340;LLM&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;KI
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15043v1 Announce Type: cross  Abstract: Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered "interactor" role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model's response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KI
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.14859</link><description>&lt;p&gt;
&#20869;&#22312;&#30340;&#29436;&#65306;&#36890;&#36807;MLLM&#25805;&#20316;&#21592;&#21521;MLLM&#31038;&#20250;&#20013;&#28183;&#20837;&#24694;&#24847;
&lt;/p&gt;
&lt;p&gt;
The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14859
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35770;&#25991;&#25506;&#35752;&#20102;&#22312;MLLM&#31038;&#20250;&#20013;&#36890;&#36807;&#21333;&#20010;&#25805;&#20316;&#21592;&#38388;&#25509;&#24433;&#21709;&#20854;&#20182;&#20195;&#29702;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#26032;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22788;&#29702;&#21644;&#21709;&#24212;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#65292;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#19981;&#26029;&#23450;&#20041;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26032;&#36793;&#30028;&#12290;&#38543;&#30528;&#36825;&#20123;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24418;&#25104;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#30340;&#21327;&#20316;&#32593;&#32476;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#12298;&#20869;&#22312;&#30340;&#29436;&#12299;&#25506;&#35752;&#20102;MLLM&#31038;&#20250;&#20013;&#30340;&#19968;&#31181;&#26032;&#22411;&#28431;&#27934; - &#24694;&#24847;&#20869;&#23481;&#30340;&#38388;&#25509;&#20256;&#25773;&#12290;&#19982;&#30452;&#25509;&#20026;MLLM&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#20010;MLLM&#20195;&#29702;&#22914;&#20309;&#34987;&#24494;&#22937;&#22320;&#24433;&#21709;&#65292;&#20197;&#29983;&#25104;&#20877;&#27425;&#35825;&#20351;&#31038;&#20250;&#20013;&#20854;&#20182;MLLM&#20195;&#29702;&#36755;&#20986;&#24694;&#24847;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#32780;&#24378;&#26377;&#21147;&#30340;&#38388;&#25509;&#24433;&#21709;&#26041;&#27861;&#26631;&#24535;&#30528;&#19982;MLLM&#30456;&#20851;&#30340;&#23433;&#20840;&#39118;&#38505;&#30340;&#26174;&#33879;&#21319;&#32423;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#20960;&#20046;&#27809;&#26377;&#25110;&#26159;&#26681;&#26412;&#27809;&#26377;&#35775;&#38382;MLLM&#21442;&#25968;&#65292;&#19968;&#20010;MLLM&#20195;&#29702;&#65292;&#24403;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14859v1 Announce Type: cross  Abstract: Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.14856</link><description>&lt;p&gt;
&#22312;&#25512;&#29702;&#24605;&#32500;&#20013;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20986;&#23427;&#20204;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#24605;&#32500;&#22312;&#21046;&#23450;&#20581;&#20840;&#21644;&#36830;&#36143;&#35770;&#28857;&#26041;&#38754;&#25198;&#28436;&#20102;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#20801;&#35768;&#20010;&#20307;&#26681;&#25454;&#25152;&#25552;&#20379;&#20449;&#24687;&#30340;&#30495;&#20540;&#24471;&#20986;&#36923;&#36753;&#19978;&#30340;&#32467;&#35770;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#20854;&#25512;&#29702;&#34892;&#20026;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#21407;&#29702;&#65292;&#36890;&#36807;&#23545;&#23427;&#20204;&#23545;&#21629;&#39064;&#36923;&#36753;&#38382;&#39064;&#30340;&#21709;&#24212;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#26469;&#30740;&#31350;LLMs&#37319;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#25512;&#29702;&#27169;&#24335;&#65292;&#21253;&#25324;&#35832;&#22914;&#8220;&#20551;&#23450;&#36319;&#38543;&#8221;&#25110;&#8220;&#38142;&#26500;&#24314;&#8221;&#31561;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;arXiv:2402.14856v1 Announce Type: cross
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14856v1 Announce Type: cross  Abstract: Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the ar
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.14809</link><description>&lt;p&gt;
CriticBench&#65306;&#20026;&#25209;&#21028;&#24615;-&#27491;&#30830;&#25512;&#29702;&#35780;&#20272;LLMs&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14809
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#25209;&#21028;&#21644;&#32416;&#27491;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#36923;&#36753;&#20219;&#21153;&#26356;&#26131;&#20110;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25209;&#21028;&#21644;&#23436;&#21892;&#20854;&#25512;&#29702;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;&#35780;&#20272;&#12289;&#21453;&#39304;&#25552;&#20379;&#21644;&#33258;&#25105;&#25913;&#36827;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CriticBench&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25209;&#21028;&#21644;&#32416;&#27491;&#20854;&#25512;&#29702;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;CriticBench&#21253;&#21547;&#20116;&#20010;&#25512;&#29702;&#39046;&#22495;&#65306;&#25968;&#23398;&#12289;&#24120;&#35782;&#12289;&#31526;&#21495;&#12289;&#32534;&#30721;&#21644;&#31639;&#27861;&#12290;&#23427;&#25972;&#21512;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;LLM&#31995;&#21015;&#30340;&#21709;&#24212;&#12290;&#21033;&#29992;CriticBench&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#21078;&#26512;&#20102;17&#20010;LLMs&#22312;&#29983;&#25104;&#12289;&#25209;&#21028;&#21644;&#20462;&#27491;&#25512;&#29702;&#65288;&#21363;GQC&#25512;&#29702;&#65289;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;1&#65289;GQC&#33021;&#21147;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#25209;&#21028;&#24615;&#35757;&#32451;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20462;&#27491;&#25928;&#26524;&#22312;&#20219;&#21153;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#20197;&#36923;&#36753;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#26356;&#23481;&#26131;&#20462;&#27491;&#65307;&#65288;3&#65289;GQC&#30693;&#35782;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14809v1 Announce Type: cross  Abstract: The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14526</link><description>&lt;p&gt;
&#24102;&#32858;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24179;&#34913;&#25968;&#25454;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Balanced Data Sampling for Language Model Training with Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClusterClip Sampling&#30340;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20026;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#21644;&#32452;&#25104;&#65292;&#20294;&#30830;&#23450;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#25277;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;LLM&#20351;&#29992;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25277;&#26679;&#31574;&#30053;&#24573;&#35270;&#20102;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#24615;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClusterClip Sampling&#65292;&#20197;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#30340;&#25991;&#26412;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ClusterClip Sampling&#21033;&#29992;&#25968;&#25454;&#32858;&#31867;&#26469;&#21453;&#26144;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#26681;&#25454;&#32858;&#31867;&#32467;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24120;&#35265;&#26679;&#26412;&#21644;&#31232;&#26377;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#37325;&#22797;&#35009;&#21098;&#25805;&#20316;&#26469;&#20943;&#36731;&#30001;&#20110;&#26469;&#33258;&#26576;&#20123;&#32858;&#31867;&#30340;&#26679;&#26412;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;ClusterClip Sampling&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14526v1 Announce Type: cross  Abstract: Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12843</link><description>&lt;p&gt;
&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;:&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#19981;&#23436;&#32654;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12843
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#38451;&#33021;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#30417;&#25511;&#21644;&#32500;&#25252;&#26041;&#27861;&#26469;&#30830;&#20445;&#22826;&#38451;&#33021;&#38754;&#26495;&#23433;&#35013;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#22826;&#38451;&#33021;&#38754;&#26495;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#36816;&#34892;&#38382;&#39064;&#21644;&#35780;&#20272;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#38754;&#26495;&#20998;&#21106;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#25163;&#21160;&#26631;&#27880;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#21171;&#21160;&#23494;&#38598;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#24182;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;SSL&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#20026;&#20581;&#22766;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12843v1 Announce Type: cross  Abstract: The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12789</link><description>&lt;p&gt;
&#26080;&#38656;&#20844;&#24179;&#35757;&#32451;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#65306;&#19968;&#31181;&#21463;&#24433;&#21709;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12789
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24212;&#35813;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#32676;&#20307;&#30340;&#20154;&#20204;&#21463;&#30410;&#65292;&#32780;&#32676;&#20307;&#20449;&#24687;&#24448;&#24448;&#26159;&#25935;&#24863;&#30340;&#65292;&#19981;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20294;&#25490;&#38500;&#25935;&#24863;&#23646;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#32780;&#19981;&#23454;&#29616;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20855;&#26377;&#36866;&#24403;&#20998;&#24067;&#20559;&#31227;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20844;&#24179;&#24046;&#36317;&#30340;&#19978;&#38480;&#21644;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#21487;&#20197;&#21516;&#27493;&#25552;&#39640;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25277;&#26679;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#35775;&#38382;&#26032;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.11338</link><description>&lt;p&gt;
&#20855;&#26377;&#37096;&#20998;&#21453;&#39304;&#30340;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32570;&#20047;&#37096;&#20998;&#21453;&#39304;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#30830;&#20445;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#34987;&#25506;&#32034;&#12289;&#38450;&#27490;&#38169;&#35823;&#20998;&#31867;&#12289;&#20197;&#21450;&#25910;&#25947;&#21040;&#26399;&#26395;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39044;&#27979;&#22330;&#26223;&#65288;&#20363;&#22914;&#20449;&#36151;&#25918;&#27454;&#65289;&#20013;&#65292;&#21482;&#26377;&#36807;&#21435;&#34987;&#31215;&#26497;&#20998;&#31867;&#30340;&#26679;&#26412;&#25165;&#20250;&#35266;&#23519;&#21040;&#30495;&#23454;&#32467;&#26524;&#12290;&#36825;&#20123;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#24418;&#25104;&#20102;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#32570;&#20047;&#20851;&#20110;&#36807;&#21435;&#65288;&#38169;&#35823;&#22320;&#65289;&#34987;&#36127;&#38754;&#20998;&#31867;&#30340;&#26679;&#26412;&#32467;&#26524;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#25506;&#32034;&#31574;&#30053;&#26469;&#25910;&#38598;&#20851;&#20110;&#21542;&#21017;&#20250;&#34987;&#24573;&#30053;&#30340;&#23376;&#32676;&#20307;&#30340;&#32467;&#26524;&#25968;&#25454;&#12290;&#23545;&#20110;&#20219;&#20309;&#25506;&#32034;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#37117;&#20855;&#26377;&#20197;&#19979;&#20445;&#35777;&#65306;&#65288;1&#65289;&#25152;&#26377;&#23376;&#32676;&#20307;&#37117;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#65288;2&#65289;&#20551;&#38451;&#24615;&#30340;&#27604;&#20363;&#21463;&#21040;&#20102;&#38480;&#21046;&#65292;&#65288;3&#65289;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#25910;&#25947;&#21040;&#19968;&#20010;&#8220;&#26399;&#26395;&#8221;&#30340;&#20998;&#31867;&#22120;&#12290;&#27491;&#30830;&#30340;&#25506;&#32034;&#31574;&#30053;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#65307;&#23427;&#21487;&#20197;&#36873;&#25321;&#20197;&#25913;&#21892;&#23398;&#20064;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>BioFusionNet&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#19982;&#22522;&#22240;&#21644;&#20020;&#24202;&#25968;&#25454;&#34701;&#21512;&#65292;&#23454;&#29616;ER+&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#12290;</title><link>https://arxiv.org/abs/2402.10717</link><description>&lt;p&gt;
BioFusionNet&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#29305;&#24449;&#19982;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;ER+&#20083;&#33146;&#30284;&#20013;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;
&lt;/p&gt;
&lt;p&gt;
BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10717
&lt;/p&gt;
&lt;p&gt;
BioFusionNet&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#19982;&#22522;&#22240;&#21644;&#20020;&#24202;&#25968;&#25454;&#34701;&#21512;&#65292;&#23454;&#29616;ER+&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#39118;&#38505;&#20998;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VA...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10717v1 Announce Type: cross  Abstract: Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to captur
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09445</link><description>&lt;p&gt;
iMove: &#25506;&#32034;&#29992;&#20110;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#30340;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#30740;&#31350;&#35777;&#26126;&#29983;&#29289;&#38459;&#25239;&#20256;&#24863;&#25216;&#26415;&#21487;&#20197;&#25913;&#36827;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#65292;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21644;&#31934;&#30830;&#30340;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#23545;&#20110;&#20419;&#36827;&#20581;&#24247;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#21270;&#39044;&#38450;&#24615;&#21307;&#30103;&#20855;&#26377;&#30410;&#22788;&#12290;&#34429;&#28982;IMU&#30446;&#21069;&#26159;&#20027;&#35201;&#30340;&#20581;&#36523;&#36861;&#36394;&#27169;&#24335;&#65292;&#20294;&#36890;&#36807;iMove&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#21487;&#20197;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#21892;&#22522;&#20110;IMU&#30340;&#20581;&#36523;&#36861;&#36394;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#21253;&#25324;&#21313;&#20010;&#21463;&#35797;&#32773;&#22312;&#20116;&#22825;&#20869;&#36827;&#34892;&#30340;&#20845;&#31181;&#19978;&#36523;&#20581;&#36523;&#27963;&#21160;&#65292;&#20197;&#25910;&#38598;&#26469;&#33258;&#20004;&#21482;&#25163;&#33109;&#30340;&#29983;&#29289;&#38459;&#25239;&#21644;&#24038;&#25163;&#33109;IMU&#30340;&#21516;&#27493;&#25968;&#25454;&#12290;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#20004;&#31181;&#27169;&#24577;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20165;&#22522;&#20110;IMU&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#29983;&#29289;&#38459;&#25239;&#21482;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#35201;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20837;&#21333;&#20010;IMU&#30340;&#24179;&#22343;&#23439;F1&#20998;&#25968;&#25552;&#39640;&#20102;3.22&#65285;&#65292;&#36798;&#21040;84.71&#65285;&#65292;&#32780;IMU&#22522;&#32447;&#27169;&#22411;&#20026;81.49&#65285;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#29289;&#38459;&#25239;&#22914;&#20309;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09445v1 Announce Type: cross  Abstract: Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU baseline model. We have also shown how bio-impedance can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09259</link><description>&lt;p&gt;
SyntaxShap&#65306;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#35821;&#27861;&#24863;&#30693;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyntaxShap: Syntax-aware Explainability Method for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#21363;&#20351;&#29992;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SyntaxShap&#65292;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;Shapley&#20540;&#25193;&#23637;&#21040;&#32771;&#34385;&#22522;&#20110;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#12290;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#27604;&#36739;SyntaxShap&#21450;&#20854;&#21152;&#26435;&#24418;&#24335;&#19982;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22797;&#26434;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#35299;&#37322;&#19982;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09259v1 Announce Type: cross Abstract: To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08961</link><description>&lt;p&gt;
HyCubE: &#39640;&#25928;&#30340;&#30693;&#35782;&#36229;&#22270;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#27169;&#22411;&#32467;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#35821;&#20041;&#30693;&#35782;&#65292;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#35745;&#31639;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29305;&#24449;&#20132;&#20114;&#21644;&#25552;&#21462;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;&#27169;&#22411;HyCubE&#65292;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;3D&#29615;&#24418;&#21367;&#31215;&#26680;&#30340;&#22823;&#23567;&#65292;&#24182;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#65292;HyCubE&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#23454;&#20307;&#25513;&#30721;&#30340;1-N&#22810;&#32447;&#24615;&#35780;&#20998;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08961v1 Announce Type: new Abstract: Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant. Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency. In this paper, we propose a feature interaction and extraction-enhanced 3D circular convolutional embedding model, HyCubE, which designs a novel 3D circular convolutional neural network and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding. By adaptively adjusting the 3D circular convolution kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency. In addition, we use 1-N multilinear scoring based on the entity mask me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.08671</link><description>&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#22312;&#21305;&#37197;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Semi-Dense Detector-Free Methods Good at Matching Local Features?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#65292;&#22914;LoFTR&#65292;&#30446;&#21069;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#21305;&#37197;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;SDF&#26041;&#27861;&#34987;&#35757;&#32451;&#29992;&#20110;&#22312;&#20004;&#24133;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20960;&#20046;&#21482;&#20351;&#29992;&#30456;&#23545;&#20301;&#23039;&#20272;&#35745;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#22312;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#24471;&#21040;&#30340;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;&#65288;SAM&#65289;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;MegaDepth&#21644;HPatches&#65289;&#19978;&#23637;&#31034;&#19968;&#20010;&#36870;&#30452;&#35273;&#30340;&#32467;&#26524;&#65306;&#19968;&#26041;&#38754;&#65292;SAM&#22312;&#20301;&#23039;/&#21333;&#24212;&#24615;&#20272;&#35745;&#25351;&#26631;&#26041;&#38754;&#35201;&#20040;&#20248;&#20110;SDF&#26041;&#27861;&#65292;&#35201;&#20040;&#19982;&#20043;&#30456;&#24403;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;SAM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07875</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26410;&#35265;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#19981;&#28982;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#32463;&#24120;&#23637;&#29616;&#20986;&#19968;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#38544;&#24615;&#20559;&#24046;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26368;&#20248;&#25511;&#21046;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#21364;&#20102;&#35299;&#24471;&#36739;&#23569;&#12290;&#22312;&#37027;&#37324;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#24212;&#29992;&#20110;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#34987;&#31216;&#20026;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19988;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#31243;&#24230;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#26041;&#38754;&#30340;&#38544;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#20197;&#22522;&#26412;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#20026;&#37325;&#28857;&#65292;&#30830;&#31435;&#20102;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#24341;&#36215;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
&lt;/p&gt;</description></item><item><title>CyberMetric&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;10000&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#36890;&#36807;&#21512;&#20316;&#36807;&#31243;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.07688</link><description>&lt;p&gt;
CyberMetric: &#19968;&#20221;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CyberMetric: A Benchmark Dataset for Evaluating Large Language Models Knowledge in Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07688
&lt;/p&gt;
&lt;p&gt;
CyberMetric&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;10000&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#36890;&#36807;&#21512;&#20316;&#36807;&#31243;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#35780;&#20272;LLMs&#30340;&#30693;&#35782;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLMs&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#21307;&#23398;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#32593;&#32476;&#23433;&#20840;&#36825;&#20010;&#28085;&#30422;&#23494;&#30721;&#23398;&#12289;&#36870;&#21521;&#24037;&#31243;&#21644;&#39118;&#38505;&#35780;&#20272;&#31561;&#22810;&#26679;&#21270;&#39046;&#22495;&#30340;&#25361;&#25112;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CyberMetric&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#26631;&#20934;&#12289;&#35748;&#35777;&#12289;&#30740;&#31350;&#35770;&#25991;&#12289;&#20070;&#31821;&#21644;&#20854;&#20182;&#20986;&#29256;&#29289;&#30340;1&#19975;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#36807;&#19968;&#31181;&#21327;&#20316;&#36807;&#31243;&#21019;&#24314;&#65292;&#21363;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;LLM&#65288;&#21253;&#25324;GPT-3.5&#21644;Falcon-180B&#65289;&#30456;&#32467;&#21512;&#12290;&#20154;&#31867;&#19987;&#23478;&#33457;&#36153;&#20102;&#36229;&#36807;200&#23567;&#26102;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;&#38500;&#20102;&#35780;&#20272;LLM&#30340;&#30693;&#35782;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20419;&#36827;&#20154;&#31867;&#19982;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;80&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#22810;&#20010;&#20027;&#39064;&#65292;&#24182;&#35753;30&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel across various domains, from computer vision to medical diagnostics. However, understanding the diverse landscape of cybersecurity, encompassing cryptography, reverse engineering, and managerial facets like risk assessment, presents a challenge, even for human experts. In this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000 questions sourced from standards, certifications, research papers, books, and other publications in the cybersecurity domain. The questions are created through a collaborative process, i.e., merging expert knowledge with LLMs, including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's main goal is to facilitate a fair comparison between humans and different LLMs in cybersecurity. To achieve this, we carefully selected 80 questions covering a wide range of topics within cybersecurity and involved 30 participants 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05724</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#24182;&#19981;&#27604;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24179;&#22343;&#22330;&#21338;&#24328;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#20989;&#25968;&#36924;&#36817;&#19979;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#31574;&#30053;&#24615;&#25506;&#32034;&#20197;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#37096;&#20998;&#22522;&#20110;&#27169;&#22411;&#30340;Eluder&#32500;&#24230;&#65288;P-MBED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#27169;&#22411;&#31867;&#22797;&#26434;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;P-MBED&#21487;&#20197;&#34913;&#37327;&#20174;&#32473;&#23450;&#30340;&#24179;&#22343;&#22330;&#27169;&#22411;&#31867;&#36716;&#25442;&#32780;&#26469;&#30340;&#21333;&#20010;&#26234;&#33021;&#20307;&#27169;&#22411;&#31867;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#28508;&#22312;&#19978;&#21487;&#33021;&#27604;\citet{huang2023statistical}&#25552;&#20986;&#30340;MBED&#25351;&#25968;&#32423;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#65292;&#20855;&#26377;&#26032;&#39062;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;P-MBED&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#26412;&#21487;&#23454;&#29616;&#24615;&#21644;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#19979;&#65292;&#23398;&#20064;&#24179;&#22343;&#22330;&#21338;&#24328;&#30340;&#32435;&#20160;&#22343;&#34913;&#24182;&#19981;&#27604;&#35299;&#20915;&#23545;&#25968;&#20010;&#21333;&#20010;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26356;&#20855;&#32479;&#35745;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#22810;&#31867;&#22411;&#24179;&#22343;&#22330;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04601</link><description>&lt;p&gt;
Alirector: &#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;CGEC&#65289;&#22312;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26102;&#38754;&#20020;&#20005;&#37325;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#20915;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20462;&#27491;&#27169;&#22411;&#65292;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#21021;&#22987;&#20462;&#27491;&#12290;&#28982;&#21518;&#65292;&#23558;&#28304;&#21477;&#23376;&#19982;&#21021;&#22987;&#20462;&#27491;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#21478;&#19968;&#36718;&#20462;&#27491;&#65292;&#20197;&#20419;&#20351;&#23545;&#40784;&#27169;&#22411;&#19987;&#27880;&#20110;&#28508;&#22312;&#30340;&#36807;&#24230;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#35782;&#21035;&#32454;&#24494;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#28304;&#21477;&#23376;&#21644;&#21021;&#22987;&#20462;&#27491;&#30340;&#36870;&#21521;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#40784;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;CGEC&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04050</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#22312;&#23558;&#20854;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#25237;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#23613;&#31649;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#36890;&#24120;&#36873;&#25321;&#23558;&#20854;&#20316;&#20026;&#40657;&#30418;&#25552;&#20379;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#65288;CraFT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#40657;&#30418;VLMs fine-tuning&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;CraFT&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#25991;&#26412;&#25552;&#31034;&#30340;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#20197;&#27531;&#24046;&#26041;&#24335;&#22686;&#24378;&#36755;&#20986;&#39044;&#27979;&#30340;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20419;&#36827;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#19968;&#33268;&#20248;&#21270;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#35757;&#32451;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.02805</link><description>&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20013;&#30340;&#22270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#20005;&#37325;&#38477;&#32423;&#30340;&#38382;&#39064;&#65292;&#31361;&#26174;&#20102;LLMs&#22312;&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23558;LLMs&#20316;&#20026;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#35745;&#21010;&#25512;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39034;&#24207;&#21644;&#24182;&#34892;&#35268;&#21010;&#20197;&#20248;&#21270;&#26102;&#38388;&#25104;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25104;&#21151;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#22823;&#35268;&#27169;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;GPT-4&#21644;LLaMA-2&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;AsyncHow&#20013;&#65292;&#22312;&#27809;&#26377;&#25552;&#20379;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#30340;&#25554;&#22270;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Plan Like a Graph (PLaG)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#22270;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;PLaG&#33021;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;LLMs&#20173;&#28982;&#36973;&#21463;&#20005;&#37325;&#38477;&#32423;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;LLMs&#27169;&#25311;&#25968;&#23383;&#35774;&#22791;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#35270;&#20026;&#23558;LLMs&#29992;&#20316;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents.
&lt;/p&gt;</description></item><item><title>KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.02801</link><description>&lt;p&gt;
KS-Lottery: &#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#35777;&#24425;&#31080;
&lt;/p&gt;
&lt;p&gt;
KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02801
&lt;/p&gt;
&lt;p&gt;
KS-Lottery&#26159;&#19968;&#31181;&#23547;&#25214;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#21442;&#25968;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#23884;&#20837;&#23618;&#20013;&#21487;&#20197;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#24494;&#35843;&#20013;&#33719;&#24471;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#35777;&#20551;&#35828;&#35748;&#20026;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#8220;&#20013;&#22870;&#31080;&#8221;&#12290;&#22312;&#24494;&#35843;&#22330;&#26223;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#20013;&#22870;&#31080;&#65311;&#25105;&#20204;&#22914;&#20309;&#25214;&#21040;&#36825;&#26679;&#30340;&#20013;&#22870;&#31080;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KS-Lottery&#65292;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#22810;&#35821;&#35328;&#24494;&#35843;&#20013;&#39640;&#24230;&#26377;&#25928;&#30340;LLM&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#20998;&#26512;&#24494;&#35843;&#21069;&#21518;&#21442;&#25968;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#29702;&#35770;&#35777;&#26126;&#20102;KS-Lottery&#21487;&#20197;&#22312;&#23884;&#20837;&#23618;&#20013;&#25214;&#21040;&#35748;&#35777;&#30340;&#20013;&#22870;&#31080;&#65292;&#24494;&#35843;&#36825;&#20123;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#23558;KS-Lottery&#19982;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KS-Lottery&#25214;&#21040;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#21442;&#25968;&#38598;&#26469;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;18&#20010;&#26631;&#35760;&#30340;&#23884;&#20837;&#23618;
&lt;/p&gt;
&lt;p&gt;
The lottery ticket hypothesis posits the existence of ``winning tickets'' within a randomly initialized neural network. Do winning tickets exist for LLMs in fine-tuning scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of LLM parameters highly effective in multilingual fine-tuning. Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after fine-tuning. We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, fine-tuning on the found parameters is guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for fine-tuning while achieving the comparable performance as full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens' embeddin
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02713</link><description>&lt;p&gt;
&#19968;&#31687;&#20301;&#32622;&#35770;&#25991;: &#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26377;&#20160;&#20040;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: What Can Large Language Models Tell Us about Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02713
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#25552;&#21319;&#20915;&#31574;&#25928;&#29575;&#65292;&#25512;&#21160;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#30340;&#26222;&#21450;&#21270;&#12290;&#36825;&#31181;&#36827;&#23637;&#21487;&#20197;&#24102;&#26469;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#31561;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20855;&#22791;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#33021;&#21147;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#21457;&#23637;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#37327;&#27169;&#22411;&#35843;&#25972;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#30446;&#21069;&#30340;LLM&#20855;&#26377;&#39072;&#35206;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#25928;&#30340;&#20915;&#31574;&#21644;&#25512;&#36827;&#21521;&#26356;&#26222;&#36866;&#24418;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26234;&#33021;&#21457;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#21487;&#20197;&#25171;&#24320;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#21253;&#25324;&#27169;&#24577;&#20999;&#25442;&#21644;&#26102;&#38388;&#24207;&#21015;&#38382;&#31572;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#35748;&#35782;&#21040;LLM&#22312;&#25512;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#23545;&#36825;&#20123;&#30456;&#20851;&#24037;&#20316;&#30340;&#20449;&#20219;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.01789</link><description>&lt;p&gt;
LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Political Preferences of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01789
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;LLM&#30340;&#25919;&#27835;&#20559;&#22909;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#34920;&#29616;&#20986;&#24038;&#32764;&#35266;&#28857;&#65292;&#20294;&#38656;&#35880;&#24910;&#35299;&#35835;&#22522;&#30784;&#27169;&#22411;&#22312;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#20013;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#36825;&#37324;&#25253;&#21578;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20869;&#23884;&#30340;&#25919;&#27835;&#20559;&#22909;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;24&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#22411;LLM&#36827;&#34892;&#20102;11&#39033;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#65292;&#26088;&#22312;&#30830;&#23450;&#27979;&#35797;&#32773;&#30340;&#25919;&#27835;&#20559;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#20855;&#26377;&#25919;&#27835;&#21547;&#20041;&#30340;&#38382;&#39064;/&#38472;&#36848;&#36827;&#34892;&#25506;&#31350;&#26102;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#22411;LLM&#20542;&#21521;&#20110;&#29983;&#25104;&#34987;&#22823;&#22810;&#25968;&#25919;&#27835;&#27979;&#35797;&#20202;&#22120;&#35786;&#26029;&#20026;&#24038;&#32764;&#35266;&#28857;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#23545;&#20110;&#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#20248;&#21270;&#30340;LLM&#22522;&#30784;&#27169;&#22411;&#24182;&#38750;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#22522;&#30784;&#27169;&#22411;&#22312;&#36830;&#36143;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#20854;&#25919;&#27835;&#20542;&#21521;&#27979;&#35797;&#30340;&#20998;&#31867;&#36827;&#34892;&#35880;&#24910;&#35299;&#35835;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#23450;&#35770;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#26377;&#36259;&#30340;&#20551;&#35774;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#21363;&#25919;&#27835;&#20559;&#22909;&#20250;&#23884;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00839</link><description>&lt;p&gt;
X-CBA: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;CatBoosted Anomal-E&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;X-CBA&#30340;&#26032;&#39062;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#20197;&#22788;&#29702;&#32593;&#32476;&#23041;&#32961;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23041;&#32961;&#26085;&#30410;&#22797;&#26434;&#30340;&#26102;&#20195;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#30340;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20026;&#35782;&#21035;&#35745;&#31639;&#26426;&#32593;&#32476;&#20013;&#30340;&#25915;&#20987;&#21644;&#24322;&#24120;&#25552;&#20379;&#20102;&#39640;&#25928;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;IDS&#20013;&#20351;&#29992;ML&#21644;DL&#27169;&#22411;&#23548;&#33268;&#20102;&#20449;&#20219;&#36196;&#23383;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#12290;&#36825;&#31181;IDS&#30740;&#31350;&#20013;&#30340;&#36879;&#26126;&#24230;&#24046;&#36317;&#26174;&#33879;&#65292;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#38382;&#36131;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#22411;IDS&#26041;&#27861;&#65292;&#31216;&#20026;X-CBA&#65292;&#23427;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32467;&#26500;&#20248;&#21183;&#26469;&#26377;&#25928;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;GNN&#20026;&#22522;&#30784;&#30340;IDS&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#32593;&#32476;&#27969;&#37327;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#36824;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65292;&#21253;&#25324;&#36793;&#23646;&#24615;&#65292;&#26469;&#21033;&#29992;&#26356;&#24191;&#27867;&#30340;&#27969;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00138</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Decomposable Submodular Maximization in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#20248;&#21270;&#35774;&#32622;&#29992;&#20110;&#22312;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#31169;&#26377;&#30340;&#32452;&#20214;&#20989;&#25968;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#21644;&#38598;&#20013;&#32858;&#21512;&#30340;&#26041;&#24335;&#26469;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31119;&#21033;&#26368;&#22823;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#20989;&#25968;&#20197;&#21450;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#21450;&#20854;&#20248;&#21270;&#38382;&#39064;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#32452;&#20998;&#20989;&#25968;&#30340;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#32452;&#20998;&#20989;&#25968;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#65288;&#20363;&#22914;&#21487;&#33021;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#20989;&#25968;&#65289;&#65292;&#19981;&#33021;&#24191;&#27867;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#20998;&#35299;&#23376;&#27169;&#20989;&#25968;&#20248;&#21270;&#30340;&#8220;&#32852;&#37030;&#20248;&#21270;&#8221;&#35774;&#32622;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#20989;&#25968;&#65292;&#38656;&#35201;&#26368;&#22823;&#21270;&#36825;&#20123;&#20559;&#22909;&#30340;&#21152;&#26435;&#21644;&#12290;&#25105;&#20204;&#22312;&#35813;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#27969;&#34892;&#30340;&#8220;&#36830;&#32493;&#36138;&#23146;&#8221;&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#30340;&#26041;&#24335;&#26397;&#30528;&#23616;&#37096;&#35299;&#21521;&#21069;&#36808;&#20986;&#23567;&#30340;&#23616;&#37096;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#23616;&#37096;&#21464;&#21270;&#32858;&#21512;&#21040;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2401.17244</link><description>&lt;p&gt;
LLaMP: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#26448;&#26009;&#30693;&#35782;&#26816;&#32034;&#21644;&#25552;&#28860;&#20013;&#30340;&#24378;&#22823;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17244
&lt;/p&gt;
&lt;p&gt;
LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLM&#22825;&#29983;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#29486;&#21644;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#12289;&#20020;&#26102;&#30340;&#21644;&#19981;&#21487;&#36991;&#20813;&#20855;&#26377;&#20559;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMP&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#30001;&#22810;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#25512;&#29702;&#19982;&#34892;&#21160;&#65288;ReAct&#65289;&#26234;&#33021;&#20307;&#21160;&#24577;&#19982;Materials Project (MP)&#19978;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;LLaMP&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26041;&#24335;&#30340;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#21363;&#26102;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#23384;&#20648;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#65288;&#22914;&#26230;&#20307;&#32467;&#26500;&#21644;&#24377;&#24615;&#24352;&#37327;&#65289;&#65292;&#24182;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#23558;&#39057;&#32321;&#35760;&#24405;&#30340;&#33021;&#24102;&#38388;&#38553;MAPE&#38477;&#20302;&#20102;5.21%&#65292;&#23558;&#26174;&#33879;&#30340;&#38169;&#35823;&#38477;&#20302;&#20102;1103.54%
&lt;/p&gt;
&lt;p&gt;
Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Graph Anomaly Detection via Normal Structure Regularisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#24335;GAD&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#27491;&#24120;&#33410;&#28857;&#21644;&#24322;&#24120;&#33410;&#28857;&#65288;&#31216;&#20026;&#24050;&#30693;&#24322;&#24120;&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#26080;&#27861;&#23637;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26102;&#24322;&#24120;&#12290;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;GAD&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#26816;&#27979;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#25311;&#21512;&#24050;&#30693;&#24322;&#24120;&#65292;&#23548;&#33268;&#23545;&#26410;&#30693;&#24322;&#24120;&#65288;&#21363;&#26410;&#34987;&#26631;&#35760;&#30340;&#24322;&#24120;&#33410;&#28857;&#65289;&#30340;&#24369;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#22788;&#29702;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#65292;&#26410;&#33021;&#26377;&#25928;&#25429;&#25417;GAD&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24503;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#24335;GAD&#26041;&#27861;&#65292;&#21363;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#65288;NSReg&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08540</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20551;&#35774;&#65306;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context by Gradient Descent?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#26159;&#21542;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#20551;&#35774;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#20551;&#35774;&#23384;&#22312;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20351;&#20854;&#19982;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#35821;&#22659;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#27169;&#22411;&#30340;&#35266;&#23519;&#21644;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;ICL&#21644;GD&#22312;&#35266;&#23519;&#28436;&#31034;&#39034;&#24207;&#19978;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#30340;In-Context Learning&#65288;ICL&#65289;&#30340;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#29616;&#35937;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#20102;&#35299;&#29978;&#23569;&#12290;&#20026;&#20102;&#35299;&#37322;ICL&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#23558;&#20854;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#38382;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#23454;&#38469;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26159;&#21542;&#25104;&#31435;&#65311;&#25105;&#20204;&#24378;&#35843;&#20808;&#21069;&#20316;&#21697;&#20013;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#20351;&#24471;&#23427;&#20204;&#30340;&#35821;&#22659;&#19982;&#35821;&#35328;&#27169;&#22411;&#23454;&#38469;&#35757;&#32451;&#26102;&#30340;&#23454;&#38469;&#35821;&#22659;&#24046;&#21035;&#24456;&#22823;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#25163;&#24037;&#26500;&#36896;&#30340;&#26435;&#37325;&#20855;&#26377;&#19982;&#30495;&#23454;LLM&#19981;&#21305;&#37197;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;ICL&#30446;&#26631;&#65288;&#26126;&#30830;&#20026;ICL&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#36825;&#19982;&#37326;&#22806;&#20986;&#29616;&#30340;ICL&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#23547;&#25214;&#20102;&#30495;&#23454;&#27169;&#22411;&#20013;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#21644;GD&#23545;&#20110;&#35266;&#23519;&#28436;&#31034;&#30340;&#39034;&#24207;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#25506;&#35752;&#24182;&#27604;&#36739;ICL&#19982;GD&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08540v4 Announce Type: replace-cross  Abstract: The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models?   We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild.   We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13883</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain-Independent Dynamic Programming. (arXiv:2401.13883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#33539;&#20363;&#22914;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010; (MIP) &#21644;&#32422;&#26463;&#35268;&#21010; (CP) &#26088;&#22312;&#35299;&#32806;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#36807;&#31243;&#65292;&#36825;&#26159;&#22768;&#26126;&#24615;&#38382;&#39064;&#27714;&#35299;&#30340;&#8220;&#22307;&#26479;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#65288;DIDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010; (DP) &#30340;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;DP&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#36890;&#24120;&#23427;&#34987;&#20316;&#20026;&#19968;&#31181;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328; (DyPDL)&#65292;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;AI&#35268;&#21010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#29992;&#26469;&#27714;&#35299;DyPDL&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19971;&#31181;DIDP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;11&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#31867;&#21035;&#30340;&#22522;&#20934;&#23454;&#20363;&#19978;&#65292;&#23558;&#25105;&#20204;&#30340;DIDP&#27714;&#35299;&#22120;&#19982;&#21830;&#19994;MIP&#21644;CP&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65288;&#20998;&#21035;&#27714;&#35299;MIP&#21644;CP&#27169;&#22411;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;DIDP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20248;&#20110;MIP&#65292;&#20063;&#20248;&#20110;CP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.06127</link><description>&lt;p&gt;
E$^{2}$GAN: &#39640;&#25928;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#30340;&#39640;&#25928;GANs
&lt;/p&gt;
&lt;p&gt;
E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#35774;&#22791;&#19978;&#22270;&#20687;&#32534;&#36753;&#65292;&#19968;&#31181;&#39640;&#24230;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955; (Stable Diffusion)&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GANs) &#30340;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#26102;&#36890;&#24120;&#30001;&#39640;&#31471;&#21830;&#29992;GPU&#29305;&#23450;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#27599;&#20010;&#29983;&#25104;&#30340; GAN &#37117;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24037;&#20316;&#26469;&#33719;&#24471;&#21508;&#31181;&#27010;&#24565;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#33021;&#21542;&#20351;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860; GANs &#30340;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#20041;&#29305;&#24449;&#30340;&#22522;&#26412; GAN &#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#19981;&#21516;&#30340;&#27010;&#24565;&#65292;&#28040;&#38500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04518</link><description>&lt;p&gt;
&#12298;&#25209;&#35780;&#30340;&#25209;&#35780;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#20803;&#25209;&#35780;(MetaCritique)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#25209;&#35780;&#30340;&#36136;&#37327;&#65292;&#24182;&#20197;F1&#20998;&#25968;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#24341;&#20837;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#26469;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#22312;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#34987;&#35777;&#26126;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25209;&#35780;&#26412;&#36523;&#36136;&#37327;&#26041;&#38754;&#32570;&#20047;&#21407;&#21017;&#24615;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#21019;&#20102;&#25209;&#35780;&#30340;&#25209;&#35780;&#65292;&#31216;&#20026;&#20803;&#25209;&#35780;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#25209;&#35780;&#30340;&#26694;&#26550;&#65292;&#20174;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20004;&#20010;&#26041;&#38754;&#26469;&#35780;&#20272;&#25209;&#35780;&#12290;&#25105;&#20204;&#35745;&#31639;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#20316;&#20026;&#25972;&#20307;&#35780;&#20998;&#65292;&#31216;&#20026;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#23376;&#20449;&#24687;&#21333;&#20803;(AIUs)&#65292;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#25551;&#36848;&#25209;&#35780;&#12290;&#20803;&#25209;&#35780;&#32771;&#34385;&#27599;&#20010;AIU&#65292;&#24182;&#32858;&#21512;&#27599;&#20010;AIU&#30340;&#21028;&#26029;&#24471;&#21040;&#25972;&#20307;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#35780;&#20272;&#36807;&#31243;&#28041;&#21450;&#22797;&#26434;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#20803;&#25209;&#35780;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#35780;&#20215;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critique, as a natural language description for assessing the quality of model-generated content, has been proven to play an essential role in the training, evaluation, and refinement of Large Language Models (LLMs). However, there is a lack of principled understanding in evaluating the quality of the critique itself. In this paper, we pioneer the critique of critique, termed MetaCritique, which is a framework to evaluate the critique from two aspects, i.e., factuality as precision score and comprehensiveness as recall score. We calculate the harmonic mean of precision and recall as the overall rating called F1 score. To obtain a reliable evaluation outcome, we propose Atomic Information Units (AIUs), which describe the critique in a more fine-grained manner. MetaCritique takes each AIU into account and aggregates each AIU's judgment for the overall score. Moreover, given the evaluation process involves intricate reasoning, our MetaCritique provides a natural language rationale to supp
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02051</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20987;&#36133;&#20154;&#31867;&#65306;&#39640;&#25928;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#35774;&#35745;&#30340;&#20363;&#23376;
&lt;/p&gt;
&lt;p&gt;
An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search. (arXiv:2401.02051v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02051
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35745;&#31639;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#36827;&#21270;&#26694;&#26550;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#35774;&#35745;&#39640;&#25928;&#31639;&#27861;&#36890;&#24120;&#38750;&#24120;&#32321;&#29712;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36827;&#21270;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;AEL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#12290;AEL&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#36827;&#21270;&#35745;&#31639;&#30340;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#33258;&#21160;&#35774;&#35745;&#12289;&#32452;&#21512;&#21644;&#20462;&#25913;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AEL&#26469;&#35774;&#35745;&#24341;&#23548;&#23616;&#37096;&#25628;&#32034;&#65288;GLS&#65289;&#30340;&#24341;&#23548;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#33879;&#21517;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;AEL&#33258;&#21160;&#28436;&#21270;&#20986;&#20248;&#31168;&#30340;GLS&#31639;&#27861;&#65292;&#22312;&#20004;&#22825;&#20869;&#23454;&#29616;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#21644;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;1,000&#20010;TSP20-TSP100&#23454;&#20363;&#21644;TSPLib&#23454;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AEL&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#22312;&#30456;&#21516;&#30340;&#36845;&#20195;&#39044;&#31639;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#35774;&#35745;&#30340;GLS&#31639;&#27861;&#12290;&#22312;1,000&#27425;&#36845;&#20195;&#20013;&#65292;&#23427;&#22312;TSP20&#21644;TSP50&#19978;&#36798;&#21040;0%&#38388;&#38553;&#65292;&#22312;TSP100&#19978;&#36798;&#21040;0.032%&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26631;&#24535;&#30528;&#33258;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#26032;&#26102;&#20195;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often very tedious for human experts to design efficient algorithms. Recently, we have proposed a novel Algorithm Evolution using Large Language Model (AEL) framework for automatic algorithm design. AEL combines the power of a large language model and the paradigm of evolutionary computation to design, combine, and modify algorithms automatically. In this paper, we use AEL to design the guide algorithm for guided local search (GLS) to solve the well-known traveling salesman problem (TSP). AEL automatically evolves elite GLS algorithms in two days, with minimal human effort and no model training. Experimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show that AEL-designed GLS outperforms state-of-the-art human-designed GLS with the same iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap on TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in automatic algorithm design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;</title><link>http://arxiv.org/abs/2312.10302</link><description>&lt;p&gt;
&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#25968;&#25454;&#25506;&#32034;&#32773;&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#26159;&#26377;&#25928;&#21033;&#29992;&#20854;&#39044;&#35757;&#32451;&#33021;&#21147;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25193;&#23637;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20294;&#32570;&#20047;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#30340;&#26126;&#30830;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#20013;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;Nuggets&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#12290;Nuggets&#35780;&#20272;&#21333;&#20010;&#25351;&#23548;&#31034;&#20363;&#20316;&#20026;&#26377;&#25928;&#21333;&#27425;&#31034;&#20363;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#35782;&#21035;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#21508;&#31181;&#20219;&#21153;&#24615;&#33021;&#30340;&#31034;&#20363;&#12290;Nuggets&#21033;&#29992;&#22522;&#20110;&#20505;&#36873;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;MT-Bench&#21644;Alpaca-Ev&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.07586</link><description>&lt;p&gt;
&#29305;&#24449;&#24341;&#23548;&#65306;&#22823;&#23610;&#24230;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#23610;&#24230;&#30340;&#23548;&#21521;&#19979;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#26657;&#27491;&#65292;&#20197;&#22686;&#24378;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20943;&#23569;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#23548;&#24341;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DDPM)&#32447;&#24615;&#22320;&#23558;&#19981;&#21516;&#30340;&#26465;&#20214;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25552;&#20379;&#23545;&#26679;&#26412;&#30340;&#22686;&#24378;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#24403;&#23548;&#21521;&#23610;&#24230;&#21464;&#22823;&#26102;&#20135;&#29983;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#24341;&#23548;&#65292;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#20026;&#26080;&#20998;&#31867;&#22120;&#23548;&#21521;&#30340;DDPM&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#29702;&#30340;&#38750;&#32447;&#24615;&#26657;&#27491;&#12290;&#36825;&#31181;&#26657;&#27491;&#36843;&#20351;&#23548;&#21521;&#30340;DDPM&#36981;&#23432;&#20854;&#24213;&#23618;&#25193;&#25955;&#36807;&#31243;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#26080;&#38656;&#23548;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29305;&#24449;&#24341;&#23548;&#22686;&#24378;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#39068;&#33394;&#21644;&#26333;&#20809;&#38382;&#39064;&#65292;&#23545;&#20174;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21040;&#35299;&#20915;&#29289;&#29702;&#38382;&#39064;&#22914;&#30913;&#30456;&#21464;&#30340;&#21508;&#31181;&#24212;&#29992;&#37117;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2311.00889</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#39564;&#35777;&#65306;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generate and Pray: Using SALLMS to Evaluate the Security of LLM Generated Code. (arXiv:2311.00889v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;SALLMS&#35780;&#20272;LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#65292;&#25351;&#20986;&#29616;&#26377;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GitHub Copilot&#65292;ChatGPT&#31561;&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#26085;&#24120;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#30830;&#20445;&#36825;&#20123;&#24037;&#20855;&#29983;&#25104;&#30340;&#20195;&#30721;&#19981;&#20165;&#21151;&#33021;&#27491;&#30830;&#65292;&#32780;&#19988;&#27809;&#26377;&#28431;&#27934;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;LLM&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#20294;&#20043;&#21069;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;LLM&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#12290;&#23384;&#22312;&#20004;&#20010;&#23548;&#33268;&#19981;&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#27809;&#26377;&#20805;&#20998;&#22320;&#20195;&#34920;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#30495;&#23454;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#31454;&#25216;&#32534;&#31243;&#25361;&#25112;&#25110;&#20197;&#35838;&#22530;&#24418;&#24335;&#20026;&#22522;&#30784;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#23558;&#34987;&#38598;&#25104;&#21040;&#26356;&#22823;&#30340;&#20195;&#30721;&#24211;&#20013;&#65292;&#24341;&#20837;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#32570;&#20047;&#19987;&#27880;&#20110;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#23433;&#20840;&#24615;&#30340;&#22522;&#20934;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#20391;&#37325;&#20110;&#21151;&#33021;&#24615;&#32780;&#24573;&#35270;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of Large Language Models (e.g. GitHub Copilot, ChatGPT, etc.) in software engineers' daily practices, it is important to ensure that the code generated by these tools is not only functionally correct but also free of vulnerabilities. Although LLMs can help developers to be more productive, prior empirical studies have shown that LLMs can generate insecure code. There are two contributing factors to the insecure code generation. First, existing datasets used to evaluate Large Language Models (LLMs) do not adequately represent genuine software engineering tasks sensitive to security. Instead, they are often based on competitive programming challenges or classroom-type coding tasks. In real-world applications, the code produced is integrated into larger codebases, introducing potential security risks. There's a clear absence of benchmarks that focus on evaluating the security of the generated code. Second, existing evaluation metrics primarily focus on the func
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17261</link><description>&lt;p&gt;
&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29399;&#21644;&#29483;&#30340;&#27604;&#20363;&#20026;1:1&#26102;&#65292;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#29399;&#21644;&#29483;&#20063;&#24212;&#26356;&#22909;&#22320;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21482;&#25552;&#20379;&#20102;&#8220;&#22810;&#26679;&#24615;&#8221;&#36825;&#20010;&#35299;&#37322;&#24615;&#20043;&#22806;&#30340;&#32500;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#12290;&#21333;&#23646;&#24615;&#24046;&#24322;&#65288;SaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#21333;&#20010;&#23646;&#24615;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#21452;&#23646;&#24615;&#24046;&#24322;&#65288;PaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#19968;&#23545;&#23646;&#24615;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#23646;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#22270;&#20687;&#30340;&#23646;&#24615;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;CLIP&#35780;&#20998;&#65288;HCS&#65289;&#65292;&#23427;&#36890;&#36807;&#27979;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#21521;&#37327;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous 
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07867</link><description>&lt;p&gt;
&#24265;&#20215;&#23545;&#35805;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27169;&#25311;&#29420;&#31435;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20811;&#21171;&#31119;&#24503;&#21644;&#32034;&#36125;&#23572;&#65288;1982&#65289;&#30340;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#21457;&#36865;&#32773;&#21644;&#19968;&#20010;&#25509;&#25910;&#32773;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#25910;&#25947;&#21040;&#25509;&#36817;&#28216;&#25103;&#20808;&#39564;&#26368;&#20248;&#22343;&#34913;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#22312;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#31243;&#24230;&#32473;&#20986;&#30340;&#32435;&#20160;&#22343;&#34913;&#19979;&#65292;&#25353;&#29031;&#26368;&#22823;&#31243;&#24230;&#36827;&#34892;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#36229;&#21442;&#25968;&#21644;&#28216;&#25103;&#30340;&#22791;&#36873;&#35268;&#33539;&#31283;&#20581;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#38388;&#26032;&#20852;&#36890;&#20449;&#24037;&#20316;&#20197;&#21450;&#30001;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32452;&#25104;&#30340;&#24066;&#22330;&#20013;&#30340;&#23467;&#26007;&#32463;&#27982;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.02072</link><description>&lt;p&gt;
DeepVol&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepVol: A Deep Transfer Learning Approach for Universal Asset Volatility Modeling. (arXiv:2309.02072v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02072
&lt;/p&gt;
&lt;p&gt;
DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27874;&#21160;&#24615;&#27169;&#22411;DeepVol&#65292;&#23427;&#22312;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#12290;DeepVol&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#36164;&#20135;&#12290;&#36825;&#19982;&#35745;&#37327;&#32463;&#27982;&#23398;&#25991;&#29486;&#20013;&#30340;&#20027;&#27969;&#20570;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#38656;&#35201;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#24341;&#20837;DeepVol&#20026;&#37329;&#34701;&#34892;&#19994;&#30340;&#27874;&#21160;&#24615;&#24314;&#27169;&#21644;&#39044;&#27979;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#21487;&#33021;&#20250;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DeepVol, a promising new deep learning volatility model that outperforms traditional econometric models in terms of model generality. DeepVol leverages the power of transfer learning to effectively capture and model the volatility dynamics of all financial assets, including previously unseen ones, using a single universal model. This contrasts to the prevailing practice in econometrics literature, which necessitates training separate models for individual datasets. The introduction of DeepVol opens up new avenues for volatility modeling and forecasting in the finance industry, potentially transforming the way volatility is understood and predicted.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2308.10379</link><description>&lt;p&gt;
&#24605;&#24819;&#31639;&#27861;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#24819;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. (arXiv:2308.10379v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#24605;&#24819;&#31639;&#27861;"&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#24819;&#25506;&#32034;&#65292;&#20197;&#20302;&#25104;&#26412;&#12289;&#20302;&#23384;&#20648;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#26088;&#22312;&#36229;&#36234;&#8220;&#36830;&#32493;&#24605;&#32500;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#37319;&#29992;&#22806;&#37096;&#25805;&#20316;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20572;&#27490;&#12289;&#20462;&#25913;&#65292;&#28982;&#21518;&#24674;&#22797;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#24335;&#22686;&#21152;&#20102;&#26597;&#35810;&#35831;&#27714;&#30340;&#25968;&#37327;&#65292;&#22686;&#21152;&#20102;&#25104;&#26412;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#24819;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#31639;&#27861;&#25512;&#29702;&#36335;&#24452;&#25512;&#21160;LLM&#65292;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#31639;&#27861;&#31034;&#20363;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#24490;&#29615;&#21160;&#21147;&#23398;&#65292;&#20165;&#20351;&#29992;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#26597;&#35810;&#25193;&#23637;&#20854;&#24605;&#24819;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20248;&#20110;&#26089;&#26399;&#30340;&#21333;&#27425;&#26597;&#35810;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#37319;&#29992;&#24191;&#27867;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#30340;&#22810;&#27425;&#26597;&#35810;&#31574;&#30053;&#19981;&#30456;&#19978;&#19979;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31639;&#27861;&#25351;&#23548;LLM&#21487;&#20197;&#20351;&#24615;&#33021;&#36229;&#36234;&#31639;&#27861;&#26412;&#36523;&#65292;&#36825;&#26263;&#31034;&#30528;
&lt;/p&gt;
&lt;p&gt;
Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10182</link><description>&lt;p&gt;
&#36890;&#36807;&#30495;&#23454;&#21402;&#20999;&#29255;CT&#27169;&#25311;&#25913;&#36827;&#36229;&#20998;&#36776;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation. (arXiv:2307.10182v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#25104;&#21151;&#29983;&#25104;&#19982;&#23454;&#38469;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#25311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#25311;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19982;AAPM-Mayo's 2016&#20302;&#21058;&#37327;CT&#22823;&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#38469;&#22270;&#20687;&#23494;&#20999;&#30456;&#20284;&#30340;&#21402;&#20999;&#29255;CT&#22270;&#20687;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20551;&#35774;&#25105;&#20204;&#30340;&#27169;&#25311;&#23558;&#20135;&#29983;&#19982;&#30495;&#23454;&#22270;&#20687;&#26356;&#19968;&#33268;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;PSNR&#21644;RMSE&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#26368;&#39640;PSNR&#20540;&#20026;D45&#21644;B30&#37325;&#24314;&#26680;&#20998;&#21035;&#20026;49.7369&#177;2.5223&#21644;48.5801&#177;7.3271&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#20197;0.0068&#177;0.0020&#21644;0.0108&#177;0.0099&#30340;RMSE&#20540;&#27880;&#20876;&#26368;&#20302;&#30340;&#35823;&#24046;&#65292;&#34920;&#26126;&#20854;&#20998;&#24067;&#26356;&#25509;&#36817;&#20110;&#30495;&#23454;&#30340;&#21402;&#20999;&#29255;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop and evaluate an innovative simulation algorithm for generating thick-slice CT images that closely resemble actual images in the AAPM-Mayo's 2016 Low Dose CT Grand Challenge dataset. The proposed method was evaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error (RMSE) metrics, with the hypothesis that our simulation would produce images more congruent with their real counterparts. Our proposed method demonstrated substantial enhancements in terms of both PSNR and RMSE over other simulation methods. The highest PSNR values were obtained with the proposed method, yielding 49.7369 $\pm$ 2.5223 and 48.5801 $\pm$ 7.3271 for D45 and B30 reconstruction kernels, respectively. The proposed method also registered the lowest RMSE with values of 0.0068 $\pm$ 0.0020 and 0.0108 $\pm$ 0.0099 for D45 and B30, respectively, indicating a distribution more closely aligned with the authentic thick-slice image. Further validation of the proposed simulation al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03212</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21306;&#22495;&#23884;&#20837;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#39640;&#24230;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#22478;&#24066;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#20851;&#27880;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#65288;ROMER&#65289;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#35282;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;&#19981;&#21463;&#21018;&#24615;&#37051;&#22495;&#26465;&#20214;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#20174;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#20013;&#23398;&#20064;&#22478;&#24066;&#21306;&#22495;&#34920;&#31034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#31227;&#21160;&#27969;&#27169;&#24335;&#12289;POI&#35821;&#20041;&#21644;&#31614;&#21040;&#21160;&#24577;&#20013;&#25429;&#25417;&#22810;&#35270;&#35282;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20840;&#23616;&#22270;&#27880;&#24847;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#39030;&#28857;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#32771;&#34385;&#21644;&#20849;&#20139;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#21033;&#29992;&#22806;&#37096;&#27880;&#24847;&#21147;&#23398;&#20064;&#26435;&#37325;&#26469;&#34701;&#21512;&#22810;&#35270;&#35282;&#23884;&#20837;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban region embedding is an important and yet highly challenging issue due to the complexity and constantly changing nature of urban data. To address the challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER) to capture multi-view dependencies and learn expressive representations of urban regions without the constraints of rigid neighbourhood region conditions. Our model focus on learn urban region representation from multi-source urban data. First, we capture the multi-view correlations from mobility flow patterns, POI semantics and check-in dynamics. Then, we adopt global graph attention networks to learn similarity of any two vertices in graphs. To comprehensively consider and share features of multiple views, a two-stage fusion module is further proposed to learn weights with external attention to fuse multi-view embeddings. Extensive experiments for two downstream tasks on real-world datasets demonstrate that our model outperforms state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.16064</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#29305;&#24449;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#20302;&#25928;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#20511;&#21161;&#26032;&#20852;&#30340;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#25509;&#25910;&#21040;&#30340;&#21253;&#21547;&#36739;&#23569;&#38544;&#31169;&#20449;&#24687;&#30340;&#25552;&#31034;&#20197;&#21450;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#26032;&#26694;&#26550;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#21152;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;ImageNet&#21644;DomainNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.04974</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#20195;&#20215;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#31181;&#38169;&#35823;&#21487;&#33021;&#20250;&#38459;&#27490;&#26426;&#22120;&#23398;&#20064;&#30340;&#37096;&#32626;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20445;&#23432;&#24615;&#30340;&#27169;&#22411;&#8212;&#8212;&#24403;&#23427;&#20204;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#26102;&#21487;&#20197;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#8212;&#8212;&#21487;&#33021;&#20250;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#24322;&#24120;&#25110;&#22797;&#26434;&#31034;&#20363;&#26126;&#26174;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26080;&#27861;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#36741;&#21161;&#20266;OOD&#25968;&#25454;&#38598;&#19978;&#26368;&#23567;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#36741;&#21161;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#24863;&#20852;&#36259;&#30340;OOD&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#21644;OOD&#36755;&#20837;&#12290;&#21463;&#21040;&#36825;&#19968;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35774;&#35745;&#24072;&#35782;&#21035;&#35774;&#35745;&#20462;&#25913;&#65292;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#22312;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11308</link><description>&lt;p&gt;
&#35774;&#35745;&#20013;&#30340;&#21453;&#20107;&#23454;&#65306;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#24314;&#35758;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations. (arXiv:2305.11308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35774;&#35745;&#24072;&#35782;&#21035;&#35774;&#35745;&#20462;&#25913;&#65292;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#22312;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35774;&#35745;&#38382;&#39064;&#21453;&#20107;&#23454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#12290;&#21453;&#20107;&#23454;&#26159;&#25351;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#20915;&#31574;&#25110;&#36873;&#25321;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#21453;&#20107;&#23454;&#25628;&#32034;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#35774;&#35745;&#24314;&#35758;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#23545;&#35774;&#35745;&#36827;&#34892;&#20462;&#25913;&#65292;&#20174;&#32780;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#20419;&#36827;&#30446;&#26631;&#26435;&#34913;&#21487;&#35270;&#21270;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20108;&#32500;&#27979;&#35797;&#26696;&#20363;&#35777;&#26126;&#20102;MCD&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#19977;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;MCD&#22312;&#23454;&#38469;&#35774;&#35745;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;MCD&#22312;&#25512;&#33616;&#23545;&#26597;&#35810;&#35774;&#35745;&#36827;&#34892;&#20462;&#25913;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#34892;&#36710;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method for counterfactual optimization in design problems. Counterfactuals are hypothetical situations that can lead to a different decision or choice. In this paper, the authors frame the counterfactual search problem as a design recommendation tool that can help identify modifications to a design, leading to better functional performance. MCD improves upon existing counterfactual search methods by supporting multi-objective queries, which are crucial in design problems, and by decoupling the counterfactual search and sampling processes, thus enhancing efficiency and facilitating objective tradeoff visualization. The paper demonstrates MCD's core functionality using a two-dimensional test case, followed by three case studies of bicycle design that showcase MCD's effectiveness in real-world design problems. In the first case study, MCD excels at recommending modifications to query designs that can significantly enha
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#26469;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#65292;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10578</link><description>&lt;p&gt;
&#37327;&#21270;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Benefit of Artificial Intelligence for Scientific Research. (arXiv:2304.10578v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#26469;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#65292;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38761;&#21629;&#26377;&#21487;&#33021;&#25913;&#21464;&#20960;&#20046;&#25152;&#26377;&#30340;&#34892;&#19994;&#12290;&#38543;&#30528;AI&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#31934;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;AI&#21487;&#33021;&#20250;&#22312;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#29978;&#33267;&#21462;&#20195;&#20154;&#31867;&#19987;&#23478;&#12290;&#23613;&#31649;&#20154;&#20204;&#19981;&#26029;&#21162;&#21147;&#30740;&#31350;AI&#23545;&#21171;&#21160;&#21147;&#21644;&#32463;&#27982;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23427;&#22312;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#21644;&#36827;&#27493;&#26041;&#38754;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#65292;&#21363;AI&#30340;&#36827;&#27493;&#22914;&#20309;&#22312;&#19981;&#21516;&#23398;&#31185;&#21644;&#39046;&#22495;&#20013;&#21463;&#30410;&#20110;&#31185;&#23398;&#30740;&#31350;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#34913;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;87.6&#30334;&#19975;&#31687;&#35770;&#25991;&#21644;7.1&#30334;&#19975;&#20221;&#19987;&#21033;&#65292;&#26469;&#20272;&#35745;AI&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#22312;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#29305;&#21035;&#26159;&#33258;2015&#24180;&#20197;&#26469;&#24320;&#22987;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#19988;&#20351;&#29992;AI&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#65292;&#26356;&#21487;&#33021;&#22312;&#20869;&#22806;&#37096;&#34987;&#39640;&#24230;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing artificial intelligence (AI) revolution has the potential to change almost every line of work. As AI capabilities continue to improve in accuracy, robustness, and reach, AI may outperform and even replace human experts across many valuable tasks. Despite enormous efforts devoted to understanding AI's impact on labor and the economy and its recent success in accelerating scientific discovery and progress, we lack a systematic understanding of how advances in AI may benefit scientific research across disciplines and fields. Here we develop a measurement framework to estimate both the direct use of AI and the potential benefit of AI in scientific research by applying natural language processing techniques to 87.6 million publications and 7.1 million patents. We find that the use of AI in research appears widespread throughout the sciences, growing especially rapidly since 2015, and papers that use AI exhibit an impact premium, more likely to be highly cited both within and out
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13335</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#28436;&#31034;&#32780;&#27809;&#26377;&#35775;&#38382;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#30340;&#23398;&#20064;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#23558;&#19987;&#23478;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#27010;&#29575;p(a|s)&#65288;&#20363;&#22914;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;BC&#65289;&#65292;&#35201;&#20040;&#23558;&#32852;&#21512;&#27010;&#29575;p(s,a)&#24314;&#27169;&#65288;&#20363;&#22914;&#65292;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#65289;&#12290;&#23613;&#31649;&#34892;&#20026;&#20811;&#38534;&#23545;&#20110;&#24314;&#27169;&#26465;&#20214;&#27010;&#29575;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#12290;&#34429;&#28982;&#23545;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#36973;&#21463;&#27969;&#24418;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#37319;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#19987;&#23478;&#34892;&#20026;&#65292;&#24182;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26681;&#25454;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.01246</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20248;&#21270;&#26234;&#33021;&#20307;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimizing Agent Collaboration through Heuristic Multi-Agent Planning. (arXiv:2301.01246v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#24863;&#30693;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#35299;&#20915;QDec-POMDP&#30340;SOTA&#31639;&#27861;QDec-FP&#21644;QDec-FPS&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#26234;&#33021;&#20307;&#37319;&#21462;&#30456;&#21516;&#30340;&#35745;&#21010;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;QDec-FP&#21644;QDec-FPS&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SOTA algorithms for addressing QDec-POMDP issues, QDec-FP and QDec-FPS, are unable to effectively tackle problems that involve different types of sensing agents. We propose a new algorithm that addresses this issue by requiring agents to adopt the same plan if one agent is unable to take a sensing action but the other can. Our algorithm performs significantly better than both QDec-FP and QDec-FPS in these types of situations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.02295</link><description>&lt;p&gt;
&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#21327;&#35758;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#30340;&#21033;&#29992;&#29575;&#36234;&#26469;&#36234;&#39640;&#65292;&#25317;&#22622;&#26356;&#20026;&#39057;&#32321;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#20002;&#21253;&#29575;&#22686;&#21152;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#35774;&#35745;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#65292;&#38656;&#35201;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#26367;&#20195;&#20154;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32593;&#32476;&#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#65292;&#30446;&#21069;&#19981;&#21487;&#33021;&#22312;&#32593;&#32476;&#35774;&#22791;&#19978;&#37096;&#32626;AI&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;[arXiv:2207.02295]&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#35745;&#31639;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23558;&#20854;&#25512;&#29702;&#26102;&#38388;&#38477;&#20302;&#20102;500&#20493;&#65292;&#20351;&#20854;&#22312;&#956;&#31186;&#32423;&#20915;&#31574;&#26102;&#38388;&#35201;&#27714;&#20869;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#65292;&#19988;&#23545;&#36136;&#37327;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#26102;&#38598;&#32676;&#20013;&#37096;&#32626;&#20102;&#36716;&#25442;&#21518;&#30340;&#31574;&#30053;&#65292;&#24182;&#19982;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#37096;&#32626;&#30340;&#27969;&#34892;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31867;&#20284;&#30340;&#27969;&#37327;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#23614;&#37096;&#24310;&#36831;&#29575;&#25552;&#39640;&#20102;x%&#65292;&#23558;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#38477;&#20302;&#20102;y%&#12290;
&lt;/p&gt;
&lt;p&gt;
As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2201.05745</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Deep Optimal Transport for Domain Adaptation on SPD Manifolds. (arXiv:2201.05745v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05745
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;SPD&#27969;&#24418;&#19978;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#21327;&#26041;&#24046;&#30697;&#38453;&#25805;&#20316;&#30340;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30028;&#23545;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#35299;&#20915;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#38382;&#39064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#20852;&#36259;&#12290;&#36825;&#31181;&#20852;&#36259;&#28304;&#20110;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#22797;&#26434;&#31070;&#32463;&#29289;&#29702;&#25968;&#25454;&#65288;&#22914;&#33041;&#30005;&#22270;&#12289;&#33041;&#30913;&#22270;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#12290;&#36825;&#20123;&#25968;&#25454;&#34920;&#31034;&#20197;&#20449;&#21495;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24418;&#24335;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23545;&#31216;&#24615;&#21644;&#27491;&#23450;&#24615;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#22797;&#26434;&#25805;&#20316;&#29305;&#24615;&#65292;&#30452;&#25509;&#23558;&#20808;&#21069;&#30340;&#32463;&#39564;&#21644;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;DA&#38382;&#39064;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#26368;&#20248;&#20256;&#36755;&#12290;&#36825;&#19968;&#31867;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#24182;&#21033;&#29992;SPD&#27969;&#24418;&#30340;&#23545;&#25968;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in solving the domain adaptation (DA) problem on symmetric positive definite (SPD) manifolds within the machine learning community. This interest stems from the fact that complex neurophysiological data generated by medical equipment, such as electroencephalograms, magnetoencephalograms, and diffusion tensor imaging, often exhibit a shift in data distribution across different domains. These data representations, represented by signal covariance matrices, possess properties of symmetry and positive definiteness. However, directly applying previous experiences and solutions to the DA problem poses challenges due to the manipulation complexities of covariance matrices.To address this, our research introduces a category of deep learning-based transfer learning approaches called deep optimal transport. This category utilizes optimal transport theory and leverages the Log-Euclidean geometry for SPD manifolds. Additionally, we present a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2110.03443</link><description>&lt;p&gt;
&#25581;&#24320;&#40657;&#30418;&#23376;&#65306;&#35843;&#25511;&#31639;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#31639;&#27861;&#20915;&#31574;&#36827;&#34892;&#26368;&#20248;&#35843;&#25511;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38480;&#21046;&#20195;&#29702;&#20351;&#29992;&#36879;&#26126;&#24230;&#36275;&#22815;&#39640;&#30340;&#39044;&#27979;&#20989;&#25968;&#26159;&#20302;&#25928;&#30340;&#65292;&#32780;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#30340;&#30446;&#26631;&#21270;&#24037;&#20855;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25913;&#21892;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#20195;&#29702;&#20351;&#29992;&#22797;&#26434;&#30340;&#8220;&#40657;&#30418;&#8221;&#39044;&#27979;&#20989;&#25968;&#36827;&#34892;&#20915;&#31574;&#65288;&#22914;&#36151;&#27454;&#12289;&#21307;&#30103;&#27979;&#35797;&#25110;&#25307;&#32856;&#65289;&#19988;&#22996;&#25176;&#20154;&#22312;&#20102;&#35299;&#20195;&#29702;&#30340;&#40657;&#30418;&#27169;&#22411;&#26041;&#38754;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#22320;&#35843;&#25511;&#39044;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#35825;&#23548;&#19981;&#36275;&#65292;&#19988;&#26368;&#20248;&#39044;&#27979;&#20989;&#25968;&#36275;&#22815;&#22797;&#26434;&#65292;&#23558;&#20195;&#29702;&#38480;&#21046;&#22312;&#36275;&#22815;&#36879;&#26126;&#30340;&#39044;&#27979;&#20989;&#25968;&#20013;&#26159;&#20302;&#25928;&#30340;&#12290;&#31639;&#27861;&#23457;&#35745;&#26377;&#21161;&#20110;&#25552;&#39640;&#31119;&#21033;&#65292;&#20294;&#20854;&#25910;&#30410;&#21462;&#20915;&#20110;&#23457;&#35745;&#24037;&#20855;&#30340;&#35774;&#35745;&#12290;&#35768;&#22810;&#35299;&#37322;&#24037;&#20855;&#20542;&#21521;&#20110;&#26368;&#23567;&#21270;&#25972;&#20307;&#20449;&#24687;&#25439;&#22833;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38598;&#20013;&#20110;&#35299;&#37322;&#39044;&#27979;&#20989;&#25968;&#30340;&#24179;&#22343;&#34892;&#20026;&#12290;&#38024;&#23545;&#24615;&#30340;&#24037;&#20855;&#65292;&#22914;&#38024;&#23545;&#28608;&#21169;&#20559;&#24046;&#28304;&#22836;&#65288;&#22914;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#25110;&#31181;&#26063;&#24046;&#24322;&#65289;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#35777;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
&lt;/p&gt;</description></item></channel></rss>