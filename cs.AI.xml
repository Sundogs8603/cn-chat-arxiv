<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07921</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification. (arXiv:2308.07921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-4&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;PaLM-2&#22312;&#35299;&#20915;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23588;&#20854;&#26159;OpenAI&#30340;&#26368;&#26032;&#29256;&#26412;GPT-4 Code Interpreter&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#25506;&#35752;&#20102;&#20195;&#30721;&#23545;&#25552;&#21319;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20854;&#22312;&#29983;&#25104;&#21644;&#25191;&#34892;&#20195;&#30721;&#12289;&#35780;&#20272;&#20195;&#30721;&#25191;&#34892;&#32467;&#26524;&#20197;&#21450;&#20462;&#27491;&#35299;&#20915;&#26041;&#26696;&#26102;&#30340;&#24378;&#22823;&#25216;&#24039;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#33258;&#25105;&#39564;&#35777;&#65288;CSV&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;GPT-4 Code Interpreter&#30340;&#25968;&#23398;&#25512;&#29702;&#28508;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;GPT-4 Code Interpreter&#19978;&#37319;&#29992;&#38646;-shot&#25552;&#31034;&#65292;&#40723;&#21169;&#20854;&#20351;&#29992;&#20195;&#30721;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hierarchical Distance Query&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#26469;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;</title><link>http://arxiv.org/abs/2308.07903</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#29983;&#25104;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;
&lt;/p&gt;
&lt;p&gt;
Relightable and Animatable Neural Avatar from Sparse-View Video. (arXiv:2308.07903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#35270;&#35282;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hierarchical Distance Query&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36317;&#31163;&#26469;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#26410;&#30693;&#29031;&#26126;&#26465;&#20214;&#19979;&#30340;&#31232;&#30095;&#35270;&#35282;&#65288;&#29978;&#33267;&#21333;&#30446;&#65289;&#35270;&#39057;&#20013;&#21019;&#24314;&#21487;&#37325;&#20809;&#21644;&#21487;&#21160;&#21270;&#30340;&#31070;&#32463;&#21270;&#36523;&#30340;&#25361;&#25112;&#12290;&#19982;&#24037;&#20316;&#23460;&#29615;&#22659;&#30456;&#27604;&#65292;&#36825;&#20010;&#35774;&#32622;&#26356;&#23454;&#38469;&#21644;&#21487;&#34892;&#65292;&#20294;&#26159;&#38754;&#20020;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#36870;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#31070;&#32463;&#20154;&#31867;&#37325;&#24314;&#26041;&#27861;&#33021;&#22815;&#20351;&#29992;&#21464;&#24418;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#20174;&#31232;&#30095;&#35270;&#35282;&#37325;&#24314;&#21487;&#21160;&#21270;&#30340;&#21270;&#36523;&#65292;&#20294;&#26080;&#27861;&#24674;&#22797;&#29992;&#20110;&#37325;&#20809;&#30340;&#26448;&#26009;&#21442;&#25968;&#12290;&#34429;&#28982;&#21487;&#24494;&#36870;&#28210;&#26579;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#38745;&#24577;&#23545;&#35937;&#30340;&#26448;&#26009;&#65292;&#20294;&#23545;&#20110;&#21160;&#24577;&#20154;&#31867;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#21160;&#24577;&#20154;&#20307;&#26159;&#19981;&#30452;&#35266;&#30340;&#65292;&#22240;&#20026;&#22312;&#21464;&#24418;SDF&#19978;&#35745;&#31639;&#20687;&#32032;-&#34920;&#38754;&#30456;&#20132;&#21644;&#20809;&#33021;&#35265;&#24230;&#23545;&#20110;&#36870;&#28210;&#26579;&#26469;&#35828;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36317;&#31163;&#26597;&#35810;&#65288;HDQ&#65289;&#31639;&#27861;&#26469;&#36817;&#20284;&#20219;&#24847;&#20154;&#20307;&#23039;&#24577;&#19979;&#30340;&#19990;&#30028;&#31354;&#38388;&#36317;&#31163;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20272;&#31639;&#20102;&#31895;&#30053;&#30340;&#36317;&#31163;&#20540;&#65292;&#28982;&#21518;&#20351;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#25552;&#39640;&#36317;&#31163;&#20272;&#31639;&#30340;&#31934;&#24230;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20272;&#31639;&#20986;&#30340;&#36317;&#31163;&#20540;&#36827;&#34892;&#26448;&#26009;&#24674;&#22797;&#21644;&#37325;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of creating relightable and animatable neural avatars from sparse-view (or even monocular) videos of dynamic humans under unknown illumination. Compared to studio environments, this setting is more practical and accessible but poses an extremely challenging ill-posed problem. Previous neural human reconstruction methods are able to reconstruct animatable avatars from sparse views using deformed Signed Distance Fields (SDF) but cannot recover material parameters for relighting. While differentiable inverse rendering-based methods have succeeded in material recovery of static objects, it is not straightforward to extend them to dynamic humans as it is computationally intensive to compute pixel-surface intersection and light visibility on deformed SDFs for inverse rendering. To solve this challenge, we propose a Hierarchical Distance Query (HDQ) algorithm to approximate the world space distances under arbitrary human poses. Specifically, we estimate coarse
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20272;&#30340;&#22810;&#31687;&#35770;&#25991;&#65292;&#24635;&#32467;&#20986;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65306;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.07902</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Through the Lens of Core Competency: Survey on Evaluation of Large Language Models. (arXiv:2308.07902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26680;&#24515;&#31454;&#20105;&#21147;&#30340;&#35270;&#35282;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20272;&#30340;&#22810;&#31687;&#35770;&#25991;&#65292;&#24635;&#32467;&#20986;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65306;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24050;&#32463;&#35265;&#35777;&#20102;&#24555;&#36895;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26497;&#20854;&#22256;&#38590;&#65292;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;&#20256;&#32479;&#30340;NLP&#20219;&#21153;&#22312;LLM&#34920;&#29616;&#20986;&#33394;&#20043;&#21518;&#21464;&#24471;&#19981;&#22815;&#36866;&#29992;&#65307;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#20219;&#21153;&#38590;&#20197;&#36319;&#19978;&#23454;&#38469;&#22330;&#26223;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20934;&#26469;&#26356;&#22909;&#22320;&#35780;&#20272;LLM&#12290;&#20026;&#20102;&#28548;&#28165;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#19982;LLM&#35780;&#20272;&#30456;&#20851;&#30340;&#20247;&#22810;&#35770;&#25991;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31687;&#20851;&#20110;LLM&#35780;&#20272;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;LLM&#30340;4&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#21253;&#25324;&#25512;&#29702;&#33021;&#21147;&#12289;&#30693;&#35782;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20854;&#23450;&#20041;&#12289;&#23545;&#24212;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20010;&#26680;&#24515;&#31454;&#20105;&#21147;&#20307;&#31995;&#65292;&#31867;&#20284;&#30340;&#20219;&#21153;&#21487;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar ta
&lt;/p&gt;</description></item><item><title>CrystalShift&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26448;&#26009;&#30740;&#31350;&#20013;&#30340;&#27010;&#29575;XRD&#30456;&#20301;&#26631;&#35760;&#21644;&#26230;&#26684;&#32454;&#21270;&#12290;&#23427;&#36890;&#36807;&#37319;&#29992;&#23545;&#31216;&#32422;&#26463;&#30340;&#20266;&#32454;&#21270;&#20248;&#21270;&#12289;&#26368;&#20339;&#20248;&#20808;&#26641;&#25628;&#32034;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#26469;&#20272;&#35745;&#30456;&#20301;&#32452;&#21512;&#30340;&#27010;&#29575;&#65292;&#26080;&#38656;&#30456;&#20301;&#31354;&#38388;&#20449;&#24687;&#25110;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;CrystalShift&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#39640;&#36890;&#37327;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.07897</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#20027;&#26448;&#26009;&#30740;&#31350;&#30340;&#27010;&#29575;&#30456;&#20301;&#26631;&#35760;&#21644;&#26230;&#26684;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Phase Labeling and Lattice Refinement for Autonomous Material Research. (arXiv:2308.07897v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07897
&lt;/p&gt;
&lt;p&gt;
CrystalShift&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#20027;&#26448;&#26009;&#30740;&#31350;&#20013;&#30340;&#27010;&#29575;XRD&#30456;&#20301;&#26631;&#35760;&#21644;&#26230;&#26684;&#32454;&#21270;&#12290;&#23427;&#36890;&#36807;&#37319;&#29992;&#23545;&#31216;&#32422;&#26463;&#30340;&#20266;&#32454;&#21270;&#20248;&#21270;&#12289;&#26368;&#20339;&#20248;&#20808;&#26641;&#25628;&#32034;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#26469;&#20272;&#35745;&#30456;&#20301;&#32452;&#21512;&#30340;&#27010;&#29575;&#65292;&#26080;&#38656;&#30456;&#20301;&#31354;&#38388;&#20449;&#24687;&#25110;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;CrystalShift&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#39640;&#36890;&#37327;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#23556;&#32447;&#34893;&#23556;&#65288;XRD&#65289;&#26159;&#19968;&#31181;&#30830;&#23450;&#26448;&#26009;&#26230;&#20307;&#32467;&#26500;&#30340;&#22522;&#26412;&#25216;&#26415;&#65292;&#22312;&#33258;&#20027;&#31185;&#23398;&#21457;&#29616;&#36807;&#31243;&#20013;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#12290;&#28982;&#32780;&#65292;&#24555;&#36895;&#12289;&#33258;&#21160;&#21270;&#21644;&#21487;&#38752;&#30340;XRD&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#19982;&#36755;&#20837;&#25968;&#25454;&#36895;&#29575;&#30456;&#21305;&#37197;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrystalShift&#65292;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;XRD&#30456;&#20301;&#26631;&#35760;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#23545;&#31216;&#32422;&#26463;&#30340;&#20266;&#32454;&#21270;&#20248;&#21270;&#12289;&#26368;&#20339;&#20248;&#20808;&#26641;&#25628;&#32034;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65292;&#21487;&#20197;&#20272;&#35745;&#30456;&#20301;&#32452;&#21512;&#30340;&#27010;&#29575;&#65292;&#32780;&#26080;&#38656;&#30456;&#20301;&#31354;&#38388;&#20449;&#24687;&#25110;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CrystalShift&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#27010;&#29575;&#20272;&#35745;&#65292;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#39640;&#36890;&#37327;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#38500;&#20102;&#39640;&#25928;&#30340;&#30456;&#20301;&#26144;&#23556;&#65292;CrystalShift&#36824;&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#20869;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
X-ray diffraction (XRD) is an essential technique to determine a material's crystal structure in high-throughput experimentation, and has recently been incorporated in artificially intelligent agents in autonomous scientific discovery processes. However, rapid, automated and reliable analysis method of XRD data matching the incoming data rate remains a major challenge. To address these issues, we present CrystalShift, an efficient algorithm for probabilistic XRD phase labeling that employs symmetry-constrained pseudo-refinement optimization, best-first tree search, and Bayesian model comparison to estimate probabilities for phase combinations without requiring phase space information or training. We demonstrate that CrystalShift provides robust probability estimates, outperforming existing methods on synthetic and experimental datasets, and can be readily integrated into high-throughput experimental workflows. In addition to efficient phase-mapping, CrystalShift offers quantitative ins
&lt;/p&gt;</description></item><item><title>EduSAT&#26159;&#19968;&#20010;&#25945;&#23398;&#24037;&#20855;&#65292;&#29992;&#20110;&#25903;&#25345;SAT&#21644;SMT&#27714;&#35299;&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#12290;&#23427;&#25552;&#20379;&#20102;&#20851;&#38190;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;SAT&#21644;SMT&#30340;&#35299;&#20915;&#20197;&#21450;&#20854;&#20182;NP&#23436;&#20840;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.07890</link><description>&lt;p&gt;
EduSAT: &#29992;&#20110;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#29702;&#35770;&#21644;&#24212;&#29992;&#30340;&#25945;&#23398;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
EduSAT: A Pedagogical Tool for Theory and Applications of Boolean Satisfiability. (arXiv:2308.07890v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07890
&lt;/p&gt;
&lt;p&gt;
EduSAT&#26159;&#19968;&#20010;&#25945;&#23398;&#24037;&#20855;&#65292;&#29992;&#20110;&#25903;&#25345;SAT&#21644;SMT&#27714;&#35299;&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#12290;&#23427;&#25552;&#20379;&#20102;&#20851;&#38190;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;SAT&#21644;SMT&#30340;&#35299;&#20915;&#20197;&#21450;&#20854;&#20182;NP&#23436;&#20840;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#21644;&#21487;&#28385;&#36275;&#24615;&#27169;&#22411;&#29702;&#35770;&#65288;SMT&#65289;&#22312;&#33258;&#21160;&#39564;&#35777;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#32570;&#20047;&#19987;&#38376;&#29992;&#20110;&#25945;&#32946;&#30446;&#30340;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EduSAT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25903;&#25345;SAT&#21644;SMT&#27714;&#35299;&#30340;&#23398;&#20064;&#21644;&#29702;&#35299;&#32780;&#24320;&#21457;&#30340;&#25945;&#23398;&#24037;&#20855;&#12290;EduSAT&#25552;&#20379;&#20102;&#20851;&#38190;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#22914;Davis-Putnam-Logemann-Loveland&#65288;DPLL&#65289;&#31639;&#27861;&#21644;&#31616;&#21270;&#30340;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;ROBDD&#65289;&#29992;&#20110;SAT&#27714;&#35299;&#12290;&#27492;&#22806;&#65292;EduSAT&#36824;&#20026;SAT&#21644;SMT&#20043;&#22806;&#30340;&#20116;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#25552;&#20379;&#27714;&#35299;&#22120;&#25277;&#35937;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;EduSAT&#36827;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#21644;&#39564;&#35777;&#23545;SAT&#21644;SMT&#27714;&#35299;&#25216;&#26415;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#37197;&#26377;&#20840;&#38754;&#30340;&#25991;&#26723;&#21644;&#25945;&#31243;&#65292;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#20197;&#21450;&#23454;&#29992;&#30340;&#21151;&#33021;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#21644;SAT&#21644;SMT&#20844;&#24335;&#29983;&#25104;&#22120;&#65292;&#36825;&#20063;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean Satisfiability (SAT) and Satisfiability Modulo Theories (SMT) are widely used in automated verification, but there is a lack of interactive tools designed for educational purposes in this field. To address this gap, we present EduSAT, a pedagogical tool specifically developed to support learning and understanding of SAT and SMT solving. EduSAT offers implementations of key algorithms such as the Davis-Putnam-Logemann-Loveland (DPLL) algorithm and the Reduced Order Binary Decision Diagram (ROBDD) for SAT solving. Additionally, EduSAT provides solver abstractions for five NP-complete problems beyond SAT and SMT. Users can benefit from EduSAT by experimenting, analyzing, and validating their understanding of SAT and SMT solving techniques. Our tool is accompanied by comprehensive documentation and tutorials, extensive testing, and practical features such as a natural language interface and SAT and SMT formula generators, which also serve as a valuable opportunity for learners to d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23545;7&#20010;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;4&#31181;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20197;&#21450;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19979;&#30340;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.07889</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#23398;&#20064;&#30340;&#20851;&#31995;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning. (arXiv:2308.07889v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#36890;&#36807;&#23545;7&#20010;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;4&#31181;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20197;&#21450;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19979;&#30340;&#33021;&#21147;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20851;&#31995;&#27169;&#24335;&#26159;&#25351;&#20855;&#26377;&#29305;&#23450;&#35821;&#20041;&#30340;&#20851;&#31995;&#65292;&#23637;&#31034;&#22270;&#24418;&#27169;&#24335;&#65292;&#26159;&#24433;&#21709;KGE&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23613;&#31649;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;KGE&#27169;&#22411;&#22312;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#26356;&#22909;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;&#19982;KGC&#26356;&#22909;&#24615;&#33021;&#20043;&#38388;&#30340;&#31895;&#30053;&#20851;&#32852;&#65292;&#20294;&#23545;&#20110;KGE&#27169;&#22411;&#22312;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#32508;&#21512;&#23450;&#37327;&#20998;&#26512;&#23578;&#26410;&#23436;&#25104;&#65292;&#22240;&#27492;&#19981;&#30830;&#23450;KGE&#23545;&#20110;&#20851;&#31995;&#27169;&#24335;&#30340;&#29702;&#35770;&#25903;&#25345;&#22914;&#20309;&#24433;&#21709;&#19982;&#27492;&#20851;&#31995;&#27169;&#24335;&#30456;&#20851;&#30340;&#19977;&#20803;&#32452;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;7&#20010;KGE&#27169;&#22411;&#22312;4&#20010;&#24120;&#35265;&#20851;&#31995;&#27169;&#24335;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;2&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#29702;&#35770;&#12289;&#23454;&#20307;&#39057;&#29575;&#21644;&#25972;&#20307;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#30452;&#35266;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) has proven to be an effective approach to solving the Knowledge Graph Completion (KGC) task. Relational patterns which refer to relations with specific semantics exhibiting graph patterns are an important factor in the performance of KGE models. Though KGE models' capabilities are analyzed over different relational patterns in theory and a rough connection between better relational patterns modeling and better performance of KGC has been built, a comprehensive quantitative analysis on KGE models over relational patterns remains absent so it is uncertain how the theoretical support of KGE to a relational pattern contributes to the performance of triples associated to such a relational pattern. To address this challenge, we evaluate the performance of 7 KGE models over 4 common relational patterns on 2 benchmarks, then conduct an analysis in theory, entity frequency, and part-to-whole three aspects and get some counterintuitive conclusions. Finally, we int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#36793;&#32536;&#22238;&#24402;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;TGN&#20316;&#20026;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.07883</link><description>&lt;p&gt;
&#36808;&#21521;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#65306;&#20851;&#20110;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Temporal Edge Regression: A Case Study on Agriculture Trade Between Nations. (arXiv:2308.07883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22269;&#23478;&#38388;&#20892;&#19994;&#36152;&#26131;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#36793;&#32536;&#22238;&#24402;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;TGN&#20316;&#20026;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21160;&#24577;&#22270;&#20219;&#21153;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#22238;&#24402;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#37325;&#35201;&#29616;&#23454;&#24212;&#29992;&#30340;&#26102;&#24577;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;GNNs&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#35774;&#32622;&#19979;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#39044;&#27979;&#22269;&#23478;&#20043;&#38388;&#30340;&#39135;&#21697;&#21644;&#20892;&#19994;&#36152;&#26131;&#20215;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;&#22269;&#36152;&#26131;&#25968;&#25454;&#38598;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20010;&#38745;&#24577;&#21644;&#19977;&#20010;&#21160;&#24577;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22522;&#32447;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#20986;&#24322;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#29616;&#26377;GNNs&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;TGN&#20248;&#20110;&#20854;&#20182;GNN&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;TGN&#26159;&#36793;&#32536;&#22238;&#24402;&#20219;&#21153;&#30340;&#26356;&#21512;&#36866;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35757;&#32451;&#26679;&#26412;&#20013;&#36127;&#36793;&#30340;&#27604;&#20363;&#26174;&#33879;&#24433;&#21709;&#20102;&#27979;&#35797;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Graph Neural Networks (GNNs) have shown promising performance in tasks on dynamic graphs such as node classification, link prediction and graph regression. However, few work has studied the temporal edge regression task which has important real-world applications. In this paper, we explore the application of GNNs to edge regression tasks in both static and dynamic settings, focusing on predicting food and agriculture trade values between nations. We introduce three simple yet strong baselines and comprehensively evaluate one static and three dynamic GNN models using the UN Trade dataset. Our experimental results reveal that the baselines exhibit remarkably strong performance across various settings, highlighting the inadequacy of existing GNNs. We also find that TGN outperforms other GNN models, suggesting TGN is a more appropriate choice for edge regression tasks. Moreover, we note that the proportion of negative edges in the training samples significantly affects the test p
&lt;/p&gt;</description></item><item><title>1000&#19975;&#32654;&#20803;&#30340;ANA Avatar XPRIZE&#31454;&#36187;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#23454;&#26102;&#20256;&#36865;&#20154;&#31867;&#23384;&#22312;&#24863;&#21040;&#36828;&#31243;&#20301;&#32622;&#30340;&#21270;&#36523;&#31995;&#32479;&#12290;&#31454;&#36187;&#20915;&#36187;&#20013;&#65292;&#21442;&#19982;&#32773;&#23637;&#31034;&#20102;&#23545;&#36828;&#31243;&#19982;&#20154;&#31867;&#36827;&#34892;&#20114;&#21160;&#30340;&#25903;&#25345;&#12289;&#25506;&#32034;&#26032;&#29615;&#22659;&#20197;&#21450;&#20351;&#29992;&#19987;&#19994;&#25216;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07878</link><description>&lt;p&gt;
1000&#19975;&#32654;&#20803;&#30340;ANA Avatar XPRIZE&#31454;&#36187;&#20808;&#36827;&#30340;&#27785;&#28024;&#24335;&#36828;&#31243;&#20132;&#20114;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The $10 Million ANA Avatar XPRIZE Competition Advanced Immersive Telepresence Systems. (arXiv:2308.07878v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07878
&lt;/p&gt;
&lt;p&gt;
1000&#19975;&#32654;&#20803;&#30340;ANA Avatar XPRIZE&#31454;&#36187;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#23454;&#26102;&#20256;&#36865;&#20154;&#31867;&#23384;&#22312;&#24863;&#21040;&#36828;&#31243;&#20301;&#32622;&#30340;&#21270;&#36523;&#31995;&#32479;&#12290;&#31454;&#36187;&#20915;&#36187;&#20013;&#65292;&#21442;&#19982;&#32773;&#23637;&#31034;&#20102;&#23545;&#36828;&#31243;&#19982;&#20154;&#31867;&#36827;&#34892;&#20114;&#21160;&#30340;&#25903;&#25345;&#12289;&#25506;&#32034;&#26032;&#29615;&#22659;&#20197;&#21450;&#20351;&#29992;&#19987;&#19994;&#25216;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1000&#19975;&#32654;&#20803;&#30340;ANA Avatar XPRIZE&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#23454;&#26102;&#20256;&#36865;&#20154;&#31867;&#23384;&#22312;&#24863;&#21040;&#36828;&#31243;&#20301;&#32622;&#30340;&#21270;&#36523;&#31995;&#32479;&#12290;&#36825;&#20010;&#22810;&#24180;&#30340;&#31454;&#36187;&#20013;&#65292;&#21442;&#19982;&#32773;&#24320;&#21457;&#20102;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#20351;&#36816;&#33829;&#32773;&#33021;&#22815;&#20197;&#30495;&#23454;&#23384;&#22312;&#30340;&#26041;&#24335;&#22312;&#36828;&#31243;&#29615;&#22659;&#20013;&#30475;&#21040;&#12289;&#21548;&#21040;&#21644;&#20132;&#20114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36828;&#31243;&#29615;&#22659;&#20013;&#30340;&#20154;&#20204;&#20250;&#35273;&#24471;&#25805;&#20316;&#32773;&#23601;&#22312;&#21270;&#36523;&#26426;&#22120;&#20154;&#20869;&#12290;&#22312;2022&#24180;11&#26376;&#20110;&#32654;&#22269;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#38271;&#28393;&#20030;&#34892;&#30340;&#31454;&#36187;&#20915;&#36187;&#20013;&#65292;&#21270;&#36523;&#31995;&#32479;&#26681;&#25454;&#20854;&#23545;&#36828;&#31243;&#19982;&#20154;&#31867;&#36827;&#34892;&#20114;&#21160;&#30340;&#25903;&#25345;&#12289;&#25506;&#32034;&#26032;&#29615;&#22659;&#20197;&#21450;&#20351;&#29992;&#19987;&#19994;&#25216;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#31454;&#36187;&#38454;&#27573;&#30340;&#20219;&#21153;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25253;&#21578;&#20102;&#32467;&#26524;&#65292;&#20171;&#32461;&#20102;&#33719;&#32988;&#22242;&#38431;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#24471;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $10M ANA Avatar XPRIZE aimed to create avatar systems that can transport human presence to remote locations in real time. The participants of this multi-year competition developed robotic systems that allow operators to see, hear, and interact with a remote environment in a way that feels as if they are truly there. On the other hand, people in the remote environment were given the impression that the operator was present inside the avatar robot. At the competition finals, held in November 2022 in Long Beach, CA, USA, the avatar systems were evaluated on their support for remotely interacting with humans, exploring new environments, and employing specialized skills. This article describes the competition stages with tasks and evaluation procedures, reports the results, presents the winning teams' approaches, and discusses lessons learned.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2308.07871</link><description>&lt;p&gt;
&#24773;&#24863;&#23884;&#20837;&#8212;&#8212;&#20174;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#23450;&#19988;&#22343;&#21248;&#30340;&#25277;&#35937;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Emotion Embeddings $\unicode{x2014}$ Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets. (arXiv:2308.07871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#24773;&#24863;&#23884;&#20837;&#65292;&#29420;&#31435;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#26631;&#31614;&#24418;&#24335;&#65292;&#20174;&#32780;&#23558;&#20197;&#24448;&#23545;&#19981;&#21516;&#31867;&#22411;&#24322;&#36136;&#24773;&#24863;&#25968;&#25454;&#30340;&#30740;&#31350;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#24773;&#24863;&#36890;&#36807;&#22810;&#31181;&#20132;&#27969;&#26041;&#24335;&#21644;&#23186;&#20307;&#26684;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35745;&#31639;&#30740;&#31350;&#21516;&#26679;&#22810;&#26679;&#21270;&#65292;&#28041;&#21450;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#31561;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#24773;&#24863;&#34987;&#20197;&#19981;&#21516;&#30340;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65288;&#26497;&#24615;&#23610;&#24230;&#12289;&#22522;&#26412;&#24773;&#24863;&#31867;&#21035;&#12289;&#32500;&#24230;&#26041;&#27861;&#12289;&#35780;&#20215;&#29702;&#35770;&#31561;&#65289;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#12289;&#39044;&#27979;&#27169;&#22411;&#21644;&#24773;&#24863;&#20998;&#26512;&#36719;&#20214;&#24037;&#20855;&#30340;&#22810;&#26679;&#21270;&#22686;&#38271;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#22312;&#34920;&#36798;&#21644;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#36843;&#20999;&#38656;&#35201;&#32479;&#19968;&#20197;&#24448;&#23545;&#36234;&#26469;&#36234;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#26631;&#31614;&#31867;&#22411;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#31181;&#20849;&#20139;&#30340;&#24773;&#24863;&#28508;&#22312;&#34920;&#31034;&#65292;&#21363;&#25152;&#35859;&#24773;&#24863;&#23884;&#20837;&#65292;&#19981;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20132;&#27969;&#26041;&#24335;&#12289;&#23186;&#20307;&#25110;&#34920;&#31034;&#26631;&#31614;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human emotion is expressed in many communication modalities and media formats and so their computational study is equally diversified into natural language processing, audio signal analysis, computer vision, etc. Similarly, the large variety of representation formats used in previous research to describe emotions (polarity scales, basic emotion categories, dimensional approaches, appraisal theory, etc.) have led to an ever proliferating diversity of datasets, predictive models, and software tools for emotion analysis. Because of these two distinct types of heterogeneity, at the expressional and representational level, there is a dire need to unify previous work on increasingly diverging data and label types. This article presents such a unifying computational model. We propose a training procedure that learns a shared latent representation for emotions, so-called emotion embeddings, independent of different natural languages, communication modalities, media or representation label form
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07870</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#23454;&#29616;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#26412;&#19990;&#32426;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;AI&#39046;&#22495;&#21462;&#24471;&#30340;&#22823;&#37096;&#20998;&#25104;&#26524;&#37117;&#26159;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#21450;&#24212;&#29992;&#24050;&#32463;&#20984;&#26174;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#38590;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#32570;&#20047;&#40065;&#26834;&#24615;&#12289;&#19981;&#21487;&#38752;&#24615;&#21644;&#29983;&#29289;&#23398;&#19978;&#30340;&#19981;&#21512;&#29702;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#38656;&#35201;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#30340;&#21551;&#21457;&#21644;&#25351;&#23548;&#30340;&#26041;&#26696;&#12290;&#20854;&#20013;&#19968;&#31181;&#29702;&#35770;&#31216;&#20026;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#65306;PC&#21487;&#20197;&#27169;&#25311;&#19981;&#21516;&#33041;&#21306;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#35748;&#30693;&#25511;&#21046;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#24182;&#22312;&#21464;&#20998;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.07857</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Impression-Aware Recommender Systems. (arXiv:2308.07857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07857
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#25968;&#25454;&#28304;&#20026;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#36136;&#37327;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#21360;&#35937;&#26159;&#19968;&#31181;&#21253;&#21547;&#36807;&#21435;&#25512;&#33616;&#65288;&#23637;&#31034;&#30340;&#39033;&#30446;&#65289;&#21644;&#20256;&#32479;&#20114;&#21160;&#30340;&#26032;&#22411;&#25968;&#25454;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#21360;&#35937;&#26469;&#20248;&#21270;&#29992;&#25143;&#20559;&#22909;&#24182;&#20811;&#26381;&#24403;&#21069;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#12290;&#21360;&#35937;&#30340;&#30456;&#20851;&#24615;&#21644;&#20852;&#36259;&#24230;&#36880;&#24180;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31867;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20851;&#20110;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#20391;&#37325;&#20110;&#30740;&#31350;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#19977;&#20010;&#20998;&#31867;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#27599;&#31687;&#32508;&#36848;&#35770;&#25991;&#65292;&#25551;&#36848;&#20102;&#20855;&#26377;&#21360;&#35937;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#25991;&#29486;&#20013;&#32570;&#22833;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that
&lt;/p&gt;</description></item><item><title>REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.07832</link><description>&lt;p&gt;
REFORMS: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07832
&lt;/p&gt;
&lt;p&gt;
REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#37319;&#29992;&#20063;&#20276;&#38543;&#30528;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22833;&#36133;&#12290;&#36825;&#20123;&#22833;&#36133;&#21487;&#33021;&#20250;&#38459;&#30861;&#31185;&#23398;&#36827;&#23637;&#65292;&#23548;&#33268;&#23545;&#26080;&#25928;&#32467;&#35770;&#30340;&#38169;&#35823;&#20849;&#35782;&#65292;&#24182;&#21066;&#24369;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#30340;&#21487;&#20449;&#24230;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#31185;&#20013;&#24120;&#24120;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#24212;&#29992;&#19988;&#22833;&#36133;&#12290;&#20986;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25552;&#20379;&#28165;&#26224;&#30340;&#25253;&#21578;&#26631;&#20934;&#12290;&#22522;&#20110;&#23545;&#36807;&#21435;&#25991;&#29486;&#30340;&#24191;&#27867;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFORMS&#26816;&#26597;&#34920;&#65288;$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience&#65289;&#12290;&#23427;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#37197;&#22871;&#30340;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#12290;REFORMS&#26159;&#22522;&#20110;19&#20301;&#30740;&#31350;&#20154;&#21592;&#30340;&#20849;&#35782;&#24320;&#21457;&#30340;&#65292;&#36825;&#20123;&#20154;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#12290;REFORMS&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29366;&#24577;&#35782;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#35782;&#21035;&#20851;&#38190;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#12290;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#21892;&#26234;&#33021;&#20307;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07795</link><description>&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#35782;&#21035;&#24378;&#21270;&#23398;&#20064;&#30340;&#20851;&#38190;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Learning to Identify Critical States for Reinforcement Learning from Videos. (arXiv:2308.07795v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29366;&#24577;&#35782;&#21035;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#35782;&#21035;&#20851;&#38190;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#12290;&#35813;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#21892;&#26234;&#33021;&#20307;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#26377;&#20851;&#22909;&#31574;&#30053;&#30340;&#31639;&#27861;&#20449;&#24687;&#21487;&#20197;&#20174;&#32570;&#20047;&#25191;&#34892;&#21160;&#20316;&#26126;&#30830;&#20449;&#24687;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25110;&#26426;&#22120;&#20154;&#30340;&#35270;&#39057;&#21487;&#33021;&#20256;&#36798;&#20102;&#24456;&#22810;&#26377;&#20851;&#22870;&#21169;&#21160;&#20316;&#24207;&#21015;&#30340;&#38544;&#21547;&#20449;&#24687;&#65292;&#20294;&#26159;&#24819;&#35201;&#20174;&#36825;&#20123;&#35270;&#39057;&#20013;&#35266;&#23519;&#24182;&#33719;&#30410;&#30340;DRL&#26426;&#22120;&#24517;&#39035;&#39318;&#20808;&#36890;&#36807;&#33258;&#36523;&#23398;&#20064;&#26469;&#35782;&#21035;&#21644;&#35748;&#21487;&#30456;&#20851;&#30340;&#29366;&#24577;/&#21160;&#20316;/&#22870;&#21169;&#12290;&#22312;&#19981;&#20381;&#36182;&#20110;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#28145;&#24230;&#29366;&#24577;&#35782;&#21035;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#39044;&#27979;&#34987;&#32534;&#30721;&#20026;&#35270;&#39057;&#30340;&#36820;&#22238;&#20540;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#26469;&#25552;&#21462;/&#35782;&#21035;&#37325;&#35201;&#30340;&#20851;&#38190;&#29366;&#24577;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35299;&#21644;&#25913;&#21892;&#26234;&#33021;&#20307;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/AI-Initiative-KAUST/VideoRLCS &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states. Extensive experiments showcase our method's potential for understanding and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.07791</link><description>&lt;p&gt;
&#20026;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20449;&#24687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07791
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#25552;&#21462;&#36807;&#31243;&#20013;&#24212;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36234;&#26469;&#36234;&#24378;&#30340;&#33021;&#21147;&#65292;&#24050;&#25104;&#20026;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#25991;&#26412;&#22788;&#29702;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20173;&#28982;&#21463;&#21040;&#20043;&#21069;&#19968;&#20195;&#20165;&#32534;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Informed Named Entity Recognition Decoding&#65288;iNERD&#65289;&#65292;&#23427;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35270;&#20026;&#19968;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#23427;&#20197;&#38754;&#21521;&#26410;&#26469;&#30340;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#25552;&#21462;&#30340;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#28040;&#38500;&#20102;&#20219;&#20309;&#24187;&#35273;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#21512;&#24182;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#19978;&#31895;&#35843;&#20248;&#21270;&#20102;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#20843;&#20010;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, espec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#30693;&#35782;&#36861;&#36394;&#20013;&#24120;&#35265;&#30340;&#31572;&#26696;&#20559;&#35265;&#29616;&#35937;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COunterfactual REasoning (CORE)&#26694;&#26550;&#65292;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#20110;&#23398;&#29983;&#30693;&#35782;&#29366;&#24577;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07779</link><description>&lt;p&gt;
&#25105;&#20204;&#20805;&#20998;&#29702;&#35299;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21527;&#65311;&#35782;&#21035;&#21644;&#20943;&#36731;&#30693;&#35782;&#36861;&#36394;&#20013;&#30340;&#31572;&#26696;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Do We Fully Understand Students' Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing. (arXiv:2308.07779v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30693;&#35782;&#36861;&#36394;&#20013;&#24120;&#35265;&#30340;&#31572;&#26696;&#20559;&#35265;&#29616;&#35937;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COunterfactual REasoning (CORE)&#26694;&#26550;&#65292;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#20110;&#23398;&#29983;&#30693;&#35782;&#29366;&#24577;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#29983;&#19982;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#23398;&#20064;&#20114;&#21160;&#26469;&#30417;&#27979;&#23398;&#29983;&#19981;&#26029;&#21464;&#21270;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#23398;&#29983;&#22312;&#26410;&#26469;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#38388;&#25509;&#35780;&#20272;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#19968;&#20010;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#31572;&#26696;&#20559;&#35265;&#65292;&#21363;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#21644;&#38169;&#35823;&#31572;&#26696;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#31572;&#26696;&#20559;&#35265;&#20316;&#20026;&#19968;&#20010;&#25463;&#24452;&#26469;&#23454;&#29616;&#22312;KT&#20013;&#30340;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#26410;&#33021;&#20805;&#20998;&#29702;&#35299;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;KT&#20219;&#21153;&#12290;&#39318;&#20808;&#24314;&#31435;&#20102;KT&#30340;&#22240;&#26524;&#22270;&#65292;&#20174;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#23398;&#29983;&#21453;&#24212;&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#31574;&#21453;&#20107;&#23454;&#25512;&#29702;&#65288;CORE&#65289;&#26694;&#26550;&#36827;&#34892;KT&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20998;&#21035;&#25429;&#25417;&#20102;&#24635;&#22240;&#26524;&#25928;&#24212;&#21644;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#36816;&#21160;&#25511;&#21046;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#26469;&#23454;&#29616;&#22810;&#32423;&#35268;&#21010;&#21644;&#21327;&#35843;&#32930;&#20307;&#36816;&#21160;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.07775</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Hierarchical generative modelling for autonomous robots. (arXiv:2308.07775v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#36816;&#21160;&#25511;&#21046;&#30340;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#26469;&#23454;&#29616;&#22810;&#32423;&#35268;&#21010;&#21644;&#21327;&#35843;&#32930;&#20307;&#36816;&#21160;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#19982;&#21608;&#22260;&#29615;&#22659;&#20114;&#21160;&#26102;&#21487;&#20197;&#20135;&#29983;&#22797;&#26434;&#30340;&#25972;&#20307;&#36523;&#20307;&#21160;&#20316;&#65292;&#36890;&#36807;&#35745;&#21010;&#12289;&#25191;&#34892;&#21644;&#32452;&#21512;&#21508;&#20010;&#32930;&#20307;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#36816;&#21160;&#25511;&#21046;&#30340;&#36825;&#19968;&#22522;&#26412;&#26041;&#38754;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#29983;&#25104;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#37197;&#22791;&#20102;&#22810;&#32423;&#35268;&#21010;&#65292;&#20197;&#27169;&#20223;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#30340;&#28145;&#23618;&#26102;&#38388;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#26102;&#38388;&#28145;&#24230;&#26159;&#25351;&#21069;&#21521;&#25110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36830;&#32493;&#23618;&#27425;&#30340;&#23884;&#22871;&#26102;&#38388;&#23610;&#24230;&#65292;&#20363;&#22914;&#65292;&#20132;&#20184;&#19968;&#20010;&#29289;&#20307;&#38656;&#35201;&#19968;&#20010;&#20840;&#23616;&#35745;&#21010;&#26469;&#19978;&#19979;&#25991;&#21270;&#22810;&#20010;&#32930;&#20307;&#30340;&#24555;&#36895;&#21327;&#35843;&#12290;&#36825;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#20998;&#31163;&#20063;&#25512;&#21160;&#20102;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#21151;&#33021;&#30340;&#24863;&#30693;&#21160;&#20316;&#25511;&#21046;&#65292;&#20197;&#20998;&#23618;&#32467;&#26500;&#21270;&#35268;&#21010;&#21644;&#20302;&#23618;&#32930;&#20307;&#36816;&#21160;&#25511;&#21046;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#20540;&#21644;&#29289;&#29702;&#27169;&#25311;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07774</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#22270;&#32534;&#30721;&#35299;&#30721;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#24322;&#24120;&#31243;&#24230;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#12290;&#27169;&#22411;&#30340;&#27744;&#21270;&#36807;&#31243;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#27744;&#21270;&#25805;&#20316;&#65292;&#23427;&#26088;&#22312;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#27744;&#21270;&#31574;&#30053;&#20381;&#36182;&#20110;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#33719;&#24471;&#30340;&#20998;&#37197;&#30697;&#38453;&#65292;&#35813;&#30697;&#38453;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#27744;&#21270;&#36807;&#31243;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22270;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#23545;&#33410;&#28857;&#36827;&#34892;&#25490;&#24207;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#12290;&#22312;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#27744;&#21270;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;LCPool&#65292;&#23427;&#21033;&#29992;&#23616;&#37096;&#32422;&#26463;&#32447;&#24615;&#32534;&#30721;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#65292;&#36890;&#36807;&#27714;&#35299;&#24102;&#26377;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#30340;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#32858;&#31867;&#20998;&#37197;&#30697;&#38453;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#32422;&#26463;&#65292;LCPool&#34987;&#35774;&#35745;&#25104;&#20813;&#36153;
&lt;/p&gt;
&lt;p&gt;
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free fr
&lt;/p&gt;</description></item><item><title>MOLE&#26159;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#27169;&#22359;&#21270;&#26041;&#24335;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07772</link><description>&lt;p&gt;
MOLE: MOdular Learning FramEwork&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MOLE: MOdular Learning FramEwork via Mutual Information Maximization. (arXiv:2308.07772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07772
&lt;/p&gt;
&lt;p&gt;
MOLE&#26159;&#19968;&#31181;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#27169;&#22359;&#21270;&#26041;&#24335;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MOLE&#65288;MOdular Learning Framework&#65289;&#30340;&#24322;&#27493;&#21644;&#26412;&#22320;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#26041;&#24335;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22359;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#27599;&#20010;&#27169;&#22359;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#25353;&#39034;&#24207;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#12290;MOLE&#20351;&#24471;&#35757;&#32451;&#21464;&#24471;&#20855;&#26377;&#23616;&#37096;&#20248;&#21270;&#21644;&#27169;&#22359;&#20043;&#38388;&#26799;&#24230;&#38548;&#31163;&#30340;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#26696;&#22312;&#29983;&#29289;&#23398;&#19978;&#26356;&#20855;&#21487;&#34892;&#24615;&#65292;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#21152;&#21512;&#29702;&#12290;&#25105;&#20204;&#22312;&#21521;&#37327;&#12289;&#32593;&#26684;&#21644;&#22270;&#24418;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#35299;&#20915;&#22270;&#24418;&#31867;&#22411;&#25968;&#25454;&#30340;&#22270;&#24418;&#32423;&#21644;&#33410;&#28857;&#32423;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;MOLE&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#24320;&#28304;&#24037;&#20855;&#22312;MRI&#30315;&#30187;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;MATLAB&#31561;&#24037;&#20855;&#30340;&#33539;&#22260;&#21644;&#20351;&#29992;&#29575;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#20182;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#22312;&#30913;&#20849;&#25391;&#30315;&#30187;&#22270;&#20687;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.07762</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#28304;&#24037;&#20855;MRI&#30315;&#30187;&#22270;&#20687;&#39044;&#26399;&#32467;&#26524;&#30340;&#21407;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating the anticipated outcomes of MRI seizure image from open-source tool- Prototype approach. (arXiv:2308.07762v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#24320;&#28304;&#24037;&#20855;&#22312;MRI&#30315;&#30187;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;MATLAB&#31561;&#24037;&#20855;&#30340;&#33539;&#22260;&#21644;&#20351;&#29992;&#29575;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#20182;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#22312;&#30913;&#20849;&#25391;&#30315;&#30187;&#22270;&#20687;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#22823;&#33041;&#20013;&#24322;&#24120;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#24433;&#21709;&#20840;&#29699;&#36817;7000&#19975;&#20154;&#21475;&#65288;Ngugi et al., 2010&#65289;&#12290;&#35768;&#22810;&#24320;&#28304;&#31070;&#32463;&#24433;&#20687;&#24037;&#20855;&#29992;&#20110;&#20195;&#35874;&#26816;&#26597;&#21644;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35832;&#22914;MATLAB&#65292;Slicer 3D&#65292;Brain Suite21a&#65292;SPM&#21644;MedCalc&#31561;&#24320;&#28304;&#24037;&#20855;&#30340;&#33539;&#22260;&#12290;60%&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;MATLAB&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#65292;10%&#30340;&#20154;&#20351;&#29992;&#33258;&#26377;&#36719;&#20214;&#12290;&#36229;&#36807;30%&#30340;&#30740;&#31350;&#20154;&#21592;&#22312;&#22788;&#29702;&#25216;&#26415;&#20013;&#20351;&#29992;&#20854;&#20182;&#24320;&#28304;&#36719;&#20214;&#24037;&#20855;&#26469;&#30740;&#31350;&#30913;&#20849;&#25391;&#30315;&#30187;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epileptic Seizure is an abnormal neuronal exertion in the brain, affecting nearly 70 million of the world's population (Ngugi et al., 2010). So many open-source neuroimaging tools are used for metabolism checkups and analysis purposes. The scope of open-source tools like MATLAB, Slicer 3D, Brain Suite21a, SPM, and MedCalc are explained in this paper. MATLAB was used by 60% of the researchers for their image processing and 10% of them use their proprietary software. More than 30% of the researchers use other open-source software tools with their processing techniques for the study of magnetic resonance seizure images
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23884;&#20837;&#23610;&#23544;&#35774;&#32622;&#38382;&#39064;&#65292;&#23558;&#21160;&#24577;&#23884;&#20837;&#23610;&#23544;&#25628;&#32034;&#24314;&#27169;&#20026;&#19968;&#20010;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#20998;&#26512;&#21644;&#37327;&#21270;&#24433;&#21709;&#26368;&#20339;&#23884;&#20837;&#23610;&#23544;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.07760</link><description>&lt;p&gt;
&#27969;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#23567;&#21518;&#24724;&#21160;&#24577;&#23884;&#20837;&#23610;&#23544;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System. (arXiv:2308.07760v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27969;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#23884;&#20837;&#23610;&#23544;&#35774;&#32622;&#38382;&#39064;&#65292;&#23558;&#21160;&#24577;&#23884;&#20837;&#23610;&#23544;&#25628;&#32034;&#24314;&#27169;&#20026;&#19968;&#20010;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#20998;&#26512;&#21644;&#37327;&#21270;&#24433;&#21709;&#26368;&#20339;&#23884;&#20837;&#23610;&#23544;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25512;&#33616;&#31995;&#32479;&#24456;&#38590;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#12290;&#39640;&#21534;&#21520;&#37327;&#30340;&#25968;&#25454;&#35201;&#27714;&#27169;&#22411;&#21450;&#26102;&#26356;&#26032;&#20197;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#21160;&#24577;&#65292;&#36825;&#23548;&#33268;&#20102;&#27969;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#20986;&#29616;&#12290;&#30001;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26222;&#21450;&#65292;&#23884;&#20837;&#23618;&#24191;&#27867;&#37319;&#29992;&#20197;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#29992;&#25143;&#12289;&#29289;&#21697;&#21644;&#20854;&#20182;&#29305;&#24449;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#35774;&#32622;&#30456;&#21516;&#21644;&#38745;&#24577;&#30340;&#23884;&#20837;&#23610;&#23544;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#20869;&#23384;&#25104;&#26412;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27969;&#24335;&#25512;&#33616;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20102;&#27969;&#24335;&#27169;&#22411;&#26356;&#26032;&#36807;&#31243;&#65292;&#24182;&#23558;&#21160;&#24577;&#23884;&#20837;&#23610;&#23544;&#25628;&#32034;&#24314;&#27169;&#20026;&#19968;&#20010;&#24378;&#30423;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#20998;&#26512;&#21644;&#37327;&#21270;&#24433;&#21709;&#26368;&#20339;&#23884;&#20837;&#23610;&#23544;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments. The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems. Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors. However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations. To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem. Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective. Based on this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33310;&#36424;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23039;&#21183;&#21644;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#20307;&#21160;&#20316;&#35270;&#39057;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24039;&#22937;&#22320;&#21033;&#29992;T2I&#25193;&#25955;&#27169;&#22411;&#36830;&#32493;&#29983;&#25104;&#35270;&#39057;&#24103;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#25345;&#20154;&#29289;&#29305;&#24449;&#21644;&#26381;&#35013;&#19968;&#33268;&#24615;&#20197;&#21450;&#32972;&#26223;&#36830;&#32493;&#24615;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#35774;&#35745;&#24103;&#20869;&#23545;&#40784;&#27169;&#22359;&#65292;&#30830;&#20445;&#25972;&#20010;&#35270;&#39057;&#20013;&#20154;&#29289;&#22806;&#35266;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.07749</link><description>&lt;p&gt;
&#33310;&#36424;&#21270;&#36523;&#65306;&#36890;&#36807;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#21463;&#23039;&#21183;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#20154;&#20307;&#21160;&#20316;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model. (arXiv:2308.07749v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33310;&#36424;&#21270;&#36523;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23039;&#21183;&#21644;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#20307;&#21160;&#20316;&#35270;&#39057;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#24039;&#22937;&#22320;&#21033;&#29992;T2I&#25193;&#25955;&#27169;&#22411;&#36830;&#32493;&#29983;&#25104;&#35270;&#39057;&#24103;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#25345;&#20154;&#29289;&#29305;&#24449;&#21644;&#26381;&#35013;&#19968;&#33268;&#24615;&#20197;&#21450;&#32972;&#26223;&#36830;&#32493;&#24615;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#35774;&#35745;&#24103;&#20869;&#23545;&#40784;&#27169;&#22359;&#65292;&#30830;&#20445;&#25972;&#20010;&#35270;&#39057;&#20013;&#20154;&#29289;&#22806;&#35266;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#39046;&#22495;&#20013;&#21019;&#24314;&#36924;&#30495;&#21270;&#36523;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#38656;&#35201;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#21644;&#23039;&#21183;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#20307;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33310;&#36424;&#21270;&#36523;&#65292;&#26088;&#22312;&#36890;&#36807;&#23039;&#21183;&#21644;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20154;&#20307;&#36816;&#21160;&#35270;&#39057;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;T2I&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#27599;&#20010;&#35270;&#39057;&#24103;&#12290;&#21019;&#26032;&#30340;&#20851;&#38190;&#22312;&#20110;&#25105;&#20204;&#24039;&#22937;&#22320;&#21033;&#29992;T2I&#25193;&#25955;&#27169;&#22411;&#26469;&#36830;&#32493;&#29983;&#25104;&#35270;&#39057;&#24103;&#65292;&#21516;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#22312;&#19981;&#21516;&#23039;&#21183;&#19979;&#20445;&#25345;&#20154;&#29289;&#29305;&#24449;&#21644;&#26381;&#35013;&#19968;&#33268;&#24615;&#20197;&#21450;&#22312;&#21508;&#31181;&#20154;&#20307;&#36816;&#21160;&#20013;&#20445;&#25345;&#32972;&#26223;&#36830;&#32493;&#24615;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#30830;&#20445;&#25972;&#20010;&#35270;&#39057;&#20013;&#20154;&#29289;&#22806;&#35266;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24103;&#20869;&#23545;&#40784;&#27169;&#22359;&#12290;&#36825;&#20010;&#27169;&#22359;&#23558;&#25991;&#26412;&#24341;&#23548;&#30340;&#21512;&#25104;&#20154;&#29289;&#30693;&#35782;&#34701;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;T2I&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#32467;&#21512;&#20102;ChatGPT&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#21644;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#65292;&#24182;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;</title><link>http://arxiv.org/abs/2308.07748</link><description>&lt;p&gt;
&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Automotive Radar Object Detection Networks. (arXiv:2308.07748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27773;&#36710;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#21644;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#65292;&#24182;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#24863;&#30693;&#29615;&#22659;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#26159;&#36825;&#31867;&#31995;&#32479;&#30340;&#19968;&#20010;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;CNN&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#21367;&#31215;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#23427;&#23558;&#24378;&#22823;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#26816;&#27979;&#19982;&#20302;&#35745;&#31639;&#36164;&#28304;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38647;&#36798;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#31232;&#30095;&#26680;&#24515;&#28857;&#26609;&#20307;&#65288;SKPP&#65289;&#21644;&#21452;&#20307;&#32032;&#28857;&#21367;&#31215;&#65288;DVPC&#65289;&#26469;&#35299;&#20915;&#32593;&#26684;&#28210;&#26579;&#21644;&#31232;&#30095;&#39592;&#24178;&#32467;&#26500;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SKPP-DPVCN&#26550;&#26500;&#65292;&#22312;Car AP4.0&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;5.89%&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20248;&#27169;&#22411;4.19%&#12290;&#27492;&#22806;&#65292;SKPP-DPVCN&#36824;&#23558;&#24179;&#22343;&#23610;&#24230;&#35823;&#24046;&#65288;ASE&#65289;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20943;&#23569;&#20102;21.41%&#12290;
&lt;/p&gt;
&lt;p&gt;
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#25351;&#23548;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#20915;&#31574;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#20223;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#25628;&#32034;&#25110;&#26368;&#23567;&#24310;&#36831;&#24773;&#20917;&#19979;&#20316;&#20026;&#23436;&#25972;&#31574;&#30053;&#20351;&#29992;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.07738</link><description>&lt;p&gt;
&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#30340;DAgger&#31639;&#27861;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods. (arXiv:2308.07738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#25351;&#23548;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#20915;&#31574;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#27169;&#20223;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#20302;&#24310;&#36831;&#25628;&#32034;&#25110;&#26368;&#23567;&#24310;&#36831;&#24773;&#20917;&#19979;&#20316;&#20026;&#23436;&#25972;&#31574;&#30053;&#20351;&#29992;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#32467;&#21512;&#24418;&#24335;&#26041;&#27861;&#12289;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#22312;&#22823;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31361;&#21069;&#35270;&#30028;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#22411;&#26816;&#27979;&#25216;&#26415;&#25351;&#23548;MCTS&#31639;&#27861;&#65292;&#22312;MDP&#30340;&#19968;&#32452;&#20195;&#34920;&#24615;&#29366;&#24577;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#20915;&#31574;&#30340;&#31163;&#32447;&#26679;&#26412;&#12290;&#36825;&#20123;&#26679;&#26412;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#27169;&#20223;&#29992;&#20110;&#29983;&#25104;&#23427;&#20204;&#30340;&#31574;&#30053;&#12290;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20316;&#20026;&#20302;&#24310;&#36831;&#30340;MCTS&#22312;&#32447;&#25628;&#32034;&#30340;&#25351;&#23548;&#65292;&#20063;&#21487;&#20197;&#22312;&#38656;&#35201;&#26368;&#23567;&#24310;&#36831;&#26102;&#20316;&#20026;&#19968;&#20010;&#23436;&#25972;&#30340;&#31574;&#30053;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#26816;&#27979;&#26469;&#26816;&#27979;&#38656;&#35201;&#39069;&#22806;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#36825;&#20123;&#39069;&#22806;&#26679;&#26412;&#38598;&#20013;&#22312;&#23398;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#19982;&#65288;&#35745;&#31639;&#26114;&#36149;&#30340;&#65289;&#31163;&#32447;&#31574;&#30053;&#19981;&#19968;&#33268;&#30340;&#37197;&#32622;&#19978;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#20912;&#28246;&#21644;&#21507;&#35910;&#20154;&#29615;&#22659;&#30340;MDP&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments --
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#20013;&#30340;&#20869;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#21363;&#24494;&#23567;&#30340;&#35268;&#21010;&#20248;&#20808;&#32423;&#21464;&#21270;&#20250;&#23548;&#33268;&#22823;&#35268;&#27169;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#25913;&#21892;&#22303;&#22320;&#21033;&#29992;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07714</link><description>&lt;p&gt;
&#12298;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#20013;&#38544;&#34255;&#30340;&#20869;&#22312;&#19981;&#31283;&#23450;&#24615;&#30340;&#26631;&#24535;&#12299;
&lt;/p&gt;
&lt;p&gt;
Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning. (arXiv:2308.07714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#20013;&#30340;&#20869;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#21363;&#24494;&#23567;&#30340;&#35268;&#21010;&#20248;&#20808;&#32423;&#21464;&#21270;&#20250;&#23548;&#33268;&#22823;&#35268;&#27169;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#25913;&#21892;&#22303;&#22320;&#21033;&#29992;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#21033;&#29992;&#20915;&#31574;&#36827;&#31243;&#38271;&#26399;&#20197;&#26469;&#20135;&#29983;&#20102;&#20840;&#29699;&#26222;&#36941;&#30340;&#31995;&#32479;&#24615;&#20844;&#24179;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#38382;&#39064;&#12290;&#23450;&#37327;&#30340;&#12289;&#22522;&#20110;&#20248;&#21270;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#22914;&#22810;&#30446;&#26631;&#22303;&#22320;&#20998;&#37197;&#65288;MOLA&#65289;&#65292;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#26126;&#30830;&#35780;&#20272;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#20301;&#32622;&#26469;&#25552;&#39640;&#23458;&#35266;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#35268;&#21010;&#26041;&#27861;&#20351;&#29992;&#36890;&#29992;&#35268;&#21010;&#26631;&#20934;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#19981;&#31283;&#23450;&#30340;&#8220;&#38378;&#28857;&#8221;&#65292;&#24494;&#23567;&#30340;&#35268;&#21010;&#20248;&#20808;&#32423;&#21464;&#21270;&#20250;&#23548;&#33268;&#22823;&#35268;&#27169;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#23450;&#37327;&#30340;&#35770;&#35777;&#65292;&#35777;&#26126;&#25105;&#20204;&#22312;MOLA&#27169;&#22411;&#20013;&#21457;&#29616;&#30340;&#38378;&#28857;&#26159;&#19968;&#31181;&#26356;&#26222;&#36941;&#30340;&#19981;&#31283;&#23450;&#24615;&#30340;&#20363;&#23376;&#65292;&#26080;&#35770;&#36825;&#20123;&#35268;&#21010;&#22240;&#32032;&#26159;&#26126;&#30830;&#36824;&#26159;&#38544;&#21547;&#65292;&#21482;&#35201;&#35268;&#21010;&#32771;&#34385;&#21040;&#21327;&#35843;&#22320;&#28857;&#20043;&#38388;&#21644;&#22320;&#28857;&#20869;&#30340;&#20351;&#29992;&#22240;&#32032;&#65292;&#37117;&#20250;&#20986;&#29616;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#31283;&#23450;&#24615;&#20250;&#23548;&#33268;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#22320;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Land-use decision-making processes have a long history of producing globally pervasive systemic equity and sustainability concerns. Quantitative, optimization-based planning approaches, e.g. Multi-Objective Land Allocation (MOLA), seemingly open the possibility to improve objectivity and transparency by explicitly evaluating planning priorities by the type, amount, and location of land uses. Here, we show that optimization-based planning approaches with generic planning criteria generate a series of unstable "flashpoints" whereby tiny changes in planning priorities produce large-scale changes in the amount of land use by type. We give quantitative arguments that the flashpoints we uncover in MOLA models are examples of a more general family of instabilities that occur whenever planning accounts for factors that coordinate use on- and between-sites, regardless of whether these planning factors are formulated explicitly or implicitly. We show that instabilities lead to regions of ambigui
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.07687</link><description>&lt;p&gt;
DiffGuard&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#35821;&#20041;&#24102;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#19982;&#21512;&#27861;&#31867;&#21035;&#20869;&#23481;&#22312;&#35821;&#20041;&#19978;&#30340;&#19981;&#21305;&#37197;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;DiffGuard&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;DiffGuard&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#65292;&#30456;&#36739;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26131;&#20110;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#26465;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;DiffGuard&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#22270;&#20687;&#21644;&#26631;&#31614;&#20316;&#20026;&#26465;&#20214;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#65292;DiffGuard&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26799;&#24230;&#35843;&#33410;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#31454;&#20105;&#24378;&#24230;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.07686</link><description>&lt;p&gt;
Boosting Multi-modal Model Performance with Adaptive Gradient Modulation. (arXiv:2308.07686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Boosting Multi-modal Model Performance with Adaptive Gradient Modulation. (arXiv:2308.07686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26799;&#24230;&#35843;&#33410;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21319;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#31454;&#20105;&#24378;&#24230;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#26631;&#20934;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#24335;&#30340;&#32570;&#38519;&#24050;&#32463;&#21464;&#24471;&#26126;&#26174;&#12290;&#30740;&#31350;&#25351;&#20986;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#20122;&#20248;&#30340;&#21407;&#22240;&#26159;&#27169;&#24577;&#31454;&#20105;&#29616;&#35937;&#12290;&#29616;&#26377;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35843;&#25511;&#35757;&#32451;&#36807;&#31243;&#26469;&#25913;&#21892;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#21482;&#33021;&#24212;&#29992;&#20110;&#21518;&#26399;&#34701;&#21512;&#27169;&#22411;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#27169;&#24577;&#31454;&#20105;&#30340;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26799;&#24230;&#35843;&#33410;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;&#20855;&#26377;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#35843;&#25511;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23545;&#27169;&#24577;&#31454;&#20105;&#20197;&#21450;&#25105;&#20204;&#35843;&#33410;&#26041;&#27861;&#25928;&#26524;&#32972;&#21518;&#30340;&#26426;&#21046;&#26377;&#37327;&#21270;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#31454;&#20105;&#24378;&#24230;&#30340;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#34987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
While the field of multi-modal learning keeps growing fast, the deficiency of the standard joint training paradigm has become clear through recent studies. They attribute the sub-optimal performance of the jointly trained model to the modality competition phenomenon. Existing works attempt to improve the jointly trained model by modulating the training process. Despite their effectiveness, those methods can only apply to late fusion models. More importantly, the mechanism of the modality competition remains unexplored. In this paper, we first propose an adaptive gradient modulation method that can boost the performance of multi-modal models with various fusion strategies. Extensive experiments show that our method surpasses all existing modulation methods. Furthermore, to have a quantitative understanding of the modality competition and the mechanism behind the effectiveness of our modulation method, we introduce a novel metric to measure the competition strength. This metric is built 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EQ-Net&#65292;&#19968;&#31181;&#24377;&#24615;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#28789;&#27963;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#25506;&#32034;&#24377;&#24615;&#37327;&#21270;&#31354;&#38388;&#21644;&#24341;&#20837;&#26435;&#37325;&#20998;&#24067;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#20998;&#32452;&#28176;&#36827;&#24341;&#23548;&#25439;&#22833;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#22330;&#26223;&#37325;&#22797;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07650</link><description>&lt;p&gt;
EQ-Net&#65306;&#24377;&#24615;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EQ-Net: Elastic Quantization Neural Networks. (arXiv:2308.07650v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EQ-Net&#65292;&#19968;&#31181;&#24377;&#24615;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#28789;&#27963;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#25506;&#32034;&#24377;&#24615;&#37327;&#21270;&#31354;&#38388;&#21644;&#24341;&#20837;&#26435;&#37325;&#20998;&#24067;&#27491;&#21017;&#21270;&#25439;&#22833;&#21644;&#20998;&#32452;&#28176;&#36827;&#24341;&#23548;&#25439;&#22833;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#22330;&#26223;&#37325;&#22797;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27169;&#22411;&#37327;&#21270;&#26041;&#27861;&#22312;&#20943;&#23569;&#23384;&#20648;&#31354;&#38388;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30828;&#20214;&#25152;&#25903;&#25345;&#30340;&#37327;&#21270;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#36890;&#24120;&#26159;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#36827;&#34892;&#37325;&#22797;&#20248;&#21270;&#12290;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#28789;&#27963;&#37327;&#21270;&#24418;&#24335;&#30340;&#27169;&#22411;&#36824;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#32593;&#32476;&#37327;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;&#24377;&#24615;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;EQ-Net&#65289;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#26435;&#37325;&#20849;&#20139;&#37327;&#21270;&#36229;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24377;&#24615;&#37327;&#21270;&#31354;&#38388;&#65288;&#21253;&#25324;&#24377;&#24615;&#20301;&#23485;&#12289;&#31890;&#24230;&#21644;&#23545;&#31216;&#24615;&#65289;&#20197;&#36866;&#24212;&#21508;&#31181;&#20027;&#27969;&#37327;&#21270;&#24418;&#24335;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26435;&#37325;&#20998;&#24067;&#27491;&#21017;&#21270;&#25439;&#22833;&#65288;WDR-Loss&#65289;&#21644;&#20998;&#32452;&#28176;&#36827;&#24341;&#23548;&#25439;&#22833;&#65288;GPG-Loss&#65289;&#26469;&#24357;&#21512;&#24377;&#24615;&#37327;&#21270;&#31354;&#38388;&#24046;&#24322;&#20013;&#26435;&#37325;&#21644;&#36755;&#20986;&#26631;&#20934;&#24046;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#38454;&#25628;&#32034;&#31639;&#27861;&#21644;&#20108;&#20998;&#26597;&#25214;&#31639;&#27861;&#30830;&#23450;&#26368;&#20248;&#37327;&#21270;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD) &#30340;&#26032;&#39062;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#36890;&#36807;&#38480;&#21046;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#30697;&#38453;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#23427;&#22312;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07641</link><description>&lt;p&gt;
&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299;&#20316;&#20026;&#32447;&#24615;&#26144;&#23556;&#20013;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping. (arXiv:2308.07641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD) &#30340;&#26032;&#39062;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#36890;&#36807;&#38480;&#21046;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#30697;&#38453;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#23427;&#22312;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#32447;&#24615;&#26144;&#23556;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#19977;&#20803;&#22855;&#24322;&#20540;&#20998;&#35299; (TSVD)&#65292;&#20197;&#23454;&#29616;&#21331;&#36234;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#19981;&#21516;&#65292;TSVD&#23558;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;$U$&#21644;$V$&#30697;&#38453;&#38480;&#21046;&#20026;&#19977;&#20803;&#30697;&#38453;&#24418;&#24335;&#65292;&#21363;$\{ \pm 1, 0 \}$&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35745;&#31639;$U(\cdot)$&#21644;$V(\cdot)$&#26102;&#65292;TSVD&#21482;&#38656;&#35201;&#21152;&#27861;&#25351;&#20196;&#65292;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20056;&#27861;&#25351;&#20196;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30452;&#25509;&#36716;&#25442;&#31639;&#27861;&#21644;&#35757;&#32451;&#36807;&#28193;&#31639;&#27861;&#65292;&#22914;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30452;&#25509;&#36716;&#25442;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TSVD&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#32593;&#32476;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21387;&#32553;&#24615;&#33021;&#65292;&#21253;&#25324;&#24403;&#21069;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#22914;ConvNext&#12289;Swim&#12289;BERT&#20197;&#21450;&#31867;&#20284;OPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).  Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.  We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.  In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;LLM-specific Mini-CEX&#35780;&#20272;&#26631;&#20934;&#21644;&#20351;&#29992;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;LLMs&#35780;&#20272;&#20013;&#30340;&#32479;&#19968;&#21644;&#20840;&#38754;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07635</link><description>&lt;p&gt;
LLM-Mini-CEX: &#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation. (arXiv:2308.07635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;LLM-specific Mini-CEX&#35780;&#20272;&#26631;&#20934;&#21644;&#20351;&#29992;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;LLMs&#35780;&#20272;&#20013;&#30340;&#32479;&#19968;&#21644;&#20840;&#38754;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#30340;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#26631;&#20934;&#23548;&#33268;&#26080;&#27861;&#35780;&#20272;&#21307;&#23398;LLMs&#30340;&#36136;&#37327;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;LLMs&#22312;&#21307;&#30103;&#27835;&#30103;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;LLMs&#36827;&#34892;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#20132;&#20114;&#20197;&#33719;&#21462;&#35786;&#26029;&#23545;&#35805;&#21644;&#20154;&#24037;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#21407;&#22987;&#30340;Mini-CEX&#24314;&#31435;&#20102;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#31216;&#20026;LLM-specific Mini-CEX&#65292;&#20197;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#35786;&#26029;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#21171;&#21160;&#23494;&#38598;&#22411;&#20132;&#20114;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24739;&#32773;&#27169;&#25311;&#22120;&#19982;LLMs&#33258;&#21160;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#21033;&#29992;ChatGPT&#33258;&#21160;&#35780;&#20272;&#35786;&#26029;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimenta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31454;&#36187;&#29702;&#35770;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20803;&#23431;&#23449;&#26381;&#21153;&#20013;&#29992;&#25143;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#20043;&#38388;&#30340;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12290;&#36890;&#36807;&#20943;&#23569;&#20256;&#36755;&#25968;&#25454;&#37327;&#21644;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#20248;&#21270;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#21516;&#27493;&#21644;&#20943;&#23569;&#32593;&#32476;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#20803;&#23431;&#23449;&#26381;&#21153;&#30340;&#35821;&#20041;&#36890;&#20449;&#65306;&#31454;&#36187;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach. (arXiv:2308.07618v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31454;&#36187;&#29702;&#35770;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20803;&#23431;&#23449;&#26381;&#21153;&#20013;&#29992;&#25143;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#20043;&#38388;&#30340;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12290;&#36890;&#36807;&#20943;&#23569;&#20256;&#36755;&#25968;&#25454;&#37327;&#21644;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#20248;&#21270;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#21516;&#27493;&#21644;&#20943;&#23569;&#32593;&#32476;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#20316;&#20026;&#19968;&#20010;&#23089;&#20048;&#12289;&#31038;&#20132;&#21644;&#24037;&#20316;&#24179;&#21488;&#30340;&#27969;&#34892;&#65292;&#23548;&#33268;&#20102;&#23545;&#34394;&#25311;&#19990;&#30028;&#20013;&#26080;&#32541;&#21270;&#36523;&#38598;&#25104;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#22312;&#20803;&#23431;&#23449;&#20013;&#65292;&#24517;&#39035;&#26356;&#26032;&#21644;&#28210;&#26579;&#21270;&#36523;&#20197;&#21453;&#26144;&#29992;&#25143;&#30340;&#34892;&#20026;&#12290;&#23454;&#29616;&#34394;&#25311;&#20301;&#32622;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#23454;&#26102;&#21516;&#27493;&#26159;&#22797;&#26434;&#30340;&#65292;&#23545;&#20803;&#23431;&#23449;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;MSP&#65289;&#30340;&#28210;&#26579;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#25552;&#20986;&#20102;&#24456;&#39640;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#31454;&#36187;&#29702;&#35770;&#26469;&#27169;&#25311;&#29992;&#25143;&#21644;MSP&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#30830;&#23450;&#27599;&#20010;&#29992;&#25143;&#30340;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;&#12290;&#20026;&#20102;&#20943;&#23569;&#26080;&#32447;&#20256;&#36755;&#20013;&#32593;&#32476;&#36164;&#28304;&#30340;&#28040;&#32791;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#20041;&#36890;&#20449;&#25216;&#26415;&#26469;&#20943;&#23569;&#38656;&#35201;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#35774;&#32622;&#20013;&#65292;&#32534;&#30721;&#30340;&#35821;&#20041;&#25968;&#25454;&#21482;&#21253;&#21547;&#20102;51&#20010;&#23383;&#33410;&#30340;&#39592;&#39612;&#22352;&#26631;&#65292;&#32780;&#19981;&#26159;8.243&#20806;&#23383;&#33410;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#28145;&#24230;Q&#32593;&#32476;&#26469;&#20248;&#21270;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of Metaverse as an entertainment, social, and work platform has led to a great need for seamless avatar integration in the virtual world. In Metaverse, avatars must be updated and rendered to reflect users' behaviour. Achieving real-time synchronization between the virtual bilocation and the user is complex, placing high demands on the Metaverse Service Provider (MSP)'s rendering resource allocation scheme. To tackle this issue, we propose a semantic communication framework that leverages contest theory to model the interactions between users and MSPs and determine optimal resource allocation for each user. To reduce the consumption of network resources in wireless transmission, we use the semantic communication technique to reduce the amount of data to be transmitted. Under our simulation settings, the encoded semantic data only contains 51 bytes of skeleton coordinates instead of the image size of 8.243 megabytes. Moreover, we implement Deep Q-Network to optimize rewar
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SGDiff&#30340;&#39118;&#26684;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#27169;&#24577;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#21019;&#36896;&#24615;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#39118;&#26684;&#24341;&#23548;&#12289;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#20197;&#21450;&#20811;&#26381;&#25991;&#26412;&#36755;&#20837;&#38480;&#21046;&#31561;&#26041;&#24335;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;SG-Fashion&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07605</link><description>&lt;p&gt;
SGDiff: &#19968;&#31181;&#29992;&#20110;&#26102;&#23578;&#21512;&#25104;&#30340;&#39118;&#26684;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SGDiff: A Style Guided Diffusion Model for Fashion Synthesis. (arXiv:2308.07605v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07605
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SGDiff&#30340;&#39118;&#26684;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#27169;&#24577;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#21019;&#36896;&#24615;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#39118;&#26684;&#24341;&#23548;&#12289;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#20197;&#21450;&#20811;&#26381;&#25991;&#26412;&#36755;&#20837;&#38480;&#21046;&#31561;&#26041;&#24335;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;SG-Fashion&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#26684;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65288;SGDiff&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#22266;&#26377;&#30340;&#19968;&#20123;&#24369;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;SGDiff&#23558;&#22270;&#20687;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20419;&#36827;&#21019;&#36896;&#24615;&#30340;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#30340;&#39118;&#26684;&#24341;&#23548;&#26469;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#20811;&#26381;&#20102;&#21482;&#26377;&#25991;&#26412;&#36755;&#20837;&#26102;&#25511;&#21046;&#21512;&#25104;&#26679;&#24335;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;-SG-Fashion&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26102;&#23578;&#22270;&#20687;&#21512;&#25104;&#24212;&#29992;&#65292;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21644;&#24191;&#27867;&#30340;&#26381;&#35013;&#31867;&#21035;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#24212;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#25928;&#26524;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#31867;&#21035;&#30340;&#26102;&#23578;&#22270;&#20687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports on the development of \textbf{a novel style guided diffusion model (SGDiff)} which overcomes certain weaknesses inherent in existing models for image synthesis. The proposed SGDiff combines image modality with a pretrained text-to-image diffusion model to facilitate creative fashion image synthesis. It addresses the limitations of text-to-image diffusion models by incorporating supplementary style guidance, substantially reducing training costs, and overcoming the difficulties of controlling synthesized styles with text-only inputs. This paper also introduces a new dataset -- SG-Fashion, specifically designed for fashion image synthesis applications, offering high-resolution images and an extensive range of garment categories. By means of comprehensive ablation study, we examine the application of classifier-free guidance to a variety of conditions and validate the effectiveness of the proposed model for generating fashion images of the desired categories, product at
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;&#28216;&#25103;&#35282;&#33394;&#31574;&#30053;&#65292;&#20197;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#23545;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#36827;&#34892;&#21152;&#26435;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.07598</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#28216;&#25103;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Generating Personas for Games with Multimodal Adversarial Imitation Learning. (arXiv:2308.07598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;&#28216;&#25103;&#35282;&#33394;&#31574;&#30053;&#65292;&#20197;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#23545;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#36827;&#34892;&#21152;&#26435;&#65292;&#26377;&#25928;&#22320;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#30340;&#29609;&#23478;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22797;&#26434;&#30340;&#22870;&#21169;&#24037;&#31243;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#24448;&#24448;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#12290;&#20026;&#20102;&#24314;&#27169;&#21508;&#31181;&#20154;&#31867;&#28216;&#25103;&#39118;&#26684;&#65292;&#36229;&#36234;&#24378;&#21270;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#29992;&#22870;&#21169;&#20989;&#25968;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#20010;&#35282;&#33394;&#31574;&#30053;&#29992;&#20110;&#28216;&#25103;&#27979;&#35797;&#12290;&#22810;&#27169;&#24577;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;MultiGAIL&#65289;&#20351;&#29992;&#36741;&#21161;&#36755;&#20837;&#21442;&#25968;&#65292;&#20351;&#29992;&#21333;&#26234;&#33021;&#20307;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;MultiGAIL&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#27604;&#36739;&#26234;&#33021;&#20307;&#21644;&#19981;&#21516;&#30340;&#19987;&#23478;&#31574;&#30053;&#26469;&#25512;&#26029;&#29615;&#22659;&#22870;&#21169;&#12290;&#27599;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#26681;&#25454;&#36741;&#21161;&#36755;&#20837;&#36827;&#34892;&#21152;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31354;&#38388;&#21518;&#22788;&#29702;&#23454;&#29616;&#33258;&#21160;&#21270;&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;&#12290;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#36947;&#36335;&#20960;&#20309;&#21644;&#27773;&#36710;&#20132;&#36890;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#34903;&#26223;&#22270;&#20687;&#25968;&#25454;&#33021;&#22815;&#20934;&#30830;&#12289;&#24555;&#36895;&#22320;&#35780;&#20272;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#39569;&#34892;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07580</link><description>&lt;p&gt;
AutoLTS&#65306;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31354;&#38388;&#21518;&#22788;&#29702;&#23454;&#29616;&#33258;&#21160;&#21270;&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing. (arXiv:2308.07580v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31354;&#38388;&#21518;&#22788;&#29702;&#23454;&#29616;&#33258;&#21160;&#21270;&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;&#12290;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#36947;&#36335;&#20960;&#20309;&#21644;&#27773;&#36710;&#20132;&#36890;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#34903;&#26223;&#22270;&#20687;&#25968;&#25454;&#33021;&#22815;&#20934;&#30830;&#12289;&#24555;&#36895;&#22320;&#35780;&#20272;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#30340;&#39569;&#34892;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;&#26159;&#19968;&#31181;&#34913;&#37327;&#39569;&#34892;&#32773;&#21463;&#21040;&#24314;&#31569;&#29615;&#22659;&#21644;&#27773;&#36710;&#20132;&#36890;&#24433;&#21709;&#30340;&#20027;&#35266;&#21387;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#39569;&#34892;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#39569;&#34892;&#36335;&#32447;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35745;&#31639;&#39569;&#34892;&#21387;&#21147;&#30340;&#26041;&#27861;&#36895;&#24230;&#24930;&#19988;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34903;&#26223;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#25903;&#25345;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#22823;&#35268;&#27169;&#30340;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#21033;&#29992;&#39569;&#34892;&#21387;&#21147;&#26631;&#31614;&#30340;&#24207;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65307;ii&#65289;&#20351;&#29992;&#21518;&#22788;&#29702;&#25216;&#26415;&#23558;&#31354;&#38388;&#24179;&#28369;&#24615;&#32435;&#20837;&#25105;&#20204;&#30340;&#39044;&#27979;&#20013;&#12290;&#22312;&#21152;&#25343;&#22823;&#22810;&#20262;&#22810;&#25910;&#38598;&#30340;39,153&#20010;&#36947;&#36335;&#27573;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#36947;&#36335;&#20960;&#20309;&#21644;&#27773;&#36710;&#20132;&#36890;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#39569;&#34892;&#21387;&#21147;&#35780;&#20272;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cycling stress assessment, which quantifies cyclists' perceived stress imposed by the built environment and motor traffics, increasingly informs cycling infrastructure planning and cycling route recommendation. However, currently calculating cycling stress is slow and data-intensive, which hinders its broader application. In this paper, We propose a deep learning framework to support accurate, fast, and large-scale cycling stress assessments for urban road networks based on street-view images. Our framework features i) a contrastive learning approach that leverages the ordinal relationship among cycling stress labels, and ii) a post-processing technique that enforces spatial smoothness into our predictions. On a dataset of 39,153 road segments collected in Toronto, Canada, our results demonstrate the effectiveness of our deep learning framework and the value of using image data for cycling stress assessment in the absence of high-quality road geometry and motor traffic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07575</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#26469;&#36827;&#34892;&#25925;&#20107;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#21644;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#38590;&#28857;&#22312;&#20110;&#19981;&#20165;&#38656;&#35201;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21576;&#29616;&#35270;&#35273;&#32454;&#33410;&#65292;&#36824;&#38656;&#35201;&#23545;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20026;&#27599;&#20010;&#21477;&#23376;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#20294;&#22312;&#32473;&#23450;&#27573;&#33853;&#20013;&#32534;&#30721;&#19978;&#19979;&#25991;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#35828;&#26381;&#21147;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#35282;&#33394;&#25110;&#36866;&#24403;&#30340;&#22330;&#26223;&#32972;&#26223;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35760;&#24518;&#26550;&#26500;&#65292;&#29992;&#20110;&#21452;&#21521;Transformer&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#22686;&#24378;&#29983;&#25104;&#22810;&#20010;&#20266;&#25551;&#36848;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34917;&#20805;&#30417;&#30563;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25512;&#29702;&#20013;&#30340;&#35821;&#35328;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;Pororo-SV&#21644;Flintstones-SV&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;FID&#12289;&#23383;&#31526;...
&lt;/p&gt;
&lt;p&gt;
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#20998;&#31867;&#34892;&#21160;&#31867;&#21035;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#26469;&#39044;&#27979;&#34892;&#21160;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26377;&#36129;&#29486;&#65292;&#22522;&#20110;&#34892;&#21160;&#26631;&#31614;&#25991;&#26412;&#30340;&#20851;&#31995;&#39044;&#27979;&#26356;&#20934;&#30830;&#65292;&#36890;&#36807;&#32508;&#21512;&#20004;&#31181;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07558</link><description>&lt;p&gt;
&#36328;&#22810;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#26816;&#27979;&#21644;&#20998;&#31867;&#34892;&#21160;&#31867;&#21035;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Action Class Relation Detection and Classification Across Multiple Video Datasets. (arXiv:2308.07558v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#20998;&#31867;&#34892;&#21160;&#31867;&#21035;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#26469;&#39044;&#27979;&#34892;&#21160;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26377;&#36129;&#29486;&#65292;&#22522;&#20110;&#34892;&#21160;&#26631;&#31614;&#25991;&#26412;&#30340;&#20851;&#31995;&#39044;&#27979;&#26356;&#20934;&#30830;&#65292;&#36890;&#36807;&#32508;&#21512;&#20004;&#31181;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta Video Dataset (MetaVD)&#25552;&#20379;&#20102;&#20027;&#35201;&#35270;&#39057;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#34892;&#21160;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#27880;&#37322;&#12290;&#34429;&#28982;&#36825;&#20123;&#20851;&#31995;&#27880;&#37322;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#38598;&#25193;&#20805;&#65292;&#20294;&#21482;&#36866;&#29992;&#20110;MetaVD&#35206;&#30422;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#22806;&#37096;&#25968;&#25454;&#38598;&#35201;&#20139;&#21463;&#30456;&#21516;&#30340;&#22909;&#22788;&#65292;&#38656;&#35201;&#30830;&#23450;&#20854;&#34892;&#21160;&#31867;&#21035;&#19982;MetaVD&#20013;&#34892;&#21160;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65306;&#34892;&#21160;&#31867;&#21035;&#20851;&#31995;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#34892;&#21160;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;(i) &#38024;&#23545;&#25991;&#26412;&#21644;&#35270;&#39057;&#30340;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#39640;&#39044;&#27979;&#24615;&#33021;&#26377;&#36129;&#29486;&#65292;(ii) &#22522;&#20110;&#34892;&#21160;&#26631;&#31614;&#25991;&#26412;&#30340;&#20851;&#31995;&#39044;&#27979;&#27604;&#22522;&#20110;&#35270;&#39057;&#26356;&#20934;&#30830;&#65292;(iii) &#36890;&#36807;&#23558;&#20004;&#31181;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32508;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#20013;&#30340;&#20919;&#21551;&#21160;&#39057;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;Q&#23398;&#20064;&#21644;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#39044;&#26399;&#38656;&#27714;&#30340;&#22522;&#30784;&#19978;&#25552;&#21069;&#21021;&#22987;&#21270;&#20989;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20919;&#21551;&#21160;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.07541</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#20013;&#20919;&#21551;&#21160;&#39057;&#29575;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) Augmented Cold Start Frequency Reduction in Serverless Computing. (arXiv:2308.07541v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#20013;&#30340;&#20919;&#21551;&#21160;&#39057;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;Q&#23398;&#20064;&#21644;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#39044;&#26399;&#38656;&#27714;&#30340;&#22522;&#30784;&#19978;&#25552;&#21069;&#21021;&#22987;&#21270;&#20989;&#25968;&#65292;&#20174;&#32780;&#20943;&#23569;&#20919;&#21551;&#21160;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#21363;&#26381;&#21153;&#26159;&#19968;&#31181;&#20113;&#35745;&#31639;&#33539;&#20363;&#65292;&#20026;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#20107;&#20214;&#39537;&#21160;&#25191;&#34892;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#20174;&#24320;&#21457;&#32773;&#37027;&#37324;&#28040;&#38500;&#36164;&#28304;&#31649;&#29702;&#36131;&#20219;&#65292;&#25552;&#20379;&#36879;&#26126;&#21644;&#25353;&#38656;&#21487;&#25193;&#23637;&#24615;&#26469;&#23454;&#29616;&#26080;&#26381;&#21153;&#22120;&#29305;&#24615;&#12290;&#20856;&#22411;&#30340;&#26080;&#26381;&#21153;&#22120;&#24212;&#29992;&#31243;&#24207;&#23545;&#21709;&#24212;&#26102;&#38388;&#21644;&#21487;&#25193;&#23637;&#24615;&#26377;&#20005;&#26684;&#35201;&#27714;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#37096;&#32626;&#30340;&#26381;&#21153;&#20026;&#23458;&#25143;&#25552;&#20379;&#24555;&#36895;&#21644;&#23481;&#38169;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#20989;&#25968;&#21363;&#26381;&#21153;&#33539;&#20363;&#22312;&#38656;&#35201;&#25353;&#38656;&#21021;&#22987;&#21270;&#20989;&#25968;&#26102;&#23384;&#22312;&#38750;&#24120;&#21487;&#35266;&#30340;&#24310;&#36831;&#65292;&#21363;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20943;&#23569;&#24179;&#21488;&#19978;&#30340;&#20919;&#21551;&#21160;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Q&#23398;&#20064;&#65292;&#24182;&#32771;&#34385;&#20989;&#25968;&#30340;CPU&#21033;&#29992;&#29575;&#12289;&#24050;&#26377;&#20989;&#25968;&#23454;&#20363;&#21644;&#21709;&#24212;&#22833;&#36133;&#29575;&#31561;&#25351;&#26631;&#65292;&#26681;&#25454;&#39044;&#26399;&#38656;&#27714;&#25552;&#21069;&#20027;&#21160;&#21021;&#22987;&#21270;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;Kubeless&#19978;&#23454;&#29616;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Function-as-a-Service is a cloud computing paradigm offering an event-driven execution model to applications. It features serverless attributes by eliminating resource management responsibilities from developers and offers transparent and on-demand scalability of applications. Typical serverless applications have stringent response time and scalability requirements and therefore rely on deployed services to provide quick and fault-tolerant feedback to clients. However, the FaaS paradigm suffers from cold starts as there is a non-negligible delay associated with on-demand function initialization. This work focuses on reducing the frequency of cold starts on the platform by using Reinforcement Learning. Our approach uses Q-learning and considers metrics such as function CPU utilization, existing function instances, and response failure rate to proactively initialize functions in advance based on the expected demand. The proposed solution was implemented on Kubeless and was evaluated usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#22312;&#21482;&#26377;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07538</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#29109;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30340;&#30495;&#20266;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation via Minimax Entropy for Real/Bogus Classification of Astronomical Alerts. (arXiv:2308.07538v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22825;&#25991;&#35686;&#25253;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#22312;&#21482;&#26377;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22495;&#22825;&#25991;&#23398;&#27491;&#26397;&#30528;&#23454;&#26102;&#20998;&#26512;&#22810;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#20419;&#20351;&#22810;&#27969;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20351;&#29992;&#22235;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;HiTS&#12289;DES&#12289;ATLAS&#21644;ZTF&#65289;&#30740;&#31350;&#20102;&#22825;&#25991;&#35686;&#25253;&#30340;&#30495;&#20266;&#20998;&#31867;&#30340;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24494;&#35843;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#65288;MME&#65289;&#25913;&#36827;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#28304;&#30446;&#26631;&#24773;&#26223;&#19979;&#30340;&#24179;&#34913;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;MME&#27169;&#22411;&#37117;&#33021;&#26174;&#33879;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#20174;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#21482;&#26377;&#19968;&#20010;&#26631;&#35760;&#30340;&#39033;&#65292;&#32780;&#19988;MME&#27169;&#22411;&#19981;&#20250;&#25439;&#23475;&#28304;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07520</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25214;&#21040;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#12290;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#24050;&#32463;&#34987;&#27979;&#37327;&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#20010;&#21464;&#37327;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#26377;&#26102;&#20027;&#35201;&#20851;&#27880;&#30340;&#21464;&#37327;&#24182;&#38750;&#30452;&#25509;&#21487;&#35266;&#23519;&#65292;&#32780;&#26159;&#36890;&#36807;&#23427;&#20204;&#22312;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#26469;&#25512;&#29702;&#20986;&#26469;&#30340;&#12290;&#36825;&#20123;&#34987;&#31216;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19968;&#20010;&#24191;&#27867;&#34987;&#30693;&#36947;&#30340;&#20363;&#23376;&#26159;&#24515;&#29702;&#26500;&#36896;&#30340;&#26234;&#21830;&#65292;&#22240;&#20026;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#65292;&#25152;&#20197;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#22914;&#26234;&#21830;&#27979;&#35797;&#26469;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#21644;&#28508;&#22312;&#21464;&#37327;&#19982;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#36830;&#25509;&#65292;&#20174;&#32780;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#32479;&#35745;&#19968;&#33268;&#24615;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReFixMatch&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#39640;&#21644;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#26469;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20266;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.07509</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#25509;&#39640;&#21644;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#26469;&#25552;&#21319;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Semi-Supervised Learning by bridging high and low-confidence predictions. (arXiv:2308.07509v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReFixMatch&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#39640;&#21644;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#26469;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20266;&#26631;&#31614;&#38382;&#39064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20154;&#24037;&#26631;&#31614;&#65292;&#20174;&#32780;&#22312;&#30417;&#30563;&#35774;&#32622;&#19979;&#21516;&#26102;&#35757;&#32451;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#20266;&#26631;&#31614;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#21487;&#33021;&#19981;&#24635;&#26159;&#20934;&#30830;&#65292;&#23548;&#33268;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23545;&#26131;&#23398;&#20363;&#23376;&#36807;&#25311;&#21512;&#65292;&#24573;&#35270;&#38590;&#23398;&#20363;&#23376;&#65292;&#20174;&#32780;&#23548;&#33268;&#8220;&#39532;&#22826;&#25928;&#24212;&#8221;&#65292;&#21363;&#24378;&#32773;&#26356;&#24378;&#65292;&#24369;&#32773;&#26356;&#24369;&#12290;&#31532;&#19977;&#65292;&#30001;&#20110;&#20351;&#29992;&#20102;&#39640;&#38408;&#20540;&#65292;&#22823;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#34987;&#20002;&#24323;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ReFixMatch&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-labeling is a crucial technique in semi-supervised learning (SSL), where artificial labels are generated for unlabeled data by a trained model, allowing for the simultaneous training of labeled and unlabeled data in a supervised setting. However, several studies have identified three main issues with pseudo-labeling-based approaches. Firstly, these methods heavily rely on predictions from the trained model, which may not always be accurate, leading to a confirmation bias problem. Secondly, the trained model may be overfitted to easy-to-learn examples, ignoring hard-to-learn ones, resulting in the \textit{"Matthew effect"} where the already strong become stronger and the weak weaker. Thirdly, most of the low-confidence predictions of unlabeled data are discarded due to the use of a high threshold, leading to an underutilization of unlabeled data during training. To address these issues, we propose a new method called ReFixMatch, which aims to utilize all of the unlabeled data dur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#19981;&#33391;&#34892;&#20026;&#32773;&#33021;&#22815;&#33258;&#21160;&#22635;&#20889;&#22312;&#32447;&#38382;&#21367;&#65292;&#23041;&#32961;&#21040;&#25968;&#25454;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#38656;&#35201;&#20381;&#36182;&#19981;&#33391;&#34892;&#20026;&#32773;&#30340;&#19981;&#31215;&#26497;&#24615;&#26469;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.07499</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26816;&#27979;&#22312;&#32447;&#38382;&#21367;&#30340;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
Detecting The Corruption Of Online Questionnaires By Artificial Intelligence. (arXiv:2308.07499v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07499
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#19981;&#33391;&#34892;&#20026;&#32773;&#33021;&#22815;&#33258;&#21160;&#22635;&#20889;&#22312;&#32447;&#38382;&#21367;&#65292;&#23041;&#32961;&#21040;&#25968;&#25454;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#38656;&#35201;&#20381;&#36182;&#19981;&#33391;&#34892;&#20026;&#32773;&#30340;&#19981;&#31215;&#26497;&#24615;&#26469;&#20445;&#35777;&#25968;&#25454;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#38382;&#21367;&#20351;&#29992;&#20247;&#21253;&#24179;&#21488;&#25307;&#21215;&#21442;&#19982;&#32773;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#22240;&#20026;&#23427;&#20204;&#26131;&#20110;&#20351;&#29992;&#19988;&#25104;&#26412;&#20302;&#24265;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#19981;&#33391;&#34892;&#20026;&#32773;&#33021;&#22815;&#33258;&#21160;&#22635;&#20889;&#22312;&#32447;&#34920;&#21333;&#65292;&#21253;&#25324;&#20026;&#24320;&#25918;&#24615;&#20219;&#21153;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#36827;&#27493;&#23041;&#32961;&#21040;&#20351;&#29992;&#22312;&#32447;&#38382;&#21367;&#30340;&#30740;&#31350;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#29992;&#20110;&#22312;&#32447;&#30740;&#31350;&#30446;&#30340;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#20197;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#20154;&#24037;&#26234;&#33021;&#26816;&#27979;&#31995;&#32479;&#26816;&#27979;&#20986;&#26469;&#12290;&#34429;&#28982;&#20154;&#31867;&#33021;&#22815;&#27491;&#30830;&#36776;&#21035;&#25991;&#26412;&#30340;&#20316;&#32773;&#36523;&#20221;&#65288;76%&#30340;&#20934;&#30830;&#29575;&#65289;&#65292;&#20294;&#20182;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#20302;&#20110;&#30830;&#20445;&#20196;&#20154;&#28385;&#24847;&#30340;&#25968;&#25454;&#36136;&#37327;&#25152;&#38656;&#30340;&#27700;&#24179;&#12290;&#30446;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#21482;&#33021;&#20381;&#36182;&#19981;&#33391;&#34892;&#20026;&#32773;&#30340;&#19981;&#31215;&#26497;&#24615;&#26469;&#25104;&#21151;&#20351;&#29992;&#24320;&#25918;&#24615;&#22238;&#31572;&#20316;&#20026;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#33258;&#21160;&#20154;&#24037;&#26234;&#33021;&#26816;&#27979;&#31995;&#32479;&#30446;&#21069;&#26080;&#27861;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online questionnaires that use crowd-sourcing platforms to recruit participants have become commonplace, due to their ease of use and low costs. Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. These technological advances threaten the data quality for studies that use online questionnaires. This study tested if text generated by an AI for the purpose of an online study can be detected by both humans and automatic AI detection systems. While humans were able to correctly identify authorship of text above chance level (76 percent accuracy), their performance was still below what would be required to ensure satisfactory data quality. Researchers currently have to rely on the disinterest of bad actors to successfully use open-ended responses as a useful tool for ensuring data quality. Automatic AI detection systems are currently completely unusab
&lt;/p&gt;</description></item><item><title>DREAMWALKER&#26159;&#19968;&#31181;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#36830;&#32493;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#37096;&#25277;&#35937;&#19990;&#30028;&#20013;&#27169;&#25311;&#21644;&#35780;&#20272;&#21487;&#33021;&#30340;&#35745;&#21010;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.07498</link><description>&lt;p&gt;
DREAMWALKER:&#36830;&#32493;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#30340;&#24515;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation. (arXiv:2308.07498v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07498
&lt;/p&gt;
&lt;p&gt;
DREAMWALKER&#26159;&#19968;&#31181;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#36830;&#32493;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#36890;&#36807;&#22312;&#20869;&#37096;&#25277;&#35937;&#19990;&#30028;&#20013;&#27169;&#25311;&#21644;&#35780;&#20272;&#21487;&#33021;&#30340;&#35745;&#21010;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
VLN-CE&#26159;&#19968;&#39033;&#26368;&#36817;&#21457;&#24067;&#30340;&#20219;&#21153;&#65292;AI&#20195;&#29702;&#38656;&#35201;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#22312;&#33258;&#30001;&#21487;&#31359;&#36234;&#30340;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#36798;&#19968;&#20010;&#36828;&#22788;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#30001;&#20110;&#21487;&#33021;&#31574;&#30053;&#30340;&#24040;&#22823;&#31354;&#38388;&#65292;&#36825;&#20010;&#20219;&#21153;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#25105;&#20204;&#30456;&#20449;&#33021;&#22815;&#39044;&#26399;&#26410;&#26469;&#34892;&#21160;&#30340;&#21518;&#26524;&#23545;&#20110;&#26234;&#33021;&#21644;&#21487;&#35299;&#37322;&#30340;&#35268;&#21010;&#34892;&#20026;&#30340;&#20986;&#29616;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;VLN-CE&#20195;&#29702;DREAMWALKER&#12290;&#36825;&#20010;&#19990;&#30028;&#27169;&#22411;&#23558;&#22797;&#26434;&#36830;&#32493;&#29615;&#22659;&#30340;&#35270;&#35273;&#12289;&#25299;&#25169;&#21644;&#21160;&#24577;&#29305;&#24615;&#24635;&#32467;&#20026;&#31163;&#25955;&#12289;&#32467;&#26500;&#21270;&#21644;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;DREAMWALKER&#21487;&#20197;&#22312;&#36825;&#31181;&#20869;&#37096;&#25277;&#35937;&#19990;&#30028;&#20013;&#27169;&#25311;&#21644;&#35780;&#20272;&#21487;&#33021;&#30340;&#35745;&#21010;&#65292;&#28982;&#21518;&#20877;&#25191;&#34892;&#26114;&#36149;&#30340;&#34892;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;VLN-CE&#20195;&#29702;&#21482;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20316;&#36138;&#23146;&#20915;&#31574;&#30456;&#21453;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#30701;&#35270;&#34892;&#20026;&#65292;DREAMWALKER&#33021;&#22815;&#36890;&#36807;&#22823;&#37327;&#30340;&#35268;&#21010;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amou
&lt;/p&gt;</description></item><item><title>ST-MLP&#26159;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07496</link><description>&lt;p&gt;
ST-MLP&#65306;&#19968;&#31181;&#22522;&#20110;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#32423;&#32852;&#26102;&#31354;&#32447;&#24615;&#26694;&#26550;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting. (arXiv:2308.07496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07496
&lt;/p&gt;
&lt;p&gt;
ST-MLP&#26159;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#20248;&#21270;&#20132;&#36890;&#27969;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#22240;&#20854;&#36866;&#24212;&#36947;&#36335;&#22270;&#32467;&#26500;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#36190;&#35465;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;STGNNs&#26550;&#26500;&#30340;&#30740;&#31350;&#24120;&#24120;&#20248;&#20808;&#32771;&#34385;&#22797;&#26434;&#30340;&#35774;&#35745;&#65292;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#21152;&#37325;&#65292;&#20165;&#22312;&#31934;&#24230;&#19978;&#26377;&#23569;&#35768;&#25552;&#21319;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ST-MLP&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#27905;&#30340;&#22522;&#20110;&#32423;&#32852;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#21644;&#32447;&#24615;&#23618;&#30340;&#26102;&#31354;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#20998;&#32452;&#29420;&#31435;&#31574;&#30053;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25216;&#26415;&#65292;&#23558;&#26102;&#38388;&#20449;&#24687;&#12289;&#31354;&#38388;&#20449;&#24687;&#21644;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;ST-MLP&#22312;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;STGNNs&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#40723;&#21169;&#20102;&#26377;&#20851;&#20132;&#36890;&#39044;&#27979;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.07469</link><description>&lt;p&gt;
Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Omega-Regular Reward Machines. (arXiv:2308.07469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#35774;&#35745;&#36866;&#24403;&#30340;&#22870;&#21169;&#26426;&#21046;&#23545;&#20854;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#30446;&#26631;&#30340;&#22797;&#26434;&#24615;&#36229;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#33021;&#21147;&#33539;&#22260;&#65292;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;&#22870;&#21169;&#26426;&#22120;&#21644;&#969;-&#27491;&#35268;&#35821;&#35328;&#26159;&#29992;&#20110;&#34920;&#31034;&#23450;&#37327;&#21644;&#23450;&#24615;&#30446;&#26631;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#20004;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#65292;&#23558;&#22870;&#21169;&#26426;&#22120;&#19982;&#969;-&#27491;&#35268;&#35821;&#35328;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;RL&#25552;&#20379;&#20102;&#19968;&#31181;&#34920;&#36798;&#33021;&#21147;&#24378;&#19988;&#26377;&#25928;&#30340;&#22870;&#21169;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#26469;&#35745;&#31639;&#38024;&#23545;&#969;-&#27491;&#35268;&#22870;&#21169;&#26426;&#22120;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07462</link><description>&lt;p&gt;
&#29609;&#24324;&#25991;&#23383;&#65306;&#27604;&#36739;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#21644;ChatGPT&#31561;&#24037;&#20855;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#21487;&#20197;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#36825;&#23545;&#35835;&#32773;&#30340;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22521;&#35757;&#26159;&#21542;&#20250;&#20135;&#29983;&#24433;&#21709;&#20855;&#26377;&#35768;&#22810;&#21547;&#20041;&#65311;&#23427;&#26159;&#21542;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#28436;&#21464;&#65311;&#25105;&#20204;&#20851;&#27880;&#35821;&#35328;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65306;&#35789;&#35821;&#65307;&#22312;&#32534;&#20889;&#32473;&#23450;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#22686;&#21152;&#25110;&#20943;&#23569;&#20351;&#29992;&#30340;&#35789;&#27719;&#37327;&#25110;&#35789;&#27719;&#20016;&#23500;&#24230;&#65288;&#29702;&#35299;&#20026;&#20070;&#38754;&#25110;&#21475;&#22836;&#34920;&#36798;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#35789;&#27719;&#25968;&#37327;&#65289;&#65311;&#36825;&#23545;&#35789;&#35821;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#26410;&#21253;&#21547;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#30340;&#35789;&#35821;&#24448;&#24448;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21463;&#27426;&#36814;&#65292;&#24182;&#26368;&#32456;&#21487;&#33021;&#28040;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#20174;&#20132;&#36890;&#26426;&#26500;&#30340;&#35282;&#24230;&#25552;&#39640;&#25928;&#29575;&#21644;&#22686;&#21152;&#21033;&#29992;&#29575;&#65292;&#20197;&#25913;&#21892;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#26469;&#28304;&#19982;&#25968;&#25454;&#12289;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07457</link><description>&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Smart Transportation. (arXiv:2308.07457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#20174;&#20132;&#36890;&#26426;&#26500;&#30340;&#35282;&#24230;&#25552;&#39640;&#25928;&#29575;&#21644;&#22686;&#21152;&#21033;&#29992;&#29575;&#65292;&#20197;&#25913;&#21892;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#26469;&#28304;&#19982;&#25968;&#25454;&#12289;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#26377;&#36229;&#36807;7,000&#23478;&#20844;&#20849;&#20132;&#36890;&#26426;&#26500;&#65288;&#36824;&#26377;&#26356;&#22810;&#30340;&#31169;&#33829;&#26426;&#26500;&#65289;&#65292;&#23427;&#20204;&#19968;&#24180;&#20026;600&#20159;&#20056;&#23458;&#25552;&#20379;&#26381;&#21153;&#12290;&#19968;&#20010;&#36816;&#34892;&#33391;&#22909;&#30340;&#20132;&#36890;&#31995;&#32479;&#21487;&#20197;&#20419;&#36827;&#20225;&#19994;&#30340;&#22686;&#38271;&#21644;&#25193;&#24352;&#65292;&#20998;&#21457;&#31038;&#20250;&#21644;&#32463;&#27982;&#31119;&#21033;&#65292;&#24182;&#36830;&#25509;&#31038;&#21306;&#25104;&#21592;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20182;&#20204;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#30340;&#25104;&#23601;&#12290;&#30001;&#20110;&#32463;&#27982;&#23454;&#24800;&#30340;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#26159;&#35768;&#22810;&#31038;&#21306;&#30340;&#25903;&#26609;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#20174;&#20132;&#36890;&#26426;&#26500;&#30340;&#35282;&#24230;&#25552;&#39640;&#25928;&#29575;&#21644;&#22686;&#21152;&#21033;&#29992;&#29575;&#12290;&#36825;&#31687;&#20070;&#31456;&#35752;&#35770;&#20102;&#19982;&#35774;&#35745;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30456;&#20851;&#30340;&#20027;&#35201;&#38656;&#27714;&#12289;&#30446;&#26631;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#25968;&#25454;&#26469;&#28304;&#21644;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24110;&#21161;&#20132;&#36890;&#20915;&#31574;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are more than 7,000 public transit agencies in the U.S. (and many more private agencies), and together, they are responsible for serving 60 billion passenger miles each year. A well-functioning transit system fosters the growth and expansion of businesses, distributes social and economic benefits, and links the capabilities of community members, thereby enhancing what they can accomplish as a society. Since affordable public transit services are the backbones of many communities, this work investigates ways in which Artificial Intelligence (AI) can improve efficiency and increase utilization from the perspective of transit agencies. This book chapter discusses the primary requirements, objectives, and challenges related to the design of AI-driven smart transportation systems. We focus on three major topics. First, we discuss data sources and data. Second, we provide an overview of how AI can aid decision-making with a focus on transportation. Lastly, we discuss computational prob
&lt;/p&gt;</description></item><item><title>GRU-D-Weibull&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#34928;&#20943;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#20013;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#32456;&#28857;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07452</link><description>&lt;p&gt;
GRU-D-Weibull:&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction. (arXiv:2308.07452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07452
&lt;/p&gt;
&lt;p&gt;
GRU-D-Weibull&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#34928;&#20943;&#25216;&#26415;&#65292;&#29992;&#20110;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#24739;&#32773;&#38431;&#21015;&#20013;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#32456;&#28857;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#30340;&#20010;&#20307;&#32423;&#32456;&#28857;&#21644;&#32456;&#28857;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;GRU-D-Weibull&#65292;&#23427;&#23558;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#19982;&#34928;&#20943;&#25216;&#26415;&#65288;GRU-D&#65289;&#32467;&#21512;&#65292;&#26469;&#24314;&#27169;&#23041;&#24067;&#23572;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23454;&#26102;&#20010;&#20307;&#21270;&#32456;&#28857;&#39044;&#27979;&#21644;&#32676;&#20307;&#32423;&#39118;&#38505;&#31649;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;6,879&#21517;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#65288;CKD4&#65289;&#24739;&#32773;&#30340;&#38431;&#21015;&#65292;&#35780;&#20272;&#20102;GRU-D-Weibull&#22312;&#32456;&#28857;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#25351;&#26631;&#26085;&#26399;&#65292;GRU-D-Weibull&#30340;C-&#25351;&#25968;&#32422;&#20026;0.7&#65292;&#38543;&#30528;4.3&#24180;&#30340;&#38543;&#35775;&#65292;C-&#25351;&#25968;&#25552;&#39640;&#21040;&#32422;0.77&#65292;&#31867;&#20284;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CKD4&#25351;&#26631;&#26085;&#26399;&#30340;&#32477;&#23545;L1&#25439;&#22833;&#32422;&#20026;1.1&#24180;&#65288;&#26631;&#20934;&#24046;0.95&#65289;&#65292;&#38543;&#35775;4&#24180;&#21518;&#30340;&#26368;&#23567;L1&#25439;&#22833;&#32422;&#20026;0.45&#24180;&#65288;&#26631;&#20934;&#24046;0.3&#65289;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GRU-D-Weibull&#22312;&#20107;&#20214;&#21457;&#29983;&#26102;&#30340;&#39044;&#27979;&#29983;&#23384;&#27010;&#29575;&#33539;&#22260;&#26356;&#23567;&#19988;&#26356;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#33021;&#21147;&#25351;&#26631;&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#36716;&#31227;&#33021;&#21147;&#25351;&#26631;&#26080;&#27861;&#21487;&#38752;&#22320;&#20272;&#35745;&#30446;&#26631;&#22312;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07444</link><description>&lt;p&gt;
&#36716;&#31227;&#33021;&#21147;&#25351;&#26631;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#36716;&#21270;&#20026;&#21307;&#23398;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The Performance of Transferability Metrics does not Translate to Medical Tasks. (arXiv:2308.07444v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#33021;&#21147;&#25351;&#26631;&#22312;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#36716;&#31227;&#33021;&#21147;&#25351;&#26631;&#26080;&#27861;&#21487;&#38752;&#22320;&#20272;&#35745;&#30446;&#26631;&#22312;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#65292;&#36716;&#31227;&#23398;&#20064;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#32791;&#23613;&#25152;&#26377;&#20505;&#36873;&#32773;&#30340;&#23581;&#35797;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#20419;&#20351;&#20154;&#20204;&#23547;&#25214;&#26356;&#20415;&#23452;&#30340;&#36873;&#25321;&#26041;&#27861;&#12290;&#36716;&#31227;&#24615;&#35780;&#20998;&#26041;&#27861;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#20986;&#19982;&#20219;&#20309;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#26550;&#26500;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36716;&#31227;&#24615;&#35780;&#20998;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#65292;&#23427;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#20351;&#29992;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#27861;&#20351;&#20174;&#19994;&#20154;&#21592;&#21463;&#30410;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#23545;&#19977;&#20010;&#21307;&#23398;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#19971;&#20010;&#36716;&#31227;&#33021;&#21147;&#35780;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#36328;&#20998;&#24067;&#30340;&#24773;&#26223;&#12290;&#23613;&#31649;&#22312;&#36890;&#29992;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#19968;&#20010;&#36716;&#31227;&#33021;&#21147;&#35780;&#20998;&#33021;&#22815;&#21487;&#38752;&#19988;&#19968;&#33268;&#22320;&#20272;&#35745;&#30446;&#26631;&#22312;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.07441</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#38477;&#20302;&#32852;&#21512;&#39044;&#27979;&#27694;&#27687;&#21270;&#29289;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides. (arXiv:2308.07441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#20013;&#30340;&#27694;&#27687;&#21270;&#29289;&#65288;NOx&#65289;&#20027;&#35201;&#26469;&#33258;&#29123;&#26009;&#29123;&#28903;&#65292;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#26377;&#26126;&#26174;&#30340;&#24613;&#24615;&#21644;&#24930;&#24615;&#24433;&#21709;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#22312;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#19979;&#39044;&#27979;&#22320;&#38754;&#19978;NOx&#27987;&#24230;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#22823;&#27668;&#27745;&#26579;&#21160;&#21147;&#23398;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#30693;&#35782;&#65292;&#21487;&#33021;&#23384;&#22312;&#39640;&#20272;&#20559;&#24046;&#12290;&#21270;&#23398;&#20256;&#36755;&#27169;&#22411;&#65288;CTMs&#65289;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65307;&#28982;&#32780;&#65292;&#20934;&#30830;&#39044;&#27979;&#22320;&#38754;&#27987;&#24230;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#21518;&#26657;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#20808;&#39564;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#30721;&#24179;&#27969;-&#25193;&#25955;&#26426;&#21046;&#21644;&#27969;&#20307;&#21160;&#21147;&#23398;&#32422;&#26463;&#26469;&#32852;&#21512;&#39044;&#27979;NO2&#21644;NOx&#65292;&#24182;&#23558;ML&#27169;&#22411;&#30340;&#20559;&#24046;&#38477;&#20302;&#20102;21-42&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#21040;&#20102;NO2&#21644;NOx&#30340;&#32454;&#31890;&#24230;&#20256;&#36755;&#65292;&#29983;&#25104;&#20102;&#24378;&#22823;&#30340;&#31354;&#38388;&#22806;&#25512;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#23558;CTM&#30340;&#30693;&#35782;&#39537;&#21160;&#30340;&#29289;&#29702;&#21270;&#23398;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07439</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24863;&#30693;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#23545;&#20110;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20010;&#21035;&#39550;&#39542;&#21592;&#30340;&#20010;&#24615;&#21270;&#39550;&#39542;&#27169;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#27969;&#31243;&#65306;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#23450;&#39550;&#39542;&#25968;&#25454;&#38024;&#23545;&#27599;&#20010;&#39550;&#39542;&#21592;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;&#20154;&#26426;&#21327;&#21516;&#20223;&#30495;&#26469;&#25910;&#38598;&#20010;&#24615;&#21270;&#30340;&#33258;&#28982;&#39550;&#39542;&#36712;&#36857;&#21450;&#20854;&#30456;&#24212;&#30340;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22522;&#20110;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07429</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Loss for Neural Source Code Summarization. (arXiv:2308.07429v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22522;&#20110;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#12290;&#20195;&#30721;&#25688;&#35201;&#26159;&#32534;&#20889;&#28304;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#12290;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36825;&#20123;&#25551;&#36848;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#12290;&#20960;&#20046;&#25152;&#26377;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#65292;&#20363;&#22914;GPT&#12289;Codex&#12289;LLaMA&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20351;&#29992;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;CCE&#65289;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32593;&#32476;&#20248;&#21270;&#12290;CCE&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#23427;&#19968;&#27425;&#35745;&#31639;&#27599;&#20010;&#21333;&#35789;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#32780;&#19981;&#26159;&#35780;&#20272;&#25972;&#20010;&#21477;&#23376;&#65307;2&#65289;&#23427;&#35201;&#27714;&#23436;&#32654;&#39044;&#27979;&#65292;&#19981;&#20801;&#35768;&#23545;&#21516;&#20041;&#35789;&#32473;&#20104;&#37096;&#20998;&#20449;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35745;&#31639;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each 
&lt;/p&gt;</description></item><item><title>UniBrain &#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21517;&#20026;Versatile Diffusion&#30340;&#28508;&#21464;&#37327;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;fMRI&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07428</link><description>&lt;p&gt;
UniBrain: &#32479;&#19968;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#20110;&#19968;&#20307;&#30340;&#20154;&#33041;&#27963;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. (arXiv:2308.07428v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07428
&lt;/p&gt;
&lt;p&gt;
UniBrain &#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21517;&#20026;Versatile Diffusion&#30340;&#28508;&#21464;&#37327;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;fMRI&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35270;&#35273;&#21050;&#28608;&#24341;&#36215;&#30340;&#33041;&#27963;&#21160;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;&#20154;&#33041;&#19982;&#35270;&#35273;&#24863;&#30693;&#31995;&#32479;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#20013;&#20351;&#29992;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#26159;&#22312;&#20445;&#25345;&#20302;&#23618;&#32454;&#33410;&#21644;&#39640;&#35821;&#20041;&#20445;&#30495;&#24230;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#36924;&#30495;&#30340;&#26631;&#39064;&#21644;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniBrain&#65306;&#36890;&#36807;&#20154;&#33041;&#27963;&#21160;&#30340;&#32479;&#19968;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#22312;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#36890;&#36807;&#21517;&#20026;Versatile Diffusion&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23558;&#22270;&#20687;&#37325;&#24314;&#21644;&#26631;&#39064;&#29983;&#25104;&#20174;&#35270;&#35273;&#35825;&#23548;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;fMRI&#20307;&#32032;&#36716;&#25442;&#20026;&#25991;&#26412;&#21644;&#22270;&#20687;&#28508;&#21464;&#37327;&#65292;&#29992;&#20110;&#20302;&#23618;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;fMRI&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#26465;&#20214;&#20174;CLIP&#23548;&#24341;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#29983;&#25104;&#36924;&#30495;&#30340;&#26631;&#39064;&#21644;&#22270;&#20687;&#12290;UniBrain&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#20114;&#21160;&#27169;&#25311;&#65292;&#21253;&#25324;&#20004;&#20010;Agent&#30340;&#35848;&#21028;&#21644;&#20845;&#20010;Agent&#30340;&#35851;&#26432;&#20043;&#35868;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2308.07411</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;Agent&#30340;&#24314;&#27169;&#30340;&#20132;&#21449;&#28857;
&lt;/p&gt;
&lt;p&gt;
Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering. (arXiv:2308.07411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07411
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#20114;&#21160;&#27169;&#25311;&#65292;&#21253;&#25324;&#20004;&#20010;Agent&#30340;&#35848;&#21028;&#21644;&#20845;&#20010;Agent&#30340;&#35851;&#26432;&#20043;&#35868;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#31038;&#20250;&#31995;&#32479;&#30340;&#20934;&#30830;&#34920;&#31034;&#26159;&#19968;&#20010;&#26368;&#32456;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#22522;&#20110;Agent&#30340;&#24314;&#27169;&#65288;ABM&#65289;&#26088;&#22312;&#30740;&#31350;&#19968;&#20010;&#26356;&#22823;&#31995;&#32479;&#20013;&#30340;Agent&#30340;&#34892;&#20026;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#39537;&#21160;&#34892;&#20026;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#25104;&#20026;&#28508;&#22312;&#30340;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#25506;&#32034;&#20154;&#31867;&#39537;&#21160;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21463;Park&#31561;&#20154;&#65288;2023&#24180;&#65289;&#21551;&#21457;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#35843;&#26597;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20154;&#31867;&#20114;&#21160;&#27169;&#25311;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#21487;&#20449;&#20195;&#29702;&#30340;&#20223;&#30495;&#65306;&#19968;&#20010;&#26159;&#20004;&#20010;Agent&#30340;&#35848;&#21028;&#65292;&#21478;&#19968;&#20010;&#26159;&#20845;&#20010;Agent&#30340;&#35851;&#26432;&#20043;&#35868;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.
&lt;/p&gt;</description></item><item><title>PARIS&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#33410;&#29289;&#20307;&#30340;&#37096;&#20998;&#32423;&#21035;&#37325;&#24314;&#19982;&#36816;&#21160;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#26469;&#23398;&#20064;&#38544;&#24335;&#24418;&#29366;&#21644;&#22806;&#35266;&#27169;&#22411;&#65292;&#24182;&#22312;&#27809;&#26377;3D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#36816;&#21160;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;PARIS&#26041;&#27861;&#22312;&#37325;&#24314;&#21644;&#36816;&#21160;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#23545;&#35937;&#30340;&#37325;&#24314;&#36317;&#31163;&#20943;&#23569;&#20102;45.2%&#65292;&#23545;&#20110;&#37096;&#20998;&#30340;&#37325;&#24314;&#36317;&#31163;&#20943;&#23569;&#20102;84.5%&#12290;</title><link>http://arxiv.org/abs/2308.07391</link><description>&lt;p&gt;
PARIS: &#38754;&#21521;&#20851;&#33410;&#29289;&#20307;&#30340;&#37096;&#20998;&#32423;&#21035;&#37325;&#24314;&#19982;&#36816;&#21160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects. (arXiv:2308.07391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07391
&lt;/p&gt;
&lt;p&gt;
PARIS&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#33410;&#29289;&#20307;&#30340;&#37096;&#20998;&#32423;&#21035;&#37325;&#24314;&#19982;&#36816;&#21160;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#26469;&#23398;&#20064;&#38544;&#24335;&#24418;&#29366;&#21644;&#22806;&#35266;&#27169;&#22411;&#65292;&#24182;&#22312;&#27809;&#26377;3D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#36816;&#21160;&#21442;&#25968;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;PARIS&#26041;&#27861;&#22312;&#37325;&#24314;&#21644;&#36816;&#21160;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#23545;&#35937;&#30340;&#37325;&#24314;&#36317;&#31163;&#20943;&#23569;&#20102;45.2%&#65292;&#23545;&#20110;&#37096;&#20998;&#30340;&#37325;&#24314;&#36317;&#31163;&#20943;&#23569;&#20102;84.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20851;&#33410;&#29289;&#20307;&#30340;&#37096;&#20998;&#32423;&#21035;&#37325;&#24314;&#21644;&#36816;&#21160;&#21442;&#25968;&#20272;&#35745;&#30340;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#29289;&#20307;&#22312;&#20004;&#20010;&#38745;&#24577;&#20851;&#33410;&#29366;&#24577;&#19979;&#30340;&#20004;&#32452;&#22810;&#35270;&#35282;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#21487;&#31227;&#21160;&#37096;&#20998;&#19982;&#38745;&#24577;&#37096;&#20998;&#20998;&#31163;&#20986;&#26469;&#65292;&#22312;&#39044;&#27979;&#36816;&#21160;&#21442;&#25968;&#30340;&#21516;&#26102;&#37325;&#24314;&#24418;&#29366;&#21644;&#22806;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;PARIS&#65292;&#35813;&#26550;&#26500;&#23398;&#20064;&#20102;&#37096;&#20998;&#32423;&#21035;&#30340;&#38544;&#24335;&#24418;&#29366;&#21644;&#22806;&#35266;&#27169;&#22411;&#65292;&#24182;&#22312;&#27809;&#26377;&#20219;&#20309;3D&#30417;&#30563;&#12289;&#36816;&#21160;&#25110;&#35821;&#20041;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#20248;&#21270;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29289;&#20307;&#31867;&#21035;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;3D&#28857;&#20113;&#20316;&#20026;&#36755;&#20837;&#30340;&#22522;&#20934;&#32447;&#21644;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#25913;&#36827;&#20102;&#37325;&#24314;&#25928;&#26524;&#65292;&#23545;&#20110;&#23545;&#35937;&#30340;Chamfer-L1&#36317;&#31163;&#20943;&#23569;&#20102;3.94&#65288;45.2%&#65289;&#65292;&#23545;&#20110;&#37096;&#20998;&#30340;&#36317;&#31163;&#20943;&#23569;&#20102;26.79&#65288;84.5%&#65289;&#65292;&#24182;&#22312;10&#20010;&#23545;&#35937;&#31867;&#21035;&#20013;&#23454;&#29616;&#20102;5%&#30340;&#36816;&#21160;&#20272;&#35745;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of simultaneous part-level reconstruction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appearance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annotation. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input. Our approach improves reconstruction relative to state-of-the-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 object categories.  Video summar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#25351;&#23548;&#33258;&#21160;&#21270;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.07358</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#19987;&#23478;&#24341;&#23548;&#30340;&#32593;&#26684;&#21010;&#20998;&#30340;&#30830;&#35748;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks. (arXiv:2308.07358v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#25351;&#23548;&#33258;&#21160;&#21270;&#29983;&#25104;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#22312;&#19981;&#21516;&#30340;&#24037;&#31243;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20934;&#30830;&#30340;&#27169;&#25311;&#20381;&#36182;&#20110;&#20223;&#30495;&#22495;&#30340;&#36866;&#24403;&#32593;&#26684;&#21010;&#20998;&#12290;&#34429;&#28982;&#39640;&#24230;&#31934;&#32454;&#30340;&#32593;&#26684;&#21487;&#20197;&#30830;&#20445;&#31934;&#24230;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#31867;&#20284;&#22320;&#65292;&#33258;&#36866;&#24212;&#37325;&#32593;&#26684;&#25216;&#26415;&#38656;&#35201;&#22810;&#27425;&#20223;&#30495;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36825;&#24847;&#21619;&#30528;&#32593;&#26684;&#21010;&#20998;&#36807;&#31243;&#20381;&#36182;&#20110;&#19987;&#19994;&#30693;&#35782;&#21644;&#22810;&#24180;&#30340;&#32463;&#39564;&#12290;&#33258;&#21160;&#21270;&#32593;&#26684;&#29983;&#25104;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#24182;&#24102;&#26469;&#26356;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#19987;&#23478;&#25351;&#23548;&#26469;&#33258;&#21160;&#29983;&#25104;&#39134;&#26426;&#27169;&#22411;&#30340;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#32593;&#26684;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20004;&#20010;&#20808;&#36827;&#27169;&#22411;&#65288;PointNet++&#21644;PointMLP&#65289;&#30340;&#26032;&#30340;&#19977;&#32500;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#38754;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#19977;&#32500;&#32593;&#26684;&#20998;&#21106;&#27169;&#22411;&#20013;&#25237;&#23556;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation model
&lt;/p&gt;</description></item><item><title>CORNET&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#31034;&#20363;&#33258;&#21160;&#23398;&#20064;&#30005;&#23376;&#34920;&#26684;&#26465;&#20214;&#26684;&#24335;&#21270;&#35268;&#21017;&#30340;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#24402;&#32435;&#31243;&#24207;&#21512;&#25104;&#21644;&#31526;&#21495;&#35268;&#21017;&#26522;&#20030;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#22120;&#29983;&#25104;&#20934;&#30830;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2308.07357</link><description>&lt;p&gt;
CORNET&#28436;&#31034;: &#19968;&#31181;&#36890;&#36807;&#31034;&#20363;&#23398;&#20064;&#30005;&#23376;&#34920;&#26684;&#26684;&#24335;&#35268;&#21017;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Demonstration of CORNET: A System For Learning Spreadsheet Formatting Rules By Example. (arXiv:2308.07357v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07357
&lt;/p&gt;
&lt;p&gt;
CORNET&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#31034;&#20363;&#33258;&#21160;&#23398;&#20064;&#30005;&#23376;&#34920;&#26684;&#26465;&#20214;&#26684;&#24335;&#21270;&#35268;&#21017;&#30340;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#24402;&#32435;&#31243;&#24207;&#21512;&#25104;&#21644;&#31526;&#21495;&#35268;&#21017;&#26522;&#20030;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#22120;&#29983;&#25104;&#20934;&#30830;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#26469;&#23436;&#25104;&#12290;&#22823;&#22810;&#25968;&#30005;&#23376;&#34920;&#26684;&#24179;&#21488;&#20013;&#19968;&#20010;&#21463;&#27426;&#36814;&#30340;&#21151;&#33021;&#26159;&#23450;&#20041;&#25968;&#25454;&#30456;&#20851;&#30340;&#26684;&#24335;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#21487;&#20197;&#34920;&#36798;&#22914;&#8220;&#23558;&#26576;&#21015;&#20013;&#25152;&#26377;&#36127;&#25968;&#26631;&#35760;&#20026;&#32418;&#33394;&#8221;&#25110;&#8220;&#21152;&#31895;&#25152;&#26377;&#19981;&#21253;&#21547;&#38169;&#35823;&#25110;&#25925;&#38556;&#30340;&#34892;&#8221;&#30340;&#25805;&#20316;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24819;&#35201;&#20351;&#29992;&#36825;&#20010;&#21151;&#33021;&#30340;&#29992;&#25143;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#26465;&#20214;&#26684;&#24335;&#21270;&#35268;&#21017;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;CORNET&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#20174;&#29992;&#25143;&#31034;&#20363;&#20013;&#33258;&#21160;&#23398;&#20064;&#36825;&#20123;&#26465;&#20214;&#26684;&#24335;&#21270;&#35268;&#21017;&#30340;&#31995;&#32479;&#12290;CORNET&#20511;&#37492;&#20102;&#24402;&#32435;&#31243;&#24207;&#21512;&#25104;&#30340;&#24605;&#24819;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#21322;&#30417;&#30563;&#32858;&#31867;&#21644;&#36845;&#20195;&#20915;&#31574;&#26641;&#23398;&#20064;&#30340;&#31526;&#21495;&#35268;&#21017;&#26522;&#20030;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#22120;&#29983;&#25104;&#20934;&#30830;&#30340;&#26465;&#20214;&#26684;&#24335;&#21270;&#35268;&#21017;&#12290;&#22312;&#36825;&#20010;&#28436;&#31034;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CORNET&#20316;&#20026;Microsoft Excel&#30340;&#19968;&#20010;&#31616;&#21333;&#25554;&#20214;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#29992;&#25143;&#25552;&#20379;&#19968;&#20010;&#25110;&#20004;&#20010;&#26684;&#24335;&#21270;&#30340;&#21333;&#20803;&#26684;&#31034;&#20363;&#21518;&#65292;CORNET&#20250;&#29983;&#25104;&#26684;&#24335;&#21270;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data management and analysis tasks are often carried out using spreadsheet software. A popular feature in most spreadsheet platforms is the ability to define data-dependent formatting rules. These rules can express actions such as "color red all entries in a column that are negative" or "bold all rows not containing error or failure." Unfortunately, users who want to exercise this functionality need to manually write these conditional formatting (CF) rules. We introduce CORNET, a system that automatically learns such conditional formatting rules from user examples. CORNET takes inspiration from inductive program synthesis and combines symbolic rule enumeration, based on semi-supervised clustering and iterative decision tree learning, with a neural ranker to produce accurate conditional formatting rules. In this demonstration, we show CORNET in action as a simple add-in to Microsoft Excel. After the user provides one or two formatted cells as examples, CORNET generates formatting rule s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#24320;&#21457;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#39044;&#27979;&#24615;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.07352</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#31243;&#32435;&#31859;&#39063;&#31890;&#22312;&#21463;&#27745;&#26579;&#21547;&#27700;&#23618;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer. (arXiv:2308.07352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#24320;&#21457;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#39044;&#27979;&#24615;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#26377;&#35768;&#22810;&#21463;&#27745;&#26579;&#30340;&#22320;&#19979;&#27700;&#22330;&#22320;&#38656;&#35201;&#31215;&#26497;&#30340;&#20462;&#22797;&#35745;&#21010;&#26469;&#24674;&#22797;&#24403;&#22320;&#29983;&#24577;&#31995;&#32479;&#21644;&#29615;&#22659;&#12290;&#24037;&#31243;&#32435;&#31859;&#39063;&#31890;&#65288;ENPs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22320;&#19979;&#27700;&#20013;&#27745;&#26579;&#29289;&#21407;&#20301;&#38477;&#35299;&#30340;&#26377;&#25928;&#21453;&#24212;&#21058;&#12290;&#34429;&#28982;&#36825;&#20123;ENPs&#22312;&#23454;&#39564;&#23460;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#22320;&#26465;&#20214;&#19979;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;ENPs&#30340;&#22797;&#26434;&#36755;&#36816;&#21644;&#28382;&#30041;&#26426;&#21046;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#20462;&#22797;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#39044;&#27979;&#24615;&#24037;&#20855;&#26469;&#29702;&#35299;ENPs&#30340;&#36755;&#36816;&#21644;&#28382;&#30041;&#34892;&#20026;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20855;&#20027;&#35201;&#26159;&#20197;&#25968;&#20540;&#27169;&#25311;&#22120;&#20026;&#20027;&#65292;&#23545;&#31232;&#30095;&#25968;&#25454;&#38598;&#21644;&#21547;&#27700;&#23618;&#24322;&#36136;&#24615;&#30340;&#23384;&#22312;&#20855;&#26377;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;B-PINN&#65289;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#21547;&#27700;&#23618;&#20013;&#32435;&#31859;&#39063;&#31890;&#30340;&#36816;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;actor-critic&#26694;&#26550;&#20013;&#20351;&#29992;Q&#20989;&#25968;&#26469;&#36873;&#25321;&#28304;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.07351</link><description>&lt;p&gt;
IOB&#65306;&#38598;&#25104;&#20248;&#21270;&#20256;&#36882;&#21644;&#34892;&#20026;&#20256;&#36882;&#29992;&#20110;&#22810;&#31574;&#30053;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse. (arXiv:2308.07351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;actor-critic&#26694;&#26550;&#20013;&#20351;&#29992;Q&#20989;&#25968;&#26469;&#36873;&#25321;&#28304;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#37325;&#22797;&#21033;&#29992;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#26469;&#24555;&#36895;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#20063;&#21487;&#20197;&#36890;&#36807;&#20174;&#28304;&#31574;&#30053;&#21521;&#30456;&#20851;&#30446;&#26631;&#20219;&#21153;&#20256;&#36882;&#30693;&#35782;&#26469;&#20570;&#21040;&#21516;&#26679;&#30340;&#20107;&#24773;&#12290;&#20256;&#36882;RL&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#31574;&#30053;&#20248;&#21270;&#30446;&#26631;&#65288;&#20248;&#21270;&#20256;&#36882;&#65289;&#25110;&#32773;&#24433;&#21709;&#34892;&#20026;&#31574;&#30053;&#65288;&#34892;&#20026;&#20256;&#36882;&#65289;&#26469;&#21033;&#29992;&#28304;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#28304;&#31574;&#30053;&#26469;&#24341;&#23548;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32452;&#20214;&#65292;&#27604;&#22914;&#23618;&#27425;&#31574;&#30053;&#25110;&#32773;&#28304;&#31574;&#30053;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38750;&#24179;&#31283;&#30340;&#31574;&#30053;&#20248;&#21270;&#25110;&#32773;&#22823;&#37327;&#30340;&#37319;&#26679;&#25104;&#26412;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20256;&#36882;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36882;RL&#26041;&#27861;&#65292;&#36873;&#25321;&#28304;&#31574;&#30053;&#32780;&#26080;&#38656;&#35757;&#32451;&#39069;&#22806;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;actor-critic&#26694;&#26550;&#20013;&#30340;Q&#20989;&#25968;&#26469;&#25351;&#23548;&#31574;&#30053;&#36873;&#25321;&#65292;&#36873;&#25321;&#28304;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source poli
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30740;&#31350;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#37327;&#21270;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07350</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural PDE-Solvers using Quantization Aware Training. (arXiv:2308.07350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07350
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#39640;&#25928;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30740;&#31350;&#20102;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#37327;&#21270;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20316;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32463;&#20856;&#25968;&#20540;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#25104;&#20026;&#36825;&#20010;&#26377;&#30528;&#30334;&#24180;&#21382;&#21490;&#30340;&#25968;&#23398;&#39046;&#22495;&#28508;&#22312;&#33539;&#24335;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#21487;&#34892;&#24615;&#26041;&#38754;&#65292;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;PDE&#23450;&#20041;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#26469;&#20943;&#36731;&#36825;&#20010;&#25361;&#25112;&#12290;&#23545;&#20110;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20570;&#24471;&#26356;&#22909;&#65306;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36827;&#34892;&#37327;&#21270;&#21487;&#20197;&#25104;&#21151;&#38477;&#20302;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#26631;&#20934;PDE&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#32593;&#32476;&#26550;&#26500;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#36866;&#29992;&#20110;&#21508;&#31181;&#35774;&#32622;&#21644;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;FLOPs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#23454;&#35777;&#30340;&#26041;&#24335;&#35777;&#26126;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24182;&#34892;&#38598;&#21512;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#27714;&#35299;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#36229;&#36234;&#21333;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.07347</link><description>&lt;p&gt;
&#38024;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#24182;&#34892;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A Parallel Ensemble of Metaheuristic Solvers for the Traveling Salesman Problem. (arXiv:2308.07347v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24182;&#34892;&#38598;&#21512;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20803;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#27714;&#35299;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#36229;&#36234;&#21333;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#26159;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#22810;&#30340;NP-hard&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;TSP&#27714;&#35299;&#22120;&#26159;Lin-Kernighan-Helsgaun&#65288;LKH&#65289;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;Edge Assembly&#20132;&#21449;&#31639;&#27861;&#65288;EAX&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24102;&#26377;&#37325;&#21551;&#26426;&#21046;&#30340;EAX&#22312;&#24191;&#27867;&#30340;TSP&#23454;&#20363;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#30740;&#31350;&#20165;&#38480;&#20110;&#28041;&#21450;2000&#20010;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;2000&#21040;85900&#20010;&#22478;&#24066;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#22240;&#38382;&#39064;&#31867;&#22411;&#32780;&#24322;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27714;&#35299;&#22120;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#38598;&#21512;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#21333;&#29420;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#38598;&#21512;&#24335;&#35774;&#32622;&#26159;&#21033;&#29992;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#38500;&#20102;EAX&#21644;LKH&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;EAX&#21644;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;MGA&#65289;&#30340;&#22810;&#20010;&#29256;&#26412;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;MGA&#21644;EAX&#30340;&#28151;&#21512;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#35299;&#20915;&#19968;&#20123;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#28151;&#21512;&#29256;&#26412;&#30340;&#38598;&#21512;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The travelling salesman problem (TSP) is one of the well-studied NP-hard problems in the literature. The state-of-the art inexact TSP solvers are the Lin-Kernighan-Helsgaun (LKH) heuristic and Edge Assembly crossover (EAX). A recent study suggests that EAX with restart mechanisms perform well on a wide range of TSP instances. However, this study is limited to 2,000 city problems. We study for problems ranging from 2,000 to 85,900. We see that the performance of the solver varies with the type of the problem. However, combining these solvers in an ensemble setup, we are able to outperform the individual solver's performance. We see the ensemble setup as an efficient way to make use of the abundance of compute resources. In addition to EAX and LKH, we use several versions of the hybrid of EAX and Mixing Genetic Algorithm (MGA). A hybrid of MGA and EAX is known to solve some hard problems. We see that the ensemble of the hybrid version outperforms the state-of-the-art solvers on problems 
&lt;/p&gt;</description></item><item><title>Py-Tetrad&#21644;RPy-Tetrad&#26159;&#25552;&#20379;&#20102;&#26032;&#30340;Python&#21644;R&#25509;&#21475;&#30340;&#39033;&#30446;&#65292;&#29992;&#20110;&#23558;Python&#21644;R&#19982;Tetrad&#22240;&#26524;&#25628;&#32034;&#24037;&#20855;&#36827;&#34892;&#25509;&#21475;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07346</link><description>&lt;p&gt;
Py-Tetrad&#21644;RPy-Tetrad&#65306;&#25903;&#25345;Tetrad&#22240;&#26524;&#25628;&#32034;&#30340;&#26032;Python&#30028;&#38754;&#19982;R&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for Tetrad Causal Search. (arXiv:2308.07346v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07346
&lt;/p&gt;
&lt;p&gt;
Py-Tetrad&#21644;RPy-Tetrad&#26159;&#25552;&#20379;&#20102;&#26032;&#30340;Python&#21644;R&#25509;&#21475;&#30340;&#39033;&#30446;&#65292;&#29992;&#20110;&#23558;Python&#21644;R&#19982;Tetrad&#22240;&#26524;&#25628;&#32034;&#24037;&#20855;&#36827;&#34892;&#25509;&#21475;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;Tetrad&#39033;&#30446;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;Python&#21644;R&#25509;&#21475;&#65292;&#29992;&#20110;&#22240;&#26524;&#24314;&#27169;&#12289;&#25628;&#32034;&#21644;&#20272;&#35745;&#65288;&#20351;&#29992;Java&#32534;&#20889;&#65289;&#12290;Tetrad&#39033;&#30446;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25166;&#26681;30&#22810;&#24180;&#65292;&#23427;&#30340;&#19968;&#20123;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#32463;&#20856;&#65292;&#22914;PC&#21644;FCI&#65292;&#32780;&#20854;&#20182;&#31639;&#27861;&#21017;&#26159;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20174;Python&#25110;R&#20013;&#35775;&#38382;&#24213;&#23618;&#30340;Java&#20195;&#30721;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#22815;&#28385;&#36275;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#12289;&#26368;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;JPype Python-Java&#25509;&#21475;&#21644;Reticulate Python-R&#25509;&#21475;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#28155;&#21152;&#19968;&#20123;&#31616;&#21333;&#30340;&#24037;&#20855;&#21644;&#25552;&#20379;Python&#21644;R&#30340;&#24037;&#20316;&#31034;&#20363;&#65292;&#20351;&#29992;JPype&#21644;Reticulate&#23558;Python&#21644;R&#19982;Tetrad&#36827;&#34892;&#25509;&#21475;&#21270;&#21464;&#24471;&#31616;&#21333;&#30452;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give novel Python and R interfaces for the (Java) Tetrad project for causal modeling, search, and estimation. The Tetrad project is a mainstay in the literature, having been under consistent development for over 30 years. Some of its algorithms are now classics, like PC and FCI; others are recent developments. It is increasingly the case, however, that researchers need to access the underlying Java code from Python or R. Existing methods for doing this are inadequate. We provide new, up-to-date methods using the JPype Python-Java interface and the Reticulate Python-R interface, directly solving these issues. With the addition of some simple tools and the provision of working examples for both Python and R, using JPype and Reticulate to interface Python and R with Tetrad is straightforward and intuitive.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#21644;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#21450;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#26469;&#26377;&#25928;&#35299;&#20915;&#35013;&#22635;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07335</link><description>&lt;p&gt;
&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#29992;&#20110;&#35013;&#22635;&#22278;&#24418;
&lt;/p&gt;
&lt;p&gt;
An Encoder-Decoder Approach for Packing Circles. (arXiv:2308.07335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;-&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#30721;&#21644;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#21450;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#26469;&#26377;&#25928;&#35299;&#20915;&#35013;&#22635;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20960;&#21313;&#24180;&#20197;&#26469;&#65292;&#23558;&#36739;&#23567;&#30340;&#29289;&#20307;&#23436;&#20840;&#25918;&#32622;&#22312;&#36739;&#22823;&#30340;&#29289;&#20307;&#20869;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38500;&#20102;&#35201;&#27714;&#36739;&#23567;&#30340;&#29289;&#20307;&#24517;&#39035;&#23436;&#20840;&#20301;&#20110;&#36739;&#22823;&#30340;&#29289;&#20307;&#20869;&#65292;&#36824;&#35201;&#27714;&#23427;&#20204;&#19981;&#37325;&#21472;&#25110;&#23613;&#37327;&#26368;&#23567;&#21270;&#37325;&#21472;&#38754;&#31215;&#12290;&#22240;&#27492;&#65292;&#35013;&#22635;&#38382;&#39064;&#25104;&#20026;&#19968;&#20010;&#38750;&#20984;&#38382;&#39064;&#65292;&#20854;&#26368;&#20248;&#35299;&#30340;&#33719;&#21462;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#33719;&#21462;&#20122;&#26368;&#20248;&#35299;&#65292;&#24182;&#38024;&#23545;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#33719;&#24471;&#20102;&#21487;&#35777;&#26126;&#26368;&#20248;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#22359;&#12289;&#25200;&#21160;&#22359;&#21644;&#35299;&#30721;&#22120;&#22359;&#65292;&#29992;&#20110;&#23558;&#30456;&#21516;&#30340;&#22278;&#24418;&#25918;&#32622;&#22312;&#19968;&#20010;&#36739;&#22823;&#30340;&#22278;&#24418;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#32534;&#30721;&#22120;&#20197;&#35201;&#25918;&#32622;&#30340;&#22278;&#30340;&#32034;&#24341;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#21270;&#23618;&#36755;&#20986;&#20854;&#20013;&#24515;&#65292;&#25200;&#21160;&#23618;&#23545;&#20013;&#24515;&#28155;&#21152;&#21463;&#25511;&#25200;&#21160;&#65292;&#20197;&#30830;&#20445;&#23427;&#19981;&#20250;&#36229;&#20986;&#36793;&#30028;&#12290;&#35299;&#30721;&#22120;&#36890;&#36807;&#21453;&#36716;&#32534;&#30721;&#22120;&#36807;&#31243;&#26469;&#24674;&#22797;&#22278;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#35757;&#32451;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22278;&#24418;&#35013;&#22635;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Notation3&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#37096;&#20998;Notation3&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Notation3&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07332</link><description>&lt;p&gt;
Notation3&#20316;&#20026;&#19968;&#31181;&#23384;&#22312;&#35268;&#21017;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Notation3 as an Existential Rule Language. (arXiv:2308.07332v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Notation3&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#37096;&#20998;Notation3&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Notation3&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Notation3&#36923;&#36753;&#65288;\nthree&#65289;&#26159;RDF&#30340;&#25193;&#23637;&#65292;&#20801;&#35768;&#29992;&#25143;&#32534;&#20889;&#24341;&#20837;&#26032;&#30340;&#31354;&#30333;&#33410;&#28857;&#21040;RDF&#22270;&#20013;&#30340;&#35268;&#21017;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#26412;&#20307;&#26144;&#23556;&#65289;&#20381;&#36182;&#20110;&#27492;&#21151;&#33021;&#65292;&#22240;&#20026;&#31354;&#30333;&#33410;&#28857;&#22312;Web&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#30452;&#25509;&#20351;&#29992;&#25110;&#20316;&#20026;&#36741;&#21161;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#28085;&#30422;&#35813;&#36923;&#36753;&#38750;&#24120;&#37325;&#35201;&#21151;&#33021;&#30340;&#24555;&#36895;\nthree&#25512;&#29702;&#22120;&#30340;&#25968;&#37327;&#30456;&#23545;&#26377;&#38480;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;VLog&#25110;Nemo&#20043;&#31867;&#30340;&#24341;&#25806;&#19981;&#30452;&#25509;&#25903;&#25345;&#35821;&#20041;Web&#35268;&#21017;&#26684;&#24335;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#20026;&#38750;&#24120;&#30456;&#20284;&#30340;&#26500;&#36896;&#65288;&#23384;&#22312;&#35268;&#21017;&#65289;&#24320;&#21457;&#21644;&#20248;&#21270;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31354;&#30333;&#33410;&#28857;&#30340;\nthree&#35268;&#21017;&#19982;&#23384;&#22312;&#35268;&#21017;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#26144;&#23556;&#21040;&#23384;&#22312;&#35268;&#21017;&#30340;\nthree&#23376;&#38598;&#65292;&#24182;&#23450;&#20041;&#20102;&#36825;&#26679;&#19968;&#20010;&#26144;&#23556;&#65292;&#20445;&#25345;&#20102;\nthree&#20844;&#24335;&#30340;&#31561;&#20215;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;\nthree&#25512;&#29702;&#21487;&#20197;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;&#36716;&#25442;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26144;&#23556;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Notation3 Logic (\nthree) is an extension of RDF that allows the user to write rules introducing new blank nodes to RDF graphs. Many applications (e.g., ontology mapping) rely on this feature as blank nodes -- used directly or in auxiliary constructs -- are omnipresent on the Web. However, the number of fast \nthree reasoners covering this very important feature of the logic is rather limited. On the other hand, there are engines like VLog or Nemo which do not directly support Semantic Web rule formats but which are developed and optimized for very similar constructs: existential rules. In this paper, we investigate the relation between \nthree rules with blank nodes in their heads and existential rules. We identify a subset of \nthree which can be mapped directly to existential rules and define such a mapping preserving the equivalence of \nthree formulae. In order to also illustrate that in some cases \nthree reasoning could benefit from our translation, we then employ this mapping i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#29260;&#22534;&#22823;&#23567;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;Blackjack&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#21464;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#30340;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#26377;&#20851;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#35748;&#35782;&#21040;&#29260;&#22534;&#22823;&#23567;&#26159;&#24433;&#21709;Blackjack&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.07329</link><description>&lt;p&gt;
Blackjack&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variations on the Reinforcement Learning performance of Blackjack. (arXiv:2308.07329v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#29260;&#22534;&#22823;&#23567;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;Blackjack&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#21464;&#21270;&#12290;&#30740;&#31350;&#21457;&#29616;&#31639;&#27861;&#30340;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#26377;&#20851;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#35748;&#35782;&#21040;&#29260;&#22534;&#22823;&#23567;&#26159;&#24433;&#21709;Blackjack&#34920;&#29616;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blackjack&#25110;&#31216;&#20026;&#8220;21&#28857;&#8221;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#25169;&#20811;&#29260;&#30340;&#36816;&#27668;&#21644;&#25216;&#24039;&#28216;&#25103;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#36229;&#36807;21&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#27604;&#24196;&#23478;&#26356;&#39640;&#30340;&#25163;&#29260;&#24635;&#25968;&#12290;&#29702;&#24819;&#30340;&#40657;&#26480;&#20811;&#31574;&#30053;&#23558;&#22312;&#38271;&#26399;&#20869;&#26368;&#22823;&#21270;&#36130;&#21153;&#22238;&#25253;&#65292;&#21516;&#26102;&#36991;&#20813;&#36172;&#24466;&#30340;&#30772;&#20135;&#12290;&#30001;&#20110;&#40657;&#26480;&#20811;&#30340;&#38543;&#26426;&#29615;&#22659;&#21644;&#22266;&#26377;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#36825;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#29609;&#27861;&#30340;Q-learning&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30740;&#31350;&#20102;&#31639;&#27861;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#19982;&#29260;&#22534;&#22823;&#23567;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#20801;&#35768;&#20351;&#29992;&#36890;&#29992;&#40657;&#26480;&#20811;&#35268;&#21017;&#30340;&#27169;&#25311;&#22120;&#65292;&#20197;&#23637;&#31034;&#19968;&#20010;&#20351;&#29992;&#22522;&#26412;&#31574;&#30053;&#21644;HI-LO&#31995;&#32479;&#30340;&#29260;&#25968;&#23436;&#32654;&#35745;&#25968;&#22120;&#22914;&#20309;&#20351;&#24196;&#23478;&#30772;&#20135;&#65292;&#20197;&#21450;&#29615;&#22659;&#21464;&#21270;&#23545;&#36825;&#20010;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#29260;&#22534;&#22823;&#23567;&#30340;&#24433;&#21709;&#27010;&#24565;&#24615;&#22320;&#29702;&#35299;&#20026;&#40657;&#26480;&#20811;&#20013;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blackjack or "21" is a popular card-based game of chance and skill. The objective of the game is to win by obtaining a hand total higher than the dealer's without exceeding 21. The ideal blackjack strategy will maximize financial return in the long run while avoiding gambler's ruin. The stochastic environment and inherent reward structure of blackjack presents an appealing problem to better understand reinforcement learning agents in the presence of environment variations. Here we consider a q-learning solution for optimal play and investigate the rate of learning convergence of the algorithm as a function of deck size. A blackjack simulator allowing for universal blackjack rules is also implemented to demonstrate the extent to which a card counter perfectly using the basic strategy and hi-lo system can bring the house to bankruptcy and how environment variations impact this outcome. The novelty of our work is to place this conceptual understanding of the impact of deck size in the con
&lt;/p&gt;</description></item><item><title>PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07327</link><description>&lt;p&gt;
PokerKit: &#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#30340;&#20840;&#38754;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07327
&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#21644;&#25163;&#29260;&#35780;&#20272;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#25903;&#25345;&#23569;&#37327;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#19988;&#22312;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PokerKit&#36890;&#36807;&#25903;&#25345;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#30340;&#26550;&#26500;&#20379;&#29992;&#25143;&#23450;&#20041;&#33258;&#23450;&#20041;&#28216;&#25103;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#36825;&#19968;&#33539;&#22260;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;PokerKit&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#20854;&#30452;&#35266;&#30340;&#32534;&#31243;API&#65292;&#22810;&#21464;&#20307;&#28216;&#25103;&#25903;&#25345;&#20197;&#21450;&#32479;&#19968;&#30340;&#25163;&#29260;&#35780;&#20272;&#22871;&#20214;&#22312;&#19981;&#21516;&#25163;&#29260;&#31867;&#22411;&#38388;&#30340;&#24212;&#29992;&#12290;PokerKit&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#22312;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;PokerKit&#30340;&#21487;&#38752;&#24615;&#36890;&#36807;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#12289;&#24191;&#27867;&#30340;doctest&#21644;&#21333;&#20803;&#27979;&#35797;&#26469;&#30830;&#20445;&#65292;&#36798;&#21040;&#20102;97%&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#12290;&#24341;&#20837;PokerKit&#20195;&#34920;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.07326</link><description>&lt;p&gt;
AI&#25991;&#26412;-&#34892;&#20026;&#65306;&#21487;&#25805;&#25511;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI Text-to-Behavior: A Study In Steerability. (arXiv:2308.07326v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25805;&#25511;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#33021;&#22815;&#26681;&#25454;&#25552;&#31034;&#23637;&#29616;&#29305;&#23450;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21306;&#20998;&#24230;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#21382;&#21490;&#20154;&#29289;&#30340;&#20010;&#24615;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;ChatGPT&#36845;&#20195;&#29256;&#26412;&#30340;&#21487;&#25805;&#25511;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;OCEAN&#65288;&#24320;&#25918;&#24615;&#65292;&#36131;&#20219;&#24515;&#65292;&#22806;&#21521;&#24615;&#65292;&#23452;&#20154;&#24615;&#65292;&#31070;&#32463;&#36136;&#65289;&#30340;&#34892;&#20026;&#24515;&#29702;&#23398;&#26694;&#26550;&#65292;&#25105;&#20204;&#37327;&#21270;&#35780;&#20272;&#20102;&#27169;&#22411;&#23545;&#23450;&#21046;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#24403;&#35201;&#27714;&#29983;&#25104;&#31867;&#20284;&#20110;&#22806;&#21521;&#20154;&#26684;&#30340;&#25991;&#26412;&#26102;&#65292;OCEAN&#24471;&#20998;&#23545;&#40784;&#21040;&#20102;&#35813;&#34892;&#20026;&#29305;&#36136;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#8220;&#24320;&#25918;&#24615;&#8221;&#21576;&#29616;&#20102;&#35821;&#35328;&#30340;&#27169;&#31946;&#24615;&#65292;&#32780;&#8220;&#36131;&#20219;&#24515;&#8221;&#21644;&#8220;&#31070;&#32463;&#36136;&#8221;&#22312;OCEAN&#26694;&#26550;&#20013;&#26126;&#30830;&#22320;&#34987;&#21796;&#36215;&#65292;&#32780;&#8220;&#22806;&#21521;&#24615;&#8221;&#21644;&#8220;&#23452;&#20154;&#24615;&#8221;&#21017;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#26126;&#26174;&#37325;&#21472;&#20294;&#21448;&#26377;&#26126;&#26174;&#20998;&#31163;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;GPT&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35782;&#21035;&#20197;&#21450;&#36866;&#24212;&#24494;&#22937;&#25351;&#23548;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21382;&#21490;&#20154;&#29289;&#27169;&#25311;&#31361;&#20986;&#20102;LLM&#20869;&#21270;&#21644;&#25237;&#23556;&#21487;&#25351;&#23548;&#20010;&#24615;&#30340;&#33021;&#21147;&#65292;&#31934;&#30830;&#22797;&#21046;&#20102;&#20182;&#20204;&#30340;&#21746;&#23398;&#21644;&#23545;&#35805;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. How
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#35774;&#22791;&#30340;&#26032;&#26412;&#20307;&#35770;MSLE&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#65292;&#24314;&#31435;&#19968;&#20010;&#19968;&#33268;&#30340;&#35774;&#22791;&#25551;&#36848;&#65292;&#20026;&#31185;&#23398;&#23478;&#20204;&#25552;&#20379;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#31181;&#35774;&#22791;&#31867;&#22411;&#21644;&#35268;&#26684;&#30340;&#32479;&#19968;&#25551;&#36848;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07325</link><description>&lt;p&gt;
MSLE&#65306;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#35774;&#22791;&#30340;&#26412;&#20307;&#35770;&#12290;&#29992;&#20110;&#26448;&#26009;&#34920;&#24449;&#30340;&#22823;&#35268;&#27169;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
MSLE: An ontology for Materials Science Laboratory Equipment. Large-Scale Devices for Materials Characterization. (arXiv:2308.07325v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#35774;&#22791;&#30340;&#26032;&#26412;&#20307;&#35770;MSLE&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#30340;&#26412;&#20307;&#35770;&#65292;&#24314;&#31435;&#19968;&#20010;&#19968;&#33268;&#30340;&#35774;&#22791;&#25551;&#36848;&#65292;&#20026;&#31185;&#23398;&#23478;&#20204;&#25552;&#20379;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#31181;&#35774;&#22791;&#31867;&#22411;&#21644;&#35268;&#26684;&#30340;&#32479;&#19968;&#25551;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#35774;&#22791;&#30340;&#26032;&#26412;&#20307;&#35770;&#65292;&#31216;&#20026;MSLE&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#26448;&#26009;&#31185;&#23398;&#23454;&#39564;&#23460;&#35774;&#22791;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#31185;&#23398;&#23478;&#20204;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#21644;&#22810;&#20010;&#35268;&#26684;&#30340;&#35774;&#22791;&#12290;&#20363;&#22914;&#65292;&#22312;&#21270;&#23398;&#21644;&#29289;&#29702;&#23454;&#39564;&#23460;&#20013;&#26377;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#12290;&#32479;&#19968;&#25551;&#36848;&#30340;&#19968;&#20010;&#20851;&#38190;&#21457;&#23637;&#26159;&#24314;&#31435;&#19968;&#20010;&#35774;&#22791;&#39046;&#22495;&#26412;&#20307;&#35770;&#20316;&#20026;&#22522;&#26412;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#25351;&#23548;&#29992;&#25143;&#36866;&#24403;&#22320;&#20351;&#29992;&#35774;&#22791;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#24320;&#21457;&#19968;&#31181;&#19968;&#33268;&#30340;&#35774;&#22791;&#26412;&#20307;&#35770;&#65292;&#21363;MSLE&#26412;&#20307;&#35770;&#12290;&#22312;MSLE&#20013;&#65292;&#23558;&#20004;&#20010;&#20027;&#35201;&#30340;&#29616;&#26377;&#26412;&#20307;&#35770;&#65292;&#35821;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;SSN&#65289;&#21644;&#26448;&#26009;&#35789;&#27719;&#65288;MatVoc&#65289;&#65292;&#38598;&#25104;&#21040;MSLE&#26680;&#24515;&#20013;&#20197;&#24314;&#31435;&#19968;&#20010;&#19968;&#33268;&#30340;&#26412;&#20307;&#35770;&#12290;&#30001;&#20110;&#35774;&#22791;&#20351;&#29992;&#20102;&#21508;&#31181;&#32553;&#20889;&#35789;&#21644;&#26415;&#35821;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31616;&#21333;&#30693;&#35782;&#32452;&#32455;&#31995;&#32479;&#65288;SKOS&#65289;&#26469;&#34920;&#31034;&#23558;&#36825;&#20123;&#34920;&#31034;&#22788;&#29702;&#25104;&#26412;&#20307;&#35770;&#26415;&#35821;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new ontology for Materials Science Laboratory Equipment, termed MSLE. A fundamental issue with materials science laboratory (hereafter lab) equipment in the real world is that scientists work with various types of equipment with multiple specifications. For example, there are many electron microscopes with different parameters in chemical and physical labs. A critical development to unify the description is to build an equipment domain ontology as basic semantic knowledge and to guide the user to work with the equipment appropriately. Here, we propose to develop a consistent ontology for equipment, the MSLE ontology. In the MSLE, two main existing ontologies, the Semantic Sensor Network (SSN) and the Material Vocabulary (MatVoc), have been integrated into the MSLE core to build a coherent ontology. Since various acronyms and terms have been used for equipment, this paper proposes an approach to use a Simple Knowledge Organization System (SKOS) to represent the h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#20998;&#26512;&#25216;&#26415;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#21253;&#25324;&#20248;&#21270;&#27169;&#22411;&#21644;&#22810;&#30446;&#26631;&#20915;&#31574;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#21307;&#38498;&#23481;&#37327;&#36827;&#34892;&#20449;&#24687;&#37327;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.07323</link><description>&lt;p&gt;
&#25903;&#25345;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#20998;&#26512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Analytical Techniques to Support Hospital Case Mix Planning. (arXiv:2308.07323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#20998;&#26512;&#25216;&#26415;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#21253;&#25324;&#20248;&#21270;&#27169;&#22411;&#21644;&#22810;&#30446;&#26631;&#20915;&#31574;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#21307;&#38498;&#23481;&#37327;&#36827;&#34892;&#20449;&#24687;&#37327;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#23481;&#37327;&#35780;&#20272;&#21644;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#20998;&#26512;&#25216;&#26415;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#23545;&#24050;&#26377;&#30149;&#20363;&#32452;&#21512;&#36827;&#34892;&#20462;&#25913;&#30340;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#30830;&#23450;&#20102;&#22312;&#21307;&#38498;&#36164;&#28304;&#21487;&#29992;&#24615;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20854;&#20182;&#24739;&#32773;&#31867;&#22411;&#24212;&#25353;&#27604;&#20363;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#20915;&#31574;&#25216;&#26415;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#35780;&#20215;&#33719;&#24471;&#30340;&#31454;&#20105;&#24615;&#30149;&#20363;&#32452;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#26080;&#32541;&#23884;&#20837;&#22312;Excel&#30340;Visual Basic for Applications&#65288;VBA&#65289;&#20010;&#20154;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65288;PDST&#65289;&#20013;&#65292;&#29992;&#20110;&#23545;&#21307;&#38498;&#23481;&#37327;&#36827;&#34892;&#20449;&#24687;&#37327;&#21270;&#35780;&#20272;&#12290;PDST&#25253;&#21578;&#20102;&#19981;&#21516;&#25351;&#26631;&#30340;&#24046;&#24322;&#20449;&#24687;&#65292;&#24182;&#25253;&#21578;&#20102;&#30149;&#20363;&#32452;&#21512;&#20462;&#25913;&#23545;&#20854;&#20182;&#24739;&#32773;&#31867;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24320;&#21457;&#30340;&#25216;&#26415;&#20026;&#24403;&#21069;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces analytical techniques and a decision support tool to support capacity assessment and case mix planning (CMP) approaches previously created for hospitals. First, an optimization model is proposed to analyse the impact of making a change to an existing case mix. This model identifies how other patient types should be altered proportionately to the changing levels of hospital resource availability. Then we propose multi-objective decision-making techniques to compare and critique competing case mix solutions obtained. The proposed techniques are embedded seamlessly within an Excel Visual Basic for Applications (VBA) personal decision support tool (PDST), for performing informative quantitative assessments of hospital capacity. The PDST reports informative metrics of difference and reports the impact of case mix modifications on the other types of patient present. The techniques developed in this article provide a bridge between theory and practice that is currently
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20934;&#21017;&#20248;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#21307;&#38498;&#30340;&#30149;&#20363;&#32452;&#25104;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#29983;&#25104;&#38750;&#25903;&#37197;&#30149;&#20363;&#32452;&#25104;&#23384;&#26723;&#65292;&#20197;&#25552;&#39640;&#23481;&#37327;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07322</link><description>&lt;p&gt;
&#21307;&#38498;&#30149;&#20363;&#32452;&#25104;&#30340;&#22810;&#20934;&#21017;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Multicriteria Optimization Techniques for Understanding the Case Mix Landscape of a Hospital. (arXiv:2308.07322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20934;&#21017;&#20248;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#21307;&#38498;&#30340;&#30149;&#20363;&#32452;&#25104;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#29983;&#25104;&#38750;&#25903;&#37197;&#30149;&#20363;&#32452;&#25104;&#23384;&#26723;&#65292;&#20197;&#25552;&#39640;&#23481;&#37327;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20856;&#22411;&#21307;&#38498;&#20869;&#26377;&#21508;&#31181;&#21307;&#30103;&#21644;&#25163;&#26415;&#21333;&#20301;&#65292;&#20026;&#20102;&#27835;&#30103;&#24739;&#32773;&#65292;&#36825;&#20123;&#21333;&#20301;&#31454;&#20105;&#25163;&#26415;&#23460;&#21644;&#30149;&#25151;&#36164;&#28304;&#12290;&#22914;&#20309;&#31649;&#25511;&#36825;&#31181;&#31454;&#20105;&#23545;&#21307;&#38498;&#30340;&#33021;&#21147;&#21644;&#20135;&#20986;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#21307;&#38498;&#20013;&#27835;&#30103;&#19981;&#21516;&#30149;&#20363;&#32452;&#25104;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#27599;&#31181;&#30149;&#20363;&#32452;&#25104;&#37117;&#26377;&#32463;&#27982;&#21518;&#26524;&#21644;&#29420;&#29305;&#30340;&#21307;&#38498;&#36164;&#28304;&#20351;&#29992;&#29305;&#24449;&#65292;&#22240;&#27492;&#36825;&#19968;&#32771;&#34385;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#30149;&#20363;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#20174;&#23481;&#37327;&#21033;&#29992;&#35282;&#24230;&#30830;&#23450;&#26368;&#20248;&#30340;&#30149;&#20363;&#32452;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#20934;&#21017;&#20248;&#21270;&#26041;&#27861;&#12290;&#22312;&#20856;&#22411;&#21307;&#38498;&#20013;&#26377;&#24456;&#22810;&#30149;&#24739;&#31867;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38750;&#25903;&#37197;&#65288;&#21363;&#24085;&#32047;&#25176;&#26368;&#20248;&#65289;&#30149;&#20363;&#32452;&#25104;&#30340;&#23384;&#26723;&#26159;&#19968;&#39033;&#20855;&#26377;&#35745;&#31639;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#29983;&#25104;&#26356;&#22909;&#30340;&#23384;&#26723;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24182;&#34892;&#21270;&#949;&#32422;&#26463;&#26041;&#27861; (ECM)&#12290;&#25105;&#20204;&#30340;&#24182;&#34892;&#38543;&#26426;&#20462;&#27491;&#26041;&#27861;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various medical and surgical units operate in a typical hospital and to treat their patients these units compete for infrastructure like operating rooms (OR) and ward beds. How that competition is regulated affects the capacity and output of a hospital. This article considers the impact of treating different patient case mix (PCM) in a hospital. As each case mix has an economic consequence and a unique profile of hospital resource usage, this consideration is important. To better understand the case mix landscape and to identify those which are optimal from a capacity utilisation perspective, an improved multicriteria optimization (MCO) approach is proposed. As there are many patient types in a typical hospital, the task of generating an archive of non-dominated (i.e., Pareto optimal) case mix is computationally challenging. To generate a better archive, an improved parallelised epsilon constraint method (ECM) is introduced. Our parallel random corrective approach is significantly fast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#20934;&#21017;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21307;&#38498;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;CMP&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2308.07321</link><description>&lt;p&gt;
&#22810;&#20934;&#21017;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#25928;&#33021;&#24615;&#21035;&#20316;&#29992; (arXiv&#65306;2308.07321v1[cs.AI])
&lt;/p&gt;
&lt;p&gt;
The Efficacy of Utility Functions for Multicriteria Hospital Case-Mix Planning. (arXiv:2308.07321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#20934;&#21017;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21307;&#38498;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;CMP&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#38498;&#30149;&#20363;&#32452;&#21512;&#35268;&#21010;&#65288;CMP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#22810;&#20934;&#21017;&#26041;&#27861;&#21033;&#29992;&#25928;&#29992;&#20989;&#25968;&#65288;UF&#65289;&#26469;&#34920;&#36798;&#29420;&#31435;&#20915;&#31574;&#32773;&#20851;&#20110;&#36755;&#20986;&#30340;&#20559;&#22909;&#21644;&#31435;&#22330;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#27979;&#35797;&#22522;&#20110;&#19978;&#36848;UF&#26631;&#37327;&#21270;&#30340;&#25928;&#29992;&#20989;&#25968;&#26041;&#27861;&#65288;UFM&#65289;&#26159;&#21542;&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#23450;&#37327;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#20197;&#19979;&#30446;&#26631;&#65306;i&#65289;&#23558;&#21307;&#38498;&#36164;&#28304;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#36816;&#33829;&#21333;&#20301;&#65307;ii&#65289;&#25552;&#20379;&#26356;&#22909;&#30340;&#23481;&#37327;&#20998;&#37197;&#21644;&#30149;&#20363;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#23545;&#35780;&#20272;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21307;&#38498;&#30446;&#26631;&#20043;&#38388;&#26435;&#34913;&#30340;&#38656;&#27714;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#27809;&#26377;&#32771;&#34385;&#36807;&#36825;&#31181;&#26041;&#27861;&#12290;&#27491;&#22914;&#25105;&#20204;&#23558;&#22312;&#21518;&#38754;&#23637;&#31034;&#30340;&#65292;&#36825;&#20010;&#24819;&#27861;&#35299;&#20915;&#20102;&#24403;&#21069;CMP&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#38480;&#21046;&#12289;&#24369;&#28857;&#21644;&#32570;&#38519;&#12290;&#25105;&#20204;&#23545;&#19968;&#23478;&#22823;&#22411;&#19977;&#32423;&#21307;&#38498;&#30340;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#20102;&#19978;&#36848;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new approach to perform hospital case-mix planning (CMP) is introduced in this article. Our multi-criteria approach utilises utility functions (UF) to articulate the preferences and standpoint of independent decision makers regarding outputs. The primary aim of this article is to test whether a utility functions method (UFM) based upon the scalarization of aforesaid UF is an appropriate quantitative technique to, i) distribute hospital resources to different operating units, and ii) provide a better capacity allocation and case mix. Our approach is motivated by the need to provide a method able to evaluate the trade-off between different stakeholders and objectives of hospitals. To the best of our knowledge, no such approach has been considered before in the literature. As we will later show, this idea addresses various technical limitations, weaknesses, and flaws in current CMP. The efficacy of the aforesaid approach is tested on a case study of a large tertiary hospital. Currently 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#29256;&#26412;&#30340;Evee&#25554;&#20214;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#32465;&#23450;&#21644;&#21453;&#20363;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#26412;&#20307;&#20013;&#32570;&#22833;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.07294</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#19981;&#21602;&#65311;&#29992;Evee&#35299;&#37322;&#32570;&#22833;&#30340;&#34164;&#28085;&#20851;&#31995;&#65288;&#25216;&#26415;&#25253;&#21578;&#65289;
&lt;/p&gt;
&lt;p&gt;
Why Not? Explaining Missing Entailments with Evee (Technical Report). (arXiv:2308.07294v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#29256;&#26412;&#30340;Evee&#25554;&#20214;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#32465;&#23450;&#21644;&#21453;&#20363;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#26412;&#20307;&#20013;&#32570;&#22833;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26412;&#20307;&#29992;&#25143;&#26469;&#35828;&#65292;&#29702;&#35299;&#25551;&#36848;&#36923;&#36753;&#25512;&#29702;&#22120;&#27966;&#29983;&#20986;&#30340;&#36923;&#36753;&#34164;&#28085;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#30452;&#25509;&#30340;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#24320;&#21457;&#21644;&#23454;&#29616;&#20102;&#21508;&#31181;&#29992;&#20110;&#35299;&#37322;&#34164;&#28085;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#35777;&#26126;&#21644;&#35777;&#25454;&#20316;&#20026;Prot&#233;g&#233;&#26412;&#20307;&#32534;&#36753;&#22120;&#30340;&#25554;&#20214;&#26469;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#39044;&#26399;&#26576;&#31181;&#32570;&#22833;&#30340;&#34164;&#28085;&#20851;&#31995;&#24212;&#35813;&#25104;&#31435;&#26102;&#65292;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#35299;&#37322;&#20026;&#20160;&#20040;&#23427;&#19981;&#20250;&#20174;&#26412;&#20307;&#20013;&#24471;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;EVEE&#30340;&#26032;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#20010;Prot&#233;g&#233;&#25554;&#20214;&#65292;&#29616;&#22312;&#36824;&#36890;&#36807;&#22522;&#20110;&#32465;&#23450;&#21644;&#21453;&#20363;&#30340;&#29616;&#26377;&#21644;&#26032;&#25216;&#26415;&#26469;&#25552;&#20379;&#35299;&#37322;&#32570;&#22833;&#34164;&#28085;&#20851;&#31995;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding logical entailments derived by a description logic reasoner is not always straight-forward for ontology users. For this reason, various methods for explaining entailments using justifications and proofs have been developed and implemented as plug-ins for the ontology editor Prot\'eg\'e. However, when the user expects a missing consequence to hold, it is equally important to explain why it does not follow from the ontology. In this paper, we describe a new version of $\rm E{\scriptsize VEE}$, a Prot\'eg\'e plugin that now also provides explanations for missing consequences, via existing and new techniques based on abduction and counterexamples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07074</link><description>&lt;p&gt;
#InsTag:&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30417;&#30563;&#24494;&#35843;&#30340;&#25351;&#20196;&#26631;&#27880;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models. (arXiv:2308.07074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#29992;&#20110;&#26631;&#35760;&#22522;&#20110;&#35821;&#20041;&#21644;&#24847;&#22270;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25968;&#25454;&#38598;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#21147;&#20250;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#29992;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;TagLM&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#34987;&#35748;&#20026;&#26159;&#25104;&#21151;&#30340;SFT&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#20854;&#23450;&#20041;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#32570;&#20047;&#23450;&#37327;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InsTag&#65292;&#19968;&#31181;&#24320;&#25918;&#30340;&#32454;&#31890;&#24230;&#26631;&#27880;&#22120;&#65292;&#26681;&#25454;&#35821;&#20041;&#21644;&#24847;&#22270;&#23545;SFT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#19988;&#36890;&#36807;&#26631;&#31614;&#26469;&#23450;&#20041;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;6.6K&#20010;&#26631;&#31614;&#26469;&#25551;&#36848;&#32508;&#21512;&#29992;&#25143;&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;SFT&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#33021;&#21147;&#38543;&#30528;&#26356;&#22810;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#30340;&#25968;&#25454;&#32780;&#22686;&#38271;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;InsTag&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#20174;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;6K&#20010;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;InsTag&#36873;&#25321;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TagLM&#27169;&#22411;&#22312;MT-Bench&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;SFT&#25968;&#25454;&#19978;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#39564;&#35777;&#20102;&#26597;&#35810;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and compl
&lt;/p&gt;</description></item><item><title>SAILOR&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#23614;&#33410;&#28857;&#34920;&#31034;&#19978;&#30340;&#24615;&#33021;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06801</link><description>&lt;p&gt;
SAILOR: &#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAILOR: Structural Augmentation Based Tail Node Representation Learning. (arXiv:2308.06801v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06801
&lt;/p&gt;
&lt;p&gt;
SAILOR&#26159;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#38271;&#23614;&#20998;&#24067;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;GNN&#22312;&#23614;&#33410;&#28857;&#34920;&#31034;&#19978;&#30340;&#24615;&#33021;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#26377;&#25928;&#24615;&#20027;&#35201;&#20381;&#36182;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22270;&#33410;&#28857;&#24230;&#25968;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#65292;&#21363;&#22270;&#20013;&#22823;&#37096;&#20998;&#33410;&#28857;&#37117;&#26159;&#21482;&#26377;&#23569;&#25968;&#36830;&#25509;&#36793;&#30340;&#23614;&#33410;&#28857;&#12290;&#30001;&#20110;&#32570;&#20047;&#32467;&#26500;&#20449;&#24687;&#65292;GNN&#23545;&#23614;&#33410;&#28857;&#20135;&#29983;&#36739;&#24046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#21319;GNN&#23545;&#23614;&#33410;&#28857;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#26500;&#20449;&#24687;&#19981;&#36275;&#22914;&#20309;&#24694;&#21270;&#23614;&#33410;&#28857;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAILOR&#30340;&#36890;&#29992;&#32467;&#26500;&#22686;&#24378;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22686;&#24378;&#22270;&#32467;&#26500;&#21644;&#25552;&#21462;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#23614;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06546</link><description>&lt;p&gt;
MC-DRE: &#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#29992;&#20110;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#33647;&#29289;&#30456;&#20851;&#20449;&#24687;&#22359;&#65292;&#22914;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#65292;&#23545;&#20110;&#39044;&#38450;&#30142;&#30149;&#21644;&#25327;&#25937;&#35768;&#22810;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;ADE&#26159;&#36890;&#36807;&#21307;&#30103;&#32972;&#26223;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25253;&#21578;&#30340;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#36890;&#29992;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#25972;&#21512;&#21644;&#23545;&#40784;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#26469;&#26816;&#27979;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#65292;&#21253;&#25324;&#33647;&#29289;&#20107;&#20214;&#35821;&#20041;&#12289;&#21477;&#27861;&#32467;&#26500;&#21644;&#21307;&#23398;&#39046;&#22495;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25429;&#25417;&#21644;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#29992;&#20110;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#22810;&#26041;&#38754;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#21307;&#23398;&#25991;&#26723;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#26041;&#27861;&#21253;&#25324;&#27133;&#26631;&#27880;&#20219;&#21153;&#12289;&#20027;&#35201;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#36890;&#29992;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#36827;&#34892;&#20132;&#21449;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADE are reported via an unstructured conversation with the medical context. Hence, applying a general entity recognition approach is not sufficient enough. The key is how to integrate and align multiple crucial aspects to detect drug event information, including drug event semantics, syntactic structures, and medical domain terminology. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05713</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#27979;&#35797;GPT-4
&lt;/p&gt;
&lt;p&gt;
Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems. (arXiv:2308.05713v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;&#22312;2023&#24180;6&#26376;&#33267;8&#26376;&#26399;&#38388;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#39046;&#22495;&#36827;&#34892;&#30340;105&#20010;&#21407;&#21019;&#38382;&#39064;&#30340;&#27979;&#35797;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20123;&#25554;&#20214;&#26174;&#33879;&#22686;&#24378;&#20102;GPT&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#32463;&#24120;&#20986;&#29616;&#8220;&#25509;&#21475;&#8221;&#25925;&#38556;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;GPT&#32463;&#24120;&#22312;&#38382;&#39064;&#30340;&#34920;&#36848;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#26080;&#27861;&#20174;&#25554;&#20214;&#20013;&#24471;&#21040;&#26377;&#29992;&#30340;&#31572;&#26696;&#12290;&#35299;&#20915;&#36825;&#20123;&#25509;&#21475;&#25925;&#38556;&#20284;&#20046;&#26159;&#20351;GPT&#25104;&#20026;&#21487;&#38752;&#30340;&#22823;&#23398;&#32423;&#35745;&#31639;&#38382;&#39064;&#24037;&#20855;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often "interface" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02804</link><description>&lt;p&gt;
MiAMix: &#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02804
&lt;/p&gt;
&lt;p&gt;
MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#36807;&#25311;&#21512;&#20381;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#21313;&#20998;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#31574;&#30053;&#65292;&#20294;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#22312;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;MiAMix&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#12290;MiAMix&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#28151;&#21512;&#25513;&#27169;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26174;&#33879;&#24615;&#20449;&#24687;&#65292;&#32780;MiAMix&#30340;&#35774;&#35745;&#20063;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23545;MiAMix&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#22270;&#20687;&#22522;&#20934;&#21644;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.13709</link><description>&lt;p&gt;
&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65306;&#22312;&#27809;&#26377;&#20855;&#20307;&#35780;&#20215;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#29289;&#21697;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#23646;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26080;&#32541;&#32467;&#21512;&#65292;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#23646;&#24615;&#65292;&#22914;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#21487;&#21462;&#24615;&#25110;&#24378;&#24230;&#65292;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#24050;&#30693;&#29289;&#21697;&#30340;&#36825;&#20123;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20986;&#29616;&#22312;&#37197;&#23545;&#27604;&#36739;&#25968;&#25454;&#38598;&#20013;&#30340;&#36816;&#21160;&#21592;&#30340;&#23454;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#35780;&#20998;&#65288;DBTR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#19968;&#23450;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#26410;&#30693;&#29289;&#21697;&#30340;&#20219;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#32541;&#22320;&#23558;&#20256;&#32479;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#36825;&#20010;&#26550;&#26500;&#65292;&#29992;&#20110;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#30340;&#38750;&#23545;&#31216;&#29615;&#22659;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#24120;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;DBTR&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#36825;&#20123;&#23646;&#24615;&#30340;&#39044;&#26399;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
&lt;/p&gt;</description></item><item><title>PromptMagician&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25552;&#31034;&#24037;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#21457;&#23637;&#20986;&#39640;&#25928;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#21487;&#35270;&#21270;&#21644;&#20010;&#24615;&#21270;&#25506;&#32034;&#25903;&#25345;&#29992;&#25143;&#22312;&#36755;&#20837;&#25552;&#31034;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#21644;&#36845;&#20195;&#12290;</title><link>http://arxiv.org/abs/2307.09036</link><description>&lt;p&gt;
PromptMagician&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20132;&#20114;&#24335;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation. (arXiv:2307.09036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09036
&lt;/p&gt;
&lt;p&gt;
PromptMagician&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25552;&#31034;&#24037;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#21457;&#23637;&#20986;&#39640;&#25928;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#21487;&#35270;&#21270;&#21644;&#20010;&#24615;&#21270;&#25506;&#32034;&#25903;&#25345;&#29992;&#25143;&#22312;&#36755;&#20837;&#25552;&#31034;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#21644;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22240;&#20854;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#22823;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#20026;&#26399;&#26395;&#30340;&#22270;&#20687;&#24320;&#21457;&#26377;&#25928;&#30340;&#25552;&#31034;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PromptMagician&#65292;&#19968;&#20010;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22270;&#20687;&#32467;&#26524;&#24182;&#32454;&#21270;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#20027;&#24178;&#26159;&#19968;&#20010;&#25552;&#31034;&#25512;&#33616;&#27169;&#22411;&#65292;&#23427;&#20197;&#29992;&#25143;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;DiffusionDB&#20013;&#26816;&#32034;&#31867;&#20284;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#65292;&#24182;&#35782;&#21035;&#20986;&#29305;&#27530;&#30340;&#65288;&#37325;&#35201;&#30340;&#21644;&#30456;&#20851;&#30340;&#65289;&#25552;&#31034;&#20851;&#38190;&#35789;&#12290;&#20026;&#20102;&#20419;&#36827;&#20132;&#20114;&#24335;&#25552;&#31034;&#32454;&#21270;&#65292;PromptMagician&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#32423;&#21487;&#35270;&#21270;&#65292;&#29992;&#20110;&#26816;&#32034;&#30340;&#22270;&#20687;&#21644;&#25512;&#33616;&#30340;&#20851;&#38190;&#35789;&#30340;&#36328;&#27169;&#24577;&#23884;&#20837;&#65292;&#24182;&#25903;&#25345;&#29992;&#25143;&#25351;&#23450;&#22810;&#20010;&#20010;&#24615;&#21270;&#25506;&#32034;&#30340;&#26631;&#20934;&#12290;&#36890;&#36807;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#12289;&#29992;&#25143;&#30740;&#31350;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#35777;&#26126;&#20102;PromptMagician&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.07893</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#25968;&#25454;&#26377;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32420;&#32500;&#23618;&#29255;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#37325;&#26500;&#35823;&#24046;&#20316;&#20026;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#21270;&#32420;&#32500;&#25104;&#22411;(AFP)&#30340;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#26679;&#26412;&#65292;&#32780;&#36825;&#20123;&#26679;&#26412;&#24456;&#38590;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20174;&#22522;&#30784;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#27491;&#24120;&#26679;&#26412;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#32420;&#32500;&#23618;&#29255;&#65288;tow&#65289;&#23545;&#32420;&#32500;&#38138;&#35774;&#34920;&#38754;&#30340;&#28145;&#24230;&#22270;&#36827;&#34892;&#20998;&#21106;&#25104;&#23567;&#31383;&#21475;&#12290;&#20854;&#20013;&#19981;&#21253;&#21547;&#24322;&#24120;&#30340;&#31383;&#21475;&#23376;&#38598;&#20256;&#36882;&#32473;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#36755;&#20837;&#12290;&#22240;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;&#29992;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#36825;&#20123;&#26679;&#26412;&#65292;&#23427;&#20135;&#29983;&#30340;&#37325;&#26500;&#27604;&#23545;&#20110;&#24322;&#24120;&#26679;&#26412;&#26356;&#31934;&#30830;&#12290;&#22240;&#27492;&#65292;&#37325;&#26500;&#35823;&#24046;&#30340;&#20540;&#34987;&#29992;&#20316;&#19968;&#20010;&#37327;&#21270;&#25351;&#26631;&#65292;&#29992;&#20110;&#21028;&#26029;&#26159;&#21542;&#23384;&#22312;&#28508;&#22312;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current defect detection systems for Automated Fibre Placement (AFP) are mostly based on end-to-end supervised learning methods requiring abundant labelled defective samples, which are not easily generated in sufficient numbers. To address this data scarcity problem, we introduce an autoencoder-based approach compatible with small datasets. Fortunately, the problem from a foundational point of view can be simplified as a binary classification between normal and abnormal samples. The proposed approach uses a depth map of the fibre layup surface, split into small windows aligned to each composite strip (tow). A subset of these windows that do not contain anomalies is passed to an autoencoder to reconstruct the input. Because the autoencoder is trained with normal samples, it produces more accurate reconstructions for these samples than for abnormal ones. Therefore, the value of reconstruction error is used as a quantitative metric for whether there are potential anomalies. These values a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14685</link><description>&lt;p&gt;
DiffSketcher: &#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20027;&#35201;&#35757;&#32451;&#20110;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#24341;&#23548;&#32032;&#25551;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21019;&#24314;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#12290;DiffSketcher&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#23637;&#29256;&#26412;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#19968;&#32452;&#36125;&#22622;&#23572;&#26354;&#32447;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#26629;&#26684;&#32423;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#30340;&#30690;&#37327;&#32032;&#25551;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31508;&#30011;&#21021;&#22987;&#21270;&#20197;&#21152;&#24555;&#36895;&#24230;&#12290;&#29983;&#25104;&#30340;&#32032;&#25551;&#23637;&#31034;&#20102;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#34987;&#32472;&#21046;&#20027;&#39064;&#30340;&#21487;&#35782;&#21035;&#24615;&#12289;&#22522;&#26412;&#32467;&#26500;&#21644;&#37325;&#35201;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffSketcher&#30340;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than pr
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.10656</link><description>&lt;p&gt;
&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65306;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20154;&#31867;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics. (arXiv:2306.10656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21307;&#30103;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#21892;&#36523;&#20307;&#21644;&#31934;&#31070;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65288;VHGM&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#30340;&#23646;&#24615;&#12290;VHGM&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#30721;&#24314;&#27169;&#35757;&#32451;&#65292;&#22312;&#24050;&#30693;&#23646;&#24615;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#23646;&#24615;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#21033;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#39640;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#25968;&#20540;&#35780;&#20272;&#20102;VHGM&#21450;&#20854;&#35757;&#32451;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;VHGM&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102;&#29992;&#25143;&#24773;&#22659;&#65292;&#20363;&#22914;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental conditions. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose Virtual Human Generative Model (VHGM), a machine learning model for estimating attributes about healthcare, lifestyles, and personalities. VHGM is a deep generative model trained with masked modeling to learn the joint distribution of attributes conditioned on known ones. Using heterogeneous tabular datasets, VHGM learns more than 1,800 attributes efficiently. We numerically evaluate the performance of VHGM and its training techniques. As a proof-of-concept of VHGM, we present several applications demonstrating user scenarios, such as virtual measurements of healthcare attributes and hypothesis verifications of lifestyles.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs. (arXiv:2306.07699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26088;&#22312;&#27169;&#25311;&#22270;&#20687;&#30340;&#20256;&#36882;&#24615;&#36136;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22270;&#20687;&#32467;&#26500;&#24448;&#24448;&#26159;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;TGSL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PRDC&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#38598;&#32422;&#26463;&#26469;&#27491;&#21017;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PRDC&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25351;&#23548;&#31574;&#30053;&#30340;&#26356;&#26032;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06569</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#31574;&#30053;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Regularization with Dataset Constraint for Offline Reinforcement Learning. (arXiv:2306.06569v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PRDC&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#38598;&#32422;&#26463;&#26469;&#27491;&#21017;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;PRDC&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25351;&#23548;&#31574;&#30053;&#30340;&#26356;&#26032;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#21363;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#31574;&#30053;&#27491;&#21017;&#21270;&#26469;&#32422;&#26463;&#23398;&#20064;&#31574;&#30053;&#30340;&#20998;&#24067;&#25110;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#21644;&#25903;&#25345;&#32422;&#26463;&#36807;&#20110;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#35201;&#27714;&#23398;&#20064;&#31574;&#30053;&#22312;&#29305;&#23450;&#29366;&#24577;&#19979;&#36873;&#25321;&#19982;&#34892;&#20026;&#31574;&#30053;&#30456;&#20284;&#30340;&#21160;&#20316;&#12290;&#36825;&#23558;&#38480;&#21046;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#34892;&#20026;&#31574;&#30053;&#26159;&#27425;&#20248;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23558;&#31574;&#30053;&#27491;&#21017;&#21270;&#25351;&#21521;&#26368;&#36817;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#21487;&#33021;&#26356;&#21152;&#26377;&#25928;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#31574;&#30053;&#27491;&#21017;&#21270;&#65288;PRDC&#65289;&#12290;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#26356;&#26032;&#31574;&#30053;&#26102;&#65292;PRDC&#20250;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#25509;&#36817;&#30340;&#29366;&#24577;-&#21160;&#20316;&#26679;&#26412;&#65292;&#28982;&#21518;&#29992;&#35813;&#26679;&#26412;&#30340;&#21160;&#20316;&#26469;&#32422;&#26463;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;PRDC&#21487;&#20197;&#25351;&#23548;&#31574;&#30053;&#26681;&#25454;&#26368;&#30456;&#20851;&#30340;&#29366;&#24577;-&#21160;&#20316;&#26469;&#36827;&#34892;&#26356;&#26032;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#20063;&#33021;&#22815;&#36798;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01991</link><description>&lt;p&gt;
&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65306;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#19982;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Bio-Inspired Chaos Sensor Model Based on the Perceptron Neural Network: Machine Learning Concept and Application for Computational Neuro-Science. (arXiv:2306.01991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#20063;&#33021;&#22815;&#36798;&#21040;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#29289;&#28789;&#24863;&#28151;&#27788;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31070;&#32463;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#33033;&#20914;&#24207;&#21015;&#30340;&#29109;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#38544;&#34255;&#23618;&#26377;50&#20010;&#31070;&#32463;&#20803;&#65292;&#36755;&#20986;&#23618;&#26377;1&#20010;&#31070;&#32463;&#20803;&#65292;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#36817;&#20284;&#19968;&#20010;&#30701;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#31946;&#29109;&#65292;&#20915;&#23450;&#31995;&#25968;R2&#32422;&#20026;0.9&#12290;&#20351;&#29992;Hindmarsh-Rose&#33033;&#20914;&#27169;&#22411;&#29983;&#25104;&#33033;&#20914;&#38388;&#38548;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#24863;&#30693;&#22120;&#12290;&#20351;&#29992;K-block&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#36873;&#25321;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#20272;&#35745;&#20256;&#24863;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#21363;&#20351;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#35813;&#27169;&#22411;&#20063;&#33021;&#22815;&#20197;&#33391;&#22909;&#30340;&#32467;&#26524;&#36817;&#20284;&#27169;&#31946;&#29109;&#65292;&#24182;&#19988;&#20915;&#23450;&#31995;&#25968;R2&#32422;&#20026;0.5-0.8&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#30340;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#21482;&#26377;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#19988;&#31532;&#19968;&#23618;&#30340;&#26435;&#37325;&#30456;&#31561;&#65292;&#36817;&#20284;&#30340;&#21407;&#29702;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#24179;&#22343;&#20540;&#30340;&#32447;&#24615;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study presents a bio-inspired chaos sensor model based on the perceptron neural network for the estimation of entropy of spike train in neurodynamic systems. After training, the sensor on perceptron, having 50 neurons in the hidden layer and 1 neuron at the output, approximates the fuzzy entropy of a short time series with high accuracy, with a determination coefficient of R2 ~ 0.9. The Hindmarsh-Rose spike model was used to generate time series of spike intervals, and datasets for training and testing the perceptron. The selection of the hyperparameters of the perceptron model and the estimation of the sensor accuracy were performed using the K-block cross-validation method. Even for a hidden layer with one neuron, the model approximates the fuzzy entropy with good results and the metric R2 ~ 0.5-0.8. In a simplified model with one neuron and equal weights in the first layer, the principle of approximation is based on the linear transformation of the average value of the time seri
&lt;/p&gt;</description></item><item><title>GripRank&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#30693;&#35782;&#24212;&#29992;&#20110;&#27573;&#33853;&#25490;&#24207;&#65292;&#22635;&#34917;&#26816;&#32034;&#21644;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18144</link><description>&lt;p&gt;
GripRank: &#36890;&#36807;&#29983;&#25104;&#24335;&#30693;&#35782;&#25913;&#36827;&#30340;&#27573;&#33853;&#25490;&#24207;&#22635;&#34917;&#26816;&#32034;&#21644;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking. (arXiv:2305.18144v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18144
&lt;/p&gt;
&lt;p&gt;
GripRank&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#30693;&#35782;&#24212;&#29992;&#20110;&#27573;&#33853;&#25490;&#24207;&#65292;&#22635;&#34917;&#26816;&#32034;&#21644;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20174;&#22823;&#22411;&#27573;&#33853;&#35821;&#26009;&#24211;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#26469;&#25552;&#20379;&#21512;&#36866;&#30340;&#31572;&#26696;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#29983;&#25104;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22914;&#24320;&#25918;&#22495;&#38382;&#31572;&#21644;&#30693;&#35782;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#32034;&#21644;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#21363;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#20505;&#36873;&#27573;&#33853;&#37117;&#34987;&#31561;&#21516;&#23545;&#24453;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#29983;&#25104;&#21512;&#36866;&#31572;&#26696;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24182;&#19981;&#29702;&#24819;&#29992;&#20110;&#25351;&#23548;&#31572;&#26696;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GeneRative Knowledge Improved Passage Ranking (GripRank) &#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#27573;&#33853;&#20272;&#35745;&#22120;(GPE)&#30340;&#30693;&#35782;&#25552;&#28860;&#21040;&#19968;&#20010;&#27573;&#33853;&#25490;&#24207;&#22120;&#20013;&#65292;&#20854;&#20013;GPE&#26159;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#21512;&#36866;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-enhanced text generation has shown remarkable progress on knowledge-intensive language tasks, such as open-domain question answering and knowledge-enhanced dialogue generation, by leveraging passages retrieved from a large passage corpus for delivering a proper answer given the input query. However, the retrieved passages are not ideal for guiding answer generation because of the discrepancy between retrieval and generation, i.e., the candidate passages are all treated equally during the retrieval procedure without considering their potential to generate a proper answer. This discrepancy makes a passage retriever deliver a sub-optimal collection of candidate passages to generate the answer. In this paper, we propose the GeneRative Knowledge Improved Passage Ranking (GripRank) approach, addressing the above challenge by distilling knowledge from a generative passage estimator (GPE) to a passage ranker, where the GPE is a generative language model used to measure how likely the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#23545;&#21160;&#20316;&#25191;&#34892;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#25429;&#25417;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#30340;&#24314;&#27169;&#24418;&#24335;&#12290;&#20854;&#20182;&#24037;&#20316;&#36824;&#25552;&#20986;&#20102;&#38477;&#20302;&#27169;&#22411;&#35268;&#27169;&#20197;&#25552;&#39640;&#35299;&#20915;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17018</link><description>&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#24418;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Formal Modelling for Multi-Robot Systems Under Uncertainty. (arXiv:2305.17018v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#23545;&#21160;&#20316;&#25191;&#34892;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#26356;&#20934;&#30830;&#25429;&#25417;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#30340;&#24314;&#27169;&#24418;&#24335;&#12290;&#20854;&#20182;&#24037;&#20316;&#36824;&#25552;&#20986;&#20102;&#38477;&#20302;&#27169;&#22411;&#35268;&#27169;&#20197;&#25552;&#39640;&#35299;&#20915;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#22320;&#32508;&#21512;&#21644;&#20998;&#26512;&#22810;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#25105;&#20204;&#38656;&#35201;&#20934;&#30830;&#25429;&#25417;&#22810;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#24418;&#24335;&#21270;&#20219;&#21153;&#32423;&#27169;&#22411;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#24314;&#27169;&#24418;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#35268;&#21010;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#27169;&#22411;&#26816;&#39564;&#21644;&#20223;&#30495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#24418;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#22914;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65289;&#65292;&#20197;&#21450;&#24314;&#27169;&#26426;&#22120;&#20154;&#20132;&#20114;&#23545;&#21160;&#20316;&#25191;&#34892;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#22810;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#20854;&#20182;&#24037;&#20316;&#26041;&#21521;&#25552;&#20986;&#20102;&#38477;&#20302;&#22810;&#26426;&#22120;&#20154;&#27169;&#22411;&#35268;&#27169;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#35299;&#20915;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#22312;&#29420;&#31435;&#20551;&#35774;&#19979;&#35299;&#32806;&#26426;&#22120;&#20154;&#65292;&#25110;&#32773;&#23545;&#26356;&#39640;&#23618;&#27425;&#30340;&#23439;&#21160;&#20316;&#36827;&#34892;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose of Review: To effectively synthesise and analyse multi-robot behaviour, we require formal task-level models which accurately capture multi-robot execution. In this paper, we review modelling formalisms for multi-robot systems under uncertainty, and discuss how they can be used for planning, reinforcement learning, model checking, and simulation.  Recent Findings: Recent work has investigated models which more accurately capture multi-robot execution by considering different forms of uncertainty, such as temporal uncertainty and partial observability, and modelling the effects of robot interactions on action execution. Other strands of work have presented approaches for reducing the size of multi-robot models to admit more efficient solution methods. This can be achieved by decoupling the robots under independence assumptions, or reasoning over higher level macro actions.  Summary: Existing multi-robot models demonstrate a trade off between accurately capturing robot dependencie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11087</link><description>&lt;p&gt;
&#33258;&#21160;&#25512;&#29702;&#39046;&#22495;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Online Learning for Sets of Related Problems in Automated Reasoning. (arXiv:2305.11087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#29702;&#20013;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#24182;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35843;&#25972;&#35299;&#20915;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#35777;&#26126;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#26356;&#22810;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Self-Driven Strategy Learning (sdsl)&#30340;&#36731;&#37327;&#32423;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#33258;&#21160;&#25512;&#29702;&#20013;&#38656;&#35201;&#35299;&#20915;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;sdsl&#20250;&#22312;&#35299;&#20915;&#26089;&#26399;&#38382;&#39064;&#26102;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#26469;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#23427;&#21033;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25968;&#25454;&#26469;&#35843;&#25972;&#21518;&#32493;&#38382;&#39064;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32447;&#25311;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25277;&#35937;&#30340;&#36716;&#25442;&#35268;&#21017;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;sdsl&#35745;&#31639;&#23454;&#20363;&#65292;&#20854;&#20013;&#20351;&#29992;&#26465;&#20214;&#37319;&#26679;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#24213;&#23618;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Kissat&#27714;&#35299;&#22120;&#19978;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;Kissat+sdsl&#22312;&#26368;&#26032;&#30340;&#30828;&#20214;&#27169;&#22411;&#26816;&#26597;&#31454;&#36187;&#20013;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26377;&#38480;&#27169;&#22411;&#26816;&#26597;&#26041;&#27861;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#36793;&#30028;&#21644;&#21457;&#29616;&#20102;&#26356;&#22810;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Self-Driven Strategy Learning (sdsl), a lightweight online learning methodology for automated reasoning tasks that involve solving a set of related problems. sdsl automatically gathers information, in form of a dataset, while solving earlier problems. It utilizes the learned data to adjust the solving strategy for later problems by fitting a machine learning model to the obtained data on the fly. We formally define the approach as a set of abstract transition rules. We describe a concrete instance of the sdsl calculus which uses conditional sampling for generating data and random forests as the underlying machine learning model. We implement the approach on top of the Kissat solver and show that the combination of Kissat+sdsl certifies larger bounds and finds more counter-examples than other state-of-the-art bounded model checking approaches on benchmarks obtained from the latest Hardware Model Checking Competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00136</link><description>&lt;p&gt;
&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Optimizing the AI Development Process by Providing the Best Support Environment. (arXiv:2305.00136v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#12290;ML&#30340;&#20027;&#35201;&#38454;&#27573;&#21253;&#25324;&#38382;&#39064;&#29702;&#35299;&#65292;&#25968;&#25454;&#31649;&#29702;&#65292;&#27169;&#22411;&#26500;&#24314;&#65292;&#27169;&#22411;&#37096;&#32626;&#21644;&#32500;&#25252;&#12290;&#26412;&#39033;&#30446;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#30340;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#21450;&#20854;&#38556;&#30861;&#65292;&#22240;&#20026;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#36755;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21457;&#29616;&#36825;&#19968;&#38454;&#27573;&#26368;&#22823;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#20445;&#23494;&#39046;&#22495;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#26032;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this study is to investigate the development process for Artificial inelegance (AI) and machine learning (ML) applications in order to provide the best support environment. The main stages of ML are problem understanding, data management, model building, model deployment and maintenance. This project focuses on investigating the data management stage of ML development and its obstacles as it is the most important stage of machine learning development because the accuracy of the end model is relying on the kind of data fed into the model. The biggest obstacle found on this stage was the lack of sufficient data for model learning, especially in the fields where data is confidential. This project aimed to build and develop a framework for researchers and developers that can help solve the lack of sufficient data during data management stage. The framework utilizes several data augmentation techniques that can be used to generate new data from the original dataset which can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#20114;&#28857;&#24182;&#22686;&#24378;&#21608;&#22260;&#29305;&#24449;&#65292;&#23454;&#29616;&#20004;&#24103;&#35270;&#39057;&#30340;&#38544;&#24335;&#23545;&#40784;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.10465</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#23545;&#40784;&#30340;&#38544;&#24335;&#26102;&#38388;&#24314;&#27169;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Implicit Temporal Modeling with Learnable Alignment for Video Recognition. (arXiv:2304.10465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20132;&#20114;&#28857;&#24182;&#22686;&#24378;&#21608;&#22260;&#29305;&#24449;&#65292;&#23454;&#29616;&#20004;&#24103;&#35270;&#39057;&#30340;&#38544;&#24335;&#23545;&#40784;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#22312;&#21508;&#31181;&#22270;&#20687;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#25193;&#23637;CLIP&#20197;&#36827;&#34892;&#26102;&#38388;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#23398;&#20064;&#23545;&#40784;(ILA)&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26497;&#39640;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#19968;&#24103;&#23545;&#65292;&#27599;&#24103;&#37117;&#39044;&#27979;&#19968;&#20010;&#20132;&#20114;&#28857;&#65292;&#20316;&#20026;&#24444;&#27492;&#20449;&#24687;&#20016;&#23500;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#22686;&#24378;&#20132;&#20114;&#28857;&#21608;&#22260;&#30340;&#29305;&#24449;&#65292;&#20004;&#24103;&#34987;&#38544;&#24335;&#23545;&#40784;&#12290;&#23545;&#40784;&#30340;&#29305;&#24449;&#28982;&#21518;&#34987;&#27719;&#38598;&#25104;&#19968;&#20010;&#20196;&#29260;&#65292;&#29992;&#20110;&#21518;&#32493;&#30340;&#31354;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks. However, how to extend CLIP with effective temporal modeling is still an open and crucial problem. Existing factorized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes the temporal modeling effort while achieving incredibly high performance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual information rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent sp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;</title><link>http://arxiv.org/abs/2304.06348</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#30340;&#21453;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#65288;&#36890;&#36807;&#26576;&#20123;&#31867;&#22411;&#30340;&#23485;&#24230;&#37327;&#26469;&#34913;&#37327;&#65292;&#21253;&#25324;&#26641;&#23485;&#21644;&#22242;&#23485;&#31561;&#65289;&#65292;&#20026;&#24191;&#27867;&#30340;&#36923;&#36753;&#34164;&#21547;&#38382;&#39064;&#65288;&#31616;&#31216;&#26597;&#35810;&#65289;&#30340;&#21487;&#20915;&#23450;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23637;&#29616;&#20986;&#23485;&#24230;&#26377;&#38480;&#26377;&#38480;&#36890;&#29992;&#27169;&#22411;&#38598;&#30340;&#36923;&#36753;&#65292;&#20445;&#35777;&#20102;&#21508;&#31181;&#21516;&#24577;&#23553;&#38381;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#24378;&#22823;&#30340;&#23485;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Blumensath&#30340;&#20998;&#21106;&#23485;&#24230;&#65292;&#35813;&#37327;&#21253;&#21547;&#20102;&#21508;&#31181;&#36890;&#24120;&#32771;&#34385;&#30340;&#23485;&#24230;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#30340;&#35745;&#31639;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#38024;&#23545;&#26222;&#36941;&#23637;&#29616;&#23384;&#22312;&#24615;&#35268;&#21017;&#20026;&#19968;&#20010;&#23637;&#31034;&#26696;&#20363;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#26377;&#38480;&#20998;&#21106;&#23485;&#24230;&#35268;&#21017;&#38598;&#21253;&#21547;&#20854;&#20182;&#24050;&#30693;&#30340;&#25277;&#35937;&#21487;&#20915;&#23450;&#31867;&#65292;&#20294;&#20511;&#21161;&#29616;&#26377;&#30340;&#20998;&#23618;&#21644;&#21463;&#25511;&#35268;&#21017;&#38598;&#27010;&#24565;&#65292;&#20063;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#20363;&#22914;&#27491;&#21017;&#65292;&#36830;&#25509;&#21644;&#24067;&#23572;&#36830;&#25509;&#26597;&#35810;&#12290;&#25105;&#20204;&#20197;&#23384;&#22312;&#35268;&#21017;&#30340;&#24418;&#24335;&#20026;&#37325;&#28857;&#65292;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#39640;&#32423;&#30693;&#35782;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#33410;&#28857;&#26469;&#24212;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;&#26679;&#26412;&#19981;&#36275;&#21644;&#20559;&#20506;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14061</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#35821;&#20041;&#24863;&#30693;&#33410;&#28857;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Node Synthesis for Imbalanced Heterogeneous Information Networks. (arXiv:2302.14061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14061
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#19981;&#24179;&#34913;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#33410;&#28857;&#26469;&#24212;&#23545;&#23569;&#25968;&#31867;&#21035;&#30340;&#26679;&#26412;&#19981;&#36275;&#21644;&#20559;&#20506;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#22312;&#24314;&#27169;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HINs&#65289;&#20013;&#30340;&#22797;&#26434;&#24322;&#36136;&#24615;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#25928;&#33021;&#12290;HGNNs&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#36890;&#36807;&#25552;&#21462;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;HINs&#20013;&#19981;&#21516;&#30340;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;HINs&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#20026;&#29616;&#26377;&#30340;HGNNs&#21019;&#24314;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#38500;&#20102;&#33410;&#28857;&#25968;&#37327;&#30340;&#19981;&#24179;&#34913;&#22806;&#65292;HINs&#20013;&#26356;&#20851;&#38190;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#26159;&#35821;&#20041;&#19981;&#24179;&#34913;&#12290;HINs&#20013;&#30340;&#23569;&#25968;&#31867;&#21035;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#21270;&#21644;&#36275;&#22815;&#30340;&#37051;&#23621;&#33410;&#28857;&#65292;&#23548;&#33268;&#20559;&#20506;&#21644;&#19981;&#23436;&#25972;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#35821;&#20041;&#19981;&#24179;&#34913;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#20934;&#30830;&#20998;&#31867;&#23569;&#25968;&#33410;&#28857;&#30340;&#22256;&#38590;&#65292;&#23548;&#33268;HGNNs&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#23569;&#25968;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#21644;&#34917;&#20805;&#20854;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (HGNNs) have exhibited exceptional efficacy in modeling the complex heterogeneity in heterogeneous information networks (HINs). The critical advantage of HGNNs is their ability to handle diverse node and edge types in HINs by extracting and utilizing the abundant semantic information for effective representation learning. However, as a widespread phenomenon in many real-world scenarios, the class-imbalance distribution in HINs creates a performance bottleneck for existing HGNNs. Apart from the quantity imbalance of nodes, another more crucial and distinctive challenge in HINs is semantic imbalance. Minority classes in HINs often lack diverse and sufficient neighbor nodes, resulting in biased and incomplete semantic information. This semantic imbalance further compounds the difficulty of accurately classifying minority nodes, leading to the performance degradation of HGNNs. To tackle the imbalance of minority classes and supplement their inadequate se
&lt;/p&gt;</description></item><item><title>SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12449</link><description>&lt;p&gt;
SGL-PT: &#19968;&#31181;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGL-PT: A Strong Graph Learner with Graph Prompt Tuning. (arXiv:2302.12449v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12449
&lt;/p&gt;
&lt;p&gt;
SGL-PT&#26159;&#19968;&#20010;&#20855;&#26377;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#24378;&#22823;&#22270;&#24418;&#23398;&#20064;&#22120;&#65292;&#20197;&#32553;&#23567;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#35774;&#35745;&#22270;&#24418;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21069;&#25991;&#20219;&#21153;&#21644;&#19979;&#28216;&#22270;&#24418;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#24046;&#36317;&#65292;&#36825;&#19981;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#23548;&#33268;&#36127;&#20256;&#36882;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#23545;&#40784;&#65292;&#25552;&#31034;&#35843;&#20248;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#24418;&#25552;&#31034;&#35843;&#20248;&#30340;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#22270;&#39046;&#22495;&#20013;&#21508;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#32570;&#20047;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35774;&#35745;&#19968;&#33268;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#26082;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20063;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGL-PT&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36981;&#24490;&#23398;&#20064;&#31574;&#30053;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#21644;&#39044;&#27979;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specif
&lt;/p&gt;</description></item><item><title>InfiniCity&#26159;&#19968;&#20010;&#26080;&#38480;&#35268;&#27169;&#30340;&#22478;&#24066;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2301.09637</link><description>&lt;p&gt;
InfiniCity: &#26080;&#38480;&#35268;&#27169;&#22478;&#24066;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
InfiniCity: Infinite-Scale City Synthesis. (arXiv:2301.09637v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09637
&lt;/p&gt;
&lt;p&gt;
InfiniCity&#26159;&#19968;&#20010;&#26080;&#38480;&#35268;&#27169;&#30340;&#22478;&#24066;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26080;&#38480;&#35268;&#27169;&#30340;3D&#22478;&#24066;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;InfiniCity&#65292;&#23427;&#21487;&#20197;&#20174;&#38543;&#26426;&#22122;&#38899;&#26500;&#24314;&#21644;&#28210;&#26579;&#19968;&#20010;&#26080;&#38480;&#21046;&#22823;&#23567;&#30340;3D&#29615;&#22659;&#12290;InfiniCity&#23558;&#30475;&#20284;&#19981;&#21487;&#34892;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#19977;&#20010;&#21487;&#34892;&#27169;&#22359;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;2D&#21644;3D&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#26080;&#38480;&#20687;&#32032;&#22270;&#20687;&#21512;&#25104;&#27169;&#22359;&#20174;&#40479;&#30640;&#22270;&#29983;&#25104;&#20219;&#24847;&#23610;&#24230;&#30340;2D&#22320;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#22522;&#20110;&#20843;&#21449;&#26641;&#30340;&#20307;&#32032;&#23436;&#25104;&#27169;&#22359;&#23558;&#29983;&#25104;&#30340;2D&#22320;&#22270;&#36716;&#25442;&#20026;3D&#20843;&#21449;&#26641;&#12290;&#26368;&#21518;&#65292;&#19968;&#20010;&#22522;&#20110;&#20307;&#32032;&#30340;&#31070;&#32463;&#28210;&#26579;&#27169;&#22359;&#32473;&#20307;&#32032;&#36148;&#19978;&#32441;&#29702;&#24182;&#28210;&#26579;2D&#22270;&#20687;&#12290;InfiniCity&#33021;&#22815;&#21512;&#25104;&#20219;&#24847;&#23610;&#24230;&#21644;&#21487;&#36941;&#21382;&#30340;3D&#22478;&#24066;&#29615;&#22659;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36827;&#34892;&#28789;&#27963;&#21644;&#20132;&#20114;&#24335;&#32534;&#36753;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octrees. Finally, a voxel-based neural rendering module texturizes the voxels and renders 2D images. InfiniCity can thus synthesize arbitrary-scale and traversable 3D city environments, and allow flexible and interactive editing from users. We quantitatively and qualitatively demonstrate the efficacy of the proposed framework. Project page: https://hubert0527.github.io/infinicity/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2212.13925</link><description>&lt;p&gt;
&#36136;&#37327;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
Quality at the Tail. (arXiv:2212.13925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#23384;&#22312;&#27874;&#21160;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#31995;&#32479;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32771;&#34385;&#21040;&#25512;&#29702;&#36136;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20005;&#33499;&#30340;&#29615;&#22659;&#19979;&#65292;&#35201;&#27714;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#25351;&#26631;&#30340;&#35201;&#27714;&#12290;&#24573;&#35270;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#26041;&#38754;&#37117;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21644;&#19981;&#21487;&#36870;&#30340;&#21518;&#26524;&#65292;&#21253;&#25324;&#20154;&#21592;&#20260;&#20129;&#21644;&#36130;&#20135;&#25439;&#22833;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#30740;&#31350;&#32570;&#20047;&#23545;&#36825;&#20123;&#25351;&#26631;&#30340;&#20840;&#38754;&#32771;&#34385;&#65292;&#36890;&#24120;&#22312;&#29702;&#24819;&#25110;&#23485;&#26494;&#26465;&#20214;&#19979;&#36827;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#35780;&#20272;&#26041;&#27861;&#19981;&#23436;&#25972;&#25110;&#19981;&#30452;&#35266;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#36136;&#37327;&#30340;&#27874;&#21160;&#65292;&#36827;&#19968;&#27493;&#32473;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#65292;&#24341;&#20837;&#20102;&#8220;&#23614;&#37096;&#36136;&#37327;&#8221;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#20998;&#24067;&#23614;&#37096;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and evaluating deep learning models and systems necessitate a meticulous approach to ensure comprehensive assessment. In practical applications, it is paramount to consider both the inference quality and the inference time, particularly within critical contexts, where stringent requirements demand the simultaneous satisfaction of both metrics. Neglecting either aspect can result in severe and irreversible consequences, including loss of human life and property damage. Unfortunately, many studies lack a comprehensive consideration of these metrics, often conducted under ideal or permissive conditions, thereby leading to incomplete or non-intuitive evaluation methodologies.  This study reveals that deep learning inference quality exhibits fluctuations, which further introduces complications and challenges to the benchmarking and evaluation. To better characterize the phenomenon, the concept of "tail quality" is introduced, which indicates the quality at the tail of distribut
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25512;&#26029;&#21151;&#33021;&#65292;&#24182;&#22312;&#39046;&#20808;&#30340;&#35745;&#31639;&#35774;&#26045;&#20013;&#36827;&#34892;&#20102;&#37096;&#32626;&#21644;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.11317</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#23646;&#24615;&#30340;&#31471;&#21040;&#31471;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
End-to-end AI framework for interpretable prediction of molecular and crystal properties. (arXiv:2212.11317v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11317
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;AI&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#20197;&#39044;&#27979;&#20998;&#23376;&#21644;&#26230;&#20307;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#25512;&#26029;&#21151;&#33021;&#65292;&#24182;&#22312;&#39046;&#20808;&#30340;&#35745;&#31639;&#35774;&#26045;&#20013;&#36827;&#34892;&#20102;&#37096;&#32626;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;DeepHyper&#24211;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;AI&#25512;&#26029;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#65292;&#21253;&#25324;CGCNN&#12289;PhysNet&#12289;SchNet&#12289;MPNN&#12289;MPNN-transformer&#21644;TorchMD-NET&#12290;&#25105;&#20204;&#21033;&#29992;QM9&#12289;hMOF&#21644;MD17&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22312;&#29616;&#20195;&#35745;&#31639;&#29615;&#22659;&#20013;&#39044;&#27979;&#29992;&#25143;&#25351;&#23450;&#30340;&#26448;&#26009;&#23646;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#32479;&#19968;&#12289;&#29420;&#31435;&#26694;&#26550;&#22312;&#23567;&#20998;&#23376;&#12289;&#26080;&#26426;&#26230;&#20307;&#21644;&#32435;&#31859;&#22810;&#23380;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;&#24314;&#27169;&#20013;&#30340;&#21487;&#36801;&#31227;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;Argonne&#39046;&#23548;&#35745;&#31639;&#35774;&#26045;&#30340;ThetaGPU&#36229;&#32423;&#35745;&#31639;&#26426;&#21644;&#22269;&#23478;&#36229;&#32423;&#35745;&#31639;&#24212;&#29992;&#20013;&#24515;&#30340;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#21644;&#27979;&#35797;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#21152;&#36895;&#30340;AI&#39537;&#21160;&#21457;&#29616;&#30340;&#29616;&#20195;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an end-to-end computational framework that allows for hyperparameter optimization using the DeepHyper library, accelerated model training, and interpretable AI inference. The framework is based on state-of-the-art AI models including CGCNN, PhysNet, SchNet, MPNN, MPNN-transformer, and TorchMD-NET. We employ these AI models along with the benchmark QM9, hMOF, and MD17 datasets to showcase how the models can predict user-specified material properties within modern computing environments. We demonstrate transferable applications in the modeling of small molecules, inorganic crystals and nanoporous metal organic frameworks with a unified, standalone framework. We have deployed and tested this framework in the ThetaGPU supercomputer at the Argonne Leadership Computing Facility, and in the Delta supercomputer at the National Center for Supercomputing Applications to provide researchers with modern tools to conduct accelerated AI-driven discovery in leadership-class computing env
&lt;/p&gt;</description></item><item><title>FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01197</link><description>&lt;p&gt;
FedALA: &#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01197
&lt;/p&gt;
&lt;p&gt;
FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#36825;&#20250;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated learning with Adaptive Local Aggregation&#65288;FedALA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#25429;&#25417;&#20840;&#23616;&#27169;&#22411;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;FedALA&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#23616;&#37096;&#30446;&#26631;&#33258;&#36866;&#24212;&#32858;&#21512;&#19979;&#36733;&#30340;&#20840;&#23616;&#27169;&#22411;&#21644;&#26412;&#22320;&#27169;&#22411;&#20197;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21021;&#22987;&#21270;&#26412;&#22320;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;FedALA&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20351;&#29992;&#20102;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;FedALA&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#27604;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22810;3.27%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;ALA&#27169;&#22359;&#24212;&#29992;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#22810;24.19%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#24615;&#20462;&#25913;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2206.08242</link><description>&lt;p&gt;
&#38750;&#40065;&#26834;&#24615;&#29305;&#24449;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Catastrophic overfitting can be induced with discriminative non-robust features. (arXiv:2206.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#24615;&#20462;&#25913;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#26500;&#24314;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#24555;&#36895;&#21333;&#27493;&#25915;&#20987;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20856;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#25511;&#21046;&#24615;&#20462;&#25913;&#65292;&#30740;&#31350;&#20102;&#21333;&#27493;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#20013;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#30340;&#20986;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20837;&#30475;&#20284;&#26080;&#23475;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#22312;&#27604;&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#36739;&#23567;&#30340;epsilon&#20540;&#19979;&#24341;&#21457;&#28798;&#38590;&#24615;&#36807;&#24230;&#25311;&#21512;&#12290;&#36825;&#20123;&#29305;&#24449;&#26377;&#21161;&#20110;&#38750;&#40065;&#26834;&#24615;&#20998;&#31867;&#65292;&#20294;&#19981;&#33021;&#21333;&#29420;&#23454;&#29616;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#19968;&#26032;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#30340;&#26426;&#21046;&#36824;&#19981;&#22815;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of thes
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01206</link><description>&lt;p&gt;
&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20174;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#32463;&#20856;&#30340;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#30340;&#20219;&#21153;&#26159;&#20165;&#36890;&#36807;&#19968;&#20123;&#26631;&#35760;&#20026;&#27491;&#26679;&#26412;&#21644;&#65288;&#36890;&#24120;&#65289;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#20197;&#26159;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#65289;&#26469;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;infoNCE&#23545;&#27604;&#25439;&#22833;&#30340;&#23478;&#26063;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;PU&#35774;&#32622;&#65307;&#24182;&#19988;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#20026;&#26410;&#26631;&#35760;&#26679;&#26412;&#26500;&#24314;&#20266;&#26631;&#31614;&#65307;&#36825;&#20123;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#26368;&#32456;&#30340;&#65288;&#27491;&#26679;&#26412; vs. &#36127;&#26679;&#26412;&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#26631;&#20934;PU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;PU&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#31867;&#21035;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
&lt;/p&gt;</description></item></channel></rss>