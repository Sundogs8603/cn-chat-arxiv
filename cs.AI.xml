<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2311.01043</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models for Autonomous Driving. (arXiv:2311.01043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#30340;&#21457;&#23637;&#36235;&#21183;&#65292;&#20174;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36807;&#28193;&#21040;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#24182;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33021;&#21147;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#20316;&#20026;&#25913;&#21464;&#20132;&#36890;&#21644;&#22478;&#24066;&#27969;&#21160;&#24615;&#30340;&#20652;&#21270;&#21058;&#65292;&#27491;&#36235;&#21521;&#20110;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#36716;&#21521;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21463;&#21040;&#32423;&#32852;&#27169;&#22359;&#20013;&#30340;&#32047;&#31215;&#35823;&#24046;&#21644;&#19981;&#28789;&#27963;&#30340;&#39044;&#35774;&#35268;&#21017;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#36807;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26377;&#28508;&#21147;&#36991;&#20813;&#38169;&#35823;&#32047;&#31215;&#65292;&#23613;&#31649;&#30001;&#20110;&#20854;&#40657;&#30418;&#24615;&#36136;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20915;&#31574;&#30340;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#21464;&#24471;&#22797;&#26434;&#12290;&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#29702;&#35299;&#32972;&#26223;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#31561;&#33021;&#21147;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#36171;&#20104;&#33258;&#21160;&#39550;&#39542;&#20197;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#19982;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#32467;&#21512;&#65292;&#21487;&#33021;&#25171;&#24320;&#23545;&#24320;&#25918;&#19990;&#30028;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#38376;&#65292;&#36825;&#26159;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39123;&#39118;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.20174</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GraphTransformers for Geospatial Forecasting. (arXiv:2310.20174v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#36716;&#25442;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#22270;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#39123;&#39118;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#36716;&#25442;&#22120;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#24207;&#21015;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#35266;&#23519;&#22810;&#20010;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24207;&#21015;&#20043;&#38388;&#20250;&#33258;&#21160;&#24418;&#25104;&#19968;&#20010;&#22270;&#32467;&#26500;&#65292;&#32780;&#36825;&#31181;&#32467;&#26500;&#22312;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26174;&#24335;&#21033;&#29992;&#36825;&#20010;&#22270;&#32467;&#26500;&#65292;&#22320;&#29702;&#31354;&#38388;&#36712;&#36857;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#22270;&#36716;&#25442;&#22120;&#26041;&#27861;&#22312;HURDAT&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;&#23545;&#39123;&#39118;&#36712;&#36857;&#36827;&#34892;6&#23567;&#26102;&#22522;&#30784;&#19978;&#30340;&#39044;&#27979;&#19978;&#65292;&#30456;&#27604;&#22522;&#20110;Transformer&#30340;&#22522;&#20934;&#27169;&#22411;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.19806</link><description>&lt;p&gt;
&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#31561;&#20215;&#24615;&#23646;&#24615;&#30340;&#33258;&#21160;&#39564;&#35777;-&#23398;&#22763;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#24037;&#19994;&#24212;&#29992;&#22686;&#21152;&#65292;&#23545;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20851;&#38190;&#24212;&#29992;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#31243;&#24207;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#24076;&#26395;&#26377;&#19968;&#31181;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#20248;&#21270;&#30340;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#12290;&#20174;&#24418;&#24335;&#19978;&#35762;&#65292;&#36825;&#23545;&#24212;&#20110;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20102;&#32763;&#35793;&#24037;&#20855;anthem&#12290;&#23427;&#21487;&#20197;&#19982;&#29992;&#20110;&#32463;&#20856;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#26159;&#21542;&#24378;&#31561;&#20215;&#12290;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;anthem&#20013;&#65292;&#21482;&#33021;&#39564;&#35777;&#20855;&#26377;&#21463;&#38480;&#36755;&#20837;&#35821;&#35328;&#30340;&#27491;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;&#36825;&#26159;anthem&#20013;&#23454;&#29616;&#30340;&#32763;&#35793;&#964;*&#30340;&#32467;&#26524;&#65292;&#23427;&#29983;&#25104;&#20102;here-and-there&#36923;&#36753;&#20013;&#30340;&#20844;&#24335;&#65292;&#35813;&#36923;&#36753;&#21482;&#23545;&#27491;&#31243;&#24207;&#19982;&#32463;&#20856;&#36923;&#36753;&#30456;&#19968;&#33268;&#12290;&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;anthem&#65292;&#20197;&#20415;&#21487;&#20197;&#39564;&#35777;&#26356;&#24191;&#27867;&#30340;&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.19736</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#21452;&#20995;&#21073;&#19968;&#26679;&#65292;LLMs&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#65292;&#20135;&#29983;&#19981;&#36866;&#24403;&#12289;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24555;&#36895;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#21487;&#33021;&#20986;&#29616;&#27809;&#26377;&#36275;&#22815;&#20445;&#38556;&#30340;&#36229;&#26234;&#33021;&#31995;&#32479;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#65292;&#23545;LLMs&#36827;&#34892;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;LLMs&#35780;&#20272;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#35780;&#20272;&#20998;&#20026;&#19977;&#22823;&#31867;&#21035;&#65306;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#65292;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#38500;&#20102;&#20840;&#38754;&#22238;&#39038;&#35780;&#20272;&#26041;&#27861;&#21644;&#25216;&#26415;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16842</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#23884;&#20837;&#24335;FPGA&#20013;LSTM&#21333;&#20803;&#30340;&#21534;&#21520;&#37327;&#29942;&#39048;&#65292;&#22686;&#24378;&#33021;&#25928;
&lt;/p&gt;
&lt;p&gt;
Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#23884;&#20837;&#24335;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#19968;&#32500;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#36807;&#21435;&#65292;&#32463;&#24120;&#20351;&#29992;CNN&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#29305;&#27530;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#27604;&#22914;FPGA&#26469;&#35828;&#24456;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#22411;LSTM&#21333;&#20803;&#20248;&#21270;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#30340;&#31616;&#21333;LSTM&#27169;&#22411;&#22312;FPGA XC7S15&#65288;&#26469;&#33258;Spartan-7&#31995;&#21015;&#65289;&#19978;&#27599;&#31186;&#21487;&#23454;&#29616;17534&#20010;&#25512;&#26029;&#65292;&#20165;&#28040;&#32791;&#27599;&#20010;&#25512;&#26029;3.8&#24494;&#28966;&#32819;&#30340;&#33021;&#37327;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#30340;&#21534;&#21520;&#37327;&#33267;&#23569;&#25552;&#39640;&#20102;5.4&#20493;&#65292;&#33021;&#25928;&#25552;&#39640;&#20102;1.37&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
&lt;/p&gt;</description></item><item><title>ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.13258</link><description>&lt;p&gt;
ManiCast: &#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13258
&lt;/p&gt;
&lt;p&gt;
ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#23494;&#21512;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20381;&#36182;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#22823;&#35268;&#27169;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#25805;&#20316;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20851;&#38190;&#36716;&#25240;&#28857;&#22788;&#31215;&#32047;&#20102;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#19979;&#28216;&#35268;&#21010;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19982;&#20854;&#39044;&#27979;&#26368;&#26377;&#21487;&#33021;&#30340;&#20154;&#20307;&#36816;&#21160;&#65292;&#20135;&#29983;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#23601;&#36275;&#22815;&#20102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ManiCast&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#35268;&#21010;&#22120;&#20197;&#25191;&#34892;&#21327;&#21516;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26426;&#26800;&#33218;&#22312;&#22914;&#21453;&#24212;&#25605;&#25292;&#12289;&#29289;&#20307;&#20132;&#25509;&#21644;&#21327;&#21516;&#25670;&#26700;&#31561;&#22810;&#20010;&#30495;&#23454;&#20219;&#21153;&#20013;&#30340;&#27969;&#30021;&#12289;&#23454;&#26102;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;&#36816;&#21160;&#39044;&#27979;&#21644;&#31471;&#21040;&#31471;&#30340;&#39044;&#27979;-&#35268;&#21010;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.10541</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#19968;&#22823;&#22411;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#21644;&#21442;&#25968;&#35843;&#25972;&#36807;&#31243;&#21464;&#24471;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#23558;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#35757;&#32451;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#33021;&#26377;&#25928;&#26367;&#20195;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#20165;&#20851;&#27880;&#25913;&#36827;&#23398;&#29983;&#25104;&#32489;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#35748;&#35782;&#21040;&#19987;&#23478;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#37325;&#35201;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#21518;&#32493;&#25968;&#25454;&#38598;&#31934;&#28860;&#20013;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#19987;&#23478;&#36712;&#36857;&#26102;&#65292;&#19987;&#23478;&#30340;&#24179;&#28369;&#24615;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#65292;&#20197;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07997</link><description>&lt;p&gt;
Point-NeuS&#65306;&#36890;&#36807;&#20307;&#28210;&#26579;&#36827;&#34892;&#28857;&#23548;&#21521;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering. (arXiv:2310.07997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#37325;&#24314;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#26354;&#38754;&#24050;&#25104;&#20026;&#22810;&#35270;&#35282;&#37325;&#24314;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#31934;&#24230;&#26377;&#38480;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#20127;&#38656;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Point-NeuS&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28857;&#23548;&#21521;&#26426;&#21046;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#37325;&#24314;&#12290;&#28857;&#24314;&#27169;&#26377;&#26426;&#22320;&#23884;&#20837;&#21040;&#20307;&#28210;&#26579;&#20013;&#65292;&#20197;&#22686;&#24378;&#21644;&#35268;&#33539;&#38544;&#24335;&#26354;&#38754;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#28857;&#23548;&#21521;&#21644;&#25239;&#22122;&#33021;&#21147;&#65292;&#24314;&#27169;&#20102;&#28857;&#20113;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25429;&#25417;&#22122;&#22768;&#20998;&#24067;&#24182;&#20272;&#35745;&#28857;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#25509;&#28857;&#21644;&#22270;&#20687;&#30340;&#31070;&#32463;&#25237;&#24433;&#27169;&#22359;&#65292;&#20197;&#21521;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#28155;&#21152;&#20960;&#20309;&#32422;&#26463;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34917;&#20607;&#20307;&#28210;&#26579;&#21644;&#28857;&#24314;&#27169;&#20043;&#38388;&#30340;&#20960;&#20309;&#20559;&#24046;&#65292;&#23545;&#39640;&#20445;&#30495;&#24230;&#28857;&#36827;&#34892;&#20102;&#28388;&#27874;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06422</link><description>&lt;p&gt;
&#29992;&#20110;&#20256;&#25773;&#20449;&#24687;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#25968;&#23383;&#21270;&#31038;&#20250;&#20013;&#65292;&#23459;&#20256;&#20449;&#24687;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#21644;&#30495;&#30456;&#30340;&#20256;&#25773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#24494;&#22937;&#30340;&#25805;&#32437;&#25216;&#26415;&#21644;&#35821;&#22659;&#20381;&#36182;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;SemEval-2020&#20219;&#21153;11&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;14&#31181;&#23459;&#20256;&#25216;&#26415;&#26631;&#31614;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3&#21644;GPT-4&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21508;&#31181;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#31574;&#30053;&#12290;&#36890;&#36807;&#35780;&#20272;$F1$&#20998;&#25968;&#65292;$Precision$&#21644;$Recall$&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;RoBERTa&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, thi
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17338</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#36890;&#36807;&#21435;&#25481;&#33322;&#28857;&#26469;&#25913;&#36827;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#30340;&#22810;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#26412;&#36136;&#32473;&#20934;&#30830;&#24314;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#24517;&#39035;&#26377;&#25928;&#22320;&#20174;&#36807;&#21435;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22534;&#21472;&#27169;&#22411;&#20013;&#30340;&#21333;&#29420;&#32452;&#20214;&#23398;&#20064;&#26102;&#38388;&#36816;&#21160;&#65292;&#20197;&#25429;&#25417;&#26102;&#38388;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Temporal Waypoint Dropping&#65288;TWD&#65289;&#65292;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#25216;&#26415;&#20419;&#36827;&#26174;&#24335;&#30340;&#26102;&#38388;&#23398;&#20064;&#12290;&#36890;&#36807;&#33322;&#28857;&#21435;&#38500;&#23398;&#20064;&#21487;&#20197;&#36843;&#20351;&#27169;&#22411;&#25913;&#21892;&#20854;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#32852;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#36712;&#36857;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#35266;&#27979;&#21040;&#30340;&#36712;&#36857;&#33322;&#28857;&#24207;&#21015;&#26159;&#23436;&#25972;&#30340;&#65292;&#24573;&#30053;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.14306</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#20013;&#38646;-shot&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#36798;&#21040;&#30456;&#24403;&#20110;&#26356;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20219;&#21153;&#21644;&#26410;&#30693;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#21253;&#25324;Alpaca&#12289;Vicuna&#12289;WizardLM&#21644;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#27169;&#22411;&#65288;Flan-T5-XL/XXL&#12289;T0++&#65289;&#65292;&#20197;&#30495;&#23454;&#19990;&#30028;&#30340;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#36981;&#24490;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#25351;&#20196;&#21644;&#20219;&#21153;&#23548;&#21521;&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#30340;&#12290;&#20027;&#35201;&#35752;&#35770;&#30340;&#26159;&#23427;&#20204;&#22312;&#22788;&#29702;&#25351;&#20196;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#38476;&#29983;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13234</link><description>&lt;p&gt;
&#20174;&#33041;&#30005;&#22270;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#29289;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Natural Images from EEG for Object Recognition. (arXiv:2308.13234v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#33041;&#33041;&#22270;&#65288;EEG&#65289;&#20197;&#20854;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#36866;&#24230;&#30340;&#20449;&#22122;&#27604;&#32780;&#38395;&#21517;&#12290;&#26368;&#36817;&#65292;&#33021;&#21542;&#20174;EEG&#20013;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#25104;&#20026;&#28909;&#38376;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22270;&#20687;&#21644;EEG&#32534;&#30721;&#22120;&#20174;&#37197;&#23545;&#30340;&#22270;&#20687;&#21050;&#28608;&#21644;EEG&#21709;&#24212;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#22312;EEG&#32534;&#30721;&#22120;&#20043;&#21069;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;200&#31181;&#38646;&#26679;&#26412;&#20219;&#21153;&#20013;&#65292;top-1&#20934;&#30830;&#24230;&#36798;&#21040;15.6%&#65292;top-5&#20934;&#30830;&#24230;&#36798;&#21040;42.8%&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;EEG&#20449;&#21495;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#39057;&#35889;&#21644;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a brain signal known for its high time resolution and moderate signal-to-noise ratio. Whether natural images can be decoded from EEG has been a hot issue recently. In this paper, we propose a self-supervised framework to learn image representations from EEG signals. Specifically, image and EEG encoders are first used to extract features from paired image stimuli and EEG responses. Then we employ contrastive learning to align these two modalities by constraining their similarity. Additionally, we introduce two plug-in-play modules that capture spatial correlations before the EEG encoder. Our approach achieves state-of-the-art results on the most extensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. More importantly, extensive experiments analyzing the temporal, spatial, spectral, and semantic aspects of EEG signals demonstrate good biological plausibility. These results offer valuable insights 
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#19981;&#20165;&#24212;&#20851;&#27880;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#65292;&#36824;&#24212;&#20851;&#27880;&#26234;&#33021;&#20307;&#33258;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11136</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#21407;&#21017;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is There Any Social Principle for LLM-Based Agents?. (arXiv:2308.11136v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11136
&lt;/p&gt;
&lt;p&gt;
LLM&#22522;&#20110;&#26234;&#33021;&#20307;&#19981;&#20165;&#24212;&#20851;&#27880;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#65292;&#36824;&#24212;&#20851;&#27880;&#26234;&#33021;&#20307;&#33258;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#27880;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#24212;&#35813;&#36229;&#36234;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#23545;&#40784;&#25110;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26356;&#22810;&#20851;&#27880;&#26234;&#33021;&#20307;&#26412;&#36523;&#65292;&#24182;&#25506;&#35752;&#31038;&#20250;&#31185;&#23398;&#22312;&#26234;&#33021;&#20307;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focus on Large Language Model based agents should involve more than "human-centered" alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10819</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#31034;&#27880;&#20837;&#30340;&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#22312;&#38754;&#21521;&#23458;&#25143;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20986;&#33394;&#33021;&#21147;&#20063;&#24341;&#21457;&#20102;&#23545;&#30001;&#31532;&#19977;&#26041;&#25915;&#20987;&#32773;&#27880;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#39118;&#38505;&#25918;&#22823;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLM&#30340;&#21407;&#22987;&#25351;&#20196;&#24182;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;LLM&#20934;&#30830;&#36776;&#21035;&#35201;&#36981;&#24490;&#30340;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#23545;LLM&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#22522;&#20934;&#30340;&#30446;&#26631;&#26159;&#37327;&#21270;LLM&#21463;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#36825;&#20123;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#21644;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs' original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following LLMs against adversarial instructions injected in the prompt. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WavJourney&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38899;&#39057;&#20869;&#23481;&#30340;&#21019;&#20316;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.14335</link><description>&lt;p&gt;
WavJourney&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
WavJourney: Compositional Audio Creation with Large Language Models. (arXiv:2307.14335v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;WavJourney&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38899;&#39057;&#20869;&#23481;&#30340;&#21019;&#20316;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25972;&#21512;&#19981;&#21516;&#30340;&#19987;&#23478;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#39046;&#22495;&#30340;&#21457;&#23637;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#23427;&#20204;&#22312;&#26234;&#33021;&#38899;&#39057;&#20869;&#23481;&#21019;&#20316;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#34987;&#21457;&#25496;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#25991;&#26412;&#25351;&#20196;&#21019;&#24314;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#38899;&#39057;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WavJourney&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#21508;&#31181;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20869;&#23481;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#21548;&#35273;&#22330;&#26223;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;WavJourney&#39318;&#20808;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#20110;&#38899;&#39057;&#21465;&#20107;&#30340;&#32467;&#26500;&#33050;&#26412;&#12290;&#38899;&#39057;&#33050;&#26412;&#21253;&#21547;&#20102;&#19981;&#21516;&#30340;&#38899;&#39057;&#20803;&#32032;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#26102;&#31354;&#20851;&#31995;&#36827;&#34892;&#32452;&#32455;&#12290;&#20316;&#20026;&#38899;&#39057;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#38899;&#39057;&#33050;&#26412;&#20026;&#20154;&#31867;&#21442;&#19982;&#25552;&#20379;&#20102;&#20114;&#21160;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#38543;&#21518;&#65292;&#38899;&#39057;&#33050;&#26412;&#34987;&#20256;&#36882;&#32473;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11845</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#27969;&#31243;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24335;&#25991;&#26723;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#30340;&#22686;&#38271;&#21644;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#29702;&#35299;&#22312;&#38134;&#34892;&#27969;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#22810;&#26679;&#21270;&#30340;&#38134;&#34892;&#25991;&#26723;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20808;&#36827;&#30340;&#20998;&#26512;&#25216;&#26415;&#22312;&#23458;&#25143;&#19994;&#21153;&#20013;&#25552;&#39640;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35832;&#22914;LayoutXLM&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23427;&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#12289;&#22810;&#27169;&#24335;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38134;&#34892;&#19994;&#20013;&#21508;&#31181;&#19981;&#21516;&#30340;&#25991;&#26723;&#12290;&#35813;&#27169;&#22411;&#23545;&#24503;&#22269;&#20844;&#21496;&#30331;&#35760;&#25552;&#21462;&#30340;&#25991;&#26412;&#26631;&#35760;&#20998;&#31867;&#20855;&#26377;&#22823;&#32422;80%&#30340;F1&#24471;&#20998;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#35777;&#23454;&#20102;&#24067;&#23616;&#20449;&#24687;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#25972;&#21512;&#22270;&#20687;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.07294</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Expressivity Enhancement with Efficient Quadratic Neurons for Convolutional Neural Networks. (arXiv:2306.07294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#20998;&#21106;&#31561;&#39046;&#22495;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;&#26032;&#30340;CNN&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#26469;&#33258;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25226;&#37325;&#28857;&#36716;&#21521;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#21487;&#37096;&#32626;&#24615;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#65292;&#20197;&#20165;&#26377;&#24494;&#23567;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#24320;&#38144;&#26469;&#20445;&#30041;&#38750;&#32447;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#21487;&#20197;&#26368;&#22823;&#21270;&#21033;&#29992;&#20108;&#38454;&#35745;&#31639;&#20449;&#24687;&#26469;&#25913;&#21892;&#32593;&#32476;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been successfully applied in a range of fields such as image classification and object segmentation. To improve their expressivity, various techniques, such as novel CNN architectures, have been explored. However, the performance gain from such techniques tends to diminish. To address this challenge, many researchers have shifted their focus to increasing the non-linearity of neurons, the fundamental building blocks of neural networks, to enhance the network expressivity. Nevertheless, most of these approaches incur a large number of parameters and thus formidable computation cost inevitably, impairing their efficiency to be deployed in practice. In this work, an efficient quadratic neuron structure is proposed to preserve the non-linearity with only negligible parameter and computation cost overhead. The proposed quadratic neuron can maximize the utilization of second-order computation information to improve the network performance. The experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11461</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#20174;&#35821;&#20041;&#32423;&#21035;&#21040;&#20195;&#30721;&#32423;&#21035;&#30340; SelfzCoT&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558; SelfzCoT &#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20934;&#30830;&#24615;&#20174;GSM8K&#30340;40.50%&#25552;&#39640;&#33267;82.34%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;94.7%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;94.10%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;91.30%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;82.33%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;79.70%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;&#21069;&#20004;&#20010;&#25345;&#20037;&#36335;&#24452;&#28608;&#27963;&#21040;LLM&#65292;&#29305;&#21035;&#26159;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#20351; SelfzCoT &#22312;&#25152;&#26377;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22312;GSM8K&#20013;&#65292;MzCoT&#30340;&#20934;&#30830;&#24615;&#20174;40.50%&#25552;&#39640;&#33267;76.32%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;96.97%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;92.39%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;94.60%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;79.90%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;81.50%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#39640;&#32500;&#26114;&#36149;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32780;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861; (CLMEA)&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#12289;&#36229;&#20307;&#31215;&#38750;&#25903;&#37197;&#25628;&#32034;&#21644;&#30456;&#23545;&#31232;&#30095;&#30446;&#26631;&#31354;&#38388;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#23558;&#21518;&#20195;&#21010;&#20998;&#20026;&#20960;&#20010;&#31561;&#32423;&#12290;&#19981;&#21516;&#31561;&#32423;&#30340;&#21518;&#20195;&#20351;&#29992;&#25490;&#21517;&#23398;&#20064;&#31574;&#30053;&#29983;&#25104;&#26356;&#20855;&#26377;&#21069;&#26223;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#20505;&#36873;&#35299;&#29992;&#20110;&#23454;&#38469;&#20248;&#21270;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07143</link><description>&lt;p&gt;
&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Longitudinal Car-Following Model. (arXiv:2304.07143v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36319;&#36710;&#27169;&#22411;&#26159;&#20132;&#36890;&#20223;&#30495;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#20869;&#32622;&#20110;&#35768;&#22810;&#37197;&#22791;ADAS&#30340;&#27773;&#36710;&#20013;&#12290;&#23545;&#36710;&#36319;&#36710;&#34892;&#20026;&#30340;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#30001;&#22522;&#26412;&#30340;&#36710;&#36742;&#20132;&#20114;&#36807;&#31243;&#24341;&#36215;&#30340;&#19981;&#21516;&#23439;&#35266;&#29616;&#35937;&#30340;&#26681;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21508;&#31181;&#36710;&#36319;&#36710;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12289;&#20114;&#34917;&#24615;&#21644;&#37325;&#21472;&#20043;&#22788;&#12290;&#35813;&#23457;&#26597;&#23558;&#22312;&#19981;&#21516;&#21407;&#21017;&#20013;&#27010;&#24565;&#21270;&#30340;&#36710;&#36319;&#36710;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.02858</link><description>&lt;p&gt;
&#38754;&#21521;&#31867;&#21035;&#19981;&#22343;&#38382;&#39064;&#30340;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#32508;&#36848;&#65306;&#32452;&#21512;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#35745;&#31639;&#35780;&#20272;&#65292;&#25214;&#21040;&#20102;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65288;CI&#65289;&#26159;&#25351;&#23646;&#20110;&#19968;&#20010;&#31867;&#30340;&#35266;&#27979;&#20540;&#25968;&#37327;&#20302;&#20110;&#20854;&#20182;&#31867;&#30340;&#25968;&#37327;&#12290;&#38598;&#25104;&#23398;&#20064;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#19968;&#20123;&#31574;&#30053;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22686;&#24378;&#38598;&#25104;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#26412;&#25991;&#23545;&#29992;&#20110;&#35299;&#20915;&#22522;&#20934;CI&#38382;&#39064;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;CI&#38382;&#39064;&#30340;10&#20010;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;10&#20010;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23450;&#20041;&#20102;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01664</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies. (arXiv:2304.01664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23450;&#20041;&#20102;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#26159;&#30693;&#35782;&#31649;&#29702;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#65292;&#26412;&#20307;&#26500;&#24314;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25551;&#36848;&#36923;&#36753;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20844;&#29702;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36827;&#32780;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23450;&#20041;&#23481;&#38169;&#25512;&#29702;&#20851;&#31995;&#12290;&#36890;&#36807;&#32771;&#34385;&#26576;&#20123;&#36923;&#36753;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inconsistency handling is an important issue in knowledge management. Especially in ontology engineering, logical inconsistencies may occur during ontology construction. A natural way to reason with an inconsistent ontology is to utilize the maximal consistent subsets of the ontology. However, previous studies on selecting maximum consistent subsets have rarely considered the semantics of the axioms, which may result in irrational inference. In this paper, we propose a novel approach to reasoning with inconsistent ontologies in description logics based on the embeddings of axioms. We first give a method for turning axioms into distributed semantic vectors to compute the semantic connections between the axioms. We then define an embedding-based method for selecting the maximum consistent subsets and use it to define an inconsistency-tolerant inference relation. We show the rationality of our inference relation by considering some logical properties. Finally, we conduct experiments on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.00553</link><description>&lt;p&gt;
&#20174;&#23396;&#31435;&#30340;&#23707;&#23679;&#21040;&#27867;&#22823;&#38470;&#65306;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#29992;&#20110;&#20154;&#31867;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#29702;&#35299;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#24182;&#19988;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;&#34892;&#20026;&#30340;&#29289;&#29702;&#31354;&#38388;&#21040;&#35821;&#20041;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#20250;&#26681;&#25454;&#29420;&#29305;&#30340;&#36873;&#25321;&#26500;&#24314;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#20041;&#21508;&#31181;&#31867;&#21035;&#24182;&#23558;&#22522;&#20934;&#32447;&#25512;&#21521;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#19981;&#21516;&#30340;&#31867;&#21035;&#31890;&#24230;&#65292;&#23601;&#20687;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#19968;&#26679;&#20114;&#19981;&#20860;&#23481;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;A&#20013;&#30340;&#23478;&#21153;&#21644;&#25968;&#25454;&#38598;B&#20013;&#30340;&#27927;&#30424;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#38598;&#20013;&#31038;&#21306;&#30340;&#21147;&#37327;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#19968;&#36215;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#20197;&#36861;&#27714;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#32473;&#23450;&#21160;&#35789;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#24182;&#28085;&#30422;&#22823;&#37327;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#25105;&#20204;&#30340;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#31614;&#31995;&#32479;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35821;&#20041;&#31354;&#38388;&#21644;&#32479;&#19968;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
&lt;/p&gt;</description></item><item><title>PowerPruning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#33021;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#20013;MAC&#25805;&#20316;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20248;&#21270;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#65292;&#21516;&#26102;&#27809;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.13997</link><description>&lt;p&gt;
PowerPruning: &#38024;&#23545;&#21151;&#32791;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13997
&lt;/p&gt;
&lt;p&gt;
PowerPruning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#33021;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#20013;MAC&#25805;&#20316;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20248;&#21270;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#65292;&#21516;&#26102;&#27809;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#39046;&#22495;&#37117;&#33719;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#32593;&#32476;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#23588;&#20854;&#26159;&#21151;&#32791;&#38382;&#39064;&#19978;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#22240;&#32032;&#26159;&#22823;&#37327;&#30340;&#20056;&#21152;&#65288;MAC&#65289;&#25805;&#20316;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PowerPruning&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;MAC&#25805;&#20316;&#28040;&#32791;&#26356;&#23569;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20943;&#23569;&#25968;&#23383;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21151;&#32791;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#23545;&#25152;&#36873;&#26435;&#37325;&#21450;&#20854;&#19982;&#25152;&#26377;&#28608;&#27963;&#36716;&#25442;&#30340;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#65292;&#25361;&#36873;&#20986;&#22312;&#24341;&#36215;&#36739;&#23567;&#24310;&#36831;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#19981;&#20462;&#25913;MAC&#21333;&#20803;&#65292;MAC&#21333;&#20803;&#20013;&#25935;&#24863;&#30005;&#36335;&#36335;&#24452;&#30340;&#26368;&#22823;&#24310;&#36831;&#20063;&#23558;&#34987;&#20943;&#23567;&#65292;&#20174;&#32780;&#20801;&#35768;&#28789;&#27963;&#32553;&#23567;&#20379;&#30005;&#30005;&#21387;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#21151;&#32791;&#12290;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss
&lt;/p&gt;</description></item><item><title>ControlNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#23618;&#23398;&#20064;&#22810;&#26679;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05543</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Adding Conditional Control to Text-to-Image Diffusion Models. (arXiv:2302.05543v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05543
&lt;/p&gt;
&lt;p&gt;
ControlNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#23618;&#23398;&#20064;&#22810;&#26679;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#22312;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ControlNet&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#28155;&#21152;&#31354;&#38388;&#26465;&#20214;&#25511;&#21046;&#12290;ControlNet&#38145;&#23450;&#20102;&#29983;&#20135;&#23601;&#32490;&#30340;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#20197;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#21644;&#31283;&#20581;&#30340;&#32534;&#30721;&#23618;&#20316;&#20026;&#24378;&#22823;&#30340;&#39592;&#24178;&#65292;&#20174;&#32780;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#26465;&#20214;&#25511;&#21046;&#12290;&#35813;&#31070;&#32463;&#26550;&#26500;&#19982;&#8220;&#38646;&#21367;&#31215;&#8221;&#65288;&#38646;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#23618;&#65289;&#36830;&#25509;&#65292;&#20174;&#38646;&#24320;&#22987;&#36880;&#28176;&#22686;&#21152;&#21442;&#25968;&#65292;&#24182;&#30830;&#20445;&#27809;&#26377;&#26377;&#23475;&#30340;&#22122;&#22768;&#24433;&#21709;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#26465;&#20214;&#36827;&#34892;&#31283;&#23450;&#25193;&#25955;&#27979;&#35797;&#20102;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#65292;&#20363;&#22914;&#36793;&#32536;&#12289;&#28145;&#24230;&#12289;&#20998;&#21106;&#12289;&#20154;&#20307;&#23039;&#21183;&#31561;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25110;&#27809;&#26377;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ControlNets&#30340;&#35757;&#32451;&#23545;&#20110;&#23567;&#65288;&lt;50k&#65289;&#21644;&#22823;&#65288;&gt;1m&#65289;&#25968;&#25454;&#38598;&#26159;&#40065;&#26834;&#30340;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ControlNet&#21487;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20197;&#25511;&#21046;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (&lt;50k) and large (&gt;1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.06462</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#25351;&#23548;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#26102;&#12290;&#28982;&#32780;&#65292;&#25351;&#23548;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;-&#27880;&#37322;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20854;&#21487;&#29992;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#27880;&#37322;&#38656;&#27714;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#25552;&#21462;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#37322;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#31890;&#24230;&#19978;&#25552;&#20379;&#25351;&#23548;&#20449;&#21495;&#65306;&#20174;&#25972;&#20307;&#22270;&#20687;&#21040;&#29289;&#20307;&#26694;&#65292;&#29978;&#33267;&#21040;&#20998;&#21106;&#33945;&#29256;&#12290;&#25105;&#20204;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.03829</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#25915;&#20987;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20351;&#29992;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#26377;&#20851;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#12289;&#25915;&#20987;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#19982;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#26089;&#26399;&#26816;&#27979;&#26641;&#30382;&#30002;&#34411;&#24341;&#36215;&#30340;&#26641;&#26408;&#27515;&#20129;&#26041;&#38754;&#30340;&#36807;&#21435;&#21644;&#29616;&#26377;&#36827;&#23637;&#65292;&#20174;&#26641;&#30382;&#30002;&#34411;&#19982;&#23492;&#20027;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12289;&#36965;&#24863;&#21644;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#19982;&#20197;&#24448;&#30340;&#21162;&#21147;&#30456;&#21453;&#65292;&#26412;&#32508;&#36848;&#21253;&#25324;&#20102;&#25152;&#26377;&#36965;&#24863;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26681;&#25454;&#22810;&#20809;&#35889;&#25110;&#39640;&#20809;&#35889;&#20998;&#26512;&#35299;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#20174;&#26641;&#30382;&#30002;&#34411;&#29289;&#31181;&#21644;&#25915;&#20987;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#25915;&#20987;&#30340;&#26089;&#26399;&#38454;&#27573;&#12289;&#23492;&#20027;&#26641;&#26408;&#12289;&#30740;&#31350;&#21306;&#22495;&#12289;&#36965;&#24863;&#24179;&#21488;&#21644;&#20256;&#24863;&#22120;&#12289;&#20809;&#35889;/&#31354;&#38388;/&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20809;&#35889;&#29305;&#24449;&#12289;&#20809;&#35889;&#26893;&#34987;&#25351;&#25968;&#65288;SVIs&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#23398;&#20064;&#26041;&#26696;&#12289;&#20219;&#21153;&#31867;&#21035;&#12289;&#27169;&#22411;&#12289;&#31639;&#27861;&#12289;&#31867;&#21035;/&#31751;&#12289;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#19982;&#26550;&#26500;&#26041;&#38754;&#25552;&#21462;&#30693;&#35782;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21487;&#35265;&#20809;&#21644;&#28909;&#32418;&#22806;&#31561;&#27874;&#27573;&#19978;&#26816;&#27979;&#24494;&#23567;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle &amp; host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species &amp; attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms &amp; sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks &amp; architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, therma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.10498</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#26080;&#27861;&#35268;&#21010;&#65288;LLM&#22312;&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#20013;&#30340;&#22522;&#20934;&#65289;&#12290;&#65288;arXiv:2206.10498v3 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#20174;GPT-3&#21040;PaLM&#65292;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#27491;&#22312;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#20986;&#19981;&#26029;&#25552;&#39640;&#12290;&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#22806;&#65292;&#20154;&#20204;&#23545;&#20110;&#29702;&#35299;&#27492;&#31867;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#37319;&#29992;&#20102;&#25512;&#29702;&#22522;&#20934;&#26469;&#36827;&#34892;&#27979;&#35780;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32467;&#26524;&#30475;&#20284;&#31215;&#26497;&#65292;&#36825;&#20123;&#22522;&#20934;&#22312;&#26412;&#36136;&#19978;&#26159;&#31616;&#21333;&#30340;&#65292;LLMs&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#33021;&#20316;&#20026;&#25903;&#25345;LLMs&#25512;&#29702;&#33021;&#21147;&#65288;&#26377;&#26102;&#26159;&#33618;&#35884;&#30340;&#65289;&#22768;&#31216;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21482;&#20195;&#34920;&#20102;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#38598;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#34913;&#37327;&#27492;&#31867;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#30340;&#30495;&#27491;&#38480;&#21046;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#21629;&#39064;&#36923;&#36753;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#23637;&#32780;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;GPT-3&#21644;GShard&#65289;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29978;&#33267;&#26080;&#27861;&#22788;&#29702;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#24403;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#21487;&#20197;&#35268;&#21010;&#21644;&#25512;&#29702;&#21464;&#21270;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LL
&lt;/p&gt;</description></item></channel></rss>