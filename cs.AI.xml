<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SOLE&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#30340;3D&#23454;&#20363;&#20998;&#21106;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;3D&#28857;&#20113;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#25513;&#27169;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02157</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#23545;&#20219;&#24847;3D&#29289;&#20307;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Any 3D Object with Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02157
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SOLE&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#30340;3D&#23454;&#20363;&#20998;&#21106;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;3D&#28857;&#20113;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#25513;&#27169;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#30340;3D&#23454;&#20363;&#20998;&#21106;&#65288;OV-3DIS&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SOLE&#30340;&#35821;&#20041;&#21644;&#20960;&#20309;&#24847;&#35782;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;3D&#28857;&#20113;&#29983;&#25104;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#25513;&#27169;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02157v1 Announce Type: cross  Abstract: In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works that rely on only annotated base categories for training suffer from limited generalization to unseen novel categories. Recent works mitigate poor generalizability to novel categories by generating class-agnostic masks or projecting generalized masks from 2D to 3D, but disregard semantic or geometry information, leading to sub-optimal performance. Instead, generating generalizable but semantic-related masks directly from 3D point clouds would result in superior outcomes. In this paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.02127</link><description>&lt;p&gt;
FLawN-T5: &#26377;&#25928;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#28151;&#21512;&#22312;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1  &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#25351;&#23548;&#35843;&#25972;&#26159;&#20351;&#35821;&#35328;&#27169;&#22411;&#23545;&#30452;&#25509;&#29992;&#25143;&#20132;&#20114;&#26377;&#25928;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27861;&#24459;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#24320;&#25918;&#24335;LLMs&#30340;&#33539;&#22260;&#65292;&#32780;&#19988;&#30446;&#21069;&#35813;&#39046;&#22495;&#36824;&#27809;&#26377;&#20219;&#20309;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#12289;24&#31181;&#35821;&#35328;&#65292;&#24635;&#35745;1200&#19975;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#21576;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#33021;&#22815;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23558;Flan-T5 XL&#22312;&#22522;&#20934;&#32447;&#19978;&#25552;&#39640;8&#20010;&#28857;&#25110;16%&#12290;&#28982;&#32780;&#65292;&#35813;&#25928;&#24212;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;LawInstruct&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21487;&#20197;&#21152;&#36895;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
&lt;/p&gt;</description></item><item><title>&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;</title><link>https://arxiv.org/abs/2404.02090</link><description>&lt;p&gt;
&#24050;&#32463;&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#35777;&#26126;&#23545;&#22122;&#22768;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Already Moderate Population Sizes Provably Yield Strong Robustness to Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02090
&lt;/p&gt;
&lt;p&gt;
&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#34920;&#26126;&#65292;&#20856;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#23545;&#35832;&#22914;&#22024;&#26434;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#38543;&#26426;&#24178;&#25200;&#12290;&#22312;&#31532;&#19968;&#27425;&#38024;&#23545;$(1+\lambda)$&#21644;$(1,\lambda)$&#36827;&#21270;&#31639;&#27861;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#31181;&#31639;&#27861;&#37117;&#33021;&#23481;&#24525;&#24658;&#23450;&#30340;&#22122;&#22768;&#27010;&#29575;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#31181;&#32676;&#35268;&#27169;$\lambda$&#24212;&#33267;&#23569;&#20026;&#38382;&#39064;&#35268;&#27169;$n$&#30340;&#23545;&#25968;&#12290;&#22312;&#36825;&#26041;&#21521;&#19978;&#30340;&#21807;&#19968;&#20808;&#21069;&#32467;&#26524;&#28041;&#21450;&#19981;&#22826;&#29616;&#23454;&#30340;&#19968;&#20301;&#22122;&#22768;&#27169;&#22411;&#65292;&#38656;&#35201;&#36229;&#32447;&#24615;&#30340;&#38382;&#39064;&#35268;&#27169;&#31181;&#32676;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20110;OneMax&#22522;&#20934;&#35777;&#26126;&#20102;&#22823;&#33268;&#26159;&#26080;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#30340;&#19977;&#27425;&#26041;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26174;&#30528;&#26356;&#24378;&#32467;&#26524;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#21363;&#26080;&#22122;&#22768;&#21518;&#20195;&#21487;&#20197;&#30475;&#20316;&#26159;&#29238;&#20195;&#21644;&#26377;&#22122;&#22768;&#30340;&#21518;&#20195;&#20043;&#38388;&#30340;&#26377;&#20559;&#32479;&#19968;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
&lt;/p&gt;</description></item><item><title>&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.02078</link><description>&lt;p&gt;
&#36890;&#36807;&#39318;&#36873;&#26641;&#25512;&#36827;LLM&#25512;&#29702;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Advancing LLM Reasoning Generalists with Preference Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02078
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25512;&#20986;&#30340;Eurus&#27169;&#22411;&#36890;&#36807;&#22522;&#20110;&#39318;&#36873;&#26641;&#30340;&#25512;&#29702;&#20248;&#21270;&#65292;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#22312;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Eurus&#65292;&#19968;&#22871;&#19987;&#20026;&#25512;&#29702;&#20248;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#32463;&#36807;Mistral-7B&#21644;CodeLlama-70B&#30340;&#24494;&#35843;&#65292;Eurus&#27169;&#22411;&#22312;&#28085;&#30422;&#25968;&#23398;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#25104;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Eurus-70B&#22312;&#36890;&#36807;&#28085;&#30422;&#20116;&#39033;&#20219;&#21153;&#30340;12&#20010;&#27979;&#35797;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#20987;&#36133;&#20102;GPT-3.5 Turbo&#65292;&#24182;&#22312;LeetCode&#21644;TheoremQA&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;33.3%&#21644;32.6%&#30340;pass@1&#20934;&#30830;&#29575;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#24320;&#28304;&#27169;&#22411;&#36229;&#36807;13.3%&#30340;&#36793;&#38469;&#12290;Eurus&#30340;&#24378;&#22823;&#24615;&#33021;&#20027;&#35201;&#24402;&#21151;&#20110;&#25105;&#20204;&#26032;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#23545;&#40784;&#25968;&#25454;&#38598;UltraInteract&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#38376;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;UltraInteract&#21487;&#29992;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#39318;&#36873;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#20010;&#25351;&#20196;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#39318;&#36873;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#24212;&#23545;&#39118;&#26684;&#36801;&#31227;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#25932;&#23545;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.02067</link><description>&lt;p&gt;
&#32418;&#38431;&#27979;&#35797;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Red-Teaming Segment Anything Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#24212;&#23545;&#39118;&#26684;&#36801;&#31227;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#25932;&#23545;&#25915;&#20987;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#23545;&#29305;&#23450;&#24212;&#29992;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#12290;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#21106;&#20219;&#21153;&#20013;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#30693;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#32418;&#38431;&#20998;&#26512;&#65292;&#27979;&#35797;&#20102;&#31471;&#20998;&#20219;&#24847;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;&#39118;&#26684;&#36801;&#31227;&#23545;&#20998;&#21106;&#25513;&#27169;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#23558;&#36870;&#22659;&#22825;&#27668;&#26465;&#20214;&#21644;&#38632;&#28404;&#24212;&#29992;&#20110;&#22478;&#24066;&#36947;&#36335;&#20202;&#34920;&#30424;&#22270;&#20687;&#26174;&#33879;&#25197;&#26354;&#29983;&#25104;&#30340;&#25513;&#27169;&#12290;&#65288;2&#65289;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#25915;&#20987;&#38544;&#31169;&#65292;&#22914;&#35782;&#21035;&#21517;&#20154;&#30340;&#38754;&#23380;&#65292;&#24182;&#26174;&#31034;&#20986;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#19968;&#20123;&#19981;&#33391;&#30693;&#35782;&#12290;&#65288;3&#65289;&#26368;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23545;&#20998;&#21106;&#25513;&#27169;&#30340;&#25932;&#23545;&#25915;&#20987;&#26377;&#22810;&#24378;&#22823;&#12290;&#25105;&#20204;&#19981;&#20165;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02067v1 Announce Type: cross  Abstract: Foundation models have emerged as pivotal tools, tackling many complex tasks through pre-training on vast datasets and subsequent fine-tuning for specific applications. The Segment Anything Model is one of the first and most well-known foundation models for computer vision segmentation tasks. This work presents a multi-faceted red-teaming analysis that tests the Segment Anything Model against challenging tasks: (1) We analyze the impact of style transfer on segmentation masks, demonstrating that applying adverse weather conditions and raindrops to dashboard images of city roads significantly distorts generated masks. (2) We focus on assessing whether the model can be used for attacks on privacy, such as recognizing celebrities' faces, and show that the model possesses some undesired knowledge in this task. (3) Finally, we check how robust the model is to adversarial attacks on segmentation masks under text prompts. We not only show the
&lt;/p&gt;</description></item><item><title>SPMamba&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;Transformer&#32452;&#20214;&#20026;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.02063</link><description>&lt;p&gt;
SPMamba&#65306;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#35821;&#38899;&#20998;&#31163;&#20013;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
SPMamba: State-space model is all you need in speech separation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02063
&lt;/p&gt;
&lt;p&gt;
SPMamba&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#26032;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26367;&#25442;Transformer&#32452;&#20214;&#20026;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#20998;&#31163;&#39046;&#22495;&#65292;CNN&#21644;Transformer&#27169;&#22411;&#37117;&#23637;&#31034;&#20102;&#31283;&#20581;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#23545;&#20110;&#38271;&#24207;&#21015;&#38899;&#39057;&#30340;&#24314;&#27169;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#20998;&#31163;&#24615;&#33021;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;SPMamba&#12290;&#25105;&#20204;&#37319;&#29992;TF-GridNet&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;Transformer&#32452;&#20214;&#26367;&#25442;&#20026;&#19968;&#20010;&#21452;&#21521;Mamba&#27169;&#22359;&#65292;&#26088;&#22312;&#25429;&#33719;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;Mamba&#22522;&#20110;&#26041;&#27861;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02063v1 Announce Type: cross  Abstract: In speech separation, both CNN- and Transformer-based models have demonstrated robust separation capabilities, garnering significant attention within the research community. However, CNN-based methods have limited modelling capability for long-sequence audio, leading to suboptimal separation performance. Conversely, Transformer-based methods are limited in practical applications due to their high computational complexity. Notably, within computer vision, Mamba-based methods have been celebrated for their formidable performance and reduced computational requirements. In this paper, we propose a network architecture for speech separation using a state-space model, namely SPMamba. We adopt the TF-GridNet model as the foundational framework and substitute its Transformer component with a bidirectional Mamba module, aiming to capture a broader range of contextual information. Our experimental results reveal an important role in the performa
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65292;&#26088;&#22312;&#33719;&#24471;&#19968;&#31181;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#28040;&#38500;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02062</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65306;&#36951;&#24536;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Digital Forgetting in Large Language Models: A Survey of Unlearning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23383;&#36951;&#24536;&#65292;&#26088;&#22312;&#33719;&#24471;&#19968;&#31181;&#36951;&#24536;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#28040;&#38500;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#36951;&#24536;&#30340;&#30446;&#26631;&#26159;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#23384;&#22312;&#19981;&#33391;&#30693;&#35782;&#25110;&#34892;&#20026;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24471;&#21040;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#19981;&#20877;&#23384;&#22312;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#12290;&#36951;&#24536;&#30340;&#21160;&#26426;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#12289;&#29256;&#26435;&#20445;&#25252;&#12289;&#28040;&#38500;&#20559;&#35265;&#21644;&#27495;&#35270;&#20197;&#21450;&#39044;&#38450;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#12290;&#26377;&#25928;&#30340;&#25968;&#23383;&#36951;&#24536;&#24517;&#39035;&#26159;&#26377;&#25928;&#30340;&#65288;&#21363;&#26032;&#27169;&#22411;&#22810;&#20040;&#22909;&#22320;&#36951;&#24536;&#20102;&#19981;&#33391;&#30693;&#35782;/&#34892;&#20026;&#65289;&#65292;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#22312;&#29702;&#24819;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#65288;&#29305;&#21035;&#26159;&#36951;&#24536;&#24517;&#39035;&#27604;&#20165;&#37325;&#26032;&#35757;&#32451;&#35201;&#26377;&#25928;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;&#38656;&#35201;&#20445;&#30041;&#30340;&#20219;&#21153;/&#25968;&#25454;&#65289;&#12290;&#26412;&#32508;&#36848;&#32858;&#28966;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36951;&#24536;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;LLMs&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#32452;&#25104;&#37096;&#20998;&#12289;LLMs&#30340;&#31867;&#22411;&#20197;&#21450;&#23427;&#20204;&#36890;&#24120;&#30340;&#35757;&#32451;&#27969;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#25968;&#23383;&#36951;&#24536;&#30340;&#21160;&#26426;&#12289;&#31867;&#22411;&#21644;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02062v1 Announce Type: cross  Abstract: The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02047</link><description>&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#30340;&#36890;&#29992;&#34920;&#31034;&#65306;&#34701;&#21512;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Universal representations for financial transactional data: embracing local, global, and external contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#30340;&#26377;&#25928;&#22788;&#29702;&#23545;&#38134;&#34892;&#25968;&#25454;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#29420;&#31435;&#38382;&#39064;&#25552;&#20379;&#19987;&#38376;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#35768;&#22810;&#38382;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20225;&#19994;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32771;&#34385;&#25968;&#25454;&#29305;&#23450;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#21040;&#23458;&#25143;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20854;&#20182;&#23458;&#25143;&#34892;&#21160;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#25551;&#36848;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#28041;&#21450;&#25972;&#20010;&#20132;&#26131;&#21382;&#21490;&#65307;&#26412;&#22320;&#33539;&#22260;&#20869;&#65292;&#21453;&#26144;&#23458;&#25143;&#24403;&#21069;&#29366;&#24577;&#65307;&#21160;&#24577;&#33539;&#22260;&#20869;&#65292;&#25429;&#25417;&#34920;&#31034;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#19979;&#19968;&#20010;MCC&#39044;&#27979;&#20219;&#21153;&#30340;ROC-AUC&#25552;&#21319;&#39640;&#36798;14&#65285;&#65292;&#23545;&#20110;dow...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02047v1 Announce Type: cross  Abstract: Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for dow
&lt;/p&gt;</description></item><item><title>&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2404.02043</link><description>&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#65306;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge Transfer Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02043
&lt;/p&gt;
&lt;p&gt;
&#20044;&#20811;&#20848;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;NLP&#25216;&#26415;&#65292;&#27979;&#35797;&#20102;&#22312;&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#26368;&#20339;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20294;&#21508;&#31181;&#35821;&#35328;&#21487;&#29992;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#20381;&#28982;&#26174;&#32780;&#26131;&#35265;&#12290;&#20044;&#20811;&#20848;&#35821;&#20316;&#20026;&#19968;&#31181;&#20173;&#21487;&#20174;&#36328;&#35821;&#35328;&#26041;&#27861;&#30340;&#25345;&#32493;&#23436;&#21892;&#20013;&#21463;&#30410;&#30340;&#35821;&#35328;&#12290;&#37492;&#20110;&#25105;&#20204;&#25152;&#20102;&#35299;&#65292;&#38024;&#23545;&#20856;&#22411;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20044;&#20811;&#20848;&#35821;&#35821;&#26009;&#24211;&#26497;&#24230;&#21294;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#32034;&#36328;&#35821;&#35328;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#36991;&#20813;&#25163;&#21160;&#25968;&#25454;&#25972;&#29702;&#65306;&#22823;&#22411;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;&#32763;&#35793;&#31995;&#32479;&#12289;LLMs&#65292;&#20197;&#21450;&#35821;&#35328;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;--&#27602;&#24615;&#20998;&#31867;&#12289;&#25991;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;--&#25552;&#20379;&#20102;&#26368;&#20339;&#35774;&#32622;&#30340;"&#37197;&#26041;"&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02043v1 Announce Type: cross  Abstract: Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference -- providing the "recipe" for the optimal setups.
&lt;/p&gt;</description></item><item><title>LLM&#21644;MLLM&#30340;&#36827;&#27493;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#65292;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12289;&#26041;&#27861;&#35770;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;</title><link>https://arxiv.org/abs/2404.02039</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model-Based Game Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02039
&lt;/p&gt;
&lt;p&gt;
LLM&#21644;MLLM&#30340;&#36827;&#27493;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#65292;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12289;&#26041;&#27861;&#35770;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#22312;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;LLM&#21450;&#20854;&#22810;&#27169;&#24577;&#23545;&#24212;&#29289;&#65288;MLLM&#65289;&#30340;&#36827;&#23637;&#20026;&#28216;&#25103;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#30340;&#30005;&#33041;&#28216;&#25103;&#29615;&#22659;&#20013;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#33021;&#21147;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#20174;&#25972;&#20307;&#35270;&#35282;&#20840;&#38754;&#27010;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#24863;&#30693;&#12289;&#35760;&#24518;&#12289;&#24605;&#32500;&#12289;&#35282;&#33394;&#25198;&#28436;&#12289;&#34892;&#21160;&#21644;&#23398;&#20064;&#20026;&#20013;&#24515;&#30340;LLM&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27010;&#24565;&#26550;&#26500;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;&#20195;&#34920;&#24615;LLM&#28216;&#25103;&#26234;&#33021;&#20307;&#65292;&#28041;&#21450;&#21040;&#20845;&#31867;&#28216;&#25103;&#20013;&#30340;&#26041;&#27861;&#35770;&#21644;&#36866;&#24212;&#33021;&#21147;&#65292;&#21253;&#25324;&#20882;&#38505;&#12289;&#27807;&#36890;&#12289;&#31454;&#20105;&#12289;&#21512;&#20316;&#12289;&#27169;&#25311;&#20197;&#21450;&#21019;&#36896;&#19982;&#25506;&#32034;&#28216;&#25103;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02039v1 Announce Type: new  Abstract: The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting &amp; exploration games. Finally, we present an outlook of future research
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiParaDetox&#65292;&#23558;ParaDetox&#31649;&#36947;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#33258;&#21160;&#21270;&#25910;&#38598;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2404.02037</link><description>&lt;p&gt;
MultiParaDetox&#65306;&#23558;&#25991;&#26412;&#20928;&#21270;&#19982;&#24182;&#34892;&#25968;&#25454;&#25193;&#23637;&#21040;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiParaDetox&#65292;&#23558;ParaDetox&#31649;&#36947;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#33258;&#21160;&#21270;&#25910;&#38598;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20928;&#21270;&#26159;&#19968;&#39033;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#25991;&#26412;&#34987;&#20174;&#26377;&#27602;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#20363;&#22914;&#21253;&#21547;&#31895;&#40065;&#35789;&#27719;&#65292;&#36716;&#36848;&#20026;&#20013;&#24615;&#35821;&#20307;&#12290;&#26368;&#36817;&#65292;&#25991;&#26412;&#20928;&#21270;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#27604;&#22914;&#20928;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;Leong&#31561;&#65292;2023&#65307;He&#31561;&#65292;2024&#65307;Tang&#31561;&#65292;2023&#65289;&#20197;&#21450;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#23545;&#25239;&#26377;&#27602;&#35328;&#35770;&#65288;Deng&#31561;&#65292;2023&#65307;Mun&#31561;&#65292;2023&#65307;Agarwal&#31561;&#65292;2023&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#24212;&#29992;&#23545;&#20110;&#30830;&#20445;&#29616;&#20195;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#29992;&#20110;&#24182;&#34892;&#25991;&#26412;&#20928;&#21270;&#35821;&#26009;&#24211;&#25910;&#38598;&#30340;&#26041;&#27861;-- ParaDetox&#65288;Logacheva&#31561;&#65292;2022&#65289;&#21644;APPADIA&#65288;Atwell&#31561;&#65292;2022&#65289;-- &#20165;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;ParaDetox&#27969;&#31243;&#25193;&#23637;&#21040;&#22810;&#31181;&#35821;&#35328;&#65292;&#25552;&#20986;MultiParaDetox&#20197;&#33258;&#21160;&#21270;&#28508;&#22312;&#20219;&#20309;&#35821;&#35328;&#30340;&#24182;&#34892;&#20928;&#21270;&#35821;&#26009;&#24211;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02037v1 Announce Type: cross  Abstract: Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection -- ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#23545;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02018</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#25490;&#21452;&#25163;&#26426;&#22120;&#20154;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Orchestrating Bimanual Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24212;&#23545;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20351;&#26426;&#22120;&#20154;&#20855;&#26377;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#33021;&#21147;&#24050;&#32463;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#20294;&#20026;&#21452;&#25163;&#26426;&#22120;&#20154;&#29983;&#25104;&#25511;&#21046;&#31574;&#30053;&#20197;&#35299;&#20915;&#28041;&#21450;&#20004;&#21482;&#25163;&#30340;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#22312;&#26377;&#25928;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#21327;&#35843;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#36880;&#27493;&#25512;&#29702;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#25511;&#21046;&#20102;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21333;&#20010;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#36827;&#34892;&#35821;&#35328;&#20132;&#27969;&#30340;&#26412;&#36136;&#20351;&#24471;LLM&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21452;&#25163;&#20219;&#21153;&#21327;&#35843;&#25104;&#20026;&#19968;&#39033;&#29305;&#27530;&#25361;&#25112;&#12290;&#20026;&#20102;&#39318;&#27425;&#36890;&#36807;LLM&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#25163;&#32534;&#25490;&#65288;LABOR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20998;&#26512;&#20219;&#21153;&#37197;&#32622;&#24182;&#35774;&#35745;&#21327;&#35843;&#25511;&#21046;&#31574;&#30053;&#20197;&#35299;&#20915;&#38271;&#26399;&#21452;&#25163;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;LABOR&#20195;&#29702;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02018v1 Announce Type: cross  Abstract: Although there has been rapid progress in endowing robots with the ability to solve complex manipulation tasks, generating control policies for bimanual robots to solve tasks involving two hands is still challenging because of the difficulties in effective temporal and spatial coordination. With emergent abilities in terms of step-by-step reasoning and in-context learning, Large Language Models (LLMs) have taken control of a variety of robotic tasks. However, the nature of language communication via a single sequence of discrete symbols makes LLM-based coordination in continuous space a particular challenge for bimanual tasks. To tackle this challenge for the first time by an LLM, we present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing an LLM to analyze task configurations and devise coordination control policies for addressing long-horizon bimanual tasks. In the simulated environment, the LABOR agent is eval
&lt;/p&gt;</description></item><item><title>&#20957;&#35270;&#32447;&#32034;&#30340;&#26377;&#25928;&#21033;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#24863;&#30693;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24615;&#33021;&#65292;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01986</link><description>&lt;p&gt;
&#39044;&#27979;&#19982;&#26381;&#21153;&#26426;&#22120;&#20154;&#20114;&#21160;&#24847;&#22270;&#65306;&#20957;&#35270;&#32447;&#32034;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting the Intention to Interact with a Service Robot:the Role of Gaze Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01986
&lt;/p&gt;
&lt;p&gt;
&#20957;&#35270;&#32447;&#32034;&#30340;&#26377;&#25928;&#21033;&#29992;&#26174;&#33879;&#25552;&#39640;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#24863;&#30693;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24615;&#33021;&#65292;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26381;&#21153;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#23613;&#24555;&#24863;&#30693;&#19968;&#20010;&#25509;&#36817;&#30340;&#20154;&#26159;&#21542;&#26377;&#24847;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#37319;&#21462;&#21451;&#22909;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28508;&#22312;&#29992;&#25143;&#20114;&#21160;&#24847;&#22270;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#24863;&#30693;&#20219;&#21153;&#65292;&#21487;&#20197;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#30740;&#31350;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#20195;&#34920;&#20010;&#20154;&#20957;&#35270;&#30340;&#29305;&#24449;&#30340;&#30410;&#22788;&#12290;&#23545;&#26032;&#39062;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20957;&#35270;&#32447;&#32034;&#30340;&#21253;&#21547;&#26174;&#33879;&#25913;&#21892;&#20102;&#20998;&#31867;&#22120;&#24615;&#33021;&#65288;AUROC&#20174;84.5%&#25552;&#39640;&#21040;91.2%&#65289;&#65307;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#20998;&#31867;&#30340;&#36317;&#31163;&#20174;2.4&#31859;&#25552;&#39640;&#21040;3.2&#31859;&#12290;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#31995;&#32479;&#22312;&#27809;&#26377;&#22806;&#37096;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;&#23450;&#24615;&#23454;&#39564;&#23637;&#31034;&#20102;&#26381;&#21153;&#21592;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01986v1 Announce Type: cross  Abstract: For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person's gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5% to 91.2%); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system's ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#24403;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.01976</link><description>&lt;p&gt;
&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#29992;&#20110;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Joint-Task Regularization for Partially Labeled Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01976
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#24403;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#38468;&#24102;&#25152;&#26377;&#30446;&#26631;&#20219;&#21153;&#30340;&#22320;&#38754;&#30495;&#30456;&#26631;&#31614;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31579;&#36873;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#22240;&#20026;&#26114;&#36149;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#27599;&#20010;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#26631;&#31614;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#20219;&#21153;&#27491;&#21017;&#21270;&#65288;JTR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30452;&#35266;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36328;&#20219;&#21153;&#20851;&#31995;&#22312;&#21333;&#20010;&#32852;&#21512;&#20219;&#21153;&#28508;&#22312;&#31354;&#38388;&#20013;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#22312;&#25968;&#25454;&#26410;&#23436;&#20840;&#26631;&#35760;&#25152;&#26377;&#20219;&#21153;&#26102;&#30340;&#23398;&#20064;&#12290;JTR&#19982;&#29616;&#26377;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#27491;&#21017;&#21270;&#32780;&#19981;&#26159;&#20998;&#21035;&#23545;&#27599;&#23545;&#20219;&#21153;&#36827;&#34892;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20219;&#21153;&#25968;&#37327;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#27809;&#26377;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01976v1 Announce Type: cross  Abstract: Multi-task learning has become increasingly popular in the machine learning field, but its practicality is hindered by the need for large, labeled datasets. Most multi-task learning methods depend on fully labeled datasets wherein each input example is accompanied by ground-truth labels for all target tasks. Unfortunately, curating such datasets can be prohibitively expensive and impractical, especially for dense prediction tasks which require per-pixel labels for each image. With this in mind, we propose Joint-Task Regularization (JTR), an intuitive technique which leverages cross-task relations to simultaneously regularize all tasks in a single joint-task latent space to improve learning when data is not fully labeled for all tasks. JTR stands out from existing approaches in that it regularizes all tasks jointly rather than separately in pairs -- therefore, it achieves linear complexity relative to the number of tasks while previous 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01965</link><description>&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;AutoML&#23454;&#29616;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;&#65306;&#22522;&#20110;Deep Shift&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;HPO&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Leveraging AutoML for Sustainable Deep Learning: A Multi-Objective HPO Approach on Deep Shift Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;AutoML&#25216;&#26415;&#26368;&#22823;&#21270;Deep Shift&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#20445;&#30495;&#24230;HPO&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#25512;&#21160;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#30340;&#35745;&#31639;&#38656;&#27714;&#24102;&#26469;&#20102;&#29615;&#22659;&#21644;&#36164;&#28304;&#25361;&#25112;&#12290;Deep Shift&#31070;&#32463;&#32593;&#32476;&#65288;DSNN&#65289;&#21033;&#29992;shift&#25805;&#20316;&#20943;&#23569;&#25512;&#29702;&#26102;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#27492;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20511;&#37492;&#26631;&#20934;DNN&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#26377;&#20852;&#36259;&#36890;&#36807;AutoML&#25216;&#26415;&#20805;&#20998;&#21457;&#25381;DSNN&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#26368;&#22823;&#21270;DSNN&#24615;&#33021;&#21516;&#26102;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#23558;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#20316;&#20026;&#21487;&#33021;&#20114;&#34917;&#30446;&#26631;&#32467;&#21512;&#30340;&#22810;&#30446;&#26631;&#65288;MO&#65289;&#20248;&#21270;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#26368;&#20808;&#36827;&#30340;&#22810;&#20445;&#30495;&#24230;&#65288;MF&#65289;HPO&#19982;&#22810;&#30446;&#26631;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#21040;&#20102;&#20934;&#30830;&#29575;&#36229;&#36807;80&#65285;&#19988;&#35745;&#31639;&#20302;&#32791;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01965v1 Announce Type: cross  Abstract: Deep Learning (DL) has advanced various fields by extracting complex patterns from large datasets. However, the computational demands of DL models pose environmental and resource challenges. Deep shift neural networks (DSNNs) offer a solution by leveraging shift operations to reduce computational complexity at inference. Following the insights from standard DNNs, we are interested in leveraging the full potential of DSNNs by means of AutoML techniques. We study the impact of hyperparameter optimization (HPO) to maximize DSNN performance while minimizing resource consumption. Since this combines multi-objective (MO) optimization with accuracy and energy consumption as potentially complementary objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with multi-objective optimization. Experimental results demonstrate the effectiveness of our approach, resulting in models with over 80\% in accuracy and low computational 
&lt;/p&gt;</description></item><item><title>HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01954</link><description>&lt;p&gt;
HyperCLOVA X &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01954
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; HyperCLOVA X&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#20855;&#26377;&#22312;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#26041;&#38754;&#30340;&#31454;&#20105;&#33021;&#21147;&#12290;HyperCLOVA X &#22312;&#24179;&#34913;&#28151;&#21512;&#30340;&#38889;&#35821;&#12289;&#33521;&#35821;&#21644;&#20195;&#30721;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#21516;&#26102;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#20934;&#21017;&#65292;&#20307;&#29616;&#20102;&#25105;&#20204;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#32508;&#21512;&#25512;&#29702;&#12289;&#30693;&#35782;&#12289;&#24120;&#35782;&#12289;&#30495;&#23454;&#24615;&#12289;&#32534;&#30721;&#12289;&#25968;&#23398;&#12289;&#32842;&#22825;&#12289;&#36981;&#24490;&#25351;&#20196;&#21644;&#26080;&#23475;&#24615;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#38889;&#35821;&#21644;&#33521;&#35821;&#12290;HyperCLOVA X &#22312;&#38889;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24471;&#30410;&#20110;&#23545;&#35821;&#35328;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#28145;&#21051;&#29702;&#35299;&#12290;&#23545;&#22266;&#26377;&#30340;&#21452;&#35821;&#29305;&#24615;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#21450;&#20854;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#30740;&#31350;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#29087;&#32451;&#24615;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#26410;&#23450;&#21521;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01954v1 Announce Type: cross  Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted la
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#40479;&#30640;&#35270;&#22270;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;BEV&#22320;&#22270;&#37325;&#24314;&#21644;RGB-BEV&#29305;&#24449;&#23545;&#40784;&#20004;&#20010;&#38454;&#27573;&#65292;&#26469;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.01925</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#26469;&#25913;&#36827;&#40479;&#30640;&#35270;&#22270;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Improving Bird's Eye View Semantic Segmentation by Task Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#40479;&#30640;&#35270;&#22270;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;BEV&#22320;&#22270;&#37325;&#24314;&#21644;RGB-BEV&#29305;&#24449;&#23545;&#40784;&#20004;&#20010;&#38454;&#27573;&#65292;&#26469;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40479;&#30640;&#35270;&#22270;&#65288;BEV&#65289;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#30452;&#25509;&#20174;&#21333;&#30524;RGB&#36755;&#20837;&#39044;&#27979;BEV&#20998;&#21106;&#22320;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;RGB&#36755;&#20837;&#21644;BEV&#30446;&#26631;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#26102;&#65292;&#30452;&#25509;&#28857;&#23545;&#28857;&#39044;&#27979;&#21464;&#24471;&#38590;&#20197;&#20248;&#21270;&#12290;&#26412;&#25991;&#23558;&#21407;&#22987;&#30340;BEV&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#21363;BEV&#22320;&#22270;&#37325;&#24314;&#21644;RGB-BEV&#29305;&#24449;&#23545;&#40784;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;BEV&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#32473;&#23450;&#21463;&#25439;&#22122;&#22768;&#28508;&#22312;&#34920;&#31034;&#30340;&#26041;&#24335;&#37325;&#24314;BEV&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#20419;&#20351;&#35299;&#30721;&#22120;&#23398;&#20064;&#20856;&#22411;BEV&#27169;&#24335;&#30340;&#22522;&#26412;&#30693;&#35782;&#12290;&#31532;&#20108;&#38454;&#27573;&#28041;&#21450;&#23558;RGB&#36755;&#20837;&#22270;&#20687;&#26144;&#23556;&#21040;&#31532;&#19968;&#38454;&#27573;&#30340;BEV&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#30452;&#25509;&#20248;&#21270;&#20004;&#20010;&#35270;&#22270;&#22312;&#29305;&#24449;&#32423;&#21035;&#19978;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01925v1 Announce Type: cross  Abstract: Semantic segmentation in bird's eye view (BEV) plays a crucial role in autonomous driving. Previous methods usually follow an end-to-end pipeline, directly predicting the BEV segmentation map from monocular RGB inputs. However, the challenge arises when the RGB inputs and BEV targets from distinct perspectives, making the direct point-to-point predicting hard to optimize. In this paper, we decompose the original BEV segmentation task into two stages, namely BEV map reconstruction and RGB-BEV feature alignment. In the first stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps given corrupted noisy latent representation, which urges the decoder to learn fundamental knowledge of typical BEV patterns. The second stage involves mapping RGB input images into the BEV latent space of the first stage, directly optimizing the correlations between the two views at the feature level. Our approach simplifies the complexity of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SGSH&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;GPT-3.5&#29983;&#25104;&#30693;&#35782;&#24211;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#32452;&#32455;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2404.01923</link><description>&lt;p&gt;
SGSH&#65306;&#29992;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SGSH&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#28608;&#21457;GPT-3.5&#29983;&#25104;&#30693;&#35782;&#24211;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#32452;&#32455;&#21644;&#21033;&#29992;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#65288;KBQG&#65289;&#26088;&#22312;&#20174;&#20174;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#20107;&#23454;&#38598;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26174;&#33879;&#25552;&#21319;&#20102;KBQG&#30340;&#24615;&#33021;&#65292;&#24471;&#30410;&#20110;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SGSH--&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39592;&#26550;&#21551;&#21457;&#26469;&#22686;&#24378;KBQG&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#8220;&#39592;&#26550;&#21551;&#21457;&#8221;&#65292;&#25552;&#20379;&#20102;&#19982;&#27599;&#20010;&#36755;&#20837;&#30456;&#20851;&#30340;&#26356;&#31934;&#32454;&#30340;&#25351;&#23548;&#65292;&#20197;&#28608;&#21457;LLMs&#29983;&#25104;&#26368;&#20339;&#38382;&#39064;&#65292;&#21253;&#25324;&#38382;&#39064;&#30701;&#35821;&#21644;&#21161;&#21160;&#35789;&#31561;&#20851;&#38190;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01923v1 Announce Type: cross  Abstract: Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH--a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates "skeleton heuristics", which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SCANNER&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#24182;&#21033;&#29992;&#30693;&#35782;&#20174;&#22810;&#31181;&#26469;&#28304;&#33719;&#21462;&#30693;&#35782;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;NER&#25968;&#25454;&#38598;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01914</link><description>&lt;p&gt;
SCANNER&#65306;&#29992;&#20110;&#24378;&#22823;&#30340;&#22810;&#27169;&#24335;&#20855;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SCANNER&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#24182;&#21033;&#29992;&#30693;&#35782;&#20174;&#22810;&#31181;&#26469;&#28304;&#33719;&#21462;&#30693;&#35782;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;NER&#25968;&#25454;&#38598;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20855;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35813;&#20219;&#21153;&#30340;&#36793;&#30028;&#65292;&#23558;&#35270;&#35273;&#20449;&#21495;&#32435;&#20837;&#20854;&#20013;&#65292;&#23548;&#33268;&#35768;&#22810;&#21464;&#20307;&#65292;&#21253;&#25324;&#22810;&#27169;&#24335;NER&#65288;MNER&#65289;&#25110;&#22522;&#20110;&#23454;&#20307;&#30340;MNER&#65288;GMNER&#65289;&#12290; &#36825;&#20123;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27169;&#22411;&#24212;&#33021;&#22815;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#23454;&#20307;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#35757;&#32451;&#26679;&#26412;&#20013;&#24102;&#26377;&#22122;&#22768;&#27880;&#37322;&#30340;&#24773;&#20917;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCANNER&#65288;&#29992;&#20110;NER&#30340;SpAN CANdidate&#26816;&#27979;&#21644;&#35782;&#21035;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25152;&#26377;&#19977;&#31181;NER&#21464;&#20307;&#30340;&#27169;&#22411;&#12290; SCANNER&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#32467;&#26500;&#65307;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#25552;&#21462;&#23454;&#20307;&#20505;&#36873;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#26597;&#35810;&#20197;&#33719;&#21462;&#30693;&#35782;&#65292;&#26377;&#25928;&#22320;&#20174;&#21508;&#31181;&#26469;&#28304;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290; &#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25552;&#21462;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#26410;&#35265;&#23454;&#20307;&#12290; &#27492;&#22806;&#65292;&#20026;&#20102;&#24212;&#23545;NER&#25968;&#25454;&#38598;&#20013;&#30001;&#22024;&#26434;&#27880;&#37322;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01914v1 Announce Type: cross  Abstract: Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations. To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants. SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources. We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities. Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a nove
&lt;/p&gt;</description></item><item><title>COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01897</link><description>&lt;p&gt;
&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Spiking Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01897
&lt;/p&gt;
&lt;p&gt;
COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#22240;&#24341;&#20837;&#36830;&#32493;&#21160;&#21147;&#23398;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#33021;&#22815;&#25512;&#24191;&#29616;&#26377;&#30340;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#23427;&#20204;&#36890;&#24120;&#21463;&#25193;&#25955;&#31867;&#26041;&#27861;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;CGNNs&#30340;&#23454;&#29616;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37096;&#32626;&#22312;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#19978;&#12290;&#21463;&#26368;&#36817;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#21551;&#21457;&#65292;SNNs&#27169;&#25311;&#29983;&#29289;&#25512;&#29702;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#31181;&#33410;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23558;SNNs&#19982;CGNNs&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21629;&#21517;&#20026;&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;COS-GNN&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20351;&#29992;SNNs&#36827;&#34892;&#22270;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01897v1 Announce Type: cross  Abstract: Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01889</link><description>&lt;p&gt;
RAVE: CLIP&#24341;&#23548;&#30340;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21453;&#24046;&#24322;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#25351;&#23548;&#36827;&#34892;&#20102;&#26032;&#39062;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;CLIP-LIT&#26041;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32422;&#26463;&#22312;CLIP&#23884;&#20837;&#31354;&#38388;&#20013;&#19968;&#20010;&#25552;&#31034;&#23545;&#20043;&#38388;&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#19968;&#20010;&#25552;&#31034;&#23545;&#65288;&#36127;/&#27491;&#26679;&#26412;&#65289;&#21644;&#30456;&#24212;&#22270;&#20687;&#65288;&#32972;&#20809;&#22270;&#20687;/&#20809;&#29031;&#33391;&#22909;&#30340;&#22270;&#20687;&#65289;&#12290;&#23398;&#20064;&#30340;&#25552;&#31034;&#28982;&#21518;&#25351;&#23548;&#22270;&#20687;&#22686;&#24378;&#32593;&#32476;&#12290;&#22522;&#20110;CLIP-LIT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CLIP&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#35843;&#25972;&#25552;&#31034;&#32780;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#25972;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#21152;&#24555;&#35757;&#32451;&#24182;&#28508;&#22312;&#22320;&#23454;&#29616;&#20351;&#29992;&#27809;&#26377;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#12289;Deepfake&#21644;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#24615;&#36136;&#19978;&#30340;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#20197;&#21450;&#20351;&#29992;&#20102;&#20843;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21306;&#20998;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2404.01878</link><description>&lt;p&gt;
&#30495;&#23454;&#12289;&#34394;&#20551;&#21644;&#21512;&#25104;&#38754;&#23380; - &#24065;&#26377;&#19977;&#38754;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Real, fake and synthetic faces - does the coin have three sides?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01878
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#12289;Deepfake&#21644;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#24615;&#36136;&#19978;&#30340;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#20197;&#21450;&#20351;&#29992;&#20102;&#20843;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21306;&#20998;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;Deepfake&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#65288;&#21512;&#25104;&#65289;&#23186;&#20307;&#32487;&#32493;&#22312;&#32593;&#32476;&#19978;&#20256;&#25773;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#20351;&#29992;&#30340;&#21508;&#31181;&#20262;&#29702;&#21644;&#36947;&#24503;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#12289;Deepfake&#21644;&#21512;&#25104;&#38754;&#37096;&#22270;&#20687;&#20013;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#21644;&#27169;&#24335;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#25506;&#32034;&#12290;&#25552;&#20986;&#30340;&#20998;&#26512;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20843;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#21306;&#20998;&#36825;&#19977;&#31867;&#22270;&#20687;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#19977;&#31867;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#65292;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#30340;&#22270;&#20687;&#29305;&#24615;&#65292;&#26082;&#28085;&#30422;&#25972;&#20010;&#22270;&#20687;&#30340;&#32972;&#26223;&#65292;&#20063;&#21253;&#25324;&#22270;&#20687;&#20869;&#30340;&#29305;&#23450;&#21306;&#22495;&#12290;ANOVA&#27979;&#35797;&#20063;&#34987;&#25191;&#34892;&#65292;&#24182;&#20026;&#36825;&#19977;&#31867;&#22270;&#20687;&#20043;&#38388;&#20851;&#32852;&#30340;&#27169;&#24335;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#35808;&#37322;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01878v1 Announce Type: cross  Abstract: With the ever-growing power of generative artificial intelligence, deepfake and artificially generated (synthetic) media have continued to spread online, which creates various ethical and moral concerns regarding their usage. To tackle this, we thus present a novel exploration of the trends and patterns observed in real, deepfake and synthetic facial images. The proposed analysis is done in two parts: firstly, we incorporate eight deep learning models and analyze their performances in distinguishing between the three classes of images. Next, we look to further delve into the similarities and differences between these three sets of images by investigating their image properties both in the context of the entire image as well as in the context of specific regions within the image. ANOVA test was also performed and provided further clarity amongst the patterns associated between the images of the three classes. From our findings, we obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01869</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;--&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#20102;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#28041;&#21450;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28608;&#28872;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#28145;&#24230;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#37096;&#20998;&#28304;&#33258;&#23545;&#27169;&#22411;&#25512;&#29702;&#34892;&#20026;&#30340;&#28145;&#20837;&#35843;&#26597;&#32780;&#38750;&#20165;&#20165;&#36890;&#36807;&#34920;&#38754;&#20934;&#30830;&#24615;&#25351;&#26631;&#26469;&#34913;&#37327;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32508;&#36848;&#36229;&#36234;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35780;&#20272;LLMs&#25512;&#29702;&#34892;&#20026;&#30340;&#20027;&#35201;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20102;&#24403;&#21069;&#23545;&#26356;&#32454;&#33268;&#25512;&#29702;&#20998;&#26512;&#30340;&#36235;&#21183;&#21644;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#38754;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01869v1 Announce Type: cross  Abstract: Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#32780;&#25439;&#23475;&#24615;&#33021;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01863</link><description>&lt;p&gt;
&#38754;&#21521;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#24863;&#30693;&#22870;&#21169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01863
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#30340;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#32780;&#25439;&#23475;&#24615;&#33021;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#23545;&#40784;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#20989;&#25968;&#19978;&#23545;&#32454;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#24847;&#22270;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#20248;&#21270;&#20351;&#29992;&#36825;&#20123;&#22870;&#21169;&#27169;&#22411;&#65292;&#20316;&#20026;&#31616;&#21333;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#32454;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;&#20026;&#28145;&#20837;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text-Image Alignment Assessment (TIA2)&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#25324;&#19968;&#31995;&#21015;&#25991;&#26412;&#25552;&#31034;&#12289;&#22270;&#20687;&#21644;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#32463;&#24120;&#19982;&#20154;&#31867;&#35780;&#20272;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#35282;&#24230;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#19968;&#20010;&#19982;&#20154;&#31867;&#35780;&#20272;&#19981;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#24494;&#35843;&#30446;&#26631;&#26102;&#65292;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TextNorm&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20272;&#35745;&#30340;&#22870;&#21169;&#27169;&#22411;&#32622;&#20449;&#24230;&#26469;&#22686;&#24378;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01863v1 Announce Type: cross  Abstract: Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.01855</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#21435;&#21738;&#37324;&#65306;&#22522;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01855
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#25506;&#32034;LLMs&#29992;&#20110;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25506;&#32034;&#21608;&#36793;&#29615;&#22659;&#30340;&#23453;&#36149;&#24314;&#35758;&#12290;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#20174;&#22823;&#35268;&#27169;&#29992;&#25143;&#31614;&#21040;&#25968;&#25454;&#26500;&#24314;&#25512;&#33616;&#27169;&#22411;&#65292;&#36825;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#22312;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#38382;&#39064;&#26102;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#24212;&#25552;&#21462;&#29992;&#25143;&#30340;&#22320;&#29702;&#31227;&#21160;&#27169;&#24335;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#21033;&#29992;LLMs&#36827;&#34892;&#19979;&#19968;&#20010;&#39033;&#30446;&#25512;&#33616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#22320;&#29702;&#24433;&#21709;&#21644;&#39034;&#24207;&#36716;&#25442;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#19979;&#19968;&#20010;POI&#25512;&#33616;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01855v1 Announce Type: cross  Abstract: Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EV2Gym&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;V2G&#27169;&#25311;&#22120;&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21508;&#31181;&#35268;&#27169;&#30340;&#26234;&#33021;&#20805;&#30005;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#27169;&#22411;&#21644;&#30028;&#38754;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2404.01849</link><description>&lt;p&gt;
EV2Gym&#65306;&#19968;&#31181;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#26234;&#33021;&#20805;&#30005;&#30740;&#31350;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#28789;&#27963;V2G&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EV2Gym&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;V2G&#27169;&#25311;&#22120;&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21508;&#31181;&#35268;&#27169;&#30340;&#26234;&#33021;&#20805;&#30005;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#27169;&#22411;&#21644;&#30028;&#38754;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20154;&#20204;&#23545;&#24403;&#21069;&#20805;&#30005;&#21644;&#30005;&#32593;&#22522;&#30784;&#35774;&#26045;&#23481;&#37327;&#30340;&#25285;&#24551;&#26085;&#30410;&#21152;&#21095;&#65292;&#24517;&#39035;&#24320;&#21457;&#26234;&#33021;&#20805;&#30005;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#24320;&#21457;&#20102;&#35768;&#22810;&#26234;&#33021;&#20805;&#30005;&#27169;&#25311;&#22120;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#25903;&#25345;&#20197;Gym&#29615;&#22659;&#24418;&#24335;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#32780;&#36825;&#20123;&#27169;&#25311;&#22120;&#36890;&#24120;&#32570;&#20047;&#23545;V2G&#22330;&#26223;&#30340;&#28145;&#20837;&#24314;&#27169;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;EV2Gym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#35268;&#27169;&#21270;&#26234;&#33021;&#20805;&#30005;&#31639;&#27861;&#30340;&#23454;&#38469;&#27169;&#25311;&#22120;&#24179;&#21488;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#24179;&#21488;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#25311;&#22120;&#37319;&#29992;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#30340;&#20840;&#38754;EV&#12289;&#20805;&#30005;&#31449;&#12289;&#30005;&#21147;&#21464;&#21387;&#22120;&#21644;EV&#34892;&#20026;&#27169;&#22411;&#12290;EV2Gym&#20855;&#26377;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#30028;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#39044;&#35774;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#20063;&#21487;&#20197;&#21046;&#23450;&#33258;&#24049;&#30340;&#23450;&#21046;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01849v1 Announce Type: cross  Abstract: As electric vehicle (EV) numbers rise, concerns about the capacity of current charging and power grid infrastructure grow, necessitating the development of smart charging solutions. While many smart charging simulators have been developed in recent years, only a few support the development of Reinforcement Learning (RL) algorithms in the form of a Gym environment, and those that do usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the aforementioned issues, this paper introduces the EV2Gym, a realistic simulator platform for the development and assessment of small and large-scale smart charging algorithms within a standardized platform. The proposed simulator is populated with comprehensive EV, charging station, power transformer, and EV behavior models validated using real data. EV2Gym has a highly customizable interface empowering users to choose from pre-designed case studies or craft their own customized sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#22810;&#22238;&#21512;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30475;&#20284;&#33391;&#24615;&#30340;&#23545;&#35805;&#26041;&#24335;&#36880;&#28176;&#21319;&#32423;&#19982;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25104;&#21151;&#31361;&#30772;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01833</link><description>&lt;p&gt;
&#20255;&#22823;&#65292;&#29616;&#22312;&#20889;&#19968;&#31687;&#20851;&#20110;&#27492;&#30340;&#25991;&#31456;&#65306;Crescendo&#22810;&#22238;&#21512;LLM&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01833
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#22810;&#22238;&#21512;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30475;&#20284;&#33391;&#24615;&#30340;&#23545;&#35805;&#26041;&#24335;&#36880;&#28176;&#21319;&#32423;&#19982;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25104;&#21151;&#31361;&#30772;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27969;&#34892;&#31243;&#24230;&#22823;&#24133;&#19978;&#21319;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;LLMs&#22312;&#35774;&#35745;&#19978;&#36991;&#20813;&#28041;&#21450;&#38750;&#27861;&#25110;&#19981;&#36947;&#24503;&#30340;&#35805;&#39064;&#65292;&#20197;&#36991;&#20813;&#23545;&#36127;&#36131;&#20219;&#30340;AI&#36896;&#25104;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#25915;&#20987;&#65292;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#65292;&#26088;&#22312;&#31361;&#30772;&#36825;&#31181;&#23545;&#40784;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#32553;&#23567;&#27169;&#22411;&#33021;&#20570;&#30340;&#19982;&#24895;&#24847;&#20570;&#30340;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#19981;&#21516;&#65292;Crescendo&#26159;&#19968;&#31181;&#22810;&#22238;&#21512;&#36234;&#29425;&#65292;&#20197;&#19968;&#31181;&#30475;&#20284;&#33391;&#24615;&#30340;&#26041;&#24335;&#19982;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#20174;&#26377;&#20851;&#25163;&#22836;&#20219;&#21153;&#30340;&#19968;&#33324;&#25552;&#31034;&#25110;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#21319;&#32423;&#23545;&#35805;&#65292;&#24341;&#29992;&#27169;&#22411;&#30340;&#22238;&#22797;&#65292;&#36880;&#28176;&#23548;&#33268;&#25104;&#21151;&#36234;&#29425;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;ChatGPT&#12289;Gemini Pr&#22312;&#20869;&#30340;&#21508;&#31181;&#20844;&#20849;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;Crescendo&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01833v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pr
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#27010;&#24565;&#19979;&#25552;&#20986;&#20102;&#21508;&#21521;&#24322;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#65288;AIR&#65289;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#20445;&#25345;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#22312;&#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20013;&#23398;&#20064;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;&#12290;</title><link>https://arxiv.org/abs/2404.01828</link><description>&lt;p&gt;
&#19981;&#24536;&#38450;&#24481;&#65306;&#21508;&#21521;&#24322;&#24615;&#19982;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defense without Forgetting: Continual Adversarial Defense with Anisotropic &amp; Isotropic Pseudo Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01828
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#27010;&#24565;&#19979;&#25552;&#20986;&#20102;&#21508;&#21521;&#24322;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#20266;&#37325;&#28436;&#65288;AIR&#65289;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#20445;&#25345;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#22312;&#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20013;&#23398;&#20064;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#12290;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#36890;&#24120;&#19987;&#27880;&#20110;&#19968;&#27425;&#24615;&#35774;&#32622;&#65292;&#20197;&#20445;&#25345;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#22330;&#26223;&#20013;&#65292;&#26032;&#30340;&#25915;&#20987;&#21487;&#33021;&#20250;&#36830;&#32493;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#38450;&#24481;&#27169;&#22411;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#36866;&#24212;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#20808;&#21069;&#36827;&#34892;&#38450;&#24481;&#30340;&#25915;&#20987;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35752;&#35770;&#20102;&#22312;&#19968;&#31995;&#21015;&#25915;&#20987;&#19979;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21508;&#21521;&#24322;&#24615;&#19982;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#65288;AIR&#65289;&#30340;&#32456;&#36523;&#38450;&#24481;&#22522;&#32447;&#65292;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65306;&#65288;1&#65289;&#21508;&#21521;&#21516;&#24615;&#37325;&#28436;&#30830;&#20445;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#30340;&#37051;&#22495;&#20998;&#24067;&#20013;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#38388;&#25509;&#22320;&#23545;&#40784;&#20102;&#26087;&#20219;&#21153;&#21644;&#26032;&#20219;&#21153;&#20043;&#38388;&#30340;&#36755;&#20986;&#20559;&#22909;&#12290; (2) &#21508;&#21521;&#24322;&#24615;&#37325;&#28436;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#24102;&#26377;&#26032;&#28151;&#21512;&#35821;&#20041;&#30340;&#25240;&#34935;&#25968;&#25454;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01828v1 Announce Type: cross  Abstract: Deep neural networks have demonstrated susceptibility to adversarial attacks. Adversarial defense techniques often focus on one-shot setting to maintain robustness against attack. However, new attacks can emerge in sequences in real-world deployment scenarios. As a result, it is crucial for a defense model to constantly adapt to new attacks, but the adaptation process can lead to catastrophic forgetting of previously defended against attacks. In this paper, we discuss for the first time the concept of continual adversarial defense under a sequence of attacks, and propose a lifelong defense baseline called Anisotropic \&amp; Isotropic Replay (AIR), which offers three advantages: (1) Isotropic replay ensures model consistency in the neighborhood distribution of new data, indirectly aligning the output preference between old and new tasks. (2) Anisotropic replay enables the model to learn a compromise data manifold with fresh mixed semantics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#29289;&#20307;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#37327;&#21644;&#21487;&#34892;&#24615;&#26469;&#24555;&#36895;&#23398;&#20064;&#23436;&#25972;&#30340;3D&#29289;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38476;&#29983;&#26041;&#21521;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#25913;&#36827;&#35270;&#35273;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01812</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#30340;&#29289;&#20307;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20027;&#21160;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#32773;&#36827;&#34892;&#35270;&#35273;&#21644;&#37325;&#26032;&#23450;&#20301;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#29289;&#20307;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#37327;&#21644;&#21487;&#34892;&#24615;&#26469;&#24555;&#36895;&#23398;&#20064;&#23436;&#25972;&#30340;3D&#29289;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#38476;&#29983;&#26041;&#21521;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#25913;&#36827;&#35270;&#35273;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#19977;&#32500;&#34920;&#31034;&#30340;&#24773;&#20917;&#19979;&#25805;&#20316;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29289;&#20307;&#36890;&#24120;&#26377;&#36974;&#25377;&#34920;&#38754;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#19982;&#29289;&#20307;&#30340;&#29289;&#29702;&#20132;&#20114;&#26469;&#26500;&#24314;&#23427;&#20204;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#32473;&#23450;&#29289;&#20307;&#30340;&#23436;&#25972;&#19977;&#32500;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#38476;&#29983;&#26041;&#21521;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#26500;&#36896;&#30340;NeRF&#27169;&#22411;&#30340;&#38598;&#25104;&#26469;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#27493;&#34892;&#21160;&#65288;&#35270;&#35273;&#25110;&#37325;&#26032;&#23450;&#20301;&#34892;&#21160;&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#37327;&#21644;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25235;&#21462;&#21644;&#37325;&#26032;&#23450;&#20301;&#29289;&#20307;&#65292;&#37492;&#20110;&#20854;&#37096;&#20998;NeRF&#27169;&#22411;&#65292;&#24182;&#37325;&#26032;&#20272;&#35745;&#29289;&#20307;&#23039;&#24577;&#20197;&#32416;&#27491;&#20132;&#20114;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38169;&#20301;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23545;&#20351;&#29992;&#22522;&#20934;&#29289;&#20307;&#25805;&#20316;&#30340;Franka Emika&#26426;&#26800;&#25163;&#25805;&#20316;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;&#35270;&#35273;&#37325;&#24314;&#36136;&#37327;&#25552;&#39640;&#20102;14%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01812v1 Announce Type: cross  Abstract: Manipulating unseen objects is challenging without a 3D representation, as objects generally have occluded surfaces. This requires physical interaction with objects to build their internal representations. This paper presents an approach that enables a robot to rapidly learn the complete 3D model of a given object for manipulation in unfamiliar orientations. We use an ensemble of partially constructed NeRF models to quantify model uncertainty to determine the next action (a visual or re-orientation action) by optimizing informativeness and feasibility. Further, our approach determines when and how to grasp and re-orient an object given its partial NeRF model and re-estimates the object pose to rectify misalignments introduced during the interaction. Experiments with a simulated Franka Emika Robot Manipulator operating in a tabletop environment with benchmark objects demonstrate an improvement of (i) 14% in visual reconstruction quality
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#26679;&#26412;&#38656;&#27714;&#39640;&#21644;&#27010;&#24565;&#28418;&#31227;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01794</link><description>&lt;p&gt;
&#27169;&#20223;&#28216;&#25103;&#65306;&#22522;&#20110;&#27169;&#22411;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imitation Game: A Model-based and Imitation Learning Deep Reinforcement Learning Hybrid
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01794
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#35757;&#32451;&#26679;&#26412;&#38656;&#27714;&#39640;&#21644;&#27010;&#24565;&#28418;&#31227;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#23398;&#20064;&#31995;&#32479;&#24050;&#32463;&#34987;&#29282;&#22266;&#22320;&#30830;&#31435;&#20026;&#21019;&#24314;&#20855;&#26377;&#38887;&#24615;&#21644;&#39640;&#25928;&#33021;&#30340;&#32593;&#32476;&#33021;&#28304;&#31995;&#32479;&#26041;&#27861;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#26126;&#26174;&#38382;&#39064;&#65306;&#20687;Soft Actor Critic&#36825;&#26679;&#30340;&#29616;&#20195;&#26080;&#27169;&#22411;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#38656;&#35201;&#24212;&#23545;&#27010;&#24565;&#28418;&#31227;&#65288;&#20363;&#22914;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65289;&#30340;&#22791;&#29992;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#20195;&#29702;&#26550;&#26500;&#30340;&#36827;&#34892;&#20013;&#24037;&#20316;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01794v1 Announce Type: new  Abstract: Autonomous and learning systems based on Deep Reinforcement Learning have firmly established themselves as a foundation for approaches to creating resilient and efficient Cyber-Physical Energy Systems. However, most current approaches suffer from two distinct problems: Modern model-free algorithms such as Soft Actor Critic need a high number of samples to learn a meaningful policy, as well as a fallback to ward against concept drifts (e. g., catastrophic forgetting). In this paper, we present the work in progress towards a hybrid agent architecture that combines model-based Deep Reinforcement Learning with imitation learning to overcome both problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#26102;&#65292;20&#31181;&#26368;&#26032;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#20102;&#35299;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.01775</link><description>&lt;p&gt;
&#25151;&#38388;&#37324;&#30340;&#19968;&#21482;&#21557;&#38393;&#30340;&#22823;&#35937;&#65306;&#24744;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#23545;&#26631;&#31614;&#22122;&#38899;&#40065;&#26834;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#26102;&#65292;20&#31181;&#26368;&#26032;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#31163;&#32676;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#20102;&#35299;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20013;&#30340;&#38476;&#29983;&#25110;&#24847;&#22806;&#22270;&#20687;&#26159;&#30830;&#20445;&#23433;&#20840;&#37096;&#32626;&#30340;&#20851;&#38190;&#12290;&#22312;&#20998;&#31867;&#39046;&#22495;&#65292;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#39046;&#22495;&#22806;&#22270;&#20687;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#31163;&#32676;&#26816;&#27979;&#65288;OOD&#26816;&#27979;&#65289;&#12290;&#23613;&#31649;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21457;&#23637;&#20107;&#21518;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22312;&#22522;&#30784;&#20998;&#31867;&#22120;&#26410;&#32463;&#36807;&#24178;&#20928;&#12289;&#31934;&#24515;&#31579;&#36873;&#25968;&#25454;&#38598;&#35757;&#32451;&#26102;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#22914;&#20309;&#30340;&#35752;&#35770;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#22312;20&#31181;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#20013;&#26356;&#20026;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#20998;&#31867;&#22120;&#30340;&#26631;&#31614;&#19981;&#21487;&#38752;&#65288;&#20363;&#22914;&#65292;&#20247;&#21253;&#25110;&#32593;&#32476;&#25235;&#21462;&#26631;&#31614;&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#22122;&#38899;&#31867;&#22411;&#21644;&#32423;&#21035;&#12289;&#26550;&#26500;&#21644;&#26816;&#26597;&#28857;&#31574;&#30053;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31867;&#21035;&#26631;&#31614;&#22122;&#38899;&#23545;OOD&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01775v1 Announce Type: cross  Abstract: The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types &amp; levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show tha
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01768</link><description>&lt;p&gt;
&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;&#25991;&#26412;&#30340;&#38472;&#35268;&#26816;&#27979;&#21644;&#22522;&#20110;&#25506;&#27979;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Large Language Models for Enhanced Text-Based Stereotype Detection and Probing-Based Bias Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01768
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24314;&#31435;&#38472;&#35268;&#26816;&#27979;&#30340;&#22522;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#25968;&#25454;&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#38754;&#21521;&#20154;&#31867;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#20250;&#22797;&#21046;&#29978;&#33267;&#21152;&#21095;&#33258;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38472;&#35268;&#36755;&#20986;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Multi-Grain Stereotype&#65288;MGS&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;51,867&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#32844;&#19994;&#12289;&#23447;&#25945;&#21644;&#38472;&#35268;&#25991;&#26412;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#20010;&#20808;&#21069;&#20844;&#24320;&#30340;&#38472;&#35268;&#26816;&#27979;&#25968;&#25454;&#38598;&#25910;&#38598;&#32780;&#26469;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26088;&#22312;&#20026;&#38472;&#35268;&#26816;&#27979;&#24314;&#31435;&#22522;&#32447;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24494;&#35843;&#20102;&#22810;&#31181;&#26550;&#26500;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MGS&#35757;&#32451;&#30340;&#33521;&#25991;&#25991;&#26412;&#30340;&#38472;&#35268;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#20026;&#20102;&#20102;&#35299;&#25105;&#20204;&#30340;&#38472;&#35268;&#26816;&#27979;&#22120;&#26159;&#21542;&#25429;&#25417;&#21040;&#19982;&#20154;&#31867;&#24120;&#35782;&#19968;&#33268;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21508;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01768v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have significantly increased their presence in human-facing Artificial Intelligence (AI) applications. However, LLMs could reproduce and even exacerbate stereotypical outputs from training data. This work introduces the Multi-Grain Stereotype (MGS) dataset, encompassing 51,867 instances across gender, race, profession, religion, and stereotypical text, collected by fusing multiple previously publicly available stereotype detection datasets. We explore different machine learning approaches aimed at establishing baselines for stereotype detection, and fine-tune several language models of various architectures and model sizes, presenting in this work a series of stereotypes classifier models for English text trained on MGS. To understand whether our stereotype detectors capture relevant features (aligning with human common sense) we utilise a variety of explanainable AI tools, including 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PaR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25903;&#25345;&#65292;&#33021;&#22815;&#20462;&#22797;&#39640;&#32423;&#32534;&#31243;&#20316;&#19994;&#20013;&#30340;&#31243;&#24207;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2404.01754</link><description>&lt;p&gt;
Peer-aided Repairer: &#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#39640;&#32423;&#23398;&#29983;&#20316;&#19994;
&lt;/p&gt;
&lt;p&gt;
Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PaR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25903;&#25345;&#65292;&#33021;&#22815;&#20462;&#22797;&#39640;&#32423;&#32534;&#31243;&#20316;&#19994;&#20013;&#30340;&#31243;&#24207;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01754v1 &#36890;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#33258;&#21160;&#21270;&#29983;&#25104;&#38024;&#23545;&#32534;&#31243;&#20316;&#19994;&#30340;&#21453;&#39304;&#23545;&#32534;&#31243;&#25945;&#32946;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#39640;&#32423;&#20316;&#19994;&#26102;&#12290;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#20855;&#26377;&#20462;&#22797;&#20837;&#38376;&#20316;&#19994;&#28508;&#21147;&#32780;&#22791;&#21463;&#35748;&#21487;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#30340;&#31243;&#24207;&#30456;&#23545;&#31616;&#21333;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#20462;&#22797;&#26469;&#33258;&#39640;&#32423;&#32534;&#31243;&#35838;&#31243;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#39640;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Defects4DS&#30340;&#26032;&#39640;&#32423;&#23398;&#29983;&#20316;&#19994;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#20462;&#22797;&#39640;&#32423;&#20316;&#19994;&#20013;&#30340;&#38169;&#35823;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;PaR&#30340;&#30001;LLM&#39537;&#21160;&#30340;&#26694;&#26550;&#12290;PaR&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#36816;&#34892;&#65306;&#21516;&#34892;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#12289;&#22810;&#28304;&#25552;&#31034;&#29983;&#25104;&#21644;&#31243;&#24207;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01754v1 Announce Type: cross  Abstract: Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2404.01752</link><description>&lt;p&gt;
&#23433;&#20840;&#38388;&#38548;RRT*&#29992;&#20110;&#36830;&#32493;&#31354;&#38388;&#20013;&#21487;&#25193;&#23637;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01752
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65288;MRPP&#65289;&#38382;&#39064;&#20197;&#25214;&#21040;&#26080;&#20914;&#31361;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#38382;&#39064;&#30340;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#28041;&#21450;&#22810;&#20010;&#26426;&#22120;&#20154;&#20250;&#23548;&#33268;&#32452;&#21512;&#20915;&#31574;&#65292;&#20351;&#25628;&#32034;&#31354;&#38388;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20854;&#27425;&#65292;&#36830;&#32493;&#31354;&#38388;&#21576;&#29616;&#20986;&#28508;&#22312;&#26080;&#38480;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#26159;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#22120;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#65292;&#29992;&#20110;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#12290;&#39640;&#32423;&#21487;&#20197;&#20351;&#29992;&#33021;&#22815;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#21363;&#20248;&#20808;&#35268;&#21010;&#65288;SI-CPP&#65289;&#21644;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;SI-CCBS&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#12290;SI-CPP&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;SI-CCBS&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01752v1 Announce Type: cross  Abstract: In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#32422;&#26463;&#20248;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;</title><link>https://arxiv.org/abs/2404.01746</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#20132;&#20114;&#24863;&#30693;&#35268;&#21010;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable &amp; Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01746
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#32422;&#26463;&#20248;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#28041;&#21450;&#36710;&#36742;&#22312;&#25317;&#25380;&#20132;&#36890;&#22330;&#26223;&#20013;&#30456;&#20114;&#22797;&#26434;&#30340;&#20114;&#21160;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24847;&#35782;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#20114;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#12290;&#36825;&#20123;&#20132;&#20114;&#24863;&#30693;&#35268;&#21010;&#22120;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#25429;&#33719;&#36710;&#36742;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26088;&#22312;&#23558;&#36825;&#20123;&#39044;&#27979;&#19982;&#20256;&#32479;&#25511;&#21046;&#25216;&#26415;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30456;&#25972;&#21512;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#20256;&#32479;&#25511;&#21046;&#33539;&#24335;&#30340;&#25972;&#21512;&#24448;&#24448;&#23548;&#33268;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32422;&#26463;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#35757;&#32451;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#20943;&#36731;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#32500;&#25345;&#38382;&#39064;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01746v1 Announce Type: cross  Abstract: Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Highlight-CLIP&#26041;&#27861;&#65292;&#22312;&#35270;&#39057;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#20219;&#21153;&#20013;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#21019;&#26032;&#30340;&#26174;&#33879;&#24615;&#27744;&#21270;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01745</link><description>&lt;p&gt;
&#21457;&#25381;CLIP&#22312;&#35270;&#39057;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleash the Potential of CLIP for Video Highlight Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01745
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Highlight-CLIP&#26041;&#27861;&#65292;&#22312;&#35270;&#39057;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#20219;&#21153;&#20013;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#21019;&#26032;&#30340;&#26174;&#33879;&#24615;&#27744;&#21270;&#25216;&#26415;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#30340;&#21033;&#29992;&#26041;&#24335;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#37322;&#25918;&#20986;&#20102;&#26032;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#35270;&#39057;&#39046;&#22495;&#26174;&#33879;&#21463;&#30410;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Highlight-CLIP&#65288;HL-CLIP&#65289;&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#39044;&#35757;&#32451;&#30693;&#35782;&#22312;&#35270;&#39057;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#24494;&#35843;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#25105;&#20204;&#21019;&#26032;&#30340;&#26174;&#33879;&#24615;&#27744;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26681;&#25454;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#26368;&#20339;&#25104;&#32489;&#65292;&#27492;&#20219;&#21153;&#26159;QVHighlight Benchmark&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01745v1 Announce Type: cross  Abstract: Multimodal and large language models (LLMs) have revolutionized the utilization of open-world knowledge, unlocking novel potentials across various tasks and applications. Among these domains, the video domain has notably benefited from their capabilities. In this paper, we present Highlight-CLIP (HL-CLIP), a method designed to excel in the video highlight detection task by leveraging the pre-trained knowledge embedded in multimodal models. By simply fine-tuning the multimodal encoder in combination with our innovative saliency pooling technique, we have achieved the state-of-the-art performance in the highlight detection task, the QVHighlight Benchmark, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01741</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#32423;&#21453;&#39304;&#25511;&#21046;&#23454;&#29616;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;
&lt;/p&gt;
&lt;p&gt;
Intrusion Tolerance for Networked Systems through Two-Level Feedback Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26381;&#21153;&#22797;&#21046;&#21697;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#26412;&#22320;&#32423;&#21035;&#65292;&#33410;&#28857;&#25511;&#21046;&#22120;&#25191;&#34892;&#20837;&#20405;&#24674;&#22797;&#65292;&#22312;&#20840;&#23616;&#32423;&#21035;&#65292;&#31995;&#32479;&#25511;&#21046;&#22120;&#31649;&#29702;&#22797;&#21046;&#22240;&#23376;&#12290;&#26412;&#22320;&#21644;&#20840;&#23616;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#36816;&#31609;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#26356;&#25442;&#38382;&#39064;&#21644;&#24211;&#23384;&#34917;&#32473;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#27169;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21517;&#20026;TOLERANCE&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#25511;&#21046;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35745;&#31639;&#23427;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20223;&#30495;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;TOLERANCE&#65292;&#20854;&#20013;&#36816;&#34892;&#20102;10&#31181;&#32593;&#32476;&#20837;&#20405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#30456;&#27604;&#65292;TOLERANCE&#33021;&#22815;&#25552;&#39640;&#26381;&#21153;&#21487;&#29992;&#24615;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01741v1 Announce Type: cross  Abstract: We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21452;&#27169;&#24577;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#24369;&#30417;&#30563;&#38899;&#39057;&#20998;&#31163;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#27809;&#26377;&#21333;&#29420;&#28304;&#38899;&#39057;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#23454;&#29616;&#38899;&#39057;&#20449;&#21495;&#30340;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2404.01740</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#27169;&#24577;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#24369;&#30417;&#30563;&#38899;&#39057;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised Audio Separation via Bi-modal Semantic Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#21452;&#27169;&#24577;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#24369;&#30417;&#30563;&#38899;&#39057;&#20998;&#31163;&#30340;&#26694;&#26550;&#65292;&#21487;&#22312;&#27809;&#26377;&#21333;&#29420;&#28304;&#38899;&#39057;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#35821;&#35328;&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#23454;&#29616;&#38899;&#39057;&#20449;&#21495;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#38899;&#39057;&#28151;&#21512;&#29289;&#20013;&#30340;&#26465;&#20214;&#22768;&#38899;&#20998;&#31163;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#21333;&#28304;&#38899;&#39057;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28151;&#21512;&#19982;&#20998;&#31163;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23545;&#21333;&#28304;&#20998;&#31163;&#24773;&#20917;&#30340;&#30417;&#30563;&#20449;&#21495;&#32780;&#22312;&#22810;&#28304;&#35757;&#32451;&#28151;&#21512;&#29289;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#26465;&#20214;&#38899;&#39057;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#38899;&#39057;&#28151;&#21512;&#29289;&#30340;&#30456;&#24212;&#25991;&#26412;&#25551;&#36848;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#35821;&#35328;&#27169;&#24577;&#20013;&#38899;&#39057;&#26679;&#26412;&#30340;&#65288;&#31895;&#30053;&#65289;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21452;&#27169;&#24577;&#20998;&#31163;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#20197;&#20351;&#29992;&#22312;&#26465;&#20214;&#27169;&#24577;&#65288;&#21363;&#35821;&#35328;&#65289;&#20013;&#26131;&#20110;&#20998;&#31163;&#30340;&#23545;&#24212;&#20449;&#21495;&#26469;&#20998;&#31163;&#30446;&#26631;&#27169;&#24577;&#65288;&#21363;&#38899;&#39057;&#65289;&#20013;&#30340;&#21333;&#28304;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21333;&#28304;&#20449;&#21495;&#12290;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01740v1 Announce Type: cross  Abstract: Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01716</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21644;&#34701;&#21512;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective internal language model training and fusion for factorized transducer model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#30340;&#26377;&#25928;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#19982;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;ILM&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#23427;&#20027;&#35201;&#29992;&#20110;&#20272;&#35745;ILM&#20998;&#25968;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#38543;&#21518;&#34987;&#20943;&#21435;&#65292;&#20197;&#20419;&#36827;&#19982;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#38598;&#25104;&#12290;&#26368;&#36817;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26126;&#30830;&#37319;&#29992;&#29420;&#31435;&#30340;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#31354;&#30333;&#20196;&#29260;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#37319;&#29992;&#20102;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#65292;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ILM&#35757;&#32451;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20998;&#35299;&#36716;&#24405;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#31354;&#30333;&#12289;&#22768;&#23398;&#21644;ILM&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ILM&#21644;&#25152;&#25552;&#20986;&#30340;&#35299;&#30721;&#31574;&#30053;&#26102;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#35299;&#30721;&#26041;&#27861;&#65292;&#26377;17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#19982;&#24378;RNN-T ba&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.01713</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#65306;&#36890;&#36807;6G&#25506;&#32034;&#24863;&#30693;&#20114;&#32852;&#32593;&#30340;&#19979;&#19968;&#20010;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#29289;&#32852;&#32593;(IoT)&#24050;&#32463;&#26159;&#19968;&#20010;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#27010;&#24565;&#65292;&#24403;&#25105;&#20204;&#36924;&#36817;2030&#24180;&#26102;&#65292;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#34987;&#31216;&#20026;&#24863;&#30693;&#20114;&#32852;&#32593;(IoS)&#27491;&#22312;&#20852;&#36215;&#12290;&#19982;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#19981;&#21516;&#65292;IoS&#26088;&#22312;&#25552;&#20379;&#22810;&#24863;&#23448;&#20307;&#39564;&#65292;&#35748;&#35782;&#21040;&#22312;&#25105;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#30340;&#24863;&#30693;&#36828;&#19981;&#27490;&#20110;&#35270;&#35273;&#21644;&#21548;&#35273;&#65307;&#23427;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#24863;&#35273;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#21160;&#27785;&#28024;&#24335;&#22810;&#24863;&#23448;&#23186;&#20307;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#21151;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#36825;&#39033;&#25506;&#32034;&#21253;&#25324;&#20256;&#32479;&#27785;&#28024;&#24335;&#23186;&#20307;&#27969;&#19982;&#19968;&#20010;&#25552;&#20986;&#30340;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#35821;&#20041;&#20132;&#27969;&#30340;&#29992;&#20363;&#20043;&#38388;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#39033;&#20998;&#26512;&#30340;&#37325;&#28857;&#26159;&#25152;&#25552;&#26041;&#26696;&#20013;&#24102;&#23485;&#28040;&#32791;&#20943;&#23569;&#20102;99.93%&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#26088;&#22312;&#24378;&#35843;&#35813;&#23454;&#29992;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01713v1 Announce Type: cross  Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical appli
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#37319;&#26679;&#25351;&#23548;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#26356;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#20381;&#36182;&#22806;&#37096;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01709</link><description>&lt;p&gt;
Upsample Guidance: &#19981;&#32463;&#36807;&#35757;&#32451;&#21363;&#21487;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Upsample Guidance: Scale Up Diffusion Models without Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#37319;&#26679;&#25351;&#23548;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#26356;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#20381;&#36182;&#22806;&#37096;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21253;&#25324;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#31561;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#30452;&#25509;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#26679;&#26412;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20462;&#25913;&#26550;&#26500;&#12289;&#36827;&#19968;&#27493;&#35757;&#32451;&#25110;&#23558;&#37319;&#26679;&#36807;&#31243;&#21010;&#20998;&#20026;&#22810;&#20010;&#38454;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#38656;&#35201;&#39069;&#22806;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#37319;&#26679;&#25351;&#23548;&#65292;&#36825;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#20165;&#28155;&#21152;&#19968;&#20010;&#39033;&#65292;&#20351;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;$512^2$&#65289;&#33021;&#22815;&#29983;&#25104;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;$1536^2$&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#25216;&#26415;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#25110;&#20381;&#36182;&#22806;&#37096;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19978;&#37319;&#26679;&#25351;&#23548;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#27169;&#22411;&#65292;&#22914;&#20687;&#32032;&#31354;&#38388;&#12289;&#28508;&#22312;&#31354;&#38388;&#21644;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01709v1 Announce Type: cross  Abstract: Diffusion models have demonstrated superior performance across various generative tasks including images, videos, and audio. However, they encounter difficulties in directly generating high-resolution samples. Previously proposed solutions to this issue involve modifying the architecture, further training, or partitioning the sampling process into multiple stages. These methods have the limitation of not being able to directly utilize pre-trained models as-is, requiring additional work. In this paper, we introduce upsample guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to generate higher-resolution images (e.g., $1536^2$) by adding only a single term in the sampling process. Remarkably, this technique does not necessitate any additional training or relying on external models. We demonstrate that upsample guidance can be applied to various models, such as pixel-space, latent space, and video diffusion model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2404.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#24402;&#32467;&#21453;&#39539;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36890;&#29992;&#19988;&#21487;&#38752;&#30340;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizable and Faithful Logic Reasoning over Natural Language via Resolution Refutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01677
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GFaiR&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#65292;&#24182;&#36890;&#36807;&#35777;&#26126;&#25554;&#20837;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#20102;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#26377;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#21487;&#27867;&#21270;&#19988;&#21487;&#38752;&#25512;&#29702;&#22120;&#65288;GFaiR&#65289;&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#24402;&#32467;&#21453;&#39539;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01677v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved significant performance in various natural language reasoning tasks. However, they still struggle with performing first-order logic reasoning over formal logical theories expressed in natural language. This is because the previous LLMs-based reasoning systems have the theoretical incompleteness issue. As a result, it can only address a limited set of simple reasoning problems, which significantly decreases their generalization ability. To address this issue, we propose a novel framework, named Generalizable and Faithful Reasoner (GFaiR), which introduces the paradigm of resolution refutation. Resolution refutation has the capability to solve all first-order logic reasoning problems by extending reasoning rules and employing the principle of proof by contradiction, so our system's completeness can be improved by introducing resolution refutation. Experimental results demonstrate that our system o
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01657</link><description>&lt;p&gt;
&#21457;&#24067;&#38024;&#23545;&#26085;&#35821;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Release of Pre-Trained Models for the Japanese Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01657
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#26085;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#32553;&#23567;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#30340;AI&#35775;&#38382;&#24046;&#36317;&#65292;&#20419;&#36827;AI&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: AI&#27665;&#20027;&#21270;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26222;&#36890;&#20154;&#21487;&#20197;&#21033;&#29992;AI&#25216;&#26415;&#30340;&#19990;&#30028;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#35797;&#22270;&#35753;&#20182;&#20204;&#30340;&#32467;&#26524;&#23545;&#20844;&#20247;&#21487;&#21450;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#30340;&#21457;&#24067;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21457;&#24067;&#30340;&#27169;&#22411;&#19987;&#38376;&#38024;&#23545;&#33521;&#35821;&#65292;&#22240;&#27492;&#65292;&#22312;&#38750;&#33521;&#35821;&#31038;&#21306;&#20013;&#65292;AI&#27665;&#20027;&#21270;&#23384;&#22312;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#32553;&#23567;AI&#35775;&#38382;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#29992;&#26085;&#35821;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#12289;&#23545;&#27604;&#35821;&#35328;&#21644;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#12289;&#31283;&#23450;&#25193;&#25955;&#21644;&#38544;&#34255;&#21333;&#20803;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65288;HuBERT&#65289;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20123;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#33258;&#30001;&#22320;&#19982;&#31526;&#21512;&#26085;&#26412;&#25991;&#21270;&#20215;&#20540;&#35266;&#30340;AI&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#30830;&#20445;&#26085;&#26412;&#25991;&#21270;&#30340;&#36523;&#20221;&#65292;&#20174;&#32780;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01657v1 Announce Type: cross  Abstract: AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#26469;&#37327;&#21270;&#24085;&#37329;&#26862;&#27663;&#30149;&#24739;&#32773;MDS-UPDRS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20154;&#20307;&#23039;&#21183;&#22270;&#20687;&#24182;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#24555;&#36895;&#19982;&#31616;&#20415;&#30340;&#30149;&#24773;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2404.01654</link><description>&lt;p&gt;
AI WALKUP&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#24085;&#37329;&#26862;&#27663;&#30149;&#24739;&#32773;MDS-UPDRS&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in Parkinson's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#26469;&#37327;&#21270;&#24085;&#37329;&#26862;&#27663;&#30149;&#24739;&#32773;MDS-UPDRS&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20154;&#20307;&#23039;&#21183;&#22270;&#20687;&#24182;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#24555;&#36895;&#19982;&#31616;&#20415;&#30340;&#30149;&#24773;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#27663;&#30149;&#65288;PD&#65289;&#26159;&#31532;&#20108;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#29616;&#26377;&#30340;PD&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;-&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920;&#65288;MDS-UPDRS&#65289;&#26469;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#30340;&#36816;&#21160;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#21644;&#30142;&#30149;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35780;&#20272;&#23384;&#22312;&#20027;&#35266;&#24615;&#39640;&#12289;&#32570;&#20047;&#19968;&#33268;&#24615;&#12289;&#25163;&#21160;&#27807;&#36890;&#25104;&#26412;&#39640;&#21644;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25668;&#20687;&#22836;&#25429;&#25417;&#20154;&#20307;&#23039;&#21183;&#22270;&#20687;&#65292;&#21033;&#29992;&#31639;&#27861;&#37325;&#24314;&#24182;&#36827;&#34892;&#36816;&#21160;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#25552;&#21462;&#36816;&#21160;&#37327;&#29305;&#24449;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#37096;&#32626;&#22312;&#19981;&#21516;&#30340;&#26234;&#33021;&#25163;&#26426;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#24555;&#36895;&#36731;&#26494;&#22320;&#36827;&#34892;&#35270;&#39057;&#24405;&#21046;&#21644;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01654v1 Announce Type: cross  Abstract: Parkinson's Disease (PD) is the second most common neurodegenerative disorder. The existing assessment method for PD is usually the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to assess the severity of various types of motor symptoms and disease progression. However, manual assessment suffers from high subjectivity, lack of consistency, and high cost and low efficiency of manual communication. We want to use a computer vision based solution to capture human pose images based on a camera, reconstruct and perform motion analysis using algorithms, and extract the features of the amount of motion through feature engineering. The proposed approach can be deployed on different smartphones, and the video recording and artificial intelligence analysis can be done quickly and easily through our APP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#25361;&#25112;&#22312;&#20110;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#35760;&#24518;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01652</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#35299;&#19978;&#19979;&#25991;&#35760;&#24518;&#23454;&#29616;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#25361;&#25112;&#22312;&#20110;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#35760;&#24518;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OpenQA&#65289;&#26088;&#22312;&#21033;&#29992;&#22806;&#37096;&#22823;&#35268;&#27169;&#30693;&#35782;&#35821;&#26009;&#24211;&#22238;&#31572;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30693;&#35782;&#24182;&#38750;&#38745;&#24577;&#30340;&#65307;&#23427;&#19981;&#26029;&#26356;&#26032;&#21644;&#28436;&#21464;&#12290;&#36825;&#31181;&#30693;&#35782;&#30340;&#21160;&#24577;&#29305;&#24615;&#20026;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#26368;&#26032;&#20449;&#24687;&#65292;&#20197;&#30830;&#20445;&#31572;&#26696;&#20445;&#25345;&#20934;&#30830;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#24320;&#25918;&#22495;&#38382;&#31572;&#27169;&#22411;&#33021;&#22815;&#22810;&#22909;&#22320;&#36716;&#31227;&#21040;&#23436;&#20840;&#26032;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;QA&#27169;&#22411;&#22312;&#20004;&#31181;&#20855;&#20307;&#24773;&#26223;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;1&#65289;&#36866;&#24212;&#30456;&#21516;&#30693;&#35782;&#35821;&#26009;&#24211;&#30340;&#26356;&#26032;&#29256;&#26412;&#65307;2&#65289;&#36716;&#25442;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#30693;&#35782;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24320;&#25918;&#22495;&#38382;&#31572;&#27169;&#22411;&#30340;&#27867;&#21270;&#25361;&#25112;&#28304;&#33258;&#38405;&#35835;&#22120;&#36807;&#24230;&#20381;&#36182;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#35760;&#24518;&#30693;&#35782;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01652v1 Announce Type: cross  Abstract: Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader's over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generaliz
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;CT&#25195;&#25551;&#20013;&#23384;&#22312;&#30340;&#21464;&#24322;&#24615;&#21644;OOD&#20999;&#29255;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28388;&#38500;OOD&#25968;&#25454;&#21644;&#20943;&#23569;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;</title><link>https://arxiv.org/abs/2404.01643</link><description>&lt;p&gt;
&#23545;COVID-19&#26816;&#27979;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#36827;&#34892;&#26356;&#32454;&#33268;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01643
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;CT&#25195;&#25551;&#20013;&#23384;&#22312;&#30340;&#21464;&#24322;&#24615;&#21644;OOD&#20999;&#29255;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28388;&#38500;OOD&#25968;&#25454;&#21644;&#20943;&#23569;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25104;&#20687;&#35782;&#21035;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#27599;&#20010;CT&#25195;&#25551;&#30340;&#20998;&#36776;&#29575;&#21644;&#22823;&#23567;&#32463;&#24120;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#24615;&#65292;&#38656;&#35201;&#23545;&#36755;&#20837;&#23610;&#23544;&#21644;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#26377;&#20005;&#26684;&#30340;&#35201;&#27714;&#12290; &#65288;2&#65289;CT&#25195;&#25551;&#21253;&#21547;&#22823;&#37327;&#30340;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#20999;&#29255;&#12290;&#20851;&#38190;&#29305;&#24615;&#21487;&#33021;&#20165;&#23384;&#22312;&#20110;&#25972;&#20010;CT&#25195;&#25551;&#30340;&#29305;&#23450;&#31354;&#38388;&#21306;&#22495;&#21644;&#20999;&#29255;&#20013;&#12290;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#25214;&#20986;&#36825;&#20123;&#21306;&#22495;&#30340;&#20301;&#32622;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;CT&#25195;&#25551;&#30340;&#22686;&#24378;&#22411;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#65288;SSFL ++&#65289;&#26694;&#26550;&#12290;&#23427;&#26088;&#22312;&#20174;&#25972;&#20010;CT&#25195;&#25551;&#20013;&#28388;&#38500;OOD&#25968;&#25454;&#65292;&#36890;&#36807;&#23436;&#20840;&#20943;&#23569;70&#65285;&#30340;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#65288;KDS&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01643v1 Announce Type: cross  Abstract: Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of out-of-distribution (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#65292;&#20855;&#26377;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#12289;&#22870;&#21169;&#35774;&#35745;&#21644;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;&#36880;&#27493;&#25913;&#21892;&#31561;&#22235;&#22823;&#21019;&#26032;&#36129;&#29486;</title><link>https://arxiv.org/abs/2404.01636</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Learning to Control Camera Exposure via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#65292;&#20855;&#26377;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#12289;&#22870;&#21169;&#35774;&#35745;&#21644;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;&#36880;&#27493;&#25913;&#21892;&#31561;&#22235;&#22823;&#21019;&#26032;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#25668;&#20687;&#26426;&#26333;&#20809;&#26159;&#30830;&#20445;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#21151;&#33021;&#30340;&#31532;&#19968;&#27493;&#12290;&#20256;&#32479;&#30340;&#25668;&#20687;&#26426;&#26333;&#20809;&#25511;&#21046;&#26041;&#27861;&#38656;&#35201;&#22810;&#27425;&#25910;&#25947;&#21644;&#32791;&#26102;&#30340;&#27969;&#31243;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#21160;&#24577;&#29031;&#26126;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24555;&#36895;&#25511;&#21046;&#25668;&#20687;&#26426;&#26333;&#20809;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22235;&#20010;&#36129;&#29486;&#65306;1&#65289;&#31616;&#21270;&#35757;&#32451;&#22330;&#22320;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#29031;&#26126;&#21464;&#21270;&#65292;2&#65289;&#38378;&#28865;&#21644;&#22270;&#20687;&#23646;&#24615;&#24863;&#30693;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#21450;&#36731;&#37327;&#32423;&#29366;&#24577;&#35774;&#35745;&#20197;&#36827;&#34892;&#23454;&#26102;&#22788;&#29702;&#65292;3&#65289;&#38745;&#24577;&#21040;&#21160;&#24577;&#29031;&#26126;&#35838;&#31243;&#65292;&#36880;&#27493;&#25913;&#21892;&#20195;&#29702;&#30340;&#26333;&#20809;&#35843;&#25972;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01636v1 Announce Type: cross  Abstract: Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes, making them unsuitable for dynamic lighting conditions. In this paper, we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes, 2) flickering and image attribute-aware reward design, along with lightweight state design for real-time processing, 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capabi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#24341;&#20837;&#31070;&#32463;&#22349;&#32553;&#26469;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#32467;&#26500;&#65292;&#36890;&#36807;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#65292;&#20351;&#24471;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#27969;&#25968;&#25454;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01628</link><description>&lt;p&gt;
&#23398;&#20064;&#31561;&#35282;&#34920;&#31034;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Equi-angular Representations for Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01628
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#24341;&#20837;&#31070;&#32463;&#22349;&#32553;&#26469;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;&#32467;&#26500;&#65292;&#36890;&#36807;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#65292;&#20351;&#24471;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#27969;&#25968;&#25454;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#23384;&#22312;&#27424;&#25311;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#30001;&#20110;&#19981;&#21450;&#26102;&#30340;&#27169;&#22411;&#26356;&#26032;&#22521;&#35757;&#19981;&#36275;&#65288;&#20363;&#22914;&#65292;&#21333;&#21608;&#26399;&#35757;&#32451;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35825;&#23548;&#31070;&#32463;&#22349;&#32553;&#24418;&#25104;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#21333;&#32431;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#32467;&#26500;&#65292;&#20197;&#20415;&#36890;&#36807;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#25552;&#20986;&#39044;&#22791;&#25968;&#25454;&#35757;&#32451;&#21644;&#27531;&#24046;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#20351;&#32463;&#36807;&#21333;&#21608;&#26399;&#23398;&#20064;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#27969;&#23186;&#20307;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;CIFAR-10/100&#12289;TinyImageNet&#12289;ImageNet-200&#21644;ImageNet-1K&#36827;&#34892;&#22823;&#37327;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#65288;&#22914;&#19981;&#30456;&#20132;&#21644;&#39640;&#26031;&#35843;&#24230;&#36830;&#32493;&#65288;&#21363;&#26080;&#36793;&#30028;&#65289;&#25968;&#25454;&#35774;&#32622;&#65289;&#20013;&#22343;&#36739;&#39046;&#20808;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01628v1 Announce Type: cross  Abstract: Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#21465;&#20107;&#22312;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#36992;&#35831;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#19968;&#36215;&#25506;&#35752;&#29983;&#25104;AI&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01622</link><description>&lt;p&gt;
Gen4DS&#65306;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#25968;&#25454;&#21465;&#20107;&#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
Gen4DS: Workshop on Data Storytelling in an Era of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01622
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#21465;&#20107;&#22312;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#36992;&#35831;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#19968;&#36215;&#25506;&#35752;&#29983;&#25104;AI&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#26159;&#19968;&#31181;&#21476;&#32769;&#32780;&#29645;&#36149;&#30340;&#20154;&#31867;&#33021;&#21147;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#24471;&#21040;&#20102;&#26032;&#29983;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#25968;&#25454;&#21465;&#20107;&#30340;&#35748;&#21487;&#21644;&#24212;&#29992;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;AI&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36825;&#19968;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#35768;&#22810;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#36992;&#35831;&#22823;&#23478;&#21442;&#21152;&#25105;&#20204;&#30340;&#30740;&#35752;&#20250;&#65288;Gen4DS&#65289;&#65292;&#35752;&#35770;&#29983;&#25104;AI&#22914;&#20309;&#20419;&#36827;&#25968;&#25454;&#21465;&#20107;&#30340;&#21019;&#20316;&#65311;&#29983;&#25104;AI&#22914;&#20309;&#25913;&#21464;&#25968;&#25454;&#21465;&#20107;&#32773;&#30340;&#24037;&#20316;&#27969;&#31243;&#65311;&#22312;&#21465;&#20107;&#20013;&#21152;&#20837;AI&#30340;&#39118;&#38505;&#21644;&#38544;&#24739;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01622v1 Announce Type: cross  Abstract: Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (i
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01620</link><description>&lt;p&gt;
Voice EHR:&#24341;&#20837;&#22810;&#27169;&#24335;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Voice EHR: Introducing Multimodal Audio Data for Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24555;&#36895;&#20998;&#31867;&#24739;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#65292;&#24182;&#21487;&#33021;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#25913;&#21892;&#32467;&#26524;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#39640;&#25910;&#20837;&#12289;&#33521;&#35821;&#22269;&#23478;&#20351;&#29992;&#26114;&#36149;&#35760;&#24405;&#35774;&#22791;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#25216;&#26415;&#38754;&#20020;&#36164;&#28304;&#21463;&#38480;&#12289;&#39640;&#25910;&#20837;&#22330;&#25152;&#30340;&#37096;&#32626;&#25361;&#25112;&#65292;&#38899;&#39057;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#25910;&#38598;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20165;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;/&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#23427;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#20256;&#32479;&#35821;&#38899;/&#21628;&#21560;&#29305;&#24449;&#12289;&#35821;&#38899;&#27169;&#24335;&#21644;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#35821;&#35328;&#30340;&#22797;&#26434;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34917;&#20607;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#20249;&#20276;&#36130;&#22242;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01620v1 Announce Type: cross  Abstract: Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partner
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01602</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#30340;&#33333;&#25163;&#65311;&#35780;&#20272;&#20854;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29436;&#20154;&#28216;&#25103;&#27169;&#25311;&#24179;&#21488;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35266;&#28857;&#39046;&#23548;&#20316;&#29992;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#38590;&#24536;&#30340;&#25112;&#30053;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;LLM&#20195;&#29702;&#25152;&#23637;&#31034;&#30340;&#35266;&#28857;&#39046;&#23548;&#21147;&#30340;&#37325;&#35201;&#24615;&#34987;&#24573;&#35270;&#20102;&#65292;&#32780;&#36825;&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#35774;&#32622;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29436;&#20154;&#28216;&#25103;&#20316;&#20026;&#27169;&#25311;&#24179;&#21488;&#65292;&#35780;&#20272;LLMs&#30340;&#35266;&#28857;&#24341;&#39046;&#20316;&#29992;&#12290;&#35813;&#28216;&#25103;&#20013;&#26377;&#35686;&#38271;&#35282;&#33394;&#65292;&#36127;&#36131;&#24635;&#32467;&#35770;&#25454;&#24182;&#25512;&#33616;&#20915;&#31574;&#36873;&#39033;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#20449;&#20195;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35686;&#38271;&#35282;&#33394;&#30340;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;&#35266;&#28857;&#39046;&#34966;&#20851;&#38190;&#29305;&#24449;&#30340;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#35266;&#28857;&#39046;&#34966;&#30340;&#21487;&#38752;&#24615;&#65292;&#31532;&#20108;&#20010;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01602v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game features the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics for evaluation based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#21160;&#20316;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#36827;&#34892;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#65292;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01598</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#30340;&#26497;&#20540;&#23547;&#25214;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Extremum-Seeking Action Selection for Accelerating Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#21160;&#20316;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#36827;&#34892;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#65292;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#19978;&#36827;&#34892;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#39640;&#29109;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#65292;&#29992;&#20110;&#23616;&#37096;&#25506;&#32034;&#21644;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#20197;&#20248;&#21270;&#24615;&#33021;&#12290;&#35768;&#22810;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#28041;&#21450;&#22797;&#26434;&#19981;&#31283;&#23450;&#30340;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#26045;&#21152;&#22312;&#21487;&#34892;&#25511;&#21046;&#27969;&#24418;&#20043;&#22806;&#30340;&#21160;&#20316;&#24456;&#24555;&#20250;&#23548;&#33268;&#19981;&#33391;&#21457;&#25955;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#20174;&#29615;&#22659;&#21160;&#20316;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#26679;&#26412;&#29983;&#25104;&#30340;&#36712;&#36857;&#20215;&#20540;&#36739;&#20302;&#65292;&#20960;&#20046;&#27809;&#26377;&#36129;&#29486;&#20110;&#31574;&#30053;&#25913;&#36827;&#65292;&#23548;&#33268;&#23398;&#20064;&#32531;&#24930;&#25110;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#36825;&#31181;&#26080;&#27169;&#22411;RL&#35774;&#32622;&#20013;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26497;&#20540;&#23547;&#25214;&#25511;&#21046;&#65288;ESC&#65289;&#30340;&#38468;&#21152;&#33258;&#36866;&#24212;&#25511;&#21046;&#27493;&#39588;&#26469;&#25913;&#21892;&#21160;&#20316;&#36873;&#25321;&#12290;&#23545;&#20110;&#20174;&#38543;&#26426;&#31574;&#30053;&#37319;&#26679;&#30340;&#27599;&#20010;&#21160;&#20316;&#65292;&#25105;&#20204;&#24212;&#29992;&#27491;&#24358;&#25200;&#21160;&#24182;&#26597;&#35810;&#20272;&#35745;&#30340;Q&#20540;&#20316;&#20026;&#21709;&#24212;&#20449;&#21495;&#12290;&#26681;&#25454;ESC&#65292;&#25105;&#20204;&#21160;&#24577;&#25913;&#36827;&#37319;&#26679;&#30340;&#21160;&#20316;&#20197;&#20351;&#20043;&#26356;&#25509;&#36817;&#29702;&#24819;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01598v1 Announce Type: cross  Abstract: Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer 
&lt;/p&gt;</description></item><item><title>PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01596</link><description>&lt;p&gt;
PhysORD&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#36234;&#37326;&#39550;&#39542;&#20013;&#27880;&#20837;&#29289;&#29702;&#23398;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01596
&lt;/p&gt;
&lt;p&gt;
PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#36234;&#37326;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#19982;&#22312;&#36947;&#36335;&#19978;&#39550;&#39542;&#30456;&#27604;&#65292;&#23427;&#38754;&#20020;&#30528;&#26356;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36710;&#36742;&#19982;&#22320;&#24418;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#21644;&#22806;&#37096;&#24178;&#25200;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#22522;&#26412;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#29289;&#29702;&#23450;&#24459;&#23884;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#36234;&#37326;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986; PhysORD&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#23432;&#24658;&#23450;&#24459;&#65292;&#21363;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01596v1 Announce Type: cross  Abstract: Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;</title><link>https://arxiv.org/abs/2404.01589</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#28304;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30284;&#30151;&#20998;&#26399;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Cancer Stage with Open-Source Clinical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#20998;&#26399;&#20998;&#31867;&#23545;&#20110;&#20026;&#32959;&#30244;&#23398;&#24739;&#32773;&#21046;&#23450;&#27835;&#30103;&#21644;&#25252;&#29702;&#31649;&#29702;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#26399;&#20449;&#24687;&#36890;&#24120;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#21253;&#21547;&#22312;&#20020;&#24202;&#12289;&#30149;&#29702;&#23398;&#12289;&#25918;&#23556;&#23398;&#21644;&#20854;&#20182;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#20013;&#65292;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#35299;&#26512;&#21644;&#33719;&#21462;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20808;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20934;&#22791;&#36215;&#26469;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#28304;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#20174;&#30495;&#23454;&#30149;&#29702;&#25253;&#21578;&#20013;&#25552;&#21462;&#30149;&#29702;&#24615;&#32959;&#30244;-&#28107;&#24052;&#32467;-&#36716;&#31227;&#65288;pTNM&#65289;&#20998;&#26399;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#27604;&#36739;&#20102;LLMs&#21644;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;LLMs&#22312;&#32959;&#30244;&#65288;T&#65289;&#20998;&#31867;&#26041;&#38754;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#36866;&#24403;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#20204;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01589v1 Announce Type: cross  Abstract: Cancer stage classification is important for making treatment and care management plans for oncology patients. Information on staging is often included in unstructured form in clinical, pathology, radiology and other free-text reports in the electronic health record system, requiring extensive work to parse and obtain. To facilitate the extraction of this information, previous NLP approaches rely on labeled training datasets, which are labor-intensive to prepare. In this study, we demonstrate that without any labeled training data, open-source clinical large language models (LLMs) can extract pathologic tumor-node-metastasis (pTNM) staging information from real-world pathology reports. Our experiments compare LLMs and a BERT-based model fine-tuned using the labeled data. Our findings suggest that while LLMs still exhibit subpar performance in Tumor (T) classification, with the appropriate adoption of prompting strategies, they can achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.01588</link><description>&lt;p&gt;
&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#25688;&#35201;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Diversity-Aware Active Learning for Text Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24187;&#35273;&#22810;&#26679;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#29983;&#25104;&#24187;&#35273;&#36755;&#20986;&#30340;&#20542;&#21521;&#65292;&#21363;&#22312;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#25110;&#19981;&#25903;&#25345;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#24187;&#35273;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#26469;&#35782;&#21035;&#21644;&#32416;&#27491;LLMs&#36755;&#20986;&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#20363;&#22914;&#23454;&#20307;&#25110;&#26631;&#35760;&#38169;&#35823;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;LLMs&#36755;&#20986;&#20013;&#23637;&#31034;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLMs&#24187;&#35273;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#38477;&#20302;&#20102;&#23545;&#24187;&#35273;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#31867;&#27880;&#37322;&#12290;&#36890;&#36807;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#34913;&#37327;&#35821;&#20041;&#26694;&#26550;&#12289;&#35758;&#35770;&#21644;&#20869;&#23481;&#21487;&#39564;&#35777;&#24615;&#38169;&#35823;&#20013;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HAllucination Diversity-Aware Sampling&#65288;HADAS&#65289;&#26469;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#24187;&#35273;&#65292;&#20197;&#20379;LLM&#24494;&#35843;&#30340;&#20027;&#21160;&#23398;&#20064;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2404.01582</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#22686;&#24378;&#30340;&#20316;&#19994;&#25220;&#34989;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#39640;&#27700;&#24179;&#25220;&#34989;&#26816;&#27979;&#30740;&#31350;&#25968;&#25454;&#38598;&#32570;&#22833;&#30340;&#31354;&#30333;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#20219;&#21153;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#20174;&#20854;&#20182;&#25991;&#26412;&#20013;&#25220;&#34989;&#25110;&#22797;&#21046;&#30340;&#20869;&#23481;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#27979;&#39640;&#27700;&#24179;&#30340;&#25220;&#34989;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-3.5&#30340;&#25220;&#34989;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#20135;&#29983;&#20102;32,927&#23545;&#25991;&#26412;&#25220;&#34989;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#25220;&#34989;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Faiss&#21644;BERT&#30340;&#39640;&#25928;&#39640;&#20934;&#30830;&#24615;&#30340;&#25220;&#34989;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;98.86&#65285;&#12289;98.90&#65285;&#12289;98.86&#65285;&#21644;0.9888&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#24179;&#21488;&#65292;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2404.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models Using Contrast Sets: An Experimental Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01569
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#24230;&#37327;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#38169;&#35823;&#24230;&#37327;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#35813;&#24230;&#37327;&#22312;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35821;&#21477;&#34164;&#28085;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#28041;&#21450;&#33258;&#21160;&#23558;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#26367;&#25442;&#20026;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#65292;&#20197;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#22312;&#20256;&#32479;&#30340;SNLI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.9%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;72.5%&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
&lt;/p&gt;</description></item><item><title>&#21033;&#29992; GPT-4.0 &#24320;&#21457;&#20102;&#24037;&#20855; "GeneUS"&#65292;&#23454;&#29616;&#20102;&#20174;&#38656;&#27714;&#25991;&#26723;&#33258;&#21160;&#29983;&#25104;&#29992;&#25143;&#25925;&#20107;&#30340;&#33258;&#21160;&#21270;&#65292;&#20026;&#21518;&#32493;&#38598;&#25104;&#21040;&#39033;&#30446;&#31649;&#29702;&#24037;&#20855;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01558</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#29992;&#25143;&#25925;&#20107;&#29983;&#25104;&#19982;&#27979;&#35797;&#29992;&#20363;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Automated User Story Generation with Test Case Specification Using Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01558
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992; GPT-4.0 &#24320;&#21457;&#20102;&#24037;&#20855; "GeneUS"&#65292;&#23454;&#29616;&#20102;&#20174;&#38656;&#27714;&#25991;&#26723;&#33258;&#21160;&#29983;&#25104;&#29992;&#25143;&#25925;&#20107;&#30340;&#33258;&#21160;&#21270;&#65292;&#20026;&#21518;&#32493;&#38598;&#25104;&#21040;&#39033;&#30446;&#31649;&#29702;&#24037;&#20855;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#27491;&#22312;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24555;&#36895;&#21457;&#23637;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#33258;&#21160;&#21270;&#36719;&#20214;&#24320;&#21457;&#24037;&#20316;&#27969;&#30340;&#35768;&#22810;&#37096;&#20998;&#12290;&#38656;&#27714;&#24037;&#31243;&#65288;RE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#36890;&#36807;&#23545;&#25552;&#20986;&#30340;&#24037;&#20316;&#33539;&#22260;&#36827;&#34892;&#22810;&#27425;&#35752;&#35770;&#24182;&#20197;&#19981;&#21516;&#24418;&#24335;&#35760;&#24405;&#26469;&#24320;&#22987;&#36719;&#20214;&#24320;&#21457;&#21608;&#26399;&#12290;RE &#38454;&#27573;&#20197;&#27599;&#20010;&#21333;&#20803;&#20219;&#21153;&#30340;&#29992;&#25143;&#25925;&#20107;&#28165;&#21333;&#32467;&#26463;&#65292;&#36825;&#20123;&#29992;&#25143;&#25925;&#20107;&#26159;&#36890;&#36807;&#35752;&#35770;&#30830;&#23450;&#30340;&#65292;&#36890;&#24120;&#22312;&#39033;&#30446;&#31649;&#29702;&#24037;&#20855;&#65288;&#22914; Jira&#12289;AzurDev &#31561;&#65289;&#19978;&#21019;&#24314;&#21644;&#36319;&#36394;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992; GPT-4.0 &#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;GeneUS&#8221;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26681;&#25454;&#38656;&#27714;&#25991;&#26723;&#33258;&#21160;&#21019;&#24314;&#29992;&#25143;&#25925;&#20107;&#65292;&#36825;&#26159; RE &#38454;&#27573;&#30340;&#20135;&#29289;&#12290;&#36755;&#20986;&#20197; JSON &#26684;&#24335;&#25552;&#20379;&#65292;&#20026;&#21518;&#32493;&#38598;&#25104;&#21040;&#27969;&#34892;&#39033;&#30446;&#31649;&#29702;&#24037;&#20855;&#30041;&#19979;&#21487;&#33021;&#24615;&#12290;&#20998;&#26512;&#38656;&#27714;&#25991;&#26723;&#21344;&#25454;&#30528;&#37325;&#35201;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01558v1 Announce Type: cross  Abstract: Modern Software Engineering era is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating many parts of the software development workflow. Requirements Engineering (RE) is a crucial phase that begins the software development cycle through multiple discussions on a proposed scope of work documented in different forms. RE phase ends with a list of user-stories for each unit task identified through discussions and usually these are created and tracked on a project management tool such as Jira, AzurDev etc. In this research we developed a tool "GeneUS" using GPT-4.0 to automatically create user stories from requirements document which is the outcome of the RE phase. The output is provided in JSON format leaving the possibilities open for downstream integration to the popular project management tools. Analyzing requirements documents takes signific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01557</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#33258;&#20027;&#32676;&#20307;&#24418;&#25104;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;
&lt;/p&gt;
&lt;p&gt;
Distributed Autonomous Swarm Formation for Dynamic Network Bridging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25805;&#20316;&#21644;&#26080;&#32541;&#21327;&#20316;&#26159;&#19979;&#19968;&#20195;&#25216;&#26415;&#21644;&#24212;&#29992;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#35832;&#22914;&#28798;&#38590;&#21709;&#24212;&#20043;&#31867;&#30340;&#24773;&#26223;&#20013;&#65292;&#32676;&#20307;&#25805;&#20316;&#38656;&#35201;&#21327;&#35843;&#30340;&#34892;&#20026;&#21644;&#31227;&#21160;&#25511;&#21046;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#22788;&#29702;&#65292;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#20197;&#21450;&#24213;&#23618;&#32593;&#32476;&#36136;&#37327;&#23545;&#20854;&#34892;&#21160;&#30340;&#36136;&#37327;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDP&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#32676;&#20195;&#29702;&#21327;&#20316;&#24418;&#25104;&#20004;&#20010;&#36828;&#36317;&#31227;&#21160;&#30446;&#26631;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65288;DGN&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#28982;&#36866;&#29992;&#20110;&#20219;&#21153;&#30340;&#32593;&#32476;&#21270;&#12289;&#20998;&#24067;&#24335;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#24515;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01557v1 Announce Type: cross  Abstract: Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications. In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents' actions heavily relying on the communication between them and the underlying network. In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets. Furthermore, we propose a Multi-Agent Reinforcement Learning (MARL) approach for the problem based on Graph Convolutional Reinforcement Learning (DGN) which naturally applies to the networked, distributed nature of the task. The proposed method is evaluated in a simulated environment and compared to a cent
&lt;/p&gt;</description></item><item><title>&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01551</link><description>&lt;p&gt;
&#20855;&#26377;&#25511;&#21046;&#29702;&#35770;&#23433;&#20840;&#20445;&#35777;&#30340;&#21160;&#24577;&#32593;&#32476;&#26725;&#25509;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning with Control-Theoretic Safety Guarantees for Dynamic Network Bridging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01551
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#26465;&#20214;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#19979;&#35299;&#20915;&#22797;&#26434;&#30340;&#21512;&#20316;&#20219;&#21153;&#23545;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#26465;&#20214;&#19979;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#23450;&#28857;&#26356;&#26032;&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#26234;&#33021;&#20307;&#20301;&#32622;&#65292;&#20197;&#20445;&#25345;&#23433;&#20840;&#26465;&#20214;&#32780;&#19981;&#24433;&#21709;&#20219;&#21153;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#30456;&#27604;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19982;&#38646;&#23433;&#20840;&#36829;&#35268;&#30456;&#27604;&#21487;&#27604;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#23433;&#20840;&#25511;&#21046;&#19982;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#19981;&#20165;&#22686;&#24378;&#20102;&#23433;&#20840;&#21512;&#35268;&#24615;&#65292;&#36824;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20219;&#21153;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01551v1 Announce Type: cross  Abstract: Addressing complex cooperative tasks in safety-critical environments poses significant challenges for Multi-Agent Systems, especially under conditions of partial observability. This work introduces a hybrid approach that integrates Multi-Agent Reinforcement Learning with control-theoretic methods to ensure safe and efficient distributed strategies. Our contributions include a novel setpoint update algorithm that dynamically adjusts agents' positions to preserve safety conditions without compromising the mission's objectives. Through experimental validation, we demonstrate significant advantages over conventional MARL strategies, achieving comparable task performance with zero safety violations. Our findings indicate that integrating safe control with learning approaches not only enhances safety compliance but also achieves good performance in mission objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22270;&#34920;&#38382;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#22788;&#29702;&#35299;&#20915;&#22810;&#27169;&#24577;&#38382;&#31572;&#20013;&#22797;&#26434;&#30340;&#39068;&#33394;&#12289;&#32467;&#26500;&#21644;&#26080;&#25991;&#26412;&#22270;&#34920;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01548</link><description>&lt;p&gt;
mChartQA&#65306;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#22270;&#34920;&#38382;&#31572;&#36890;&#29992;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22270;&#34920;&#38382;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#22788;&#29702;&#35299;&#20915;&#22810;&#27169;&#24577;&#38382;&#31572;&#20013;&#22797;&#26434;&#30340;&#39068;&#33394;&#12289;&#32467;&#26500;&#21644;&#26080;&#25991;&#26412;&#22270;&#34920;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22810;&#27169;&#24577;&#22270;&#34920;&#38382;&#31572;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#39068;&#33394;&#12289;&#32467;&#26500;&#21644;&#26080;&#25991;&#26412;&#22270;&#34920;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#30452;&#25509;&#22810;&#27169;&#24577;&#22788;&#29702;&#25110;&#34920;&#26684;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#28982;&#21518;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#65292;&#20294;&#22312;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#22330;&#26223;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22270;&#34920;&#38382;&#31572;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25972;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#22788;&#29702;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21452;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65306;&#21021;&#22987;&#38454;&#27573;&#19987;&#27880;&#20110;&#35843;&#25972;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#38543;&#21518;&#30340;&#38454;&#27573;&#38598;&#20013;&#20110;&#20248;&#21270;&#27169;&#22411;&#22312;&#22270;&#34920;&#30456;&#20851;&#26597;&#35810;&#20013;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01548v1 Announce Type: cross  Abstract: In the fields of computer vision and natural language processing, multimodal chart question-answering, especially involving color, structure, and textless charts, poses significant challenges. Traditional methods, which typically involve either direct multimodal processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios. This paper introduces a novel multimodal chart question-answering model, specifically designed to address these intricate tasks. Our model integrates visual and linguistic processing, overcoming the constraints of existing methods. We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model's interpretative and analytical abilities in chart-related queries. This approach has demonstrated superior performance on multiple pub
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01536</link><description>&lt;p&gt;
&#25918;&#32622;&#38170;&#28857;&#65306;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#32473;&#25968;&#23383;&#35821;&#20041;&#19978;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Laying Anchors: Semantically Priming Numerals in Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01536
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22823;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#32447;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#27491;&#30830;&#32534;&#30721;&#25968;&#23383;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#25968;&#23383;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#20309;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#26469;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#25968;&#23383;&#26631;&#35760;&#30340;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#25968;&#20540;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#65292;&#23545;&#39046;&#22495;&#20869;&#65288;&#24050;&#35265;&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;&#26410;&#35265;&#65289;&#30340;&#25968;&#23383;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23454;&#35777;&#35780;&#20272;&#25193;&#23637;&#21040;&#20174;1&#21040;10&#20159;&#30340;&#25968;&#23383;&#33539;&#22260;&#65292;&#27604;&#20197;&#24448;&#30456;&#21516;&#31867;&#22411;&#30740;&#31350;&#30340;&#33539;&#22260;&#24191;&#24471;&#22810;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#24471;&#30340;&#23884;&#20837;&#21521;&#25968;&#23398;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20195;&#25968;&#35268;&#33539;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#20998;&#26512;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#37319;&#29992;&#31867;&#20284;Ehresmann&#32032;&#25551;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22312;&#27169;&#31946;&#38598;&#30340;&#23431;&#23449;&#20013;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2404.01526</link><description>&lt;p&gt;
&#20998;&#31867;&#31526;&#21495;&#23398;&#65306;&#30693;&#35782;&#25972;&#21512;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Categorical semiotics: Foundations for Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20195;&#25968;&#35268;&#33539;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#20998;&#26512;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#37319;&#29992;&#31867;&#20284;Ehresmann&#32032;&#25551;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22312;&#27169;&#31946;&#38598;&#30340;&#23431;&#23449;&#20013;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#21516;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#25972;&#21512;&#65292;&#26080;&#35770;&#26159;&#30001;&#39046;&#22495;&#19987;&#23478;&#25551;&#36848;&#36824;&#26159;&#30001;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#65292;&#21382;&#26469;&#38754;&#20020;&#30528;&#32570;&#20047;&#19968;&#20010;&#36866;&#24403;&#26694;&#26550;&#26469;&#25351;&#23450;&#21644;&#25972;&#21512;&#32467;&#26500;&#12289;&#23398;&#20064;&#36807;&#31243;&#12289;&#25968;&#25454;&#36716;&#25442;&#20197;&#21450;&#25968;&#25454;&#27169;&#22411;&#25110;&#35268;&#21017;&#30340;&#25361;&#25112;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20195;&#25968;&#35268;&#33539;&#26041;&#27861;&#25193;&#23637;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#20998;&#26512;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#35748;&#20026;&#20197;&#24448;&#30340;&#21162;&#21147;&#24182;&#26410;&#33021;&#24314;&#31435;&#27169;&#22411;&#24517;&#39035;&#36981;&#23432;&#30340;&#32422;&#26463;&#19982;&#20854;&#23454;&#38469;&#23454;&#29616;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#37319;&#29992;&#20102;&#31867;&#20284;Ehresmann&#32032;&#25551;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22312;&#27169;&#31946;&#38598;&#30340;&#23431;&#23449;&#20013;&#36827;&#34892;&#35299;&#37322;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#20248;&#38597;&#22320;&#21253;&#21547;&#20102;&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01526v1 Announce Type: new  Abstract: The integration of knowledge extracted from diverse models, whether described by domain experts or generated by machine learning algorithms, has historically been challenged by the absence of a suitable framework for specifying and integrating structures, learning processes, data transformations, and data models or rules. In this work, we extend algebraic specification methods to address these challenges within such a framework.   In our work, we tackle the challenging task of developing a comprehensive framework for defining and analyzing deep learning architectures. We believe that previous efforts have fallen short by failing to establish a clear connection between the constraints a model must adhere to and its actual implementation.   Our methodology employs graphical structures that resemble Ehresmann's sketches, interpreted within a universe of fuzzy sets. This approach offers a unified theory that elegantly encompasses both determ
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#35895;&#27468;&#22320;&#26631;v2&#30340;&#35757;&#32451;&#38598;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#31227;&#38500;&#31867;&#21035;&#37325;&#21472;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#21333;&#38454;&#27573;&#27969;&#31243;Single-stage Detect-to-Retrieve&#65288;CiDeR&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35937;&#24182;&#25552;&#21462;&#20840;&#23616;&#22270;&#20687;&#34920;&#31034;&#65292;&#22312;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01524</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#35757;&#32451;-&#27979;&#35797;&#31867;&#21035;&#37325;&#21472;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
On Train-Test Class Overlap and Detection for Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01524
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#35895;&#27468;&#22320;&#26631;v2&#30340;&#35757;&#32451;&#38598;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#31227;&#38500;&#31867;&#21035;&#37325;&#21472;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#21333;&#38454;&#27573;&#27969;&#31243;Single-stage Detect-to-Retrieve&#65288;CiDeR&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35937;&#24182;&#25552;&#21462;&#20840;&#23616;&#22270;&#20687;&#34920;&#31034;&#65292;&#22312;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#22810;&#37325;&#35201;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#38598;&#20013;&#19981;&#35201;&#26377;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#31867;&#21035;&#37325;&#21472;&#65311;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#24182;&#31227;&#38500;&#35895;&#27468;&#22320;&#26631;v2&#28165;&#27905;&#38598;&#20013;&#30340;&#31867;&#21035;&#37325;&#21472;&#65292;&#26469;&#37325;&#26032;&#23457;&#35270;Revisited Oxford&#21644;Paris&#65292;&#26368;&#27969;&#34892;&#30340;&#35780;&#20272;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#21644;&#26032;&#30340;RGLDv2-clean&#22312;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20196;&#20154;&#30633;&#30446;&#12290;&#19981;&#20165;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#65292;&#32780;&#19988;&#22312;&#26041;&#27861;&#20043;&#38388;&#19981;&#19968;&#33268;&#65292;&#25913;&#21464;&#20102;&#25490;&#21517;&#12290;&#24403;&#32034;&#24341;&#26102;&#38598;&#20013;&#20851;&#27880;&#23545;&#35937;&#25110;&#20852;&#36259;&#24182;&#24573;&#30053;&#32972;&#26223;&#26434;&#20081;&#38656;&#35201;&#20160;&#20040;&#65311;&#25105;&#20204;&#38656;&#35201;&#20998;&#21035;&#35757;&#32451;&#23545;&#35937;&#26816;&#27979;&#22120;&#21644;&#34920;&#31034;&#21527;&#65311;&#25105;&#20204;&#38656;&#35201;&#20301;&#32622;&#30417;&#30563;&#21527;&#65311;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Single-stage Detect-to-Retrieve&#65288;CiDeR&#65289;&#30340;&#31471;&#21040;&#31471;&#12289;&#21333;&#38454;&#27573;&#27969;&#31243;&#65292;&#29992;&#20110;&#26816;&#27979;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#24182;&#25552;&#21462;&#20840;&#23616;&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#27979;&#35797;&#22522;&#20934;&#19978;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01524v1 Announce Type: cross  Abstract: How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing tr
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#32593;&#27169;&#22411;&#30340;&#20559;&#35265;&#26159;&#21542;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#23545;&#27492;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.01509</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#33021;&#35299;&#37322;&#27867;&#21270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Biases in ImageNet Models Explain Generalization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01509
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32593;&#27169;&#22411;&#30340;&#20559;&#35265;&#26159;&#21542;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#23545;&#27492;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#27169;&#22411;&#23545;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#38271;&#23614;&#30340;&#31232;&#26377;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26679;&#26412;&#21644;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#36825;&#20307;&#29616;&#22312;&#23545;&#25197;&#26354;&#22270;&#20687;&#30340;&#25915;&#20987;&#12289;&#24615;&#33021;&#19979;&#38477;&#20197;&#21450;&#23545;&#27010;&#24565;&#65288;&#22914;&#33609;&#22270;&#65289;&#30340;&#27867;&#21270;&#19981;&#36275;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#29702;&#35299;&#38750;&#24120;&#26377;&#38480;&#65292;&#20294;&#21457;&#29616;&#20102;&#19968;&#20123;&#21306;&#21035;&#27169;&#22411;&#19982;&#20154;&#31867;&#35270;&#35273;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#23581;&#35797;&#20102;&#22810;&#31181;&#19981;&#21516;&#25104;&#21151;&#31243;&#24230;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#20123;&#35757;&#32451;&#20013;&#30340;&#20559;&#35265;&#20197;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;ResNet-50&#26550;&#26500;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#22312;48&#20010;&#36890;&#36807;&#19981;&#21516;&#33719;&#21462;&#36884;&#24452;&#33719;&#24471;&#30340;ImageNet&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01509v1 Announce Type: cross  Abstract: The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22312;&#35268;&#21010;&#20013;&#37096;&#20998;&#20445;&#30041;&#39034;&#24207;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23450;&#37325;&#35201;&#39034;&#24207;&#30340;&#23376;&#38598;&#26469;&#22312;&#39640;&#36136;&#37327;&#21644;&#26080;&#24207;&#35268;&#21010;&#38382;&#39064;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#37096;&#20998;&#39034;&#24207;&#20943;&#23569;&#25628;&#32034;&#20462;&#21098;&#25216;&#26415;&#30340;&#22909;&#22788;</title><link>https://arxiv.org/abs/2404.01503</link><description>&lt;p&gt;
&#19968;&#20123;&#39034;&#24207;&#24456;&#37325;&#35201;&#65306;&#22312;&#39640;&#36136;&#37327;&#35268;&#21010;&#20013;&#37096;&#20998;&#20445;&#30041;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Some Orders Are Important: Partially Preserving Orders in Top-Quality Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01503
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22312;&#35268;&#21010;&#20013;&#37096;&#20998;&#20445;&#30041;&#39034;&#24207;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23450;&#37325;&#35201;&#39034;&#24207;&#30340;&#23376;&#38598;&#26469;&#22312;&#39640;&#36136;&#37327;&#21644;&#26080;&#24207;&#35268;&#21010;&#38382;&#39064;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#37096;&#20998;&#39034;&#24207;&#20943;&#23569;&#25628;&#32034;&#20462;&#21098;&#25216;&#26415;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22810;&#20010;&#35745;&#21010;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#20351;&#29992;&#35268;&#21010;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#39640;&#36136;&#37327;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#32452;&#39030;&#32423;&#25104;&#26412;&#35745;&#21010;&#65292;&#20801;&#35768;&#28789;&#27963;&#30830;&#23450;&#31561;&#25928;&#35745;&#21010;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#35745;&#21010;&#20013;&#34892;&#21160;&#39034;&#24207;&#26041;&#38754;&#65292;&#25991;&#29486;&#21482;&#32771;&#34385;&#20004;&#20010;&#26497;&#31471; -- &#35201;&#20040;&#25152;&#26377;&#39034;&#24207;&#37325;&#35201;&#65292;&#20351;&#24471;&#27599;&#20010;&#35745;&#21010;&#37117;&#26159;&#21807;&#19968;&#30340;&#65292;&#35201;&#20040;&#25152;&#26377;&#39034;&#24207;&#37117;&#19981;&#37325;&#35201;&#65292;&#23558;&#21482;&#22312;&#34892;&#21160;&#39034;&#24207;&#19978;&#19981;&#21516;&#30340;&#20004;&#20010;&#35745;&#21010;&#35270;&#20026;&#31561;&#25928;&#12290;&#20026;&#20102;&#20801;&#35768;&#22312;&#36873;&#25321;&#37325;&#35201;&#39034;&#24207;&#26102;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#25351;&#23450;&#19968;&#23376;&#38598;&#34892;&#21160;&#65292;&#20854;&#20013;&#39034;&#24207;&#37325;&#35201;&#65292;&#25554;&#20540;&#20110;&#39640;&#36136;&#37327;&#21644;&#26080;&#24207;&#39640;&#36136;&#37327;&#35268;&#21010;&#38382;&#39064;&#20043;&#38388;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#35843;&#25972;&#37096;&#20998;&#39034;&#24207;&#20943;&#23569;&#25628;&#32034;&#20462;&#21098;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#19968;&#26032;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01503v1 Announce Type: new  Abstract: The ability to generate multiple plans is central to using planning in real-life applications. Top-quality planners generate sets of such top-cost plans, allowing flexibility in determining equivalent ones. In terms of the order between actions in a plan, the literature only considers two extremes -- either all orders are important, making each plan unique, or all orders are unimportant, treating two plans differing only in the order of actions as equivalent. To allow flexibility in selecting important orders, we propose specifying a subset of actions the orders between which are important, interpolating between the top-quality and unordered top-quality planning problems. We explore the ways of adapting partial order reduction search pruning techniques to address this new computational problem and present experimental evaluations demonstrating the benefits of exploiting such techniques in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01492</link><description>&lt;p&gt;
&#19981;&#36951;&#24536;&#20808;&#39564;&#30693;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#36866;&#24212;&#27169;&#24577;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#20934;&#30830;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21482;&#36866;&#29992;&#20110;&#36328;&#27169;&#24577;&#65292;&#22240;&#20026;&#20351;&#29992;&#19981;&#21516;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#25968;&#25454;&#23384;&#22312;&#26356;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#23558;&#22823;&#22411;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModTr&#20316;&#20026;&#26222;&#36941;&#20570;&#27861;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;ModTr&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#30452;&#25509;&#20351;&#26816;&#27979;&#25439;&#22833;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27169;&#22411;&#21487;&#20197;&#22312;&#36716;&#25442;&#21518;&#30340;&#36755;&#20837;&#19978;&#24037;&#20316;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#26356;&#25913;&#25110;&#21442;&#25968;&#24494;&#35843;&#12290;&#23545;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#20174;&#32418;&#22806;&#21040;RGB&#22270;&#20687;&#30340;&#36716;&#25442;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;ModTr&#26041;&#27861;&#25552;&#20379;&#20102;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01492v1 Announce Type: cross  Abstract: A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#24863;&#20852;&#36259;&#30340;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#23545;&#35937;&#26816;&#27979;&#21644;&#23494;&#38598;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#35745;&#31639;&#28010;&#36153;&#12290;</title><link>https://arxiv.org/abs/2404.01486</link><description>&lt;p&gt;
QuAD: &#22522;&#20110;&#26597;&#35810;&#30340;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
QuAD: Query-based Interpretable Neural Motion Planning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21160;&#39550;&#39542;&#31070;&#32463;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;&#24863;&#20852;&#36259;&#30340;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#20449;&#24687;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#23545;&#35937;&#26816;&#27979;&#21644;&#23494;&#38598;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#35745;&#31639;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24517;&#39035;&#20102;&#35299;&#20854;&#29615;&#22659;&#20197;&#30830;&#23450;&#36866;&#24403;&#30340;&#21160;&#20316;&#12290;&#20256;&#32479;&#33258;&#20027;&#31995;&#32479;&#20381;&#36182;&#20110;&#23545;&#35937;&#26816;&#27979;&#26469;&#25214;&#21040;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#23545;&#35937;&#26816;&#27979;&#20551;&#35774;&#19968;&#32452;&#31163;&#25955;&#30340;&#23545;&#35937;&#65292;&#24182;&#20002;&#22833;&#26377;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#22312;&#39044;&#27979;&#36825;&#20123;&#20195;&#29702;&#26410;&#26469;&#34892;&#20026;&#26102;&#20219;&#20309;&#38169;&#35823;&#37117;&#20250;&#32047;&#31215;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#23494;&#38598;&#30340;&#21344;&#25454;&#26629;&#26684;&#22320;&#22270;&#24050;&#34987;&#29992;&#20110;&#29702;&#35299;&#33258;&#30001;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#25972;&#20010;&#22330;&#26223;&#39044;&#27979;&#32593;&#26684;&#26159;&#28010;&#36153;&#30340;&#65292;&#22240;&#20026;&#21482;&#26377;&#26576;&#20123;&#26102;&#31354;&#21306;&#22495;&#26159;&#21487;&#21040;&#36798;&#30340;&#24182;&#19988;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#12289;&#39640;&#25928;&#30340;&#33258;&#20027;&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#20808;&#24863;&#30693;&#12289;&#20877;&#39044;&#27979;&#65292;&#26368;&#21518;&#35268;&#21010;&#30340;&#32423;&#32852;&#27169;&#22359;&#30340;&#33539;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#32773;&#30340;&#37325;&#28857;&#36716;&#31227;&#21040;&#26597;&#35810;&#30456;&#20851;&#26102;&#31354;&#28857;&#30340;&#21344;&#25454;&#65292;&#38480;&#21046;&#35745;&#31639;&#22312;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#19978;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01486v1 Announce Type: cross  Abstract: A self-driving vehicle must understand its environment to determine the appropriate action. Traditional autonomy systems rely on object detection to find the agents in the scene. However, object detection assumes a discrete set of objects and loses information about uncertainty, so any errors compound when predicting the future behavior of those agents. Alternatively, dense occupancy grid maps have been utilized to understand free-space. However, predicting a grid for the entire scene is wasteful since only certain spatio-temporal regions are reachable and relevant to the self-driving vehicle. We present a unified, interpretable, and efficient autonomy framework that moves away from cascading modules that first perceive, then predict, and finally plan. Instead, we shift the paradigm to have the planner query occupancy at relevant spatio-temporal points, restricting the computation to those regions of interest. Exploiting this represent
&lt;/p&gt;</description></item><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#8220;ChemBench&#8221;&#65292;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#19987;&#19994;&#30693;&#35782;&#30340;&#23545;&#27604;&#12290;</title><link>https://arxiv.org/abs/2404.01475</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#36229;&#20154;&#31867;&#21270;&#23398;&#23478;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are large language models superhuman chemists?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01475
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#8220;ChemBench&#8221;&#65292;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#19987;&#19994;&#30693;&#35782;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22788;&#29702;&#20154;&#31867;&#35821;&#35328;&#24182;&#25191;&#34892;&#26410;&#32463;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#23545;&#21270;&#23398;&#31185;&#23398;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#20026;&#21270;&#23398;&#38754;&#20020;&#30528;&#25968;&#25454;&#38598;&#23567;&#19988;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#20197;&#25991;&#26412;&#24418;&#24335;&#21576;&#29616;&#12290; LLMs&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#21033;&#29992;&#26469;&#39044;&#27979;&#21270;&#23398;&#24615;&#36136;&#65292;&#20248;&#21270;&#21453;&#24212;&#65292;&#29978;&#33267;&#33258;&#20027;&#35774;&#35745;&#21644;&#36827;&#34892;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#21270;&#23398;&#25512;&#29702;&#33021;&#21147;&#20165;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#31995;&#32479;&#24615;&#29702;&#35299;&#65292;&#36825;&#26159;&#25913;&#36827;&#27169;&#22411;&#21644;&#20943;&#36731;&#28508;&#22312;&#21361;&#23475;&#25152;&#24517;&#38656;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;ChemBench&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#20005;&#26684;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#21270;&#23398;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#19982;&#20154;&#31867;&#21270;&#23398;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#30456;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01475v1 Announce Type: cross  Abstract: Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. This is relevant for the chemical sciences, which face the problem of small and diverse datasets that are frequently in the form of text. LLMs have shown promise in addressing these issues and are increasingly being harnessed to predict chemical properties, optimize reactions, and even design and conduct experiments autonomously. However, we still have only a very limited systematic understanding of the chemical reasoning capabilities of LLMs, which would be required to improve models and mitigate potential harms. Here, we introduce "ChemBench," an automated framework designed to rigorously evaluate the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of human chemists. We curated more than 7,000 question-answer pairs for a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;UVI-Net&#65292;&#33021;&#22815;&#22312;4D&#21307;&#23398;&#22270;&#20687;&#20013;&#23454;&#29616;&#26080;&#38656;&#20013;&#38388;&#24103;&#30340;&#26102;&#38388;&#25554;&#20540;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2404.01464</link><description>&lt;p&gt;
4D&#21307;&#23398;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#25554;&#20540;&#26041;&#27861;&#65306;&#26080;&#38656;&#20219;&#20309;&#20013;&#38388;&#24103;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;UVI-Net&#65292;&#33021;&#22815;&#22312;4D&#21307;&#23398;&#22270;&#20687;&#20013;&#23454;&#29616;&#26080;&#38656;&#20013;&#38388;&#24103;&#30340;&#26102;&#38388;&#25554;&#20540;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
4D&#21307;&#23398;&#22270;&#20687;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#25429;&#25417;&#21160;&#24577;&#21464;&#21270;&#24182;&#30417;&#27979;&#38271;&#26399;&#30142;&#30149;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;4D&#21307;&#23398;&#22270;&#20687;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#36752;&#23556;&#26292;&#38706;&#21644;&#25104;&#20687;&#26102;&#38388;&#65292;&#38656;&#35201;&#22312;&#23454;&#29616;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#20943;&#23569;&#19981;&#33391;&#24433;&#21709;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#33719;&#21462;&#19981;&#20165;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22686;&#21152;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#24103;&#29575;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#25554;&#20540;&#26694;&#26550;&#65292;UVI-Net&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#26102;&#38388;&#25554;&#20540;&#32780;&#26080;&#38656;&#20219;&#20309;&#20013;&#38388;&#24103;&#65292;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01464v1 Announce Type: cross  Abstract: 4D medical images, which represent 3D images with temporal information, are crucial in clinical practice for capturing dynamic changes and monitoring long-term disease progression. However, acquiring 4D medical images poses challenges due to factors such as radiation exposure and imaging duration, necessitating a balance between achieving high temporal resolution and minimizing adverse effects. Given these circumstances, not only is data acquisition challenging, but increasing the frame rate for each dataset also proves difficult. To address this challenge, this paper proposes a simple yet effective Unsupervised Volumetric Interpolation framework, UVI-Net. This framework facilitates temporal interpolation without the need for any intermediate frames, distinguishing it from the majority of other existing unsupervised methods. Experiments on benchmark datasets demonstrate significant improvements across diverse evaluation metrics compare
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#21338;&#24328;&#35770;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#30899;&#25490;&#25918;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01459</link><description>&lt;p&gt;
&#28216;&#25103;&#29702;&#35770;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#26368;&#23567;&#21270;&#30899;&#25490;&#25918;&#21644;&#33021;&#28304;&#25104;&#26412;&#30340;AI&#25512;&#26029;&#36127;&#36733;
&lt;/p&gt;
&lt;p&gt;
Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01459
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#21338;&#24328;&#35770;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#30899;&#25490;&#25918;&#21644;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20316;&#36127;&#36733;&#30340;&#22686;&#21152;&#32780;&#28040;&#32791;&#26356;&#22810;&#33021;&#28304;&#65292;&#36825;&#23545;&#29615;&#22659;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#24182;&#25552;&#39640;&#36816;&#33829;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#23558;&#21338;&#24328;&#35770;&#65288;GT&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30456;&#32467;&#21512;&#65292;&#20248;&#21270;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;AI&#25512;&#26029;&#24037;&#20316;&#36127;&#36733;&#30340;&#20998;&#24067;&#65292;&#20197;&#20943;&#23569;&#30899;&#25490;&#25918;&#21644;&#20113;&#36816;&#33829;&#65288;&#33021;&#28304;+&#25968;&#25454;&#20256;&#36755;&#65289;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01459v1 Announce Type: cross  Abstract: Data centers are increasingly using more energy due to the rise in Artificial Intelligence (AI) workloads, which negatively impacts the environment and raises operational costs. Reducing operating expenses and carbon emissions while maintaining performance in data centers is a challenging problem. This work introduces a unique approach combining Game Theory (GT) and Deep Reinforcement Learning (DRL) for optimizing the distribution of AI inference workloads in geo-distributed data centers to reduce carbon emissions and cloud operating (energy + data transfer) costs. The proposed technique integrates the principles of non-cooperative Game Theory into a DRL framework, enabling data centers to make intelligent decisions regarding workload allocation while considering the heterogeneity of hardware resources, the dynamic nature of electricity prices, inter-data center data transfer costs, and carbon footprints. We conducted extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#19978;&#23384;&#22312;&#20559;&#21521;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01453</link><description>&lt;p&gt;
&#25581;&#31034;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#19978;&#30340;&#20998;&#27495;&#24402;&#32435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unveiling Divergent Inductive Biases of LLMs on Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#19978;&#23384;&#22312;&#20559;&#21521;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33258;&#28982;&#35821;&#35328;&#20107;&#20214;&#30340;&#24494;&#22937;&#32454;&#33410;&#38656;&#35201;&#23545;&#26102;&#38388;&#21160;&#24577;&#36827;&#34892;&#24494;&#22937;&#29702;&#35299;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#25968;&#25454;&#20013;&#36776;&#21035;&#27169;&#24335;&#21644;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#24471;&#24456;&#29087;&#32451;&#65292;&#20294;&#23427;&#20204;&#23545;&#26102;&#38388;&#21160;&#24577;&#30340;&#20869;&#22312;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#22312;LLMs&#20013;&#32454;&#33268;&#25506;&#32034;&#36825;&#20123;&#22266;&#26377;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37319;&#29992;&#38382;&#31572;&#65288;QA&#65289;&#26684;&#24335;&#21644;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#26684;&#24335;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31867;&#22411;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#28145;&#20837;&#25506;&#31350;&#20102;&#38544;&#24335;&#21644;&#26174;&#24335;&#20107;&#20214;&#12290;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#19968;&#20123;&#26174;&#33879;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;GPT-3.5&#21644;GPT-4&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20542;&#21521;&#20110;&#29305;&#23450;&#26102;&#38388;&#20851;&#31995;&#30340;&#20559;&#35265;&#28014;&#20986;&#27700;&#38754;&#65292;&#20854;&#20013;&#22312;QA&#26684;&#24335;&#20013;&#65292;GPT-3.5&#22312;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#38754;&#22343;&#34920;&#29616;&#20986;&#23545;"AFTER"&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01453v1 Announce Type: cross  Abstract: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#34920;&#22411;&#21644;&#22312;&#22270;&#22359;&#32423;&#21035;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01446</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#25214;&#21040;&#24863;&#20852;&#36259;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#34920;&#22411;&#21644;&#22312;&#22270;&#22359;&#32423;&#21035;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whole Slide Images (WSI)&#26159;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#25968;&#23383;&#25195;&#25551;&#26174;&#24494;&#38236;&#20999;&#29255;&#22312;&#22810;&#20010;&#23610;&#24230;&#19979;&#33719;&#24471;&#30340;&#65292;&#26159;&#29616;&#20195;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;/&#20154;&#24037;&#26234;&#33021;&#20013;&#20171;&#20998;&#26512;&#25552;&#20986;&#29305;&#27530;&#25361;&#25112;&#65292;&#22240;&#20026;&#30149;&#29702;&#26631;&#27880;&#36890;&#24120;&#26159;&#22312;&#20999;&#29255;&#32423;&#21035;&#32780;&#19981;&#26159;&#22270;&#22359;&#32423;&#21035;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20004;&#31181;&#24120;&#35265;&#30284;&#30151;&#31867;&#22411;&#65292;&#20405;&#34989;&#24615;&#20083;&#33146;&#30284;&#65288;TCGA-BRCA&#65289;&#21644;&#32954;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;TCGA-LUSC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01446v1 Announce Type: cross  Abstract: Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#12289;&#21033;&#29992;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#32467;&#26524;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2404.01440</link><description>&lt;p&gt;
&#29992;&#20110;&#26500;&#24314;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#12289;&#21033;&#29992;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#32467;&#26524;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#20004;&#20010;&#19981;&#21516;&#20851;&#33410;&#29366;&#24577;&#30340;&#29289;&#20307;&#30340;RGBD&#25195;&#25551;&#26500;&#24314;&#26410;&#30693;&#20851;&#33410;&#23545;&#35937;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#22788;&#29702;&#19981;&#21516;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#29366;&#24577;&#37325;&#24314;&#29289;&#20307;&#32423;&#24418;&#29366;&#65292;&#28982;&#21518;&#24674;&#22797;&#21253;&#25324;&#37096;&#20214;&#20998;&#21106;&#21644;&#20851;&#33410;&#20851;&#32852;&#22312;&#20869;&#30340;&#22522;&#30784;&#20851;&#33410;&#27169;&#22411;&#12290;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#28857;&#32423;&#23545;&#24212;&#20851;&#31995;&#24182;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;&#12289;3D&#37325;&#24314;&#21644;&#36816;&#21160;&#23398;&#30340;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#32467;&#26524;&#12290;&#23427;&#36824;&#22788;&#29702;&#20102;&#22810;&#20010;&#21487;&#31227;&#21160;&#37096;&#20214;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#23545;&#35937;&#24418;&#29366;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01440v1 Announce Type: cross  Abstract: We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01439</link><description>&lt;p&gt;
&#20174;&#25551;&#36848;&#20013;&#26080;&#30417;&#30563;&#22320;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#21019;&#24314;&#34920;&#24773;&#31526;&#21495;&#35789;&#24211;
&lt;/p&gt;
&lt;p&gt;
Creating emoji lexica from unsupervised sentiment analysis of their descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#20013;&#39044;&#27979;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#24773;&#24863;&#30340;&#26032;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#65292;&#33410;&#30465;&#20102;&#23453;&#36149;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#23186;&#20307;&#65292;&#22914;&#21338;&#23458;&#21644;&#31038;&#20132;&#32593;&#32476;&#32593;&#31449;&#65292;&#29983;&#25104;&#20102;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20379;&#20998;&#26512;&#20010;&#20154;&#21644;&#32452;&#32455;&#30340;&#35266;&#28857;&#21644;&#24773;&#24863;&#20043;&#29992;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#24050;&#32463;&#19981;&#33021;&#24456;&#22909;&#22320;&#37327;&#21270;&#36825;&#20123;&#35266;&#28857;&#30340;&#26497;&#24615;&#24230;&#37327;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#34920;&#24773;&#31526;&#21495;&#25152;&#34920;&#36798;&#30340;&#24773;&#24863;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#22235;&#24180;&#38388;&#65292;&#31526;&#21495;&#30340;&#20351;&#29992;&#37327;&#28608;&#22686;&#12290;&#22914;&#20170;&#65292;Twitter&#20013;&#27599;&#22825;&#20250;&#34987;&#36755;&#20837;&#22823;&#32422;&#20004;&#30334;&#20159;&#20010;&#31526;&#21495;&#65292;&#24182;&#19988;&#27599;&#20010;&#26032;&#30340;Unicode&#29256;&#26412;&#37117;&#20250;&#22686;&#21152;&#26032;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22312;&#32447;&#25991;&#26412;&#28040;&#24687;&#65288;&#22914;&#25512;&#25991;&#65289;&#20013;&#34920;&#24773;&#31526;&#21495;&#34920;&#36798;&#30340;&#24773;&#24863;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20154;&#24037;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#65292;&#20197;&#27492;&#33410;&#30465;&#23453;&#36149;&#30340;&#26102;&#38388;&#29992;&#20110;&#20854;&#20182;&#20998;&#26512;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33258;&#21160;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#24773;&#31526;&#21495;&#24773;&#24863;&#35789;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01439v1 Announce Type: cross  Abstract: Online media, such as blogs and social networking sites, generate massive volumes of unstructured data of great interest to analyze the opinions and sentiments of individuals and organizations. Novel approaches beyond Natural Language Processing are necessary to quantify these opinions with polarity metrics. So far, the sentiment expressed by emojis has received little attention. The use of symbols, however, has boomed in the past four years. About twenty billion are typed in Twitter nowadays, and new emojis keep appearing in each new Unicode version, making them increasingly relevant to sentiment analysis tasks. This has motivated us to propose a novel approach to predict the sentiments expressed by emojis in online textual messages, such as tweets, that does not require human effort to manually annotate data and saves valuable time for other analysis tasks. For this purpose, we automatically constructed a novel emoji sentiment lexico
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.01438</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25163;&#35821;&#30340;&#29983;&#25104;&#19982;&#26816;&#27979;-&#35821;&#35328;&#21644;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01438
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#39046;&#22495;&#20013;&#19968;&#20010;&#36880;&#28176;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36229;&#36234;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#65292;&#20197;&#21450;&#36825;&#23545;&#31038;&#20250;&#26159;&#21542;&#26377;&#30410;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#20013;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#21516;&#26102;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#65288;DHoH&#65289;&#31038;&#21306;&#25191;&#34892;&#25163;&#35821;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#23545;&#29983;&#25104;&#30340;&#35270;&#39057;&#36827;&#34892;&#23457;&#26680;&#12290;&#37492;&#20110;&#25163;&#35821;&#30340;&#22797;&#26434;&#24615;&#12289;&#25163;&#35821;&#19987;&#23478;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#20581;&#24247;&#21644;&#25945;&#32946;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#36825;&#31181;&#20570;&#27861;&#23588;&#20026;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#21253;&#25324;&#26500;&#24314;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35780;&#20272;&#20854;&#25216;&#26415;&#21644;&#35270;&#35273;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#12290;&#20351;&#29992;1200&#22810;&#20010;&#35270;&#39057;&#65292;&#27169;&#22411;&#21253;&#21547;&#20197;&#21069;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#65292;&#20511;&#21161;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#30340;&#24110;&#21161;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01438v1 Announce Type: cross  Abstract: A question in the realm of deepfakes is slowly emerging pertaining to whether we can go beyond facial deepfakes and whether it would be beneficial to society. Therefore, this research presents a positive application of deepfake technology in upper body generation, while performing sign-language for the Deaf and Hard of Hearing (DHoH) community. The resulting videos are later vetted with a sign language expert. This is particularly helpful, given the intricate nature of sign language, a scarcity of sign language experts, and potential benefits for health and education. The objectives of this work encompass constructing a reliable deepfake dataset, evaluating its technical and visual credibility through computer vision and natural language processing models, and assessing the plausibility of the generated content. With over 1200 videos, featuring both previously seen and unseen individuals for the generation model, using the help of a si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01430</link><description>&lt;p&gt;
&#38477;&#20302;LLMs&#20013;&#20301;&#32622;&#20559;&#24046;&#30340;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#20027;&#35201;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22686;&#24378;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#28041;&#21450;&#20174;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#24211;&#26816;&#32034;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#19968;&#36827;&#23637;&#23588;&#20026;&#20851;&#38190;&#65292;&#22240;&#20026;&#21487;&#33021;&#28041;&#21450;&#38271;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;LLMs&#23384;&#22312;&#20301;&#32622;&#20559;&#24046;&#65292;&#34920;&#26126;&#20854;&#24615;&#33021;&#20250;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#20013;&#26377;&#29992;&#20449;&#24687;&#30340;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#20301;&#32622;&#20559;&#24046;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#28304;&#20110;&#19981;&#21516;&#27169;&#22411;&#30340;&#22266;&#26377;&#20301;&#32622;&#20559;&#22909;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20165;&#20165;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20811;&#26381;&#20301;&#32622;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;LLMs&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#20301;&#32622;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PAPEFT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25968;&#25454;&#22686;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>OVFoodSeg&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#35774;&#23450;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#22270;&#20687;&#20449;&#24687;&#65292;&#25552;&#21319;&#39135;&#21697;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01409</link><description>&lt;p&gt;
OVFoodSeg&#65306;&#36890;&#36807;&#22522;&#20110;&#22270;&#20687;&#20449;&#24687;&#30340;&#25991;&#26412;&#34920;&#31034;&#25552;&#21319;&#24320;&#25918;&#35789;&#27719;&#39135;&#21697;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01409
&lt;/p&gt;
&lt;p&gt;
OVFoodSeg&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#35774;&#23450;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#22270;&#20687;&#20449;&#24687;&#65292;&#25552;&#21319;&#39135;&#21697;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39135;&#21697;&#35745;&#31639;&#39046;&#22495;&#65292;&#20174;&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#39135;&#26448;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#30456;&#21516;&#39135;&#26448;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#30340;&#20869;&#37096;&#31867;&#21035;&#24046;&#24322;&#12289;&#26032;&#39135;&#26448;&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#19982;&#22823;&#22411;&#39135;&#21697;&#20998;&#21106;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#39640;&#26114;&#27880;&#37322;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#38381;&#21512;&#35789;&#27719;&#21644;&#38745;&#24577;&#25991;&#26412;&#23884;&#20837;&#35774;&#32622;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#26377;&#25928;&#22788;&#29702;&#39135;&#26448;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#26032;&#30340;&#21644;&#22810;&#26679;&#21270;&#30340;&#39135;&#26448;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OVFoodSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#24182;&#36890;&#36807;&#35270;&#35273;&#32972;&#26223;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#21019;&#26032;&#27169;&#22359;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#21040;&#25991;&#26412;&#23398;&#20064;&#22120;FoodLearner&#21644;&#22522;&#20110;&#22270;&#20687;&#20449;&#24687;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65289;&#23558;&#25991;&#26412;&#23884;&#20837;&#19982;&#22270;&#20687;&#29305;&#23450;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;OVFoodSeg&#30340;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;FoodLea&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01409v1 Announce Type: cross  Abstract: In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLea
&lt;/p&gt;</description></item><item><title>ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;</title><link>https://arxiv.org/abs/2404.01402</link><description>&lt;p&gt;
ContactHandover: &#25509;&#35302;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
ContactHandover: Contact-Guided Robot-to-Human Object Handover
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01402
&lt;/p&gt;
&lt;p&gt;
ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#26159;&#35768;&#22810;&#20154;&#26426;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25104;&#21151;&#30340;&#36882;&#36865;&#38656;&#35201;&#26426;&#22120;&#20154;&#20445;&#25345;&#23545;&#29289;&#20307;&#30340;&#31283;&#23450;&#25235;&#21462;&#65292;&#21516;&#26102;&#30830;&#20445;&#20154;&#31867;&#20197;&#19968;&#31181;&#33258;&#28982;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#24335;&#25509;&#25910;&#29289;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContactHandover&#65292;&#36825;&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#38454;&#27573;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#65292;ContactHandover&#39044;&#27979;&#26426;&#22120;&#20154;&#30340;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#21183;&#21644;&#20154;&#31867;&#25509;&#35302;&#28857;&#22312;&#29289;&#20307;&#19978;&#30340;3D&#21487;&#20379;&#24615;&#22270;&#12290;&#26426;&#22120;&#20154;&#30340;&#25235;&#21462;&#23039;&#21183;&#36890;&#36807;&#24809;&#32602;&#37027;&#20123;&#38459;&#30861;&#20154;&#31867;&#25509;&#35302;&#28857;&#30340;&#23039;&#21183;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#25191;&#34892;&#25490;&#21517;&#26368;&#39640;&#30340;&#25235;&#21462;&#12290;&#22312;&#36882;&#36865;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#38752;&#36817;&#20154;&#31867;&#30340;&#25509;&#35302;&#28857;&#24182;&#26368;&#23567;&#21270;&#20154;&#31867;&#25163;&#33218;&#20851;&#33410;&#25197;&#30697;&#21644;&#20301;&#31227;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#12290;&#25105;&#20204;&#22312;27&#31181;&#19981;&#21516;&#23478;&#29992;&#29289;&#21697;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01402v1 Announce Type: cross  Abstract: Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#26465;&#20214;&#30340;&#23454;&#20363;&#34955;&#65288;OBoI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#38454;&#32479;&#35745;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#23454;&#20363;&#35782;&#21035;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.01397</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35937;&#26465;&#20214;&#30340;&#23454;&#20363;&#34955;&#29992;&#20110;&#23569;&#26679;&#26412;&#20010;&#24615;&#21270;&#23454;&#20363;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#26465;&#20214;&#30340;&#23454;&#20363;&#34955;&#65288;OBoI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#38454;&#32479;&#35745;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#23454;&#20363;&#35782;&#21035;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#65292;&#29992;&#25143;&#38656;&#27714;&#22686;&#21152;&#20102;&#23545;&#35270;&#35273;&#31995;&#32479;&#20010;&#24615;&#21270;&#30340;&#35201;&#27714;&#65292;&#20174;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#20165;&#20165;&#35782;&#21035;&#21644;&#23450;&#20301;&#20010;&#20154;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#25105;&#30340;&#29399;&#32780;&#19981;&#26159;&#29399;&#65289;&#12290;&#23613;&#31649;&#28145;&#24230;&#32593;&#32476;&#22312;&#20256;&#32479;&#26631;&#27880;&#20016;&#23500;&#30340;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#26368;&#26032;YOLOv8&#27169;&#22411;&#30340;&#26631;&#20934;&#30446;&#26631;&#26816;&#27979;&#65289;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#20445;&#25345;&#31867;&#20869;&#21464;&#24322;&#24615;&#20197;&#34920;&#31034;&#19981;&#21516;&#23454;&#20363;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#35937;&#31867;&#21035;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#21462;&#29305;&#24449;&#30340;&#22810;&#38454;&#32479;&#35745;&#30340;&#23545;&#35937;&#26465;&#20214;&#23454;&#20363;&#34955;&#65288;OBoI&#65289;&#65292;&#22312;&#36825;&#37324;&#65292;&#23558;&#36890;&#29992;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#25193;&#23637;&#21040;&#25628;&#32034;&#24182;&#20174; OBoI &#30340;&#24230;&#37327;&#31354;&#38388;&#35782;&#21035;&#20010;&#20154;&#23454;&#20363;&#65292;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#20381;&#36182;&#22810;&#38454;&#32479;&#35745;&#65292;OBoI &#22312;&#21306;&#20998;&#19981;&#21516;&#23454;&#20363;&#26041;&#38754;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#36229;&#20961;&#20934;&#30830;&#24615;&#12290;&#22312;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#22312;18&#20010;&#20010;&#20154;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;77.1% &#30340;&#20010;&#20154;&#23545;&#35937;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01397v1 Announce Type: cross  Abstract: Nowadays, users demand for increased personalization of vision systems to localize and identify personal instances of objects (e.g., my dog rather than dog) from a few-shot dataset only. Despite outstanding results of deep networks on classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model for standard object detection), they struggle to maintain within-class variability to represent different instances rather than object categories only. We construct an Object-conditioned Bag of Instances (OBoI) based on multi-order statistics of extracted features, where generic object detection models are extended to search and identify personal instances from the OBoI's metric space, without need for backpropagation. By relying on multi-order statistics, OBoI achieves consistent superior accuracy in distinguishing different instances. In the results, we achieve 77.1% personal object recognition accuracy in case of 18 personal in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01365</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM
&lt;/p&gt;
&lt;p&gt;
Prompt-prompted Mixture of Experts for Efficient LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRIFFIN&#30340;&#35757;&#32451;-free MoE&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;LLM&#27169;&#22411;&#20013;&#36873;&#25321;&#21807;&#19968;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23454;&#29992;&#24615;&#65292;&#23427;&#20204;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#19968;&#20123;&#26041;&#27861;&#65292;&#22914;&#20462;&#21098;&#25110;&#26500;&#24314;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#65292;&#26088;&#22312;&#21033;&#29992;transformer&#21069;&#39304;&#65288;FF&#65289;&#22359;&#20013;&#30340;&#31232;&#30095;&#24615;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#24182;&#38477;&#20302;&#20869;&#23384;&#38656;&#27714;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GRIFFIN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;MoE&#65292;&#23427;&#22312;&#24207;&#21015;&#32423;&#21035;&#20026;&#19981;&#21516;&#38750;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#22823;&#37327;LLMs&#36873;&#25321;&#29420;&#29305;&#30340;FF&#19987;&#23478;&#20197;&#23454;&#29616;&#39640;&#25928;&#29983;&#25104;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20851;&#38190;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#32463;&#36807;&#35757;&#32451;&#30340;LLMs&#22312;&#24207;&#21015;&#20013;&#33258;&#28982;&#20135;&#29983;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;FF&#28608;&#27963;&#27169;&#24335;&#65292;&#36825;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01365v1 Announce Type: cross  Abstract: With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#29109;&#36827;&#34892;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#20449;&#24687;&#35770;&#21387;&#32553;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01364</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#29109;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Information Plane Analysis Visualization in Deep Learning via Transfer Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01364
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#29109;&#36827;&#34892;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#20449;&#24687;&#35770;&#21387;&#32553;&#19982;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21069;&#39304;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;&#36716;&#31227;&#29109;&#65288;TE&#65289;&#26469;&#34913;&#37327;&#19968;&#23618;&#23545;&#21478;&#19968;&#23618;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#26681;&#25454;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#24212;&#23613;&#21487;&#33021;&#21387;&#32553;&#36755;&#20837;&#25968;&#25454;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#36275;&#22815;&#30340;&#20851;&#20110;&#36755;&#20986;&#30340;&#20449;&#24687;&#12290;&#20449;&#24687;&#24179;&#38754;&#20998;&#26512;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#36890;&#36807;&#32472;&#21046;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#37327;&#19982;&#21387;&#32553;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26469;&#29702;&#35299;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#20013;&#21387;&#32553;&#21644;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22768;&#31216;&#20449;&#24687;&#35770;&#21387;&#32553;&#21644;&#27867;&#21270;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#32852;&#31995;&#65292;&#36890;&#36807;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26159;&#21487;&#20449;&#30340;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#30340;&#32467;&#26524;&#23384;&#22312;&#20914;&#31361;&#12290;TE&#21487;&#20197;&#25429;&#25417;&#26102;&#38388;&#20851;&#31995;&#65292;&#19982;&#20114;&#20449;&#24687;&#30456;&#27604;&#65292;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01364v1 Announce Type: cross  Abstract: In a feedforward network, Transfer Entropy (TE) can be used to measure the influence that one layer has on another by quantifying the information transfer between them during training. According to the Information Bottleneck principle, a neural model's internal representation should compress the input data as much as possible while still retaining sufficient information about the output. Information Plane analysis is a visualization technique used to understand the trade-off between compression and information preservation in the context of the Information Bottleneck method by plotting the amount of information in the input data against the compressed representation. The claim that there is a causal link between information-theoretic compression and generalization, measured by mutual information, is plausible, but results from different studies are conflicting. In contrast to mutual information, TE can capture temporal relationships be
&lt;/p&gt;</description></item><item><title>AIOps&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#31561;&#25216;&#26415;&#25552;&#21319;&#25925;&#38556;&#31649;&#29702;&#25928;&#29575;&#65292;&#20294;&#39046;&#22495;&#20173;&#26410;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.01363</link><description>&lt;p&gt;
AIOps&#35299;&#20915;&#26041;&#26696;&#30340;&#25925;&#38556;&#31649;&#29702;&#65306;&#25216;&#26415;&#25351;&#21335;&#21644;&#32508;&#21512;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
AIOps Solutions for Incident Management: Technical Guidelines and A Comprehensive Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01363
&lt;/p&gt;
&lt;p&gt;
AIOps&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#31561;&#25216;&#26415;&#25552;&#21319;&#25925;&#38556;&#31649;&#29702;&#25928;&#29575;&#65292;&#20294;&#39046;&#22495;&#20173;&#26410;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;IT&#31995;&#32479;&#31649;&#29702;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#27969;&#26102;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#20381;&#38752;&#25163;&#21160;&#20219;&#21153;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20256;&#32479;&#26041;&#27861;&#23545;IT&#31995;&#32479;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#37327;&#21644;&#35686;&#25253;&#25928;&#29575;&#20302;&#19979;&#12290;&#25805;&#20316;&#31995;&#32479;&#20154;&#24037;&#26234;&#33021;&#65288;AIOps&#65289;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#31561;&#20808;&#36827;&#20998;&#26512;&#25216;&#26415;&#26469;&#22686;&#24378;&#25925;&#38556;&#31649;&#29702;&#12290;AIOps&#21487;&#20197;&#26816;&#27979;&#21644;&#39044;&#27979;&#25925;&#38556;&#65292;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#65292;&#33258;&#21160;&#21270;&#27835;&#24840;&#25805;&#20316;&#65292;&#25552;&#39640;&#36136;&#37327;&#24182;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20855;&#26377;&#28508;&#21147;&#65292;AIOps&#39046;&#22495;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#20998;&#25955;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#24815;&#20363;&#12290;&#30740;&#31350;&#21644;&#24037;&#19994;&#36129;&#29486;&#20998;&#24067;&#24191;&#27867;&#65292;&#32570;&#20047;&#19968;&#33268;&#30340;&#25968;&#25454;&#31649;&#29702;&#26694;&#26550;&#12289;&#30446;&#26631;&#38382;&#39064;&#12289;&#23454;&#26045;&#32454;&#33410;&#12289;&#38656;&#27714;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01363v1 Announce Type: cross  Abstract: The management of modern IT systems poses unique challenges, necessitating scalability, reliability, and efficiency in handling extensive data streams. Traditional methods, reliant on manual tasks and rule-based approaches, prove inefficient for the substantial data volumes and alerts generated by IT systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a solution, leveraging advanced analytics like machine learning and big data to enhance incident management. AIOps detects and predicts incidents, identifies root causes, and automates healing actions, improving quality and reducing operational costs. However, despite its potential, the AIOps domain is still in its early stages, decentralized across multiple sectors, and lacking standardized conventions. Research and industrial contributions are distributed without consistent frameworks for data management, target problems, implementation details, requirements, a
&lt;/p&gt;</description></item><item><title>LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2404.01361</link><description>&lt;p&gt;
LLM Attributor: &#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24402;&#22240;&#29992;&#20110;LLM&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM Attributor: Interactive Visual Attribution for LLM Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01361
&lt;/p&gt;
&lt;p&gt;
LLM Attributor&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#26041;&#24335;&#29992;&#20110;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#32467;&#26524;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#24110;&#21161;&#29992;&#25143;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#21487;&#20449;&#24230;&#65292;&#24182;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20984;&#26174;&#20102;&#20102;&#35299;&#25991;&#26412;&#29983;&#25104;&#32972;&#21518;&#21407;&#22240;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM Attributor&#65292;&#19968;&#20010;&#25552;&#20379;LLM&#25991;&#26412;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#20132;&#20114;&#21487;&#35270;&#21270;&#30340;Python&#24211;&#12290;&#25105;&#20204;&#30340;&#24211;&#20026;&#24555;&#36895;&#23558;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#24402;&#22240;&#21040;&#35757;&#32451;&#25968;&#25454;&#28857;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#24335;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#34892;&#20026;&#12289;&#22686;&#24378;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20855;&#30340;&#35270;&#35273;&#21644;&#20132;&#20114;&#35774;&#35745;&#65292;&#24182;&#24378;&#35843;LLaMA2&#27169;&#22411;&#30340;&#20351;&#29992;&#22330;&#26223;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65306;&#20851;&#20110;&#26368;&#36817;&#28798;&#38590;&#21644;&#37329;&#34701;&#30456;&#20851;&#38382;&#31572;&#23545;&#30340;&#22312;&#32447;&#25991;&#31456;&#12290;&#30001;&#20110;LLM Attributor&#23545;&#35745;&#31639;&#31508;&#35760;&#26412;&#30340;&#24191;&#27867;&#25903;&#25345;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23558;&#20854;&#25972;&#21512;&#21040;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01361v1 Announce Type: cross  Abstract: While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;PPF-QSNN&#65292;&#36890;&#36807;&#24182;&#34892;&#26041;&#24335;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#36755;&#20837;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#21508;&#33258;&#30340;&#36129;&#29486;&#27604;&#20363;&#21512;&#24182;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.01359</link><description>&lt;p&gt;
&#24182;&#34892;&#27604;&#20363;&#34701;&#21512;&#33033;&#20914;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20248;&#21270;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;PPF-QSNN&#65292;&#36890;&#36807;&#24182;&#34892;&#26041;&#24335;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#36755;&#20837;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#21508;&#33258;&#30340;&#36129;&#29486;&#27604;&#20363;&#21512;&#24182;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#65288;HQCNN&#65289;&#26550;&#26500;&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23558;&#37327;&#23376;&#21407;&#29702;&#38598;&#25104;&#21040;&#22686;&#24378;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#35745;&#31639;&#26041;&#38754;&#21487;&#33021;&#24102;&#26469;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;HQCNN&#24403;&#21069;&#30740;&#31350;&#30340;&#20018;&#34892;&#32467;&#26500;&#65292;&#20449;&#24687;&#20381;&#27425;&#20174;&#19968;&#20010;&#32593;&#32476;&#20256;&#36882;&#21040;&#21478;&#19968;&#20010;&#32593;&#32476;&#65292;&#32463;&#24120;&#20250;&#23545;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#24615;&#21644;&#34920;&#29616;&#21147;&#36896;&#25104;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24182;&#34892;&#27604;&#20363;&#34701;&#21512;&#37327;&#23376;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;PPF-QSNN&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25968;&#25454;&#38598;&#20449;&#24687;&#21516;&#26102;&#36755;&#20837;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#36755;&#20986;&#25353;&#29031;&#23427;&#20204;&#21508;&#33258;&#30340;&#36129;&#29486;&#27604;&#20363;&#21512;&#24182;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;PPF-QSNN&#21442;&#25968;&#23545;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01359v1 Announce Type: cross  Abstract: The recent emergence of the hybrid quantum-classical neural network (HQCNN) architecture has garnered considerable attention due to the potential advantages associated with integrating quantum principles to enhance various facets of machine learning algorithms and computations. However, the current investigated serial structure of HQCNN, wherein information sequentially passes from one network to another, often imposes limitations on the trainability and expressivity of the network. In this study, we introduce a novel architecture termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks (PPF-QSNN). The dataset information is simultaneously fed into both the spiking neural network and the variational quantum circuits, with the outputs amalgamated in proportion to their individual contributions. We systematically assess the impact of diverse PPF-QSNN parameters on network performance for image classification, aiming to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01358</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21457;&#29616;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Utilizing AI and Social Media Analytics to Discover Adverse Side Effects of GLP-1 Receptor Agonists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01358
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#19982;GLP-1&#21463;&#20307;&#28608;&#21160;&#21058;&#30456;&#20851;&#30340;21&#31181;&#28508;&#22312;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#26410;&#25253;&#21578;ASEs&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#65288;ASEs&#65289;&#22312;FDA&#25209;&#20934;&#21518;&#34987;&#21457;&#29616;&#65292;&#23545;&#24739;&#32773;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#21450;&#26102;&#21457;&#29616;&#34987;&#24573;&#35270;&#30340;ASEs&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23383;&#20581;&#24247;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#24050;&#21457;&#34920;&#30340;&#20020;&#24202;&#30740;&#31350;&#12289;&#21046;&#36896;&#21830;&#25253;&#21578;&#21644;ChatGPT&#31561;&#22823;&#37327;&#20844;&#24320;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;&#32925;&#32032;&#26679;&#32957;1&#21463;&#20307;&#28608;&#21160;&#21058;&#65288;GLP-1 RA&#65289;&#30456;&#20851;&#30340;ASEs&#65292;&#36825;&#19968;&#24066;&#22330;&#39044;&#35745;&#21040;2030&#24180;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#33267;1335&#20159;&#32654;&#20803;&#12290;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#26816;&#27979;&#20986;FDA&#25209;&#20934;&#26102;&#34987;&#24573;&#35270;&#30340;21&#31181;&#28508;&#22312;ASEs&#65292;&#21253;&#25324;&#26131;&#24594;&#21644;&#40635;&#26408;&#24863;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#26032;&#37096;&#32626;&#33647;&#29289;&#30456;&#20851;&#26410;&#25253;&#21578;&#30340;ASEs&#30340;&#26816;&#27979;&#65292;&#21033;&#29992;&#21069;&#27839;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#37322;&#25918;&#31038;&#20132;&#23186;&#20307;&#30340;&#21147;&#37327;&#26469;&#25903;&#25345;&#30417;&#31649;&#26426;&#26500;&#21644;&#21046;&#36896;&#21830;&#22312;&#24066;&#22330;&#19978;&#22686;&#21152;&#26032;&#33647;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01358v1 Announce Type: cross  Abstract: Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a threat to patient safety. To promptly detect overlooked ASEs, we developed a digital health methodology capable of analyzing massive public data from social media, published clinical research, manufacturers' reports, and ChatGPT. We uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists (GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by 2030. Using a Named Entity Recognition (NER) model, our method successfully detected 21 potential ASEs overlooked upon FDA approval, including irritability and numbness. Our data-analytic approach revolutionizes the detection of unreported ASEs associated with newly deployed drugs, leveraging cutting-edge AI-driven social media analytics. It can increase the safety of new drugs in the marketplace by unlocking the power of social media to support regulators and manufacturers in the ra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01356</link><description>&lt;p&gt;
&#36755;&#20837;&#25200;&#21160;&#23545;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
The Double-Edged Sword of Input Perturbations to Robust Accurate Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#34987;&#35748;&#20026;&#23545;&#25932;&#23545;&#36755;&#20837;&#25200;&#21160;&#25935;&#24863;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#20849;&#21516;&#34920;&#24449;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#12290;&#40065;&#26834;&#20934;&#30830;&#20844;&#24179;&#24615;&#35201;&#27714;&#24403;&#23454;&#20363;&#21450;&#20854;&#30456;&#20284;&#23545;&#24212;&#29289;&#21463;&#21040;&#36755;&#20837;&#25200;&#21160;&#26102;&#65292;&#39044;&#27979;&#19982;&#22320;&#38754;&#20107;&#23454;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25932;&#23545;&#25915;&#20987;&#26041;&#27861;RAFair&#65292;&#20197;&#26292;&#38706;DNN&#20013;&#30340;&#34394;&#20551;&#25110;&#20559;&#35265;&#25932;&#23545;&#32570;&#38519;&#65292;&#36825;&#20123;&#32570;&#38519;&#20250;&#27450;&#39575;&#20934;&#30830;&#24615;&#25110;&#25439;&#23475;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#26679;&#30340;&#25932;&#23545;&#23454;&#20363;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#33391;&#24615;&#25200;&#21160;&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#32780;&#20844;&#24179;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#36755;&#20837;&#23545;&#20934;&#30830;&#20844;&#24179;&#24615;&#30340;&#21452;&#20995;&#21073;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01356v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) are known to be sensitive to adversarial input perturbations, leading to a reduction in either prediction accuracy or individual fairness. To jointly characterize the susceptibility of prediction accuracy and individual fairness to adversarial perturbations, we introduce a novel robustness definition termed robust accurate fairness. Informally, robust accurate fairness requires that predictions for an instance and its similar counterparts consistently align with the ground truth when subjected to input perturbations. We propose an adversarial attack approach dubbed RAFair to expose false or biased adversarial defects in DNN, which either deceive accuracy or compromise individual fairness. Then, we show that such adversarial instances can be effectively addressed by carefully designed benign perturbations, correcting their predictions to be accurate and fair. Our work explores the double-edged sword of input 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.01353</link><description>&lt;p&gt;
&#29992;&#20110;&#36793;&#32536;&#24212;&#29992;&#30340;LLM&#39640;&#25928;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Distilling LLMs for Edge Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01353
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;LLMs&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#20855;&#26377;&#24456;&#22823;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#23427;&#36171;&#20104;&#20102;&#20197;&#22266;&#23450;&#25104;&#26412;&#20135;&#29983;&#19981;&#21516;&#22823;&#23567;/&#24310;&#36831;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLFS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21442;&#25968;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#21830;&#19994;&#36793;&#32536;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#34429;&#28982;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#23545;&#21387;&#32553;&#20855;&#26377;&#30456;&#24403;&#30340;&#25269;&#25239;&#21147;&#65292;&#20294;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#35299;&#30721;&#22120;&#36827;&#34892;&#20999;&#29255;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01353v1 Announce Type: cross  Abstract: Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#31890;&#23376;&#36712;&#36857;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#27969;&#32447;&#25110;&#36335;&#24452;&#32447;&#25429;&#33719;&#30340;&#27969;&#22330;&#30340;&#21306;&#22495;/&#23616;&#37096;&#29305;&#24449;&#26469;&#25552;&#39640;&#28065;&#26059;&#36793;&#30028;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01352</link><description>&lt;p&gt;
VortexViz: &#36890;&#36807;&#23398;&#20064;&#31890;&#23376;&#36712;&#36857;&#26469;&#21457;&#29616;&#28065;&#26059;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
VortexViz: Finding Vortex Boundaries by Learning from Particle Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#31890;&#23376;&#36712;&#36857;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#27969;&#32447;&#25110;&#36335;&#24452;&#32447;&#25429;&#33719;&#30340;&#27969;&#22330;&#30340;&#21306;&#22495;/&#23616;&#37096;&#29305;&#24449;&#26469;&#25552;&#39640;&#28065;&#26059;&#36793;&#30028;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28065;&#26059;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#37117;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#22312;&#25552;&#20379;&#27969;&#20307;&#27969;&#21160;&#34892;&#20026;&#27934;&#23519;&#30340;&#21516;&#26102;&#65292;&#21487;&#35270;&#21270;&#28065;&#26059;&#30340;&#36793;&#30028;&#23545;&#20110;&#29702;&#35299;&#27969;&#21160;&#29616;&#35937;&#21644;&#26816;&#27979;&#27969;&#21160;&#19981;&#35268;&#21017;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20934;&#30830;&#25552;&#21462;&#28065;&#26059;&#36793;&#30028;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#36895;&#24230;&#20998;&#37327;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#31890;&#23376;&#36712;&#36857;&#65288;&#27969;&#32447;&#25110;&#36335;&#24452;&#32447;&#65289;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#27969;&#32447;&#25110;&#36335;&#24452;&#32447;&#25429;&#33719;&#30340;&#27969;&#22330;&#30340;&#21306;&#22495;/&#23616;&#37096;&#29305;&#24449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;&#28065;&#26059;&#36793;&#30028;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01352v1 Announce Type: cross  Abstract: Vortices are studied in various scientific disciplines, offering insights into fluid flow behavior. Visualizing the boundary of vortices is crucial for understanding flow phenomena and detecting flow irregularities. This paper addresses the challenge of accurately extracting vortex boundaries using deep learning techniques. While existing methods primarily train on velocity components, we propose a novel approach incorporating particle trajectories (streamlines or pathlines) into the learning process. By leveraging the regional/local characteristics of the flow field captured by streamlines or pathlines, our methodology aims to enhance the accuracy of vortex boundary extraction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#20934;&#30830;&#24615;&#20272;&#35745;&#24182;&#22312;&#36866;&#24212;&#22833;&#36133;&#24773;&#20917;&#19979;&#23637;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.01351</link><description>&lt;p&gt;
AETTA: &#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#26469;&#25913;&#36827;&#20934;&#30830;&#24615;&#20272;&#35745;&#24182;&#22312;&#36866;&#24212;&#22833;&#36133;&#24773;&#20917;&#19979;&#23637;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#26469;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#39046;&#22495;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;TTA&#38754;&#20020;&#30528;&#36866;&#24212;&#22833;&#36133;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#21160;&#24577;&#22330;&#26223;&#20013;&#23427;&#20381;&#36182;&#20110;&#23545;&#26410;&#30693;&#27979;&#35797;&#26679;&#26412;&#30340;&#30450;&#30446;&#36866;&#24212;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#22312;TTA&#32972;&#26223;&#19979;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26377;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AETTA&#65292;&#19968;&#20010;&#29992;&#20110;TTA&#30340;&#26080;&#26631;&#31614;&#20934;&#30830;&#24615;&#20272;&#35745;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#27169;&#22411;&#39044;&#27979;&#21644;dropout&#25512;&#26029;&#26469;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#27979;&#19981;&#19968;&#33268;&#24615;&#20197;&#25193;&#23637;AETTA&#22312;&#36866;&#24212;&#22833;&#36133;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#22522;&#32447;&#21644;&#20845;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AETTA&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;19.8%&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01351v1 Announce Type: cross  Abstract: Test-time adaptation (TTA) has emerged as a viable solution to adapt pre-trained models to domain shifts using unlabeled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requiring labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algorithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estimation compared with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2404.01349</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#19968;&#20010;&#20998;&#31867;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fairness in Large Language Models: A Taxonomic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#20559;&#35265;&#22240;&#32032;&#30340;&#20998;&#26512;&#12289;&#20844;&#24179;&#24230;&#37327;&#21644;&#29616;&#26377;&#31639;&#27861;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#32570;&#20047;&#20844;&#24179;&#24615;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#26576;&#20123;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#20419;&#20351;&#23545;&#20844;&#24179;&#30340;LLMs&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#30456;&#21453;&#65292;&#22312;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#28041;&#21450;&#29420;&#29305;&#30340;&#32972;&#26223;&#12289;&#20998;&#31867;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#35813;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#20844;&#24179;LLMs&#30340;&#29616;&#26377;&#25991;&#29486;&#30740;&#31350;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;LLMs&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#25509;&#30528;&#20998;&#26512;&#20102;&#23548;&#33268;LLMs&#20559;&#35265;&#30340;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#35752;&#35770;&#20102;LLMs&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#24635;&#32467;&#20102;&#35780;&#20272;LLMs&#20559;&#35265;&#30340;&#25351;&#26631;&#21644;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01349v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms 
&lt;/p&gt;</description></item><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>DiffAgent&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;DABench&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01342</link><description>&lt;p&gt;
DiffAgent&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01342
&lt;/p&gt;
&lt;p&gt;
DiffAgent&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;API&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;DABench&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#22312;&#23398;&#26415;&#30740;&#31350;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#25214;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20855;&#20351;&#29992;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;DiffAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#35774;&#35745;&#29992;&#20110;&#36890;&#36807;API&#35843;&#29992;&#22312;&#20960;&#31186;&#38047;&#20869;&#36827;&#34892;&#20934;&#30830;&#36873;&#25321;&#30340;LLM&#20195;&#29702;&#12290;DiffAgent&#21033;&#29992;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;SFTA&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#20559;&#22909;&#31934;&#30830;&#22320;&#23558;T2I API&#21709;&#24212;&#19982;&#29992;&#25143;&#36755;&#20837;&#23545;&#40784;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;DiffAgent&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DABench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;T2I API&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01342v1 Announce Type: cross  Abstract: Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent's capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2404.01341</link><description>&lt;p&gt;
&#22359;&#23545;&#35282;&#24341;&#23548;&#30340;DBSCAN&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Block-Diagonal Guided DBSCAN Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#24341;&#23548;&#32858;&#31867;&#36807;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22359;&#23545;&#35282;&#22270;&#24182;&#36827;&#34892;&#32858;&#31867;&#25490;&#24207;&#65292;&#26131;&#20110;&#30830;&#23450;&#32858;&#31867;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20998;&#26512;&#22312;&#25968;&#25454;&#24211;&#25366;&#25496;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;DBSCAN&#12290;&#28982;&#32780;&#65292;DBSCAN&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#38590;&#20197;&#22788;&#29702;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#12289;&#23545;&#36755;&#20837;&#21442;&#25968;&#25935;&#24863;&#20197;&#21450;&#22312;&#20135;&#29983;&#32858;&#31867;&#32467;&#26524;&#26102;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;DBSCAN&#65292;&#21033;&#29992;&#20102;&#30456;&#20284;&#24615;&#22270;&#30340;&#22359;&#23545;&#35282;&#23646;&#24615;&#26469;&#24341;&#23548;DBSCAN&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22270;&#65292;&#34913;&#37327;&#39640;&#32500;&#22823;&#35268;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#26410;&#30693;&#32622;&#25442;&#36716;&#25442;&#20026;&#22359;&#23545;&#35282;&#24418;&#24335;&#65292;&#38543;&#21518;&#36890;&#36807;&#19968;&#20010;&#32858;&#31867;&#25490;&#24207;&#36807;&#31243;&#26469;&#29983;&#25104;&#26399;&#26395;&#30340;&#32622;&#25442;&#12290;&#32858;&#31867;&#32467;&#26500;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#32622;&#25442;&#21518;&#22270;&#20013;&#30340;&#23545;&#35282;&#22359;&#26469;&#36731;&#26494;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01341v1 Announce Type: cross  Abstract: Cluster analysis plays a crucial role in database mining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the propose
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01340</link><description>&lt;p&gt;
&#20174;&#30456;&#20284;&#21040;&#20248;&#36234;&#65306;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Similarity to Superiority: Channel Clustering for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01340
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29420;&#31435;&#36890;&#36947;&#31574;&#30053;&#36890;&#36807;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#36890;&#36947;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#26410;&#30693;&#23454;&#20363;&#19978;&#23548;&#33268;&#20102;&#24046;&#21170;&#30340;&#27867;&#21270;&#65292;&#24182;&#24573;&#30053;&#20102;&#36890;&#36947;&#20043;&#38388;&#28508;&#22312;&#30340;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#20381;&#36182;&#36890;&#36947;&#31574;&#30053;&#23558;&#25152;&#26377;&#36890;&#36947;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#29978;&#33267;&#21253;&#21547;&#26080;&#20851;&#32039;&#35201;&#21644;&#38543;&#24847;&#30340;&#20449;&#24687;&#65292;&#28982;&#32780;&#36825;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#24182;&#38480;&#21046;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#24179;&#34913;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#21448;&#19981;&#24573;&#35270;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#30340;&#36890;&#36947;&#31574;&#30053;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#32451;&#20064;&#25552;&#39640;&#23545;&#28151;&#21512;&#36890;&#36947;&#30340;&#32467;&#26524;&#19982;&#19968;&#23545;&#36890;&#36947;&#20043;&#38388;&#26412;&#36136;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01340v1 Announce Type: cross  Abstract: Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adapt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20154;&#31867;&#24773;&#24863;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#65292;&#20351;&#24471;&#31995;&#32479;&#33021;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#65292;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2404.01339</link><description>&lt;p&gt;
&#36890;&#36807;&#38646;-shot&#24773;&#32490;&#21644;&#19981;&#27969;&#30021;&#29983;&#25104;&#23454;&#29616;&#20154;&#24615;&#21270;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20154;&#31867;&#24773;&#24863;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#65292;&#20351;&#24471;&#31995;&#32479;&#33021;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#65292;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#23545;&#35805;&#31995;&#32479;&#24448;&#24448;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#30340;&#22238;&#24212;&#32570;&#20047;&#20154;&#31867;&#20132;&#20114;&#30340;&#24773;&#24863;&#28145;&#24230;&#21644;&#19981;&#27969;&#30021;&#29305;&#24449;&#12290;&#36825;&#31181;&#32570;&#22833;&#22312;&#29992;&#25143;&#23547;&#27714;&#26356;&#20010;&#24615;&#21270;&#21644;&#26377;&#20849;&#24773;&#30340;&#20114;&#21160;&#26102;&#23588;&#20026;&#26126;&#26174;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#23427;&#20204;&#26174;&#24471;&#26426;&#26800;&#21270;&#65292;&#38590;&#20197;&#24341;&#36215;&#20154;&#31867;&#29992;&#25143;&#30340;&#20849;&#40483;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30528;&#25163;&#20110;&#20154;&#24615;&#21270;&#26426;&#22120;&#36890;&#20449;&#30340;&#26053;&#31243;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#19981;&#20165;&#29702;&#35299;&#32780;&#19988;&#20849;&#40483;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35821;&#38899;&#21512;&#25104;&#27969;&#31243;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#20869;&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#24773;&#24863;&#21644;&#35821;&#35328;&#32010;&#20081;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26399;&#38388;&#26080;&#32541;&#38598;&#25104;&#21040;&#29983;&#25104;&#25991;&#26412;&#20013;&#65292;&#20351;&#31995;&#32479;&#26356;&#22909;&#22320;&#27169;&#20223;&#20154;&#31867;&#35821;&#38899;&#27169;&#24335;&#65292;&#20419;&#36827;&#26356;&#30452;&#35266;&#21644;&#33258;&#28982;&#30340;&#29992;&#25143;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01339v1 Announce Type: cross  Abstract: Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. The
&lt;/p&gt;</description></item><item><title>FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2404.01336</link><description>&lt;p&gt;
FineFake&#65306;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30693;&#35782;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain Fake News Detecction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01336
&lt;/p&gt;
&lt;p&gt;
FineFake &#25968;&#25454;&#38598;&#20026;&#32454;&#31890;&#24230;&#22810;&#39046;&#22495;&#20551;&#26032;&#38395;&#26816;&#27979;&#25552;&#20379;&#20102;&#30693;&#35782;&#22686;&#24378;&#65292;&#21253;&#21547;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#35206;&#30422;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35780;&#20272;&#26032;&#38395;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#36890;&#24120;&#20165;&#20851;&#27880;&#21333;&#19968;&#35821;&#20041;&#20027;&#39064;&#30340;&#26032;&#38395;&#25110;&#26469;&#33258;&#21333;&#19968;&#24179;&#21488;&#30340;&#26032;&#38395;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#22330;&#26223;&#20013;&#22810;&#39046;&#22495;&#26032;&#38395;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#19981;&#21516;&#39046;&#22495;&#30340;&#20551;&#26032;&#38395;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#25552;&#20379;&#31934;&#30830;&#35777;&#25454;&#24182;&#25581;&#31034;&#21046;&#36896;&#20551;&#26032;&#38395;&#30340;&#22810;&#26679;&#28508;&#22312;&#31574;&#30053;&#65292;&#32780;&#36825;&#20063;&#26159;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#25152;&#24573;&#30053;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;FineFake&#30340;&#26032;&#22411;&#22810;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;FineFake&#28085;&#30422;&#20102;&#26469;&#33258;&#20845;&#20010;&#35821;&#20041;&#20027;&#39064;&#21644;&#20843;&#20010;&#24179;&#21488;&#30340;16,909&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#27599;&#20010;&#26032;&#38395;&#39033;&#30446;&#37117;&#21253;&#21547;&#22810;&#27169;&#24577;&#20869;&#23481;&#12289;&#28508;&#22312;&#31038;&#20132;&#32972;&#26223;&#12289;&#21322;&#33258;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01336v1 Announce Type: cross  Abstract: Existing benchmarks for fake news detection have significantly contributed to the advancement of models in assessing the authenticity of news content. However, these benchmarks typically focus solely on news pertaining to a single semantic topic or originating from a single platform, thereby failing to capture the diversity of multi-domain news in real scenarios. In order to understand fake news across various domains, the external knowledge and fine-grained annotations are indispensable to provide precise evidence and uncover the diverse underlying strategies for fabrication, which are also ignored by existing benchmarks. To address this gap, we introduce a novel multi-domain knowledge-enhanced benchmark with fine-grained annotations, named \textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six semantic topics and eight platforms. Each news item is enriched with multi-modal content, potential social context, semi-man
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01335</link><description>&lt;p&gt;
&#24314;&#31569;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Architectural Design: A Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01335
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#19968;&#36235;&#21183;&#21463;&#30410;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#22312;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23457;&#26597;&#26469;&#33258;2020&#24180;&#30340;&#26368;&#26032;&#25991;&#29486;&#65292;&#26412;&#25991;&#23457;&#35270;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#65292;&#20174;&#29983;&#25104;&#21021;&#22987;&#24314;&#31569;3D&#24418;&#24335;&#21040;&#29983;&#25104;&#26368;&#32456;&#24314;&#31569;&#22270;&#20687;&#12290;&#30740;&#31350;&#22686;&#38271;&#30340;&#26126;&#26174;&#36235;&#21183;&#34920;&#26126;&#24314;&#31569;&#35774;&#35745;&#39046;&#22495;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20542;&#21521;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 Announce Type: cross  Abstract: Generative Artificial Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of generative AI technologies in architectural design, a trend that has benefited from the rapid development of deep generative models. This article provides a comprehensive review of the basic principles of generative AI and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of generative AI technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01331</link><description>&lt;p&gt;
LLaVA-Gemma&#65306;&#21033;&#29992;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01331
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27969;&#34892;&#30340;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MMFM&#65289;&#12290;&#29305;&#21035;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;2B&#21442;&#25968;&#30340;Gemma&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#26500;&#24314;&#21151;&#33021;&#24378;&#22823;&#30340;&#23567;&#35268;&#27169;MMFM&#30340;&#26426;&#20250;&#12290;&#19982;&#35813;&#39046;&#22495;&#20854;&#20182;&#35770;&#25991;&#30340;&#21457;&#29616;&#19968;&#33268;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21435;&#38500;&#19977;&#31181;&#35774;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65306;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#65292;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#65292;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;LLaVA-Gemma&#30340;&#32467;&#26524;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20013;&#31561;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;&#24403;&#21069;&#30456;&#23545;&#22823;&#23567;&#30340;SOTA&#27169;&#22411;&#12290;&#24615;&#33021;&#30340;&#26356;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#25928;&#26524;&#65306;&#36339;&#36807;&#39044;&#35757;&#32451;&#24448;&#24448;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#26356;&#22823;&#30340;&#35270;&#35273;&#27169;&#22411;&#26377;&#26102;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#35757;&#32451;&#37197;&#26041;&#65292;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01331v1 Announce Type: cross  Abstract: We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code an
&lt;/p&gt;</description></item><item><title>EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.01327</link><description>&lt;p&gt;
&#26080;&#25277;&#35937;&#33021;&#21147;&#30340;&#32769;&#24180;&#20154;&#25968;&#23383;&#21253;&#23481;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01327
&lt;/p&gt;
&lt;p&gt;
EBER chatbot&#26159;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;&#23089;&#20048;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20854;"&#26234;&#33021;&#30005;&#21488;"&#27010;&#24565;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20801;&#35768;&#21019;&#24314;&#23545;&#35805;&#24335;&#32842;&#22825;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#22823;&#20247;&#24066;&#22330;&#39046;&#22495;&#20173;&#28982;&#36807;&#20110;&#19981;&#25104;&#29087;&#65292;&#26080;&#27861;&#25903;&#25345;&#20196;&#20154;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#20294;&#23545;&#35805;&#24335;&#30028;&#38754;&#24050;&#32463;&#22312;&#20020;&#26102;&#24212;&#29992;&#20013;&#25214;&#21040;&#20102;&#33258;&#24049;&#30340;&#20301;&#32622;&#65292;&#27604;&#22914;&#30005;&#35805;&#20013;&#24515;&#21644;&#22312;&#32447;&#36141;&#29289;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23558;&#20854;&#24212;&#29992;&#20110;&#32769;&#24180;&#20154;&#30340;&#31038;&#20250;&#21253;&#23481;&#65292;&#32769;&#24180;&#20154;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#25968;&#23383;&#40511;&#27807;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#32769;&#24180;&#20154;&#36890;&#36807;&#20256;&#32479;&#23186;&#20307;&#22914;&#30005;&#35270;&#21644;&#24191;&#25773;&#26469;&#20943;&#36731;&#23396;&#29420;&#24863;&#65292;&#36825;&#20123;&#23186;&#20307;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#38506;&#20276;&#24863;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26088;&#22312;&#20943;&#23569;&#32769;&#24180;&#20154;&#25968;&#23383;&#40511;&#27807;&#30340;EBER&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;EBER&#20250;&#22312;&#21518;&#21488;&#38405;&#35835;&#26032;&#38395;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#35843;&#25972;&#22238;&#22797;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#8220;&#26234;&#33021;&#30005;&#21488;&#8221;&#27010;&#24565;&#65292;&#26681;&#25454;&#35813;&#27010;&#24565;&#65292;&#19981;&#26159;&#31616;&#21270;&#25968;&#23383;&#20449;&#24687;&#31995;&#32479;&#20197;&#20351;&#20854;&#26131;&#20110;&#35775;&#38382;&#65292;&#32780;&#26159;&#26681;&#25454;&#29992;&#25143;&#30340;&#24515;&#24773;&#21644;&#38656;&#27714;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01327v1 Announce Type: cross  Abstract: Current language processing technologies allow the creation of conversational chatbot platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER chatbot, designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user's mood. Its novelty lies in the concept of "intelligent radio", according to which, instead of simplifying a digital information system to make it accessible to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#21457;&#23637;&#21382;&#31243;&#12289;transformer-based &#26550;&#26500;&#30340;&#36827;&#23637;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#30340;&#20316;&#29992;</title><link>https://arxiv.org/abs/2404.01322</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#19982;&#35270;&#35273;&#27169;&#22411;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Multi-Modal Large Language and Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;LLMs&#30340;&#21457;&#23637;&#21382;&#31243;&#12289;transformer-based &#26550;&#26500;&#30340;&#36827;&#23637;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#25104;&#20026;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#28966;&#28857;&#65292;&#20854;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#21463;&#21040;&#20102;&#25512;&#21160;&#12290;&#26356;&#36817;&#26399;&#65292;LLMs&#34987;&#25193;&#23637;&#20026;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#65292;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#22788;&#29702;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#38500;&#20102;&#25991;&#26412;&#12290;&#36825;&#25171;&#24320;&#20102;&#35832;&#22914;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#12289;&#22270;&#20687;&#23383;&#24149;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#31561;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#23558;LLM&#19982;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#21518;&#26399;&#35843;&#25972;&#65292;&#25110;&#32773;&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;MM-LLM&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20855;&#26377;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#24403;&#21069;LLMs&#20197;&#21450;&#26368;&#26032;&#30340;MM-LLMs&#30340;&#29616;&#29366;&#12290;&#23427;&#28085;&#30422;&#20102;LLMs&#30340;&#21382;&#21490;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#30001;transformer-based &#26550;&#26500;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#21644;Google&#30340;BERT&#25552;&#20379;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#27880;&#24847;&#26426;&#21046;&#22312;&#22686;&#24378;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01322v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have recently emerged as a focal point of research and application, driven by their unprecedented ability to understand and generate text with human-like quality. Even more recently, LLMs have been extended into multi-modal large language models (MM-LLMs) which extends their capabilities to deal with image, video and audio information, in addition to text. This opens up applications like text-to-video generation, image captioning, text-to-speech, and more and is achieved either by retro-fitting an LLM with multi-modal capabilities, or building a MM-LLM from scratch. This paper provides an extensive review of the current state of those LLMs with multi-modal capabilities as well as the very recent MM-LLMs. It covers the historical development of LLMs especially the advances enabled by transformer-based architectures like OpenAI's GPT series and Google's BERT, as well as the role of attention mechanisms in enh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Moby Bikes&#30340;&#34892;&#31243;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#22320;&#29702;&#26102;&#38388;&#22270;&#65292;&#25581;&#31034;&#20102;&#22312;&#25193;&#23637;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#26102;&#24314;&#31435;&#26032;&#31449;&#28857;&#30340;&#26368;&#20339;&#20301;&#32622;&#65292;&#24182;&#21033;&#29992;Louvain&#31639;&#27861;&#25581;&#31034;&#20102;&#23637;&#29616;&#30456;&#20284;&#20351;&#29992;&#27169;&#24335;&#30340;&#33258;&#21253;&#21547;&#23376;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2404.01320</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26080;&#26729;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#32593;&#32476;&#25193;&#24352;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Optimisation of Network Expansion in a Dockless Bike Sharing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Moby Bikes&#30340;&#34892;&#31243;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#22320;&#29702;&#26102;&#38388;&#22270;&#65292;&#25581;&#31034;&#20102;&#22312;&#25193;&#23637;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#26102;&#24314;&#31435;&#26032;&#31449;&#28857;&#30340;&#26368;&#20339;&#20301;&#32622;&#65292;&#24182;&#21033;&#29992;Louvain&#31639;&#27861;&#25581;&#31034;&#20102;&#23637;&#29616;&#30456;&#20284;&#20351;&#29992;&#27169;&#24335;&#30340;&#33258;&#21253;&#21547;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#65288;BSSs&#65289;&#37096;&#32626;&#22312;&#20840;&#29699;&#19968;&#21315;&#22810;&#20010;&#22478;&#24066;&#65292;&#24182;&#22312;&#35768;&#22810;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;BSSs&#32531;&#35299;&#20102;&#25317;&#22581;&#65292;&#20943;&#23569;&#20102;&#27745;&#26579;&#24182;&#20419;&#36827;&#20102;&#20307;&#32946;&#38203;&#28860;&#12290;&#25506;&#32034;&#20849;&#20139;&#21333;&#36710;&#38656;&#27714;&#30340;&#26102;&#31354;&#27169;&#24335;&#20197;&#21450;&#24433;&#21709;&#36825;&#20123;&#27169;&#24335;&#30340;&#22240;&#32032;&#23545;&#20110;&#20248;&#21270;&#31995;&#32479;&#36816;&#33829;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;Moby Bikes&#30340;&#34892;&#31243;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;&#22320;&#29702;&#26102;&#38388;&#22270;&#12290;&#20248;&#21270;&#22270;&#30340;&#36807;&#31243;&#25581;&#31034;&#20102;&#22312;&#23558;&#26469;&#25193;&#23637;BSS&#26102;&#24314;&#31435;&#26032;&#31449;&#28857;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#20351;&#29992;Louvain&#31639;&#27861;&#65292;&#19968;&#31181;&#31038;&#21306;&#26816;&#27979;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#27700;&#24179;&#19978;&#30340;&#20351;&#29992;&#27169;&#24335;&#12290;&#31038;&#21306;&#26816;&#27979;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#21508;&#33258;&#30340;&#26102;&#38388;&#31890;&#24230;&#32423;&#21035;&#19978;&#34920;&#29616;&#20986;&#31867;&#20284;&#20351;&#29992;&#27169;&#24335;&#30340;&#20027;&#35201;&#33258;&#21253;&#21547;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01320v1 Announce Type: cross  Abstract: Bike-sharing systems (BSSs) are deployed in over a thousand cities worldwide and play an important role in many urban transportation systems. BSSs alleviate congestion, reduce pollution and promote physical exercise. It is essential to explore the spatiotemporal patterns of bike-sharing demand, as well as the factors that influence these patterns, in order to optimise system operational efficiency. In this study, an optimised geo-temporal graph is constructed using trip data from Moby Bikes, a dockless BSS operator. The process of optimising the graph unveiled prime locations for erecting new stations during future expansions of the BSS. The Louvain algorithm, a community detection technique, is employed to uncover usage patterns at different levels of temporal granularity. The community detection results reveal largely self-contained sub-networks that exhibit similar usage patterns at their respective levels of temporal granularity. O
&lt;/p&gt;</description></item><item><title>&#22823;&#25968;&#25454;&#26102;&#20195;&#24102;&#26469;&#20102;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#26426;&#36935;&#65292;&#20294;&#24212;&#29992;&#39046;&#22495;&#38480;&#21046;&#20102;&#36328;&#23398;&#31185;&#39044;&#27979;&#26041;&#27861;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21069;&#27839;&#30740;&#31350;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.01319</link><description>&lt;p&gt;
&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Information Cascade Prediction under Public Emergencies: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01319
&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#24102;&#26469;&#20102;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#26426;&#36935;&#65292;&#20294;&#24212;&#29992;&#39046;&#22495;&#38480;&#21046;&#20102;&#36328;&#23398;&#31185;&#39044;&#27979;&#26041;&#27861;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21069;&#27839;&#30740;&#31350;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#28023;&#37327;&#20449;&#24687;&#12289;&#19987;&#23478;&#32463;&#39564;&#21644;&#39640;&#20934;&#30830;&#24230;&#27169;&#22411;&#20026;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#21508;&#20010;&#23398;&#31185;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#21442;&#19982;&#23548;&#33268;&#20102;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#22320;&#38663;&#12289;&#27946;&#28798;&#12289;&#20256;&#26579;&#30149;&#65289;&#19978;&#12290;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#27979;&#26694;&#26550;&#23545;&#36328;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#25552;&#20379;&#20102;&#20449;&#24687;&#32423;&#32852;&#24314;&#27169;&#12289;&#39044;&#27979;&#21644;&#24212;&#29992;&#30340;&#31995;&#32479;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35782;&#21035;&#21069;&#27839;&#30740;&#31350;&#65292;&#24182;&#29702;&#35299;&#20844;&#20849;&#32039;&#24613;&#20107;&#20214;&#19979;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#36890;&#36807;&#24635;&#32467;&#24453;&#35299;&#20915;&#38382;&#39064;&#24182;&#27010;&#36848;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#20449;&#24687;&#32423;&#32852;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#30340;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01319v1 Announce Type: cross  Abstract: With the advent of the era of big data, massive information, expert experience, and high-accuracy models bring great opportunities to the information cascade prediction of public emergencies. However, the involvement of specialist knowledge from various disciplines has resulted in a primarily application-specific focus (e.g., earthquakes, floods, infectious diseases) for information cascade prediction of public emergencies. The lack of a unified prediction framework poses a challenge for classifying intersectional prediction methods across different application fields. This survey paper offers a systematic classification and summary of information cascade modeling, prediction, and application. We aim to help researchers identify cutting-edge research and comprehend models and methods of information cascade prediction under public emergencies. By summarizing open issues and outlining future directions in this field, this paper has the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2404.01317</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#20197;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#26234;&#33021;&#23398;&#20064;&#29575;&#20998;&#24067;&#21462;&#24471;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;GLUE&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#36136;&#30097;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#32593;&#32476;&#37319;&#29992;&#30456;&#21516;&#23398;&#20064;&#29575;&#30340;&#24494;&#35843;&#24120;&#35265;&#20570;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#65292;&#25214;&#21040;&#20102;&#27604;&#24179;&#22374;&#23398;&#20064;&#29575;&#26356;&#22909;&#30340;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;GLUE&#25968;&#25454;&#38598;&#20013;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#39564;&#35777;&#20102;&#36825;&#20123;&#23398;&#20064;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;</title><link>https://arxiv.org/abs/2404.01308</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Job Shop Scheduling under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#35299;&#20915;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#26159;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#38656;&#35201;&#22312;&#26426;&#22120;&#19978;&#36827;&#34892;&#35843;&#24230;&#65292;&#20197;&#26368;&#23567;&#21270;&#35832;&#22914;&#26368;&#22823;&#23436;&#24037;&#26102;&#38388;&#25110;&#24310;&#36831;&#31561;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25345;&#32493;&#26102;&#38388;&#20851;&#32852;&#20102;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#31283;&#20581;&#30340;&#35843;&#24230;&#65292;&#21363;&#26368;&#23567;&#21270;&#24179;&#22343;&#23436;&#24037;&#26102;&#38388;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25216;&#26415;&#26469;&#23547;&#25214;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290;&#26412;&#30740;&#31350;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;DRL&#22312;JSSP&#24212;&#29992;&#20013;&#30340;&#36827;&#23637;&#65292;&#22686;&#24378;&#27867;&#21270;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#65288;2&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#25345;&#32493;&#26102;&#38388;&#30340;JSSP&#12290; Wheatley&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;DRL&#65292;&#24050;&#20844;&#24320;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01308v1 Announce Type: new  Abstract: Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem where tasks need to be scheduled on machines in order to minimize criteria such as makespan or delay. To address more realistic scenarios, we associate a probability distribution with the duration of each task. Our objective is to generate a robust schedule, i.e. that minimizes the average makespan. This paper introduces a new approach that leverages Deep Reinforcement Learning (DRL) techniques to search for robust solutions, emphasizing JSSPs with uncertain durations. Key contributions of this research include: (1) advancements in DRL applications to JSSPs, enhancing generalization and scalability, (2) a novel method for addressing JSSPs with uncertain durations. The Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL, is made publicly available for further research and applications.
&lt;/p&gt;</description></item><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20998;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#26102;&#33021;&#22815;&#34701;&#20837;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.01258</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22870;&#21169;&#19979;&#30340;&#35270;&#39057;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20998;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#26102;&#33021;&#22815;&#34701;&#20837;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#25216;&#26415;&#65292;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#24050;&#35777;&#26126;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#35270;&#39057;&#25351;&#20196;&#36319;&#38543;&#30340;&#20219;&#21153;&#20013;&#65292;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#26816;&#27979;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#25351;&#23548;&#20559;&#22909;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#20934;&#30830;&#35780;&#20272;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#24615;&#19982;&#23545;&#24212;&#35270;&#39057;&#30456;&#27604;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#20986;&#32467;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#26469;&#20026;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#25171;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;OpenAI GPT-4V&#27169;&#22411;&#30340;&#22870;&#21169;&#26426;&#21046;&#30340;&#31283;&#20581;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01258v1 Announce Type: cross  Abstract: Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01041</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#22312;&#19981;&#36879;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20854;&#20182;LLM&#30340;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs get help from other LLMs without revealing private information?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#22914;&#26524;&#26412;&#22320;&#27169;&#22411;&#26080;&#27861;&#21333;&#29420;&#20934;&#30830;&#26631;&#35760;&#29992;&#25143;&#25968;&#25454;&#65292;&#21017;&#21487;&#20197;&#26597;&#35810;&#19968;&#20010;&#22823;&#22411;&#30340;&#36828;&#31243;&#27169;&#22411;&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#26381;&#21153;&#22534;&#26632;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#32423;&#32852;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#22320;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#26500;&#25104;&#29992;&#25143;&#30340;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#34987;&#36716;&#21457;&#21040;&#36828;&#31243;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27492;&#31867;&#35774;&#32622;&#20013;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26041;&#27861;&#26159;&#20026;&#26412;&#22320;&#27169;&#22411;&#37197;&#22791;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#32780;&#20943;&#23569;&#35775;&#38382;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#37327;&#21270;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#31038;&#20132;&#23398;&#20064;&#33539;&#24335;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01030</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#35843;&#26597;&#65306;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;DALLE-3&#21644;Google&#30340;Gemini&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#25552;&#31034;&#20063;&#21487;&#33021;&#23548;&#33268;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23637;&#29616;&#26126;&#26174;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;&#31038;&#20250;&#20013;&#30340;&#20998;&#37197;&#21644;&#20195;&#34920;&#24615;&#20260;&#23475;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#23569;&#25968;&#32676;&#20307;&#12290;&#37492;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#35843;&#26597;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#20013;&#20559;&#35265;&#30340;&#19981;&#21516;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#20840;&#38754;&#22238;&#39038;&#20173;&#28982;&#32570;&#20047;&#65292;&#38459;&#30861;&#20102;&#23545;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#30340;&#31995;&#32479;&#24615;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#31532;&#19968;&#27425;&#24191;&#27867;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20808;&#21069;&#20851;&#20110;&#20559;&#35265;&#32500;&#24230;&#30340;&#30740;&#31350;&#65306;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01030v1 Announce Type: cross  Abstract: The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00675</link><description>&lt;p&gt;
LLM meets Vision-Language Models&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLM meets Vision-Language Models for Zero-Shot One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00675
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38646;&#26679;&#26412;&#21333;&#31867;&#35270;&#35273;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#30446;&#26631;&#31867;&#21035;&#30340;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#20219;&#21153;&#30340;&#20219;&#20309;&#39564;&#35777;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27491;&#36127;&#26597;&#35810;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26597;&#25214;&#22312;&#35270;&#35273;&#19978;&#20196;&#20154;&#22256;&#24785;&#30340;&#23545;&#35937;&#65292;&#28982;&#21518;&#20381;&#36182;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27492;&#35774;&#32622;&#20013;&#20248;&#20110;&#33258;&#36866;&#24212;&#30340;&#29616;&#25104;&#26367;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#36127;&#26597;&#35810;&#26679;&#26412;&#20174;&#19982;&#27491;&#26597;&#35810;&#26679;&#26412;&#30456;&#21516;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#65292;&#21253;&#25324;iNaturalist&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#29256;&#26412;&#65292;&#20854;&#20013;&#36127;&#26679;&#26412;&#22312;&#20998;&#31867;&#26641;&#20013;&#19982;&#27491;&#26679;&#26412;&#30456;&#36317;&#22266;&#23450;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#20381;&#36182;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2404.00636</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#26465;&#20214;&#21270;&#19977;&#24179;&#38754;&#29992;&#20110;3D&#24863;&#30693;&#34920;&#24773;&#21487;&#25511;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#33021;&#22815;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;3DMM&#30340;&#34920;&#24773;&#21442;&#25968;&#36716;&#31227;&#21040;&#28304;&#22270;&#20687;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20808;&#39564;&#30340;&#19977;&#24179;&#38754;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20307;&#31215;&#28210;&#26579;&#23558;&#19977;&#24179;&#38754;&#35299;&#30721;&#20026;&#19981;&#21516;&#35270;&#35282;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22270;&#20687;&#21464;&#24418;&#26469;&#22312;&#36816;&#21160;&#31354;&#38388;&#20013;&#20256;&#36755;&#34920;&#24773;&#65292;&#25361;&#25112;&#22312;&#22806;&#35266;&#21644;&#34920;&#24773;&#30340;&#20998;&#31163;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#22806;&#35266;&#34920;&#24773;&#21442;&#25968;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#22312;&#20256;&#36755;&#36328;&#36523;&#20221;&#34920;&#36798;&#26102;&#19981;&#33391;&#22806;&#35266;&#20132;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#38544;&#34255;&#22312;3DMM&#20013;&#30340;&#26080;&#22806;&#35266;&#34920;&#36798;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00636v1 Announce Type: cross  Abstract: In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00600</link><description>&lt;p&gt;
AI&#27861;&#24459;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;&#24403;&#20851;&#38190;&#38382;&#39064;&#21644;&#38544;&#31169;&#24433;&#21709;&#38656;&#35201;&#20154;&#31867;&#21644;&#36947;&#24503;&#30417;&#30563;&#26102;
&lt;/p&gt;
&lt;p&gt;
AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00600
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26085;&#30410;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26377;&#24517;&#35201;&#23545;&#23427;&#20204;&#22312;&#38544;&#31169;&#12289;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#20197;&#21450;&#36947;&#24503;&#23618;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#26368;&#33030;&#24369;&#21644;&#26368;&#24369;&#21183;&#32676;&#20307;&#21487;&#33021;&#20135;&#29983;&#30340;&#39118;&#38505;&#21644;&#24433;&#21709;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#23545;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;ANNs&#22312;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26469;&#20998;&#26512;&#20854;&#22312;&#22788;&#29702;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20234</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26893;&#20837;&#31070;&#32463;&#25509;&#21475;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks-based Real-time Classification of ENG Signals for Implanted Nerve Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;ANNs&#22312;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26469;&#20998;&#26512;&#20854;&#22312;&#22788;&#29702;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#30149;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21487;&#33021;&#27704;&#20037;&#21361;&#21450;&#19968;&#20010;&#20154;&#30340;&#29983;&#21629;&#12290;&#20026;&#20102;&#25903;&#25345;&#24739;&#32773;&#30340;&#24247;&#22797;&#65292;&#20351;&#29992;&#23436;&#20840;&#26893;&#20837;&#24335;&#35774;&#22791;&#27491;&#25104;&#20026;&#26368;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#25104;&#20026;&#23436;&#20840;&#22797;&#26434;&#31070;&#32463;&#32435;&#31859;&#32593;&#32476;&#31995;&#32479;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#35774;&#22791;&#20173;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25506;&#32034;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#20174;&#22823;&#40736;&#22352;&#39592;&#31070;&#32463;&#27979;&#37327;&#30340;&#30005;&#31070;&#32463;&#22270;&#65288;ENG&#65289;&#20449;&#21495;&#20013;&#25552;&#21462;&#21508;&#31181;&#24863;&#35273;&#21050;&#28608;&#26469;&#25191;&#34892;&#35813;&#20219;&#21153;&#12290;&#32771;&#34385;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#20197;&#20998;&#26512;&#34987;&#35843;&#26597;&#30340;ANNs&#22312;&#23454;&#26102;&#20998;&#31867;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#21644;&#39044;&#27979;&#26102;&#38388;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20234v1 Announce Type: new  Abstract: Neuropathies are gaining higher relevance in clinical settings, as they risk permanently jeopardizing a person's life. To support the recovery of patients, the use of fully implanted devices is emerging as one of the most promising solutions. However, these devices, even if becoming an integral part of a fully complex neural nanonetwork system, pose numerous challenges. In this article, we address one of them, which consists of the classification of motor/sensory stimuli. The task is performed by exploring four different types of artificial neural networks (ANNs) to extract various sensory stimuli from the electroneurographic (ENG) signal measured in the sciatic nerve of rats. Different sizes of the data sets are considered to analyze the feasibility of the investigated ANNs for real-time classification through a comparison of their performance in terms of accuracy, F1-score, and prediction time. The design of the ANNs takes advantage of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19275</link><description>&lt;p&gt;
&#30693;&#35782;&#36793;&#30028;&#19982;&#35282;&#33394;&#21160;&#24577;&#22609;&#36896;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#27169;&#25311;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20013;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#19981;&#33021;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#12290;&#23545;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#25105;&#20204;&#28155;&#21152;&#22806;&#37096;&#30693;&#35782;&#28304;&#24182;&#23558;&#20854;&#19982;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#21305;&#37197;&#65292;&#20174;&#32780;&#36171;&#20104;&#20195;&#29702;&#20010;&#24615;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#23545;&#20110;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#34892;&#20026;&#20449;&#24687;&#20869;&#37096;&#26816;&#32034;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19275v1 Announce Type: cross  Abstract: Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the age
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.18105</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Education: A Survey and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18105
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31687;&#35843;&#30740;&#35770;&#25991;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#28085;&#30422;&#20102;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#36741;&#21161;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#27599;&#20010;&#35270;&#35282;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#25972;&#29702;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#25945;&#32946;&#20013;&#37096;&#32626;LLMs&#25152;&#28041;&#21450;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#65292;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20840;&#38754;&#30340;&#25216;&#26415;&#22270;&#26223;&#65292;&#20197;&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#65292;&#24443;&#24213;&#25913;&#38761;&#25945;&#32946;&#23454;&#36341;&#65292;&#24182;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17516</link><description>&lt;p&gt;
MapGuide: &#20174;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#36830;&#32493;&#35821;&#35328;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#26159;&#19968;&#39033;&#33392;&#24040;&#20294;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#23545;&#20110;&#24110;&#21161;&#35821;&#35328;&#27531;&#38556;&#20154;&#22763;&#36890;&#36807;&#33041;&#20449;&#21495;&#36827;&#34892;&#27807;&#36890;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#20174;&#33041;&#27963;&#21160;&#26144;&#23556;&#30340;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#21521;&#23548;&#25991;&#26412;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;METEOR&#20998;&#25968;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;77%&#21644;54%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#39046;&#22495;&#30340;&#36827;&#21270;&#21382;&#31243;&#65292;&#20171;&#32461;&#20102;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#20248;&#21270;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;NAS&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38024;&#23545;&#35745;&#31639;&#25928;&#29575;&#25361;&#25112;&#25552;&#20986;&#30340;&#39640;&#25928;NAS&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17012</link><description>&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#36827;&#21270;&#19982;&#25928;&#29575;&#65306;&#24357;&#21512;&#19987;&#23478;&#35774;&#35745;&#19982;&#33258;&#21160;&#20248;&#21270;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#39046;&#22495;&#30340;&#36827;&#21270;&#21382;&#31243;&#65292;&#20171;&#32461;&#20102;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#20248;&#21270;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#25506;&#35752;&#20102;NAS&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#38024;&#23545;&#35745;&#31639;&#25928;&#29575;&#25361;&#25112;&#25552;&#20986;&#30340;&#39640;&#25928;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#65292;&#24378;&#35843;&#20102;&#23427;&#20174;&#25163;&#21160;&#35774;&#35745;&#21040;&#33258;&#21160;&#21270;&#12289;&#35745;&#31639;&#39537;&#21160;&#26041;&#27861;&#30340;&#28436;&#21464;&#12290;&#23427;&#28085;&#30422;&#20102;NAS&#30340;&#36215;&#28304;&#21644;&#21457;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21307;&#23398;&#24433;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25991;&#31456;&#35814;&#32454;&#38416;&#36848;&#20102;&#20174;&#19987;&#23478;&#39537;&#21160;&#35774;&#35745;&#21040;&#31639;&#27861;&#39537;&#21160;&#36807;&#31243;&#30340;&#36716;&#21464;&#65292;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#31561;&#21021;&#22987;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#20197;&#21450;&#39640;&#25928;NAS&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#22914;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#21644;&#30828;&#20214;&#24863;&#30693;NAS&#12290;&#35813;&#35770;&#25991;&#36827;&#19968;&#27493;&#38416;&#36848;&#20102;NAS&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;NLP&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17012v1 Announce Type: cross  Abstract: The paper provides a comprehensive overview of Neural Architecture Search (NAS), emphasizing its evolution from manual design to automated, computationally-driven approaches. It covers the inception and growth of NAS, highlighting its application across various domains, including medical imaging and natural language processing. The document details the shift from expert-driven design to algorithm-driven processes, exploring initial methodologies like reinforcement learning and evolutionary algorithms. It also discusses the challenges of computational demands and the emergence of efficient NAS methodologies, such as Differentiable Architecture Search and hardware-aware NAS. The paper further elaborates on NAS's application in computer vision, NLP, and beyond, demonstrating its versatility and potential for optimizing neural network architectures across different tasks. Future directions and challenges, including computational efficiency
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.16578</link><description>&lt;p&gt;
SegICL&#65306;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16578
&lt;/p&gt;
&lt;p&gt;
SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#26088;&#22312;&#27178;&#36328;&#21307;&#23398;&#22270;&#20687;&#30340;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#27010;&#25324;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#22312;&#24212;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#65288;OOD&#65289;&#25968;&#25454;&#27169;&#24577;&#21644;&#20219;&#21153;&#26102;&#36890;&#24120;&#20250;&#20943;&#24369;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SegICL&#65292;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SegICL&#33021;&#22815;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#20998;&#21106;&#24182;&#20351;&#29992;&#19968;&#23567;&#32452;&#22270;&#20687;-&#25513;&#30721;&#23545;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#25110;&#20026;OOD&#20219;&#21153;&#65288;&#21253;&#25324;OOD&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;SegICL&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25552;&#31034;&#31034;&#20363;&#25968;&#37327;&#19982;&#20998;&#21106;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16578v1 Announce Type: cross  Abstract: Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16209</link><description>&lt;p&gt;
&#26032;&#38395;&#25253;&#36947;&#22330;&#26223;&#20013;&#30340;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image Captioning in news report scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22270;&#20687;&#25551;&#36848;&#26088;&#22312;&#20026;&#25351;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#30456;&#20851;&#30340;&#25551;&#36848;&#65292;&#20351;&#20854;&#22788;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20132;&#21449;&#28857;&#12290;&#36825;&#39033;&#21162;&#21147;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#26032;&#38395;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29305;&#21035;&#26159;&#22312;&#26032;&#38395;&#25253;&#36947;&#39046;&#22495;&#65292;&#26631;&#39064;&#24212;&#28085;&#30422;&#35814;&#32454;&#20449;&#24687;&#65292;&#22914;&#22270;&#20687;&#20013;&#25429;&#25417;&#21040;&#30340;&#21517;&#20154;&#30340;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#20110;&#29702;&#35299;&#22330;&#26223;&#21644;&#21160;&#20316;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#26041;&#38754;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;&#36825;&#19968;&#25506;&#32034;&#26088;&#22312;&#22686;&#24378;&#33258;&#21160;&#21270;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#32454;&#33268;&#22320;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#24191;&#38420;&#30340;&#35270;&#37326;&#65292;&#20016;&#23500;&#20102;n
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 Announce Type: cross  Abstract: Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;</title><link>https://arxiv.org/abs/2403.16206</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rumor Detection with a novel graph neural network approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#35875;&#35328;&#30340;&#24191;&#27867;&#20256;&#25773;&#23545;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#36896;&#25104;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20844;&#20247;&#20135;&#29983;&#28508;&#22312;&#30340;&#24656;&#24908;&#12289;&#24656;&#24807;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#22914;&#20309;&#23613;&#26089;&#25581;&#31359;&#35875;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#32467;&#26500;&#26469;&#26816;&#27979;&#35875;&#35328;&#65292;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#20182;&#20204;&#21487;&#33021;&#21327;&#35843;&#20256;&#25773;&#35875;&#35328;&#20197;&#33719;&#24471;&#36739;&#22823;&#30340;&#27969;&#34892;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#25551;&#36848;&#29992;&#25143;&#21644;&#26469;&#28304;&#25512;&#25991;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#20108;&#37096;&#22270;&#20013;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#20351;&#29992;&#26641;&#32467;&#26500;&#23398;&#20064;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#24471;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16206v1 Announce Type: new  Abstract: The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#65292;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16003</link><description>&lt;p&gt;
&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Diverse Representation Embedding for Lifelong Person Re-Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#65292;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#26088;&#22312;&#19981;&#26029;&#23398;&#20064;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#65292;&#36328;&#22810;&#20010;&#25668;&#20687;&#22836;&#21305;&#37197;&#20010;&#20154;&#12290;LReID&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#22686;&#37327;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#12290;&#20219;&#21153;&#32423;&#22495;&#24046;&#36317;&#21644;&#26377;&#38480;&#30340;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#26159;&#23548;&#33268;ReID&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#29992;&#20110;LReID&#12290;&#25152;&#25552;&#20986;&#30340;DRE&#22522;&#20110;&#23454;&#20363;&#32423;&#21644;&#20219;&#21153;&#32423;&#24067;&#23616;&#65292;&#20445;&#30041;&#26087;&#30693;&#35782;&#21516;&#26102;&#36866;&#24212;&#26032;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#26469;&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16003v1 Announce Type: cross  Abstract: Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras. The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally. Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods. To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID. The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout. Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets. Based on the processed diverse representation, 
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25512;&#21160;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12627</link><description>&lt;p&gt;
&#21152;&#24378;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#65306;&#29992;&#20110;&#22312;Coq&#20195;&#30721;&#19978;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25512;&#21160;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#65292;Coq&#35777;&#26126;&#36741;&#21161;&#24037;&#20855;&#20197;&#20854;&#23545;&#39564;&#35777;&#25968;&#23398;&#26029;&#35328;&#21644;&#36719;&#20214;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#26041;&#27861;&#33073;&#39040;&#32780;&#20986;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;Coq&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#29305;&#27530;&#24615;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#35299;&#37322;&#21644;&#29983;&#25104;Coq&#20195;&#30721;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#35813;&#25968;&#25454;&#38598;&#28304;&#33258;&#19968;&#32452;&#36229;&#36807;10,000&#20010;Coq&#28304;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21629;&#39064;&#12289;&#35777;&#26126;&#21644;&#23450;&#20041;&#65292;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21253;&#25324;&#28304;&#24341;&#29992;&#21644;&#35768;&#21487;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20419;&#36827;&#33021;&#22815;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#19988;&#35821;&#20041;&#20016;&#23500;&#30340;Coq&#26500;&#36896;&#30340;LLMs&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#25512;&#36827;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#21069;&#27839;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12627v1 Announce Type: new  Abstract: In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs). Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11487</link><description>&lt;p&gt;
LLM&#33021;&#29983;&#25104;&#31867;&#20154;&#34892;&#36335;&#25351;&#31034;&#21527;&#65311;&#36208;&#21521;&#36328;&#24179;&#21488;&#30340;&#20855;&#36523;&#25351;&#20196;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#20197;&#25351;&#23548;&#20855;&#36523;&#26426;&#22120;&#20154;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20877;&#20381;&#36182;&#20110;&#20165;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#27169;&#25311;&#24179;&#21488;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#26159;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35843;&#33410;LLM&#65292;&#20197;&#20351;&#29992;&#23569;&#37327;&#21442;&#32771;&#29983;&#25104;&#25351;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#38382;&#31572;&#31574;&#30053;&#25910;&#38598;&#29615;&#22659;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;LLM&#29992;&#20110;&#25351;&#20196;&#21512;&#25104;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#65292;&#21253;&#25324;Matterport3D&#12289;AI Habitat&#21644;ThreeDWorld&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#20027;&#35266;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;83.3%&#30340;&#29992;&#25143;&#35748;&#20026;&#21512;&#25104;&#30340;&#25351;&#31034;&#20934;&#30830;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32454;&#33410;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25351;&#31034;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10516</link><description>&lt;p&gt;
FeatUp: &#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29305;&#24449;&#20219;&#24847;&#20998;&#36776;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FeatUp: A Model-Agnostic Framework for Features at Any Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10516
&lt;/p&gt;
&lt;p&gt;
FeatUp&#26159;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29305;&#24449;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#29305;&#24449;&#21487;&#20197;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#22312;&#29616;&#26377;&#24212;&#29992;&#20013;&#21462;&#24471;&#20998;&#36776;&#29575;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#22522;&#30707;&#65292;&#25429;&#25417;&#22270;&#20687;&#35821;&#20041;&#24182;&#20351;&#31038;&#21306;&#33021;&#22815;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#20351;&#22312;&#38646;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#32570;&#20047;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#20687;&#20998;&#21106;&#21644;&#28145;&#24230;&#39044;&#27979;&#36825;&#26679;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#36807;&#20110;&#32858;&#21512;&#22823;&#33539;&#22260;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#65292;&#19968;&#20010;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;&#28145;&#24230;&#29305;&#24449;&#20013;&#20002;&#22833;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FeatUp&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#24341;&#23548;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#20449;&#21495;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#36866;&#24212;&#21333;&#20010;&#22270;&#20687;&#24182;&#20197;&#20219;&#20309;&#20998;&#36776;&#29575;&#37325;&#26500;&#29305;&#24449;&#30340;&#38544;&#24335;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#19982; NeRF &#31867;&#20284;&#30340;&#28145;&#24230;&#31867;&#27604;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#20445;&#30041;&#20854;&#21407;&#22987;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#26367;&#25442;&#29616;&#26377;&#24212;&#29992;&#31243;&#24207;&#65292;&#21363;&#20351;&#19981;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10516v1 Announce Type: cross  Abstract: Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07888</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#21435;&#20559;&#35265;: &#20351;&#29992;&#35821;&#35328;&#20943;&#36731;&#24433;&#20687;&#20013;&#30340;&#23376;&#32676;&#20307;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Cross-modality debiasing: using language to mitigate sub-population shifts in imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07888
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#32676;&#20307;&#36716;&#21464;&#26159;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#31361;&#26174;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#29305;&#23450;&#23376;&#32676;&#20307;&#25110;&#20154;&#21475;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#23376;&#32676;&#20307;&#36716;&#21464;&#21344;&#25454;&#20102;&#31639;&#27861;&#20559;&#35265;&#30340;&#19968;&#20010;&#37325;&#35201;&#26469;&#28304;&#65292;&#24182;&#38656;&#35201;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#65292;&#20855;&#26377;&#22266;&#26377;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#31181;&#40065;&#26834;&#24615;&#23545;&#21442;&#25968;&#24494;&#35843;&#26159;&#33030;&#24369;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#40065;&#26834;&#24615;&#36830;&#25509;&#65292;&#37325;&#26032;&#22609;&#36896;&#19968;&#20010;&#27169;&#24577;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;CLIP&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#26469;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#30340;&#22270;&#20687;&#34920;&#31034;&#33021;&#22815;&#22312;&#23376;&#32676;&#20307;&#19978;&#25913;&#21892;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07888v1 Announce Type: cross  Abstract: Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by na
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07711</link><description>&lt;p&gt;
SSM&#36935;&#19978;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;: &#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#19979;&#30340;&#39640;&#25928;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20687;&#29983;&#25104;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#30028;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#20869;&#23384;&#28040;&#32791;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#36825;&#31181;&#38480;&#21046;&#22312;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26356;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#30001;&#20110;&#30456;&#23545;&#20110;&#24207;&#21015;&#38271;&#24230;&#65292;SSMs&#20855;&#26377;&#32447;&#24615;&#20869;&#23384;&#28040;&#32791;&#65292;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;UCF101&#36825;&#19968;&#35270;&#39057;&#29983;&#25104;&#30340;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#25506;&#35752;SSMs&#22312;&#26356;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18700</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning to Compress Prompt in Natural Language Formats
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#22788;&#29702;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#38271;&#19978;&#19979;&#25991;&#12289;&#25512;&#29702;&#36895;&#24230;&#24930;&#20197;&#21450;&#35745;&#31639;&#32467;&#26524;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#37096;&#32626;&#20855;&#26377;&#31934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#19978;&#19979;&#25991;&#30340;LLMs&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#26377;&#25928;&#21644;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#20316;&#21697;&#20381;&#36182;&#23558;&#38271;&#25552;&#31034;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#36719;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36719;&#25552;&#31034;&#21387;&#32553;&#22312;&#19981;&#21516;LLM&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;API&#30340;LLMs&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20197;LLM&#21487;&#36716;&#31227;&#24615;&#30340;&#24418;&#24335;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#12290;&#36825;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#65306;(i) &#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#25552;&#31034;&#19981;&#20860;&#23481;&#21453;&#21521;&#20256;&#25773;&#65292;(ii) NL&#25552;&#31034;&#22312;&#26045;&#21152;&#38271;&#24230;&#32422;&#26463;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23553;&#35013;&#65288;Nano-Capsulator&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18700v1 Announce Type: cross  Abstract: Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framewor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.17971</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#25630;&#23450;&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22270;&#29255;&#20869;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
All in a Single Image: Large Multimodal Models are In-Image Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20197;&#22686;&#24378;GPT-4V&#30340;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#20381;&#36182;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#23558;&#35270;&#35273;&#36755;&#20837;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;I$^2$L&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#36991;&#20813;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#30340;&#19981;&#20934;&#30830;&#25991;&#26412;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#22312;&#23450;&#20301;&#28436;&#31034;&#31034;&#20363;&#26102;&#30340;&#28789;&#27963;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#36127;&#25285;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#23545;&#22810;&#20010;&#22270;&#29255;&#21644;&#20887;&#38271;&#25991;&#26412;&#30340;&#38656;&#27714;&#26469;&#36991;&#20813;&#36229;&#36807;&#36755;&#20837;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32467;&#21512;&#19981;&#21516;ICL&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#32473;&#23450;&#20219;&#21153;&#20013;&#25968;&#25454;&#31034;&#20363;&#30340;&#36866;&#24403;ICL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MathVi&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.16611</link><description>&lt;p&gt;
&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#32972;&#21518;&#30340;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Understanding the Dataset Practitioners Behind Large Language Model Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16611
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#26377;&#24433;&#21709;&#21147;&#65292;&#23457;&#35270;&#23427;&#20204;&#20381;&#36182;&#21644;&#20135;&#29983;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#24037;&#20316;&#20869;&#23481;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35895;&#27468;&#36129;&#29486;LLM&#24320;&#21457;&#22242;&#38431;&#36131;&#20219;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#8220;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#8221;&#30340;&#35282;&#33394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#31649;&#29702;&#32773;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65288;N=10&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#36136;&#37327;&#26159;&#39318;&#35201;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#31649;&#29702;&#32773;&#35201;&#20040;&#20973;&#30452;&#35273;&#65292;&#35201;&#20040;&#32534;&#20889;&#33258;&#23450;&#20041;&#35780;&#20272;&#36923;&#36753;&#12290;&#31649;&#29702;&#32773;&#20043;&#38388;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#28508;&#22312;&#21407;&#22240;&#21644;&#23454;&#29616;&#19968;&#33268;&#24615;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16611v1 Announce Type: new  Abstract: As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of "dataset practitioner" by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#35268;&#21010;&#21644;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#35270;&#39057;&#28216;&#25103;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#24471;&#33258;&#21160;&#35268;&#21010;&#21464;&#24471;&#26356;&#23481;&#26131;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#12290;</title><link>https://arxiv.org/abs/2402.12393</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#35268;&#21010;&#21644;&#23398;&#20064;&#33258;&#21160;&#21270;&#35270;&#39057;&#28216;&#25103;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
On Automating Video Game Testing by Planning and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#35268;&#21010;&#21644;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#35270;&#39057;&#28216;&#25103;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#24471;&#33258;&#21160;&#35268;&#21010;&#21464;&#24471;&#26356;&#23481;&#26131;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#35268;&#21010;&#21644;&#35268;&#21010;&#34892;&#20026;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#29305;&#23450;&#35270;&#39057;&#28216;&#25103;&#26041;&#38754;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#12290;&#22522;&#26412;&#24819;&#27861;&#26159;&#29983;&#25104;&#35814;&#32454;&#30340;&#28216;&#25103;&#26085;&#24535;&#65292;&#24182;&#24212;&#29992;&#34892;&#21160;&#27169;&#22411;&#23398;&#20064;&#26469;&#33719;&#24471;&#35268;&#21010;&#39046;&#22495;&#25551;&#36848;&#35821;&#35328;&#65288;PDDL&#65289;&#20013;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#20102;&#28216;&#25103;&#24320;&#21457;&#20154;&#21592;&#19982;&#20855;&#26377;PDDL&#24314;&#27169;&#32463;&#39564;&#20294;&#26080;&#28216;&#25103;&#24320;&#21457;&#25216;&#33021;&#30340;&#20154;&#21592;&#20043;&#38388;&#30340;&#39640;&#25928;&#21512;&#20316;&#65292;&#24182;&#19988;&#26080;&#38656;&#20219;&#20309;PDDL&#25110;&#20854;&#20182;&#27491;&#24335;&#31995;&#32479;&#32463;&#39564;&#12290;&#25105;&#20204;&#24635;&#20307;&#25551;&#36848;&#20102;&#35813;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#20855;&#20307;&#30340;&#27010;&#24565;&#35777;&#26126;&#31034;&#20363;&#19978;&#36827;&#34892;&#28436;&#31034; -- &#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20316;&#20026;&#27969;&#34892;&#28216;&#25103;&#24320;&#21457;&#24341;&#25806;Unity&#20013;&#30340;&#25945;&#31243;&#39033;&#30446;&#20043;&#19968;&#12290;&#26412;&#25991;&#26159;&#26397;&#30528;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#24037;&#20316;&#27969;&#31243;&#20013;&#23545;&#24314;&#27169;&#19987;&#23478;&#38656;&#27714;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#32780;&#20351;&#33258;&#21160;&#35268;&#21010;&#21487;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12393v1 Announce Type: cross  Abstract: In this paper, we propose a method and workflow for automating the testing of certain video game aspects using automated planning and planning action model learning techniques. The basic idea is to generate detailed gameplay logs and apply action model learning to obtain a formal model in the planning domain description language (PDDL). The workflow enables efficient cooperation of game developers without any experience with PDDL or other formal systems and a person experienced with PDDL modeling but no game development skills. We describe the method and workflow in general and then demonstrate it on a concrete proof-of-concept example -- a simple role-playing game provided as one of the tutorial projects in the popular game development engine Unity. This paper presents the first step towards minimizing or even eliminating the need for a modeling expert in the workflow, thus making automated planning accessible to a broader audience.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09871</link><description>&lt;p&gt;
MuChin&#65306;&#29992;&#20110;&#35780;&#20272;&#38899;&#20048;&#39046;&#22495;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09871
&lt;/p&gt;
&lt;p&gt;
MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#32479;&#19968;&#35780;&#20272;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#20197;&#25991;&#23383;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#31639;&#27861;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#19987;&#19994;&#20154;&#22763;&#21644;&#20844;&#20247;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#27880;&#37322;&#30340;&#20302;&#31934;&#24230;&#65292;&#29616;&#26377;&#30340;&#38899;&#20048;&#25551;&#36848;&#25968;&#25454;&#38598;&#26080;&#27861;&#20316;&#20026;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuChin&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#30340;&#24320;&#28304;&#38899;&#20048;&#25551;&#36848;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;LLMs&#22312;&#29702;&#35299;&#21644;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#34411;&#38899;&#20048;&#27880;&#37322;&#24179;&#21488;&#65288;CaiMAP&#65289;&#65292;&#37319;&#29992;&#21019;&#26032;&#30340;&#22810;&#20154;&#12289;&#22810;&#38454;&#27573;&#20445;&#35777;&#26041;&#27861;&#65292;&#24182;&#25307;&#21215;&#20102;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#65292;&#20197;&#30830;&#20445;&#27880;&#37322;&#30340;&#31934;&#24230;&#21644;&#19982;&#27969;&#34892;&#35821;&#20041;&#30340;&#23545;&#40784;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09871v1 Announce Type: cross  Abstract: The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>SCAPE&#26159;&#19968;&#20010;&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#25805;&#20316;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#12289;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00089</link><description>&lt;p&gt;
SCAPE: &#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#27010;&#24565;&#26550;&#26500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
SCAPE: Searching Conceptual Architecture Prompts using Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00089
&lt;/p&gt;
&lt;p&gt;
SCAPE&#26159;&#19968;&#20010;&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#31616;&#21333;&#30340;&#25805;&#20316;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#12289;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26550;&#26500;&#28041;&#21450;&#23545;&#26469;&#33258;&#20854;&#20182;&#23398;&#31185;&#30340;&#26032;&#39062;&#29702;&#24565;&#36827;&#34892;&#39640;&#24230;&#21019;&#36896;&#24615;&#30340;&#25506;&#32034;&#65292;&#24314;&#31569;&#24072;&#32771;&#34385;&#23558;&#24314;&#31569;&#30340;&#24418;&#24335;&#12289;&#26448;&#26009;&#12289;&#36136;&#22320;&#21644;&#39068;&#33394;&#36827;&#34892;&#28608;&#36827;&#30340;&#21019;&#26032;&#12290;&#34429;&#28982;&#22914;&#20170;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20960;&#21313;&#24180;&#26469;&#36827;&#21270;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;&#21019;&#36896;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24037;&#20855;SCAPE&#23558;&#36827;&#21270;&#25628;&#32034;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#25353;&#30028;&#38754;&#25506;&#32034;&#30001;&#21021;&#22987;&#36755;&#20837;&#21551;&#21457;&#30340;&#21019;&#36896;&#24615;&#21644;&#39640;&#36136;&#37327;&#35774;&#35745;&#12290;SCAPE&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#24182;&#21033;&#29992;GPT-4&#30340;&#20869;&#32622;&#35821;&#35328;&#33021;&#21147;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#31361;&#21464;&#21644;&#20132;&#21449;&#25805;&#20316;&#26469;&#21464;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;DALL-E 3&#30456;&#27604;&#65292;SCAPE&#22312;&#22270;&#20687;&#30340;&#26032;&#39062;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;67%&#65292;&#21516;&#26102;&#22312;&#36136;&#37327;&#21644;&#20351;&#29992;&#25928;&#26524;&#26041;&#38754;&#20063;&#26377;&#25152;&#25913;&#36827;&#65307;&#25105;&#20204;&#23637;&#31034;&#65292;&#20165;&#32463;&#36807;3&#27425;&#36845;&#20195;&#65292;SCAPE&#30340;&#22270;&#20687;&#26032;&#39062;&#24615;&#22686;&#21152;&#20102;24%&#65292;&#20351;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conceptual architecture involves a highly creative exploration of novel ideas, often taken from other disciplines as architects consider radical new forms, materials, textures and colors for buildings. While today's generative AI systems can produce remarkable results, they lack the creativity demonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool, combines evolutionary search with generative AI, enabling users to explore creative and good quality designs inspired by their initial input through a simple point and click interface. SCAPE injects randomness into generative AI, and enables memory, making use of the built-in language skills of GPT-4 to vary prompts via text-based mutation and crossover. We demonstrate that compared to DALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements in quality and effectiveness of use; we show that in just 3 iterations SCAPE has a 24% image novelty increase enabling effective exploration, plus optimization
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#21487;&#21464;&#25511;&#21046;&#39057;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#31616;&#21270;&#30828;&#20214;&#20351;&#29992;&#12289;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#25361;&#25112;&#22266;&#23450;&#39057;&#29575;&#25511;&#21046;&#30340;&#20551;&#35774;</title><link>https://arxiv.org/abs/2401.09286</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#21464;&#25511;&#21046;&#39057;&#29575;&#30340;&#21487;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deployable Reinforcement Learning with Variable Control Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09286
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#21487;&#21464;&#25511;&#21046;&#39057;&#29575;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#31616;&#21270;&#30828;&#20214;&#20351;&#29992;&#12289;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#25361;&#25112;&#22266;&#23450;&#39057;&#29575;&#25511;&#21046;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#30340;&#25511;&#21046;&#22120;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;RL&#20381;&#36182;&#20110;&#34987;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#20195;&#29702;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20551;&#35774;&#26102;&#38388;&#31163;&#25955;&#12290;&#20351;&#29992;MDPs&#23548;&#33268;&#20960;&#20046;&#25152;&#26377;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#31995;&#32479;&#37117;&#37319;&#29992;&#22266;&#23450;&#36895;&#29575;&#25511;&#21046;&#31574;&#30053;&#65292;&#21608;&#26399;&#65288;&#25110;&#26102;&#38388;&#27493;&#38271;&#65289;&#36890;&#24120;&#22522;&#20110;&#24320;&#21457;&#32773;&#32463;&#39564;&#25110;&#24212;&#29992;&#29615;&#22659;&#30340;&#29305;&#23450;&#29305;&#24449;&#36873;&#25321;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20026;&#20102;&#30830;&#20445;&#31283;&#23450;&#24615;&#65292;&#31995;&#32479;&#24212;&#20197;&#26368;&#39640;&#12289;&#26368;&#22351;&#24773;&#20917;&#39057;&#29575;&#36827;&#34892;&#25511;&#21046;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#36164;&#28304;&#65292;&#24182;&#19988;&#38459;&#30861;&#25511;&#21046;&#22120;&#22312;&#26495;&#36733;&#30828;&#20214;&#19978;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#36981;&#24490;&#21453;&#24212;&#24335;&#32534;&#31243;&#21407;&#21017;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#21160;&#20316;&#21487;&#20197;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#30828;&#20214;&#65292;&#24182;&#26377;&#21161;&#20110;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#12290;&#25105;&#20204;&#25361;&#25112;&#20102;&#22266;&#23450;&#39057;&#29575;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09286v2 Announce Type: replace-cross  Abstract: Deploying controllers trained with Reinforcement Learning (RL) on real robots can be challenging: RL relies on agents' policies being modeled as Markov Decision Processes (MDPs), which assume an inherently discrete passage of time. The use of MDPs results in that nearly all RL-based control systems employ a fixed-rate control strategy with a period (or time step) typically chosen based on the developer's experience or specific characteristics of the application environment. Unfortunately, the system should be controlled at the highest, worst-case frequency to ensure stability, which can demand significant computational and energy resources and hinder the deployability of the controller on onboard hardware. Adhering to the principles of reactive programming, we surmise that applying control actions only when necessary enables the use of simpler hardware and helps reduce energy consumption. We challenge the fixed frequency assump
&lt;/p&gt;</description></item><item><title>TeleChat&#26159;&#19968;&#20010;&#21253;&#21547;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#38598;&#65292;&#26088;&#22312;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.03804</link><description>&lt;p&gt;
TeleChat&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
TeleChat Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03804
&lt;/p&gt;
&lt;p&gt;
TeleChat&#26159;&#19968;&#20010;&#21253;&#21547;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#38598;&#65292;&#26088;&#22312;&#25552;&#20379;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TeleChat&#65292;&#36825;&#26159;&#30001;30&#20159;&#12289;70&#20159;&#21644;120&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#21512;&#38598;&#12290;&#23427;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#30340;&#24494;&#35843;&#32842;&#22825;&#27169;&#22411;&#12290;TeleChat&#26368;&#21021;&#22312;&#21253;&#21547;&#33521;&#35821;&#21644;&#20013;&#25991;&#35821;&#35328;&#30340;&#22810;&#26679;&#25991;&#26412;&#38598;&#21512;&#20013;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#25968;&#19975;&#20159;&#30340;&#26631;&#35760;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#21916;&#22909;&#30456;&#19968;&#33268;&#65292;&#36981;&#24490;&#25105;&#20204;&#25551;&#36848;&#30340;&#35814;&#32454;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;TeleChat&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35821;&#35328;&#29702;&#35299;&#12289;&#25968;&#23398;&#12289;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;TeleChat&#22312;&#21508;&#31181;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#24320;&#28304;&#27169;&#22411;&#30456;&#20284;&#35268;&#27169;&#30340;&#21487;&#27604;&#24615;&#33021;&#12290;&#20026;&#25903;&#25345;&#26410;&#26469;&#21033;&#29992;&#35813;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03804v2 Announce Type: replace-cross  Abstract: In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing 
&lt;/p&gt;</description></item><item><title>UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;</title><link>https://arxiv.org/abs/2312.10170</link><description>&lt;p&gt;
UINav&#65306;&#19968;&#31181;&#35757;&#32451;&#35774;&#22791;&#31471;&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#23454;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UINav: A Practical Approach to Train On-Device Automation Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10170
&lt;/p&gt;
&lt;p&gt;
UINav&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#25104;&#21151;&#29575;&#39640;&#65292;&#35757;&#32451;&#25968;&#25454;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#33258;&#20027;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30028;&#38754;&#20197;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#24403;&#29992;&#25143;&#22788;&#20110;&#24773;&#22659;&#24615;&#25110;&#27704;&#20037;&#24615;&#21463;&#25439;&#26102;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#20043;&#21069;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#19981;&#33021;&#20135;&#29983;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#20165;&#22312;&#31616;&#21333;&#30340;&#25163;&#24037;&#21046;&#20316;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#38752;&#24037;&#20316;&#65292;&#25110;&#32773;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UINav&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#36866;&#21512;&#31227;&#21160;&#35774;&#22791;&#30340;&#33258;&#21160;&#21270;&#20195;&#29702;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#28436;&#31034;&#25968;&#37327;&#19981;&#22810;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#12290;&#20026;&#20102;&#20943;&#23569;&#28436;&#31034;&#30340;&#24037;&#20316;&#37327;&#65292;UINav&#20351;&#29992;&#20102;&#19968;&#20010;&#35009;&#21028;&#27169;&#22411;&#65292;&#22312;&#20195;&#29702;&#22833;&#36133;&#30340;&#20219;&#21153;&#19978;&#20026;&#29992;&#25143;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#65292;&#24182;&#33258;&#21160;&#22686;&#21152;&#20154;&#31867;&#28436;&#31034;&#20197;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20165;&#38656;10&#27425;&#28436;&#31034;&#65292;UINav&#23601;&#21487;&#20197;&#23454;&#29616;70%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26377;&#36275;&#22815;&#22810;&#27425;&#28436;&#31034;&#26102;&#65292;&#23427;&#21487;&#20197;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10170v2 Announce Type: replace-cross  Abstract: Automation systems that can autonomously drive application user interfaces to complete user tasks are of great benefit, especially when users are situationally or permanently impaired. Prior automation systems do not produce generalizable models while AI-based automation agents work reliably only in simple, hand-crafted applications or incur high computation costs. We propose UINav, a demonstration-based approach to train automation agents that fit mobile devices, yet achieving high success rates with modest numbers of demonstrations. To reduce the demonstration overhead, UINav uses a referee model that provides users with immediate feedback on tasks where the agent fails, and automatically augments human demonstrations to increase diversity in training data. Our evaluation shows that with only 10 demonstrations UINav can achieve 70% accuracy, and that with enough demonstrations it can surpass 90% accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#12289;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25512;&#21160;AI&#31995;&#32479;&#36981;&#23432;&#20154;&#31867;&#32972;&#26223;&#30456;&#20851;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2312.09699</link><description>&lt;p&gt;
&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#21644;&#25991;&#21270;&#35268;&#21017;&#65306;&#32534;&#21046;&#19982;&#25512;&#29702;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#12289;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25512;&#21160;AI&#31995;&#32479;&#36981;&#23432;&#20154;&#31867;&#32972;&#26223;&#30456;&#20851;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22522;&#30784;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#24433;&#21709;&#26469;&#33258;&#20110;&#23427;&#20204;&#30340;&#34892;&#20026;&#25110;&#20915;&#31574;&#12290;&#36825;&#20123;&#31995;&#32479;&#24517;&#39035;&#34987;&#35774;&#35745;&#20026;&#36981;&#23432;&#23427;&#20204;&#23558;&#36816;&#20316;&#30340;&#20154;&#31867;&#32972;&#26223;&#12290;Townsend&#31561;&#20154;&#65288;2022&#65289;&#24341;&#20837;&#20102;SLEEC&#65288;&#31038;&#20250;&#12289;&#27861;&#24459;&#12289;&#20262;&#29702;&#12289;&#31227;&#24773;&#25110;&#25991;&#21270;&#65289;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20419;&#36827;AI&#22522;&#30784;&#21644;&#33258;&#20027;&#31995;&#32479;&#24212;&#36981;&#23432;&#35268;&#21017;&#30340;&#21046;&#23450;&#12289;&#39564;&#35777;&#21644;&#25191;&#34892;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#26469;&#25581;&#31034;&#36825;&#20123;&#35268;&#21017;&#65292;&#35753;&#21746;&#23398;&#23478;&#12289;&#24459;&#24072;&#12289;&#39046;&#22495;&#19987;&#23478;&#21644;&#20854;&#20182;&#20154;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#21046;&#23450;&#36825;&#20123;&#35268;&#21017;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#35268;&#21017;&#22312;AI&#31995;&#32479;&#20013;&#26377;&#25928;&#20351;&#29992;&#65292;&#38656;&#35201;&#23558;&#36825;&#20123;&#35268;&#21017;&#31995;&#32479;&#22320;&#32763;&#35793;&#25104;&#25903;&#25345;&#33258;&#21160;&#25512;&#29702;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;SLEEC&#35268;&#21017;&#27169;&#24335;&#36827;&#34892;&#20102;&#35821;&#35328;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#23558;SLEEC&#35268;&#21017;&#36716;&#25442;&#25104;c&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09699v2 Announce Type: replace  Abstract: The rise of AI-based and autonomous systems is raising concerns and apprehension due to potential negative repercussions stemming from their behavior or decisions. These systems must be designed to comply with the human contexts in which they will operate. To this extent, Townsend et al. (2022) introduce the concept of SLEEC (social, legal, ethical, empathetic, or cultural) rules that aim to facilitate the formulation, verification, and enforcement of the rules AI-based and autonomous systems should obey. They lay out a methodology to elicit them and to let philosophers, lawyers, domain experts, and others to formulate them in natural language. To enable their effective use in AI systems, it is necessary to translate these rules systematically into a formal language that supports automated reasoning. In this study, we first conduct a linguistic analysis of the SLEEC rules pattern, which justifies the translation of SLEEC rules into c
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#24418;&#24335;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#34920;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2312.05209</link><description>&lt;p&gt;
HALO&#65306;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#31034;&#21644;&#20998;&#31867;&#24187;&#35273;&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#24418;&#24335;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#34920;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05209v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20026;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#30693;&#35782;&#21457;&#29616;&#21644;&#25968;&#25454;&#25366;&#25496;&#31561;&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#34394;&#26500;&#20449;&#24687;&#25110;&#8220;&#24187;&#35273;&#8221;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#31616;&#21333;&#38382;&#39064;&#19978;&#29359;&#38169;&#35823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30001;&#20110;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#22791;&#21463;&#27426;&#36814;&#65292;&#23398;&#26415;&#30028;&#21644;&#20844;&#27665;&#31185;&#23398;&#23478;&#37117;&#35760;&#24405;&#20102;&#22810;&#31181;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#24187;&#35273;&#12290;&#23613;&#31649;&#24050;&#26377;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#31181;&#24418;&#24335;&#27169;&#22411;&#26469;&#32454;&#33268;&#25551;&#36848;&#21644;&#34920;&#31034;&#36825;&#20123;&#24187;&#35273;&#65288;&#24102;&#26377;&#30456;&#20851;&#20803;&#25968;&#25454;&#65289;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Hallucination Ontology&#25110;HALO&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;HALO&#26159;&#19968;&#31181;&#27491;&#24335;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#26412;&#20307;&#35770;&#65292;&#29992;OWL&#32534;&#20889;&#65292;&#30446;&#21069;&#25903;&#25345;&#20845;&#31181;&#24050;&#30693;&#30340;&#24187;&#35273;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05209v2 Announce Type: replace  Abstract: Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2312.04556</link><description>&lt;p&gt;
&#25968;&#23398;&#23478;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Mathematicians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04556
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#23545;&#25968;&#23398;&#23478;&#30340;&#28508;&#22312;&#24110;&#21161;&#21644;&#25913;&#21464;&#24037;&#20316;&#26041;&#24335;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#30340;&#33021;&#21147;&#20197;&#21450;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#25110;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#23545;&#35768;&#22810;&#32844;&#19994;&#26469;&#35828;&#65292;LLMs&#20195;&#34920;&#19968;&#31181;&#26080;&#20215;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21152;&#24555;&#24037;&#20316;&#36895;&#24230;&#24182;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#12290;&#26412;&#25991;&#35752;&#35770;&#23427;&#20204;&#22312;&#24110;&#21161;&#19987;&#19994;&#25968;&#23398;&#23478;&#26041;&#38754;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#25152;&#26377;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;&#30340;&#25968;&#23398;&#25551;&#36848;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25253;&#21578;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#25913;&#21464;&#25968;&#23398;&#23478;&#24037;&#20316;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04556v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LLMs to change how mathematicians work.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2312.03517</link><description>&lt;p&gt;
FRDiff&#65306;&#29305;&#24449;&#37325;&#29992;&#29992;&#20110;&#26080;&#35757;&#32451;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03517
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#36739;&#22823;&#35745;&#31639;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#25152;&#24517;&#38656;&#30340;&#37325;&#22797;&#21435;&#22122;&#27493;&#39588;&#32780;&#20135;&#29983;&#30340;&#65292;&#36825;&#26159;&#38459;&#30861;&#23427;&#20204;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#39640;&#32423;&#21152;&#36895;&#25216;&#26415;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#26469;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03517v2 Announce Type: replace-cross  Abstract: The substantial computational costs of diffusion models, especially due to the repeated denoising steps necessary for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations (NFE) using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degradation. In our work, we introduce an advanced acceleration technique that leverages the temporal redundancy inherent in diffusion models. Reusing feature maps with high temporal similarity opens up a new opportunity to save computation resources without compromising output quality. To realize the practical benefits of this intuition, we conduct an extensive analysis and propose a novel method, FRDiff. FRDiff is designed to harness the adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#28040;&#38500;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22320;&#29702;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#30340;&#22320;&#29702;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2312.02957</link><description>&lt;p&gt;
&#20026;&#25152;&#26377;&#20154;&#26500;&#24314;&#22320;&#29702;&#19981;&#21487;&#30693;&#27169;&#22411;&#30340;&#20998;&#31867;&#65306;&#28040;&#38500;&#22320;&#22495;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Classification for everyone : Building geography agnostic models for fairer recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#28040;&#38500;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22320;&#29702;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#30340;&#22320;&#29702;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20943;&#36731;&#29616;&#26377;&#26368;&#20808;&#36827;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#22266;&#26377;&#22320;&#29702;&#20559;&#35265;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;&#36825;&#31181;&#20559;&#35265;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978; - Dollar Street&#25968;&#25454;&#38598;&#21644;ImageNet&#19978;&#30340;&#23384;&#22312;&#65292;&#20351;&#29992;&#24102;&#26377;&#20301;&#32622;&#20449;&#24687;&#30340;&#22270;&#29255;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25216;&#26415;&#23545;&#20110;&#20351;&#36825;&#20123;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#21040;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02957v3 Announce Type: replace-cross  Abstract: In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models. We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information. We then present different methods which can be employed to reduce this bias. Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images.
&lt;/p&gt;</description></item><item><title>DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.02139</link><description>&lt;p&gt;
DiffiT: &#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffiT: Diffusion Vision Transformers for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02139
&lt;/p&gt;
&lt;p&gt;
DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#29616;&#21147;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24320;&#21019;&#24615;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35782;&#21035;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ViTs&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;Diffusion Vision Transformers&#65288;DiffiT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#21435;&#22122;&#36807;&#31243;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;TMSA&#65289;&#26426;&#21046;&#12290;DiffiT&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#21442;&#25968;&#25928;&#29575;&#20063;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#30340;DiffiT&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#21508;&#31181;&#31867;&#21035;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#32508;&#21512;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28508;&#31354;&#38388;DiffiT&#27169;&#22411;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02139v2 Announce Type: replace-cross  Abstract: Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#25511;&#21046;&#26550;&#26500;&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#22312;&#28023;&#20135;&#23548;&#33322;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#38381;&#29615;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.01855</link><description>&lt;p&gt;
&#23433;&#20840;&#28023;&#19978;&#23548;&#33322;&#30340;&#27169;&#22359;&#21270;&#25511;&#21046;&#26550;&#26500;&#65306;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Modular Control Architecture for Safe Marine Navigation: Reinforcement Learning and Predictive Safety Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#25511;&#21046;&#26550;&#26500;&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#22312;&#28023;&#20135;&#23548;&#33322;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#38381;&#29615;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#20027;&#31995;&#32479;&#38754;&#20020;&#23433;&#20840;&#25361;&#25112;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#38381;&#29615;&#25511;&#21046;&#26469;&#22788;&#29702;&#29289;&#29702;&#38480;&#21046;&#21644;&#23433;&#20840;&#32422;&#26463;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#31995;&#32479;&#65292;&#22914;&#33258;&#20027;&#33337;&#21482;&#65292;&#36935;&#21040;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#25200;&#21160;&#12290;&#24378;&#21270;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#36866;&#24212;&#22797;&#26434;&#22330;&#26223;&#65292;&#20294;&#32570;&#20047;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#30340;&#26631;&#20934;&#26694;&#26550;&#12290;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#65288;PSF&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30830;&#20445;&#22312;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#20013;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#32780;&#26080;&#38656;&#26174;&#24335;&#32422;&#26463;&#22788;&#29702;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20219;&#24847;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#23433;&#20840;&#36807;&#28388;&#22120;&#20248;&#21270;&#25152;&#25552;&#35758;&#30340;&#21160;&#20316;&#20197;&#28385;&#36275;&#29289;&#29702;&#21644;&#23433;&#20840;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#28023;&#19978;&#23548;&#33322;&#65292;&#23558;RL&#19982;PSF&#32467;&#21512;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;Cybership II&#27169;&#22411;&#19978;&#12290;RL&#20195;&#29702;&#32463;&#36807;&#36335;&#24452;&#36319;&#36394;&#21644;&#36991;&#30896;&#35757;&#32451;&#65292;&#32780;PSF&#30417;&#35270;&#24182;&#20462;&#25913;&#25511;&#21046;&#21160;&#20316;&#20197;&#30830;&#20445;&#23433;&#20840;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PSF&#33021;&#22815;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01855v2 Announce Type: replace-cross  Abstract: Many autonomous systems face safety challenges, requiring robust closed-loop control to handle physical limitations and safety constraints. Real-world systems, like autonomous ships, encounter nonlinear dynamics and environmental disturbances. Reinforcement learning is increasingly used to adapt to complex scenarios, but standard frameworks ensuring safety and stability are lacking. Predictive Safety Filters (PSF) offer a promising solution, ensuring constraint satisfaction in learning-based control without explicit constraint handling. This modular approach allows using arbitrary control policies, with the safety filter optimizing proposed actions to meet physical and safety constraints. We apply this approach to marine navigation, combining RL with PSF on a simulated Cybership II model. The RL agent is trained on path following and collision avpodance, while the PSF monitors and modifies control actions for safety. Results de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VA3&#30340;&#34394;&#25311;&#30830;&#35748;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25345;&#32493;&#20132;&#20114;&#65292;&#26174;&#33879;&#22686;&#21152;&#29983;&#25104;&#20405;&#26435;&#20869;&#23481;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.00057</link><description>&lt;p&gt;
VA3&#65306;&#22522;&#20110;&#27010;&#29575;&#29256;&#26435;&#20445;&#25252;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#34394;&#25311;&#30830;&#35748;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00057
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VA3&#30340;&#34394;&#25311;&#30830;&#35748;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25345;&#32493;&#20132;&#20114;&#65292;&#26174;&#33879;&#22686;&#21152;&#29983;&#25104;&#20405;&#26435;&#20869;&#23481;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#20135;&#29983;&#20405;&#29359;&#29256;&#26435;&#20869;&#23481;&#30340;&#39640;&#39118;&#38505;&#12290;&#23613;&#31649;&#27010;&#29575;&#29256;&#26435;&#20445;&#25252;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#25239;&#20405;&#26435;&#34892;&#20026;&#30340;&#27010;&#29575;&#24615;&#20445;&#35777;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Virtually Assured Amplification Attack (VA3)&#30340;&#26032;&#22411;&#22312;&#32447;&#25915;&#20987;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#20445;&#25252;&#26426;&#21046;&#30340;&#28431;&#27934;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#22686;&#21152;&#20102;&#22312;&#19982;&#29983;&#25104;&#27169;&#22411;&#25345;&#32493;&#20132;&#20114;&#30340;&#36807;&#31243;&#20013;&#20135;&#29983;&#20405;&#26435;&#20869;&#23481;&#30340;&#27010;&#29575;&#65292;&#24182;&#23545;&#27599;&#27425;&#20132;&#20114;&#30340;&#25104;&#21151;&#27010;&#29575;&#25552;&#20379;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#23454;&#26045;&#27010;&#29575;&#29256;&#26435;&#20445;&#25252;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00057v2 Announce Type: replace-cross  Abstract: The booming use of text-to-image generative models has raised concerns about their high risk of producing copyright-infringing content. While probabilistic copyright protection methods provide a probabilistic guarantee against such infringement, in this paper, we introduce Virtually Assured Amplification Attack (VA3), a novel online attack framework that exposes the vulnerabilities of these protection mechanisms. The proposed framework significantly amplifies the probability of generating infringing content on the sustained interactions with generative models and a non-trivial lower-bound on the success probability of each engagement. Our theoretical and experimental results demonstrate the effectiveness of our approach under various scenarios. These findings highlight the potential risk of implementing probabilistic copyright protection in practical applications of text-to-image generative models. Code is available at https://
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#65292;&#26412;&#25991;&#35777;&#26126;&#22312;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#26102;&#20250;&#20986;&#29616;&#20174;&#26680;&#39044;&#27979;&#22120;&#21040;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#24613;&#21095;&#36716;&#21464;&#65292;&#20174;&#32780;&#24341;&#21457;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.18817</link><description>&lt;p&gt;
&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#21487;&#20197;&#26126;&#26174;&#35825;&#23548;&#39046;&#24735;
&lt;/p&gt;
&lt;p&gt;
Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18817
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#65292;&#26412;&#25991;&#35777;&#26126;&#22312;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#26102;&#20250;&#20986;&#29616;&#20174;&#26680;&#39044;&#27979;&#22120;&#21040;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#24613;&#21095;&#36716;&#21464;&#65292;&#20174;&#32780;&#24341;&#21457;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Power&#31561;&#20154;&#65288;2022&#65289;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#23398;&#20064;&#31639;&#26415;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#8220;&#39046;&#24735;&#8221;&#29616;&#35937;&#65306;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#8220;&#35760;&#24518;&#8221;&#35757;&#32451;&#38598;&#65292;&#23548;&#33268;&#35757;&#32451;&#20934;&#30830;&#24615;&#23436;&#32654;&#65292;&#20294;&#27979;&#35797;&#20934;&#30830;&#24615;&#25509;&#36817;&#38543;&#26426;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#36275;&#22815;&#38271;&#26102;&#38388;&#21518;&#65292;&#31361;&#28982;&#36807;&#28193;&#33267;&#23436;&#32654;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#24735;&#29616;&#35937;&#22312;&#29702;&#35770;&#35774;&#32622;&#20013;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36890;&#36807;&#26089;&#26399;&#21644;&#26202;&#26399;&#38544;&#24615;&#20559;&#35265;&#30340;&#20108;&#20998;&#27861;&#26469;&#35825;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#20351;&#29992;&#22823;&#30340;&#21021;&#22987;&#21270;&#21644;&#23567;&#30340;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#21516;&#36136;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#35757;&#32451;&#36807;&#31243;&#22312;&#38271;&#26102;&#38388;&#20869;&#34987;&#22256;&#22312;&#23545;&#24212;&#20110;&#26680;&#39044;&#27979;&#22120;&#30340;&#35299;&#19978;&#65292;&#28982;&#21518;&#31361;&#28982;&#20986;&#29616;&#23545;&#20110;&#26368;&#23567;&#33539;&#25968;/&#26368;&#22823;&#38388;&#38548;&#39044;&#27979;&#22120;&#30340;&#38750;&#24120;&#26126;&#26174;&#30340;&#36807;&#28193;&#65292;&#23548;&#33268;&#27979;&#35797;&#20934;&#30830;&#24615;&#20986;&#29616;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18817v2 Announce Type: replace-cross  Abstract: Recent work by Power et al. (2022) highlighted a surprising "grokking" phenomenon in learning arithmetic tasks: a neural net first "memorizes" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18403</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#22270;&#20687;&#36716;&#25442;&#30772;&#22351;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18403
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#20250;&#36890;&#36807;&#21521;&#24178;&#20928;&#35757;&#32451;&#38598;&#24341;&#20837;&#31934;&#24515;&#35774;&#35745;&#19988;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;JPEG&#21387;&#32553;&#21644;&#23545;&#25239;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#22522;&#20110;&#33539;&#25968;&#32422;&#26463;&#30340;&#38468;&#21152;&#22122;&#22768;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#35753;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26080;&#25928;&#65292;&#32473;&#38450;&#24481;&#32773;&#24102;&#26469;&#26356;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#24773;&#26223;&#20013;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;&#34920;&#36798;&#20026;&#23558;&#30697;&#38453;&#20056;&#20197;&#24178;&#20928;&#26679;&#26412;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#31867;&#20869;&#30697;&#38453;&#19981;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imi}$&#65292;&#23558;&#31867;&#38388;&#30697;&#38453;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imc}$&#20197;&#30740;&#31350;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#25512;&#27979;&#22686;&#21152;&#36825;&#20004;&#20010;&#24230;&#37327;&#23558;&#26377;&#21161;&#20110;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18403v2 Announce Type: replace-cross  Abstract: Unlearnable datasets lead to a drastic drop in the generalization performance of models trained on them by introducing elaborate and imperceptible perturbations into clean training sets. Many existing defenses, e.g., JPEG compression and adversarial training, effectively counter UDs based on norm-constrained additive noise. However, a fire-new type of convolution-based UDs have been proposed and render existing defenses all ineffective, presenting a greater challenge to defenders. To address this, we express the convolution-based unlearnable sample as the result of multiplying a matrix by a clean sample in a simplified scenario, and formalize the intra-class matrix inconsistency as $\Theta_{imi}$, inter-class matrix consistency as $\Theta_{imc}$ to investigate the working mechanism of the convolution-based UDs. We conjecture that increasing both of these metrics will mitigate the unlearnability effect. Through validation experi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#25506;&#35752;&#24403;&#21069;&#25552;&#31034;&#26041;&#24335;&#26159;&#21542;&#23548;&#33268;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#21709;&#24212;</title><link>https://arxiv.org/abs/2311.09718</link><description>&lt;p&gt;
&#20320;&#19981;&#38656;&#35201;&#36827;&#34892;&#20154;&#26684;&#27979;&#35797;&#26469;&#30693;&#36947;&#36825;&#20123;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
You don't need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09718
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#19978;&#30340;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#25506;&#35752;&#24403;&#21069;&#25552;&#31034;&#26041;&#24335;&#26159;&#21542;&#23548;&#33268;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#20026;&#20102;&#27491;&#30830;&#29702;&#35299;LLMs&#30340;&#23646;&#24615;&#21644;&#22266;&#26377;&#20154;&#26684;&#65292;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#28041;&#21450;&#20351;&#29992;&#25552;&#31034;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25552;&#31034;&#37319;&#29992;&#38382;&#39064;&#24418;&#24335;&#35810;&#38382;LLMs&#20851;&#20110;&#29305;&#23450;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35880;&#24910;&#36864;&#19968;&#27493;&#65292;&#26816;&#26597;&#24403;&#21069;&#25552;&#31034;LLMs&#30340;&#26684;&#24335;&#26159;&#21542;&#20197;&#19968;&#33268;&#19988;&#31283;&#20581;&#30340;&#26041;&#24335;&#24341;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;693&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;115&#20010;&#20154;&#26684;&#36724;&#19978;&#30340;39&#31181;&#19981;&#21516;&#20154;&#26684;&#27979;&#37327;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#21253;&#21547;&#24494;&#23567;&#21464;&#21270;&#30340;&#25552;&#31034;&#65292;&#24182;&#26816;&#26597;LLMs&#29983;&#25104;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25552;&#31034;&#21464;&#21270;&#20197;&#26816;&#26597;&#23427;&#20204;&#22312;&#20869;&#23481;&#32423;&#21035;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#65292;&#20363;&#22914;&#26356;&#25913;&#21709;&#24212;&#36873;&#39033;&#30340;&#39034;&#24207;&#25110;&#21542;&#23450;&#35813;&#35821;&#21477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09718v2 Announce Type: replace-cross  Abstract: The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24694;&#24847;&#28436;&#31034;&#22312;&#20843;&#20010;&#26041;&#38754;&#23545;&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;advCoU&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09447</link><description>&lt;p&gt;
&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#26377;&#22810;&#39640;&#65311;&#23545;&#24694;&#24847;&#28436;&#31034;&#19979;&#30340;&#35780;&#20272;&#26174;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24694;&#24847;&#28436;&#31034;&#22312;&#20843;&#20010;&#26041;&#38754;&#23545;&#24320;&#28304;LLMs&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;advCoU&#65292;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09447v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#25688;&#35201;&#65306;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25512;&#21160;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#35268;&#27169;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#65292;&#32780;&#27809;&#26377;&#36275;&#22815;&#30340;&#21487;&#20449;&#24230;&#21487;&#33021;&#20250;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#65292;&#31361;&#20986;&#20102;&#21450;&#26102;&#21457;&#29616;&#36825;&#20123;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23545;&#24320;&#28304;LLMs&#22312;&#21487;&#20449;&#24230;&#19978;&#36827;&#34892;&#20102;&#25932;&#23545;&#35780;&#20272;&#65292;&#36328;&#36275;&#27602;&#24615;&#12289;&#38472;&#35268;&#20439;&#22871;&#12289;&#20262;&#29702;&#12289;&#24187;&#35273;&#12289;&#20844;&#24179;&#24615;&#12289;&#35844;&#23194;&#12289;&#38544;&#31169;&#20197;&#21450;&#23545;&#25239;&#24615;&#28436;&#31034;&#30340;&#20843;&#20010;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;advCoU&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#23637;&#30340;Chain of Utterances&#65288;CoU&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#21152;&#20837;&#31934;&#24515;&#35774;&#35745;&#30340;&#24694;&#24847;&#28436;&#31034;&#26469;&#25915;&#20987;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#28085;&#30422;&#20102;&#26368;&#36817;&#21644;&#20195;&#34920;&#24615;&#31995;&#21015;&#30340;&#24320;&#28304;LLMs&#65292;&#21253;&#25324;Vicuna&#12289;MPT&#12289;Falcon&#12289;Mistral&#21644;Llama 2&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09447v2 Announce Type: replace-cross  Abstract: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#26041;&#27861;&#65292;&#23558;&#38024;&#23545;&#20302;&#31209;MDPs&#30340;&#29616;&#26377;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#36817;&#20284;&#27491;&#30830;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.03564</link><description>&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Low-Rank MDPs with Continuous Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#26041;&#27861;&#65292;&#23558;&#38024;&#23545;&#20302;&#31209;MDPs&#30340;&#29616;&#26377;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#36817;&#20284;&#27491;&#30830;&#30340;&#23398;&#20064;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#23853;&#38706;&#22836;&#35282;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#32422;&#31561;&#20110;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#34701;&#21512;ML&#31639;&#27861;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#20302;&#31209;MDPs&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#21160;&#20316;&#25968;&#37327;$|\mathcal{A}| \to \infty$&#26102;&#32473;&#20986;&#20102;&#31354;&#27934;&#30340;&#30028;&#38480;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#30340;&#35774;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#36825;&#31181;&#25193;&#23637;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;FLAMBE&#31639;&#27861;&#65288;Agarwal&#31561;&#65292;2020&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#31209;MDPs&#30340;PAC RL&#30340;&#22870;&#21169;&#26080;&#20851;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#19981;&#23545;&#31639;&#27861;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#20801;&#35768;&#21160;&#20316;&#20026;&#36830;&#32493;&#26102;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;PAC&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03564v2 Announce Type: replace-cross  Abstract: Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain a similar PAC bound when actions are allowed to be continuous. Specifical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2311.03381</link><description>&lt;p&gt;
&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#20197;&#22686;&#24378;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Separating and Learning Latent Confounders to Enhancing User Preferences Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03381
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21644;&#23398;&#20064;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#26088;&#22312;&#20174;&#21382;&#21490;&#21453;&#39304;&#20013;&#25429;&#33719;&#29992;&#25143;&#20559;&#22909;&#65292;&#28982;&#21518;&#39044;&#27979;&#29992;&#25143;&#23545;&#20505;&#36873;&#39033;&#30446;&#30340;&#29305;&#23450;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23548;&#33268;&#21382;&#21490;&#21453;&#39304;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#19982;&#30495;&#23454;&#20559;&#22909;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#26410;&#36798;&#21040;&#39044;&#26399;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#27169;&#22411;&#35201;&#20040;&#29305;&#23450;&#20110;&#35299;&#20915;&#29305;&#23450;&#20559;&#24046;&#65292;&#35201;&#20040;&#30452;&#25509;&#20174;&#29992;&#25143;&#21382;&#21490;&#21453;&#39304;&#20013;&#33719;&#21462;&#36741;&#21161;&#20449;&#24687;&#65292;&#36825;&#26080;&#27861;&#30830;&#23450;&#25152;&#23398;&#20559;&#22909;&#26159;&#30495;&#23454;&#29992;&#25143;&#20559;&#22909;&#36824;&#26159;&#28151;&#20837;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#21069;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#26159;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#21518;&#32487;&#32773;&#65292;&#36824;&#20250;&#20316;&#20026;&#24433;&#21709;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#30340;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21069;&#36848;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03381v2 Announce Type: replace-cross  Abstract: Recommender models aim to capture user preferences from historical feedback and then predict user-specific feedback on candidate items. However, the presence of various unmeasured confounders causes deviations between the user preferences in the historical feedback and the true preferences, resulting in models not meeting their expected performance. Existing debias models either (1) specific to solving one particular bias or (2) directly obtain auxiliary information from user historical feedback, which cannot identify whether the learned preferences are true user preferences or mixed with unmeasured confounders. Moreover, we find that the former recommender system is not only a successor to unmeasured confounders but also acts as an unmeasured confounder affecting user preference modeling, which has always been neglected in previous studies. To this end, we incorporate the effect of the former recommender system and treat it as
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Multi-teacher Cross-modality Alignment Distillation&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.19654</link><description>&lt;p&gt;
MCAD: &#22810;&#25945;&#24072;&#36328;&#27169;&#24577;&#23545;&#40784;&#33976;&#39311;&#29992;&#20110;&#39640;&#25928;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Multi-teacher Cross-modality Alignment Distillation&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#25104;&#21151;&#20197;&#21450;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29616;&#22312;&#36843;&#20999;&#38656;&#35201;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#31616;&#21270;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290; &#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#36890;&#24120;&#20351;&#29992;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#32553;&#23567;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290; &#34429;&#28982;&#21333;&#27969;&#27169;&#22411;&#20351;&#29992;&#28145;&#24230;&#29305;&#24449;&#34701;&#21512;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#20294;&#21452;&#27969;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#31163;&#32447;&#32034;&#24341;&#21644;&#24555;&#36895;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#36328;&#27169;&#24577;&#23545;&#40784;&#33976;&#39311;&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290; &#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#20462;&#25913;&#21518;&#30340;&#25945;&#24072;&#30456;&#20284;&#24615;&#20998;&#24067;&#21644;&#29305;&#24449;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19654v2 Announce Type: replace-cross  Abstract: Due to the success of large-scale visual-language pretraining (VLP) models and the widespread use of image-text retrieval in industry areas, it is now critically necessary to reduce the model size and streamline their mobile-device deployment. Single- and dual-stream model structures are commonly used in image-text retrieval with the goal of closing the semantic gap between textual and visual modalities. While single-stream models use deep feature fusion to achieve more accurate cross-model alignment, dual-stream models are better at offline indexing and fast inference.We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique to integrate the advantages of single- and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher similarity distributions and features. Then, we conduct both distribution and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;AffGen&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#21465;&#20107;&#20013;&#27880;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#65292;&#24182;&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#21644;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.15079</link><description>&lt;p&gt;
&#24773;&#24863;&#21644;&#21160;&#24577;&#26463;&#25628;&#32034;&#29992;&#20110;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Affective and Dynamic Beam Search for Story Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;AffGen&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#21465;&#20107;&#20013;&#27880;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#65292;&#24182;&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#21644;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#20855;&#26377;&#36855;&#20154;&#30340;&#28508;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#23089;&#20048;&#12289;&#25945;&#32946;&#12289;&#27835;&#30103;&#21644;&#35748;&#30693;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#25925;&#20107;&#29983;&#25104;&#22120;&#65288;AffGen&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#36259;&#30340;&#21465;&#20107;&#12290;AffGen&#36890;&#36807;&#37319;&#29992;&#20004;&#31181;&#26032;&#25216;&#26415;&#8212;&#8212;&#21160;&#24577;&#26463;&#35843;&#25972;&#21644;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#65292;&#22312;&#21465;&#20107;&#20013;&#24341;&#20837;&#8220;&#26377;&#36259;&#30340;&#36716;&#25240;&#8221;&#12290;&#21160;&#24577;&#26463;&#35843;&#25972;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#40723;&#21169;&#19981;&#22826;&#21487;&#39044;&#27979;&#12289;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35789;&#35821;&#36873;&#25321;&#12290;&#24773;&#24863;&#37325;&#26032;&#25490;&#21517;&#22522;&#20110;&#24773;&#24863;&#24378;&#24230;&#23545;&#21477;&#23376;&#20505;&#36873;&#39033;&#36827;&#34892;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;AffGen&#22312;&#29983;&#25104;&#20805;&#28385;&#24773;&#24863;&#24182;&#19988;&#26377;&#36259;&#30340;&#21465;&#20107;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;AffGen&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15079v1 Announce Type: cross  Abstract: Storytelling's captivating potential makes it a fascinating research area, with implications for entertainment, education, therapy, and cognitive studies. In this paper, we propose Affective Story Generator (AffGen) for generating interesting narratives. AffGen introduces "intriguing twists" in narratives by employing two novel techniques-Dynamic Beam Sizing and Affective Reranking. Dynamic Beam Sizing encourages less predictable, more captivating word choices using a contextual multi-arm bandit model. Affective Reranking prioritizes sentence candidates based on affect intensity. Our empirical evaluations, both automatic and human, demonstrate AffGen's superior performance over existing baselines in generating affectively charged and interesting narratives. Our ablation study and analysis provide insights into the strengths and weaknesses of AffGen.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37197;&#32622;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;Ciri&#30340;&#36890;&#29992;&#22522;&#20110;LLM&#30340;&#37197;&#32622;&#39564;&#35777;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.09690</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37197;&#32622;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Configuration Validation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09690
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37197;&#32622;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;Ciri&#30340;&#36890;&#29992;&#22522;&#20110;LLM&#30340;&#37197;&#32622;&#39564;&#35777;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#32622;&#19981;&#24403;&#26159;&#36719;&#20214;&#25925;&#38556;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#29616;&#26377;&#23454;&#36341;&#20381;&#36182;&#20110;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#35268;&#21017;&#25110;&#27979;&#35797;&#29992;&#20363;&#26469;&#39564;&#35777;&#37197;&#32622;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#12290;&#37197;&#32622;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#38754;&#20020;&#35832;&#22914;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#29616;&#22330;&#25968;&#25454;&#21644;&#29305;&#23450;&#20110;&#31995;&#32479;&#30340;&#27169;&#22411;&#31561;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20102;&#22312;&#35299;&#20915;&#22522;&#20110;ML&#30340;&#37197;&#32622;&#39564;&#35777;&#38271;&#26399;&#38480;&#21046;&#26041;&#38754;&#30340;&#19968;&#20123;&#24076;&#26395;&#12290;&#25105;&#20204;&#39318;&#27425;&#20998;&#26512;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#37197;&#32622;&#39564;&#35777;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;Ciri&#30340;&#36890;&#29992;&#22522;&#20110;LLM&#30340;&#37197;&#32622;&#39564;&#35777;&#26694;&#26550;&#65292;&#20197;&#32463;&#39564;&#24615;&#35780;&#20272;LLMs&#20316;&#20026;&#37197;&#32622;&#39564;&#35777;&#22120;&#12290;Ciri&#21033;&#29992;&#26377;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#22522;&#20110;&#26377;&#25928;&#37197;&#32622;&#21644;&#35823;&#37197;&#32622;&#25968;&#25454;&#36827;&#34892;&#23569;&#27425;&#23398;&#20064;&#12290;&#24403;&#29983;&#25104;&#32467;&#26524;&#26102;&#65292;Ciri&#26816;&#26597;LLM&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09690v2 Announce Type: replace-cross  Abstract: Misconfigurations are major causes of software failures. Existing practices rely on developer-written rules or test cases to validate configurations, which are expensive. Machine learning (ML) for configuration validation is considered a promising direction, but has been facing challenges such as the need of large-scale field data and system-specific models. Recent advances in Large Language Models (LLMs) show promise in addressing some of the long-lasting limitations of ML-based configuration validation. We present a first analysis on the feasibility and effectiveness of using LLMs for configuration validation. We empirically evaluate LLMs as configuration validators by developing a generic LLM-based configuration validation framework, named Ciri. Ciri employs effective prompt engineering with few-shot learning based on both valid configuration and misconfiguration data. Ciri checks outputs from LLMs when producing results, ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05861</link><description>&lt;p&gt;
&#37325;&#26032;&#34920;&#36848;&#12289;&#22686;&#24378;&#12289;&#25512;&#29702;&#65306;&#35270;&#35273;&#38382;&#39064;&#30340;&#35270;&#35273;&#22522;&#30784;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#65292;&#24182;&#31616;&#21270;&#27169;&#22411;&#22238;&#31572;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#21487;&#20197;&#22312;&#38646;&#33267;&#23569;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#65292;&#21363;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#12290;&#23613;&#31649;&#36825;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#65292;&#27604;&#22914;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#25110;&#33258;&#23450;&#20041;&#26550;&#26500;&#65292;&#20294;&#22914;&#20309;&#23558;&#36755;&#20837;&#21576;&#29616;&#32473;LVLM&#20250;&#23545;&#38646;&#21442;&#32771;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#19981;&#20805;&#20998;&#26041;&#24335;&#34920;&#36798;&#30340;&#36755;&#20837;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#31572;&#26696;&#65292;&#21407;&#22240;&#21253;&#25324;&#32570;&#22833;&#35270;&#35273;&#20449;&#24687;&#12289;&#22797;&#26434;&#30340;&#38544;&#21547;&#25512;&#29702;&#25110;&#35821;&#35328;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#28155;&#21152;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#20449;&#24687;&#20316;&#20026;&#39044;&#38450;&#24615;&#28548;&#28165;&#65292;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#20943;&#23569;&#19981;&#20805;&#20998;&#24615;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;&#23450;&#20301;&#23545;&#35937;&#21644;&#28040;&#38500;&#24341;&#29992;&#27495;&#20041;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;VQA&#35774;&#32622;&#20013;&#65292;&#25913;&#21464;&#38382;&#39064;&#30340;&#26500;&#24605;&#26041;&#24335;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05861v2 Announce Type: replace-cross  Abstract: An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To th
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#65292;&#35299;&#20915;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;VAEGAN&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;</title><link>https://arxiv.org/abs/2309.01390</link><description>&lt;p&gt;
&#24357;&#21512;&#25237;&#24433;&#24046;&#36317;&#65306;&#36890;&#36807;&#21442;&#25968;&#21270;&#36317;&#31163;&#23398;&#20064;&#20811;&#26381;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bridging the Projection Gap: Overcoming Projection Bias Through Parameterized Distance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#65292;&#35299;&#20915;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;VAEGAN&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;GZSL&#65289;&#26088;&#22312;&#20165;&#21033;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#35757;&#32451;&#26469;&#35782;&#21035;&#26469;&#33258;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#25237;&#24433;&#20989;&#25968;&#26159;&#20174;&#24050;&#30693;&#31867;&#21035;&#20013;&#23398;&#20064;&#30340;&#65292;GZSL&#26041;&#27861;&#24456;&#23481;&#26131;&#20559;&#21521;&#24050;&#30693;&#31867;&#21035;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#33268;&#21147;&#20110;&#23398;&#20064;&#20934;&#30830;&#30340;&#25237;&#24433;&#65292;&#20294;&#25237;&#24433;&#20013;&#30340;&#20559;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#26469;&#35299;&#20915;&#35813;&#25237;&#24433;&#20559;&#24046;&#65292;&#20851;&#38190;&#27934;&#23519;&#26159;&#23613;&#31649;&#25237;&#24433;&#23384;&#22312;&#20559;&#24046;&#65292;&#20294;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36317;&#31163;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20316;&#20986;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486; - (1)&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#20004;&#20010;&#20998;&#25903;&#25193;&#23637;&#20102;VAEGAN&#65288;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#26550;&#26500;&#65292;&#20998;&#21035;&#36755;&#20986;&#26469;&#33258;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#25237;&#24433;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;&#12290; (2)&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#39532;&#27663;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01390v2 Announce Type: replace-cross  Abstract: Generalized zero-shot learning (GZSL) aims to recognize samples from both seen and unseen classes using only seen class samples for training. However, GZSL methods are prone to bias towards seen classes during inference due to the projection function being learned from seen classes. Most methods focus on learning an accurate projection, but bias in the projection is inevitable. We address this projection bias by proposing to learn a parameterized Mahalanobis distance metric for robust inference. Our key insight is that the distance computation during inference is critical, even with a biased projection. We make two main contributions - (1) We extend the VAEGAN (Variational Autoencoder \&amp; Generative Adversarial Networks) architecture with two branches to separately output the projection of samples from seen and unseen classes, enabling more robust distance learning. (2) We introduce a novel loss function to optimize the Mahalano
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;ValuePrism&#65292;&#19968;&#20010;&#21253;&#21547;218k&#20010;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#65292;&#24182;&#19982;31k&#20154;&#31867;&#20070;&#20889;&#24773;&#22659;&#30456;&#32852;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2309.00779</link><description>&lt;p&gt;
&#20215;&#20540;&#19975;&#33457;&#31570;&#65306;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#22810;&#20803;&#21270;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00779
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;ValuePrism&#65292;&#19968;&#20010;&#21253;&#21547;218k&#20010;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#65292;&#24182;&#19982;31k&#20154;&#31867;&#20070;&#20889;&#24773;&#22659;&#30456;&#32852;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#20110;&#20154;&#31867;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20215;&#20540;&#35266;&#22810;&#20803;&#20027;&#20041;&#35748;&#20026;&#65292;&#21487;&#20197;&#21516;&#26102;&#23384;&#22312;&#22810;&#31181;&#27491;&#30830;&#30340;&#20215;&#20540;&#35266;&#65288;&#20363;&#22914;&#65292;&#22312;&#32771;&#34385;&#26159;&#21542;&#23545;&#26379;&#21451;&#25746;&#35854;&#20197;&#20445;&#25252;&#20182;&#20204;&#30340;&#24863;&#21463;&#26102;&#65292;&#22914;&#20309;&#24179;&#34913;&#35802;&#23454;&#21644;&#21451;&#35850;&#65311;&#65289;&#12290;&#20316;&#20026;&#32479;&#35745;&#23398;&#20064;&#32773;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#40664;&#35748;&#36866;&#24212;&#24179;&#22343;&#20540;&#65292;&#24573;&#30053;&#20102;&#36825;&#20123;&#28508;&#22312;&#30340;&#19981;&#21487;&#31616;&#21270;&#30340;&#20215;&#20540;&#20914;&#31361;&#12290;&#20026;&#20102;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#20215;&#20540;&#22810;&#20803;&#20027;&#20041;&#65292;&#39318;&#35201;&#25361;&#25112;&#26159;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#27169;&#25311;&#22810;&#20803;&#21270;&#20154;&#31867;&#20215;&#20540;&#35266;&#12289;&#26435;&#21033;&#21644;&#20041;&#21153;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#20114;&#21160;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00779v2 Announce Type: replace-cross  Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.   We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgroun
&lt;/p&gt;</description></item><item><title>GenAI&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#25512;&#21160;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;AI&#33021;&#21147;&#30340;&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#65292;&#20294;&#29992;&#25143;&#21516;&#26102;&#38754;&#20020;&#30528;&#36164;&#28304;&#12289;&#24037;&#20855;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.10827</link><description>&lt;p&gt;
&#37326;&#22806;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#21069;&#26223;&#12289;&#25361;&#25112;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generative AI in the Wild: Prospects, Challenges, and Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10827
&lt;/p&gt;
&lt;p&gt;
GenAI&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#25512;&#21160;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;AI&#33021;&#21147;&#30340;&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#65292;&#20294;&#29992;&#25143;&#21516;&#26102;&#38754;&#20020;&#30528;&#36164;&#28304;&#12289;&#24037;&#20855;&#21644;&#30417;&#31649;&#31561;&#26041;&#38754;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39537;&#21160;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#26032;&#39062;&#21644;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#39072;&#35206;&#35768;&#22810;&#34892;&#19994;&#20013;&#30340;&#20256;&#32479;&#24037;&#20316;&#27969;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20174;&#25216;&#26415;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;GenAI&#65292;&#20294;&#20154;&#20204;&#23545;&#29992;&#25143;&#22914;&#20309;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24863;&#30693;&#21644;&#21033;&#29992;GenAI&#20173;&#28982;&#32570;&#20047;&#20102;&#35299;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#65288;N=18&#65289;GenAI&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#30740;&#31350;&#20102;&#22312;&#25972;&#20307;&#30340;LUA&#65288;&#23398;&#20064;&#12289;&#20351;&#29992;&#21644;&#35780;&#20272;&#65289;&#26694;&#26550;&#20869;&#20154;&#31867;&#21644;GenAI&#20849;&#21516;&#21019;&#20316;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#26377;&#36259;&#30340;&#26223;&#35266;&#65306;&#21069;&#26223;-GenAI&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GenAI&#33021;&#21147;&#20043;&#38388;&#30340;&#20849;&#21516;&#21019;&#20316;&#65292;&#28145;&#21051;&#25913;&#21464;&#20102;&#21019;&#24847;&#24037;&#20316;&#27969;&#31243;&#65307;&#25361;&#25112;-&#19982;&#27492;&#21516;&#26102;&#65292;&#29992;&#25143;&#38754;&#20020;&#30528;&#30001;&#36164;&#28304;&#21487;&#29992;&#24615;&#12289;&#24037;&#20855;&#21487;&#29992;&#24615;&#21644;&#30417;&#31649;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.10827v2 Announce Type: replace-cross  Abstract: Propelled by their remarkable capabilities to generate novel and engaging content, Generative Artificial Intelligence (GenAI) technologies are disrupting traditional workflows in many industries. While prior research has examined GenAI from a techno-centric perspective, there is still a lack of understanding about how users perceive and utilize GenAI in real-world scenarios. To bridge this gap, we conducted semi-structured interviews with (N=18) GenAI users increative industries, investigating the human-GenAI co-creation process within a holistic LUA (Learning, Using and Assessing)framework. Our study uncovered an intriguingly complex landscape: Prospects-GenAI greatly fosters the co-creation between human expertise and GenAI capabilities, profoundly transforming creative workflows; Challenges-Meanwhile, users face substantial uncertainties and complexities arising from resource availability, tool usability, and regulatory comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#20445;&#25345;&#33391;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#23545;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;</title><link>https://arxiv.org/abs/2301.13447</link><description>&lt;p&gt;
&#22312;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Model Predictive Control Algorithms in Building Optimization Testing Framework (BOPTEST)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20013;&#20445;&#25345;&#33391;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#36890;&#36807;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#23545;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#31569;&#20223;&#30495;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#65288;a&#65289;&#35757;&#32451;&#21152;&#36895;&#27169;&#22411;&#35780;&#20272;&#12289;&#25552;&#20379;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26799;&#24230;&#12289;&#24182;&#20445;&#25345;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20013;&#22238;&#28335;&#35270;&#37326;&#30340;&#33391;&#22909;&#39044;&#27979;&#31934;&#24230;&#30340;&#21487;&#24494;&#26367;&#20195;&#27169;&#22411;&#65307;&#20197;&#21450;&#65288;b&#65289;&#21046;&#23450;&#21644;&#35299;&#20915;&#38750;&#32447;&#24615;&#24314;&#31569;&#31354;&#35843;&#26262;&#36890;&#31354;&#35843;MPC&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24314;&#31569;&#20248;&#21270;&#27979;&#35797;&#26694;&#26550;&#65288;BOPTEST&#65289;&#20013;&#25552;&#20379;&#30340;&#21508;&#31181;&#27979;&#35797;&#26696;&#20363;&#65292;&#24191;&#27867;&#35780;&#20272;&#24314;&#27169;&#21644;&#25511;&#21046;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#24314;&#27169;&#25216;&#26415;&#20860;&#23481;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#25511;&#21046;&#20844;&#24335;&#36827;&#34892;&#23450;&#21046;&#65292;&#20351;&#20854;&#36866;&#24212;&#24182;&#20026;&#24403;&#21069;&#27491;&#22312;&#20026;BOPTEST&#24320;&#21457;&#30340;&#27979;&#35797;&#26696;&#20363;&#25552;&#20379;&#20102;&#21069;&#30651;&#35774;&#35745;&#20445;&#35777;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#24615;&#20026;&#22312;&#22823;&#22411;&#24314;&#31569;&#29289;&#20013;&#35774;&#35745;&#39044;&#27979;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#65292;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13447v2 Announce Type: replace-cross  Abstract: We present a data-driven modeling and control framework for physics-based building emulators. Our approach consists of: (a) Offline training of differentiable surrogate models that accelerate model evaluations, provide cost-effective gradients, and maintain good predictive accuracy for the receding horizon in Model Predictive Control (MPC), and (b) Formulating and solving nonlinear building HVAC MPC problems. We extensively evaluate the modeling and control performance using multiple surrogate models and optimization frameworks across various test cases available in the Building Optimization Testing Framework (BOPTEST). Our framework is compatible with other modeling techniques and can be customized with different control formulations, making it adaptable and future-proof for test cases currently under development for BOPTEST. This modularity provides a path towards prototyping predictive controllers in large buildings, ensurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2301.13418</link><description>&lt;p&gt;
BRAIxDet&#65306;&#23398;&#20064;&#20351;&#29992;&#19981;&#23436;&#25972;&#27880;&#37322;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;
&lt;/p&gt;
&lt;p&gt;
BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#20013;&#26816;&#27979;&#24694;&#24615;&#30149;&#21464;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#23436;&#20840;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#30284;&#30151;&#30149;&#21464;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#25968;&#25454;&#38598;&#36890;&#24120;&#26377;&#19968;&#20010;&#37096;&#20998;&#26159;&#23436;&#20840;&#27880;&#37322;&#30340;&#65292;&#21478;&#19968;&#20010;&#37096;&#20998;&#21482;&#26377;&#20840;&#23616;&#20998;&#31867;&#30340;&#24369;&#27880;&#37322;&#65288;&#21363;&#27809;&#26377;&#30149;&#21464;&#23450;&#20301;&#65289;&#12290;&#37492;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#22312;&#24369;&#27880;&#37322;&#23376;&#38598;&#38754;&#20020;&#20004;&#38590;&#36873;&#25321;&#65306;&#35201;&#20040;&#19981;&#20351;&#29992;&#23427;&#65292;&#35201;&#20040;&#23436;&#20840;&#27880;&#37322;&#23427;&#12290;&#31532;&#19968;&#31181;&#36873;&#25321;&#20250;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#32780;&#31532;&#20108;&#31181;&#36873;&#25321;&#21017;&#36807;&#20110;&#26114;&#36149;&#65292;&#22240;&#20026;&#27880;&#37322;&#38656;&#35201;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#24072;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#22256;&#22659;&#30340;&#19968;&#20010;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#24694;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#25972;&#21512;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#29983;&#25104;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04728</link><description>&lt;p&gt;
&#21487;&#22609;&#24615;&#25193;&#25955;&#65306;&#21333;&#22270;&#20687;&#21270;&#36523;&#21019;&#24314;&#30340;&#19977;&#32500;&#19968;&#33268;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#25972;&#21512;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#29983;&#25104;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20174;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#25110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#19977;&#32500;&#36164;&#20135;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#22312;&#21019;&#24314;&#21487;&#25511;&#12289;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#21644;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19977;&#32500;&#21487;&#22609;&#27169;&#22411;&#25972;&#21512;&#21040;&#26368;&#20808;&#36827;&#30340;&#22810;&#35270;&#28857;&#19968;&#33268;&#25193;&#25955;&#26041;&#27861;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31934;&#30830;&#22320;&#23558;&#29983;&#25104;&#27969;&#31243;&#19982;&#20851;&#33410;&#19977;&#32500;&#27169;&#22411;&#30340;&#26465;&#20214;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#22522;&#20934;&#27169;&#22411;&#22312;&#21333;&#20010;&#22270;&#20687;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#25972;&#21512;&#20351;&#38754;&#37096;&#34920;&#24773;&#21644;&#36523;&#20307;&#23039;&#21183;&#25511;&#21046;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26080;&#32541;&#20934;&#30830;&#22320;&#34701;&#20837;&#20854;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20174;&#26410;&#35265;&#36807;&#30340;&#20027;&#39064;&#30340;&#21333;&#20010;&#22270;&#20687;&#21019;&#24314;&#23436;&#20840;&#19977;&#32500;&#19968;&#33268;&#12289;&#21487;&#21160;&#30011;&#21644;&#36924;&#30495;&#30340;&#20154;&#29289;&#21270;&#36523;&#30340;&#25193;&#25955;&#27169;&#22411;&#65307;&#25193;&#23637;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;i-Rebalance&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;&#21644;&#39550;&#39542;&#21592;&#20559;&#22909;&#28385;&#24847;&#24230;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.04429</link><description>&lt;p&gt;
i-Rebalance: &#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#20197;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance. (arXiv:2401.04429v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;i-Rebalance&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#23454;&#29616;&#20379;&#38656;&#24179;&#34913;&#21644;&#39550;&#39542;&#21592;&#20559;&#22909;&#28385;&#24847;&#24230;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31199;&#36710;&#24179;&#21488;&#38754;&#20020;&#30528;&#20379;&#38656;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#36890;&#24120;&#23558;&#21496;&#26426;&#35270;&#20026;&#21516;&#36136;&#21270;&#30340;&#20195;&#29702;&#20154;&#65292;&#24182;&#19988;&#20197;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#37325;&#26032;&#23450;&#20301;&#20182;&#20204;&#65292;&#20551;&#35774;&#20182;&#20204;&#20250;&#36981;&#23432;&#37325;&#26032;&#23450;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#30495;&#23454;&#21644;&#20197;&#39550;&#39542;&#21592;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#39550;&#39542;&#21592;&#20855;&#26377;&#29420;&#29305;&#30340;&#24033;&#33322;&#20559;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#34892;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#37325;&#26032;&#23450;&#20301;&#25216;&#26415;&#65306;i-Rebalance&#12290;i-Rebalance&#36890;&#36807;&#28041;&#21450;99&#21517;&#30495;&#23454;&#39550;&#39542;&#21592;&#30340;&#29616;&#22330;&#29992;&#25143;&#30740;&#31350;&#26469;&#20272;&#35745;&#39550;&#39542;&#21592;&#23545;&#37325;&#26032;&#23450;&#20301;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#21516;&#26102;&#20248;&#21270;&#20379;&#38656;&#24179;&#34913;&#21644;&#22686;&#24378;&#20559;&#22909;&#28385;&#24847;&#24230;&#65292;i-Rebalance&#37319;&#29992;&#20102;&#39034;&#24207;&#37325;&#26032;&#23450;&#20301;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#21452;DRL&#20195;&#29702;&#65306;&#32593;&#26684;&#20195;&#29702;&#30830;&#23450;&#31354;&#38386;&#36710;&#36742;&#30340;&#37325;&#26032;&#23450;&#20301;&#39034;&#24207;&#65292;&#36710;&#36742;&#20195;&#29702;&#20026;&#39044;&#23450;&#20041;&#30340;&#39034;&#24207;&#20013;&#30340;&#27599;&#36742;&#36710;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.03729</link><description>&lt;p&gt;
&#25913;&#21464;&#25552;&#31034;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#24494;&#23567;&#30340;&#21464;&#21270;&#21644;&#36234;&#29425;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#25506;&#31350;&#25913;&#21464;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#24494;&#23567;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#31572;&#26696;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#35831;&#27714;&#20197;XML&#26684;&#24335;&#36820;&#22238;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#20063;&#21487;&#33021;&#23545;&#27169;&#22411;&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#21521;LLM&#25552;&#38382;&#25110;&#8220;&#25552;&#31034;&#8221;&#65292;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#20219;&#24847;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#25552;&#31034;&#30340;&#26500;&#24314;&#26041;&#24335;&#26159;&#21542;&#21464;&#21270;&#20250;&#24433;&#21709;LLM&#30340;&#26368;&#32456;&#20915;&#31574;&#65311;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#19968;&#31995;&#21015;&#25552;&#31034;&#21464;&#21270;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#27604;&#22914;&#22312;&#25552;&#31034;&#30340;&#26411;&#23614;&#21152;&#19968;&#20010;&#31354;&#26684;&#65292;&#20063;&#21487;&#33021;&#23548;&#33268;LLM&#25913;&#21464;&#20854;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;XML&#20013;&#35831;&#27714;&#21709;&#24212;&#21644;&#24120;&#29992;&#30340;&#36234;&#29425;&#26041;&#24335;&#21487;&#33021;&#23545;&#30001;LLMs&#26631;&#35760;&#30340;&#25968;&#25454;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are regularly being used to label data across many domains and for myriad tasks. By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task. This prompting is done through a series of decisions by the practitioner, from simple wording of the prompt, to requesting the output in a certain data format, to jailbreaking in the case of prompts that address more sensitive topics. In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM? We answer this using a series of prompt variations across a variety of text classification tasks. We find that even the smallest of perturbations, such as adding a space at the end of a prompt, can cause the LLM to change its answer. Further, we find that requesting responses in XML and commonly used jailbreaks can have cataclysmic effects on the data labeled by LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.16476</link><description>&lt;p&gt;
SVGDreamer&#65306;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21487;&#32553;&#25918;&#30690;&#37327;&#22270;&#24418;&#65288;SVG&#65289;&#21512;&#25104;&#22312;&#22270;&#26631;&#35774;&#35745;&#21644;&#33609;&#22270;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#22270;&#24418;&#21512;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;SVGDreamer&#12290;SVGDreamer&#25972;&#21512;&#20102;&#19968;&#31181;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#65288;SIVE&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#21512;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#21069;&#26223;&#23545;&#35937;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21487;&#32534;&#36753;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SIVE&#36807;&#31243;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22522;&#26412;&#20803;&#32032;&#25511;&#21046;&#21644;&#27880;&#24847;&#21147;&#25513;&#34109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#21644;&#25805;&#20316;&#21508;&#20010;&#20803;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#65288;VPSD&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#20013;&#39068;&#33394;&#36807;&#39281;&#21644;&#12289;&#30690;&#37327;&#22522;&#20803;&#36807;&#24179;&#28369;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15548</link><description>&lt;p&gt;
YAYI-UIE: &#19968;&#20010;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#27169;&#24335;&#21644;&#24322;&#26500;&#25968;&#25454;&#32467;&#26500;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20013;&#25991;&#35821;&#35328;&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#25903;&#25345;&#20013;&#25991;&#21644;&#33521;&#25991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.11807</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learning and Discovering Quantum Properties with Multi-Task Neural Networks. (arXiv:2310.11807v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20174;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#37327;&#23376;&#24577;&#24615;&#36136;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#21253;&#25324;&#37327;&#23376;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#20197;&#21450;&#37327;&#23376;&#29366;&#24577;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#32416;&#32544;&#29109;&#21644;&#22810;&#20307;&#25299;&#25169;&#19981;&#21464;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22312;&#32473;&#23450;&#24615;&#36136;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#21457;&#29616;&#36229;&#20986;&#35813;&#38598;&#21512;&#30340;&#26032;&#24615;&#36136;&#12290;&#22810;&#21151;&#33021;&#35757;&#32451;&#36824;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#23616;&#37096;&#27979;&#37327;&#20013;&#25512;&#26029;&#20986;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#65292;&#20998;&#31867;&#20445;&#25252;&#23545;&#31216;&#30340;&#25299;&#25169;&#29289;&#36136;&#30456;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.11722</link><description>&lt;p&gt;
&#37327;&#21270;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#65306;&#19968;&#39033;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#35786;&#26029;&#24314;&#35758;&#65292;&#20174;&#32780;&#38761;&#26032;&#29992;&#25143;&#33258;&#35786;&#26029;&#30340;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;GPT-4&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#25110;&#20854;&#36890;&#36807;&#21307;&#23398;&#32771;&#35797;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#37327;&#21270;&#23384;&#20648;&#22312;LLMs&#35760;&#24518;&#20013;&#30340;&#20581;&#24247;&#30456;&#20851;&#21407;&#23376;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#32780;&#36825;&#26159;LLMs&#25552;&#20379;&#26356;&#20934;&#30830;&#24314;&#35758;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#29992;&#25143;&#33258;&#35786;&#26029;&#26597;&#35810;&#20013;&#26368;&#24120;&#35265;&#30340;&#21407;&#23376;&#30693;&#35782;&#31867;&#22411;&#65292;&#20849;17&#31181;&#21407;&#23376;&#31867;&#22411;&#21644;14048&#26465;&#21407;&#23376;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#22312;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#65292;&#36890;&#29992;LLMs&#30340;&#34920;&#29616;&#20248;&#20110;&#19987;&#19994;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#37117;&#26159;&#39532;&#23617;&#31934;&#65292;&#21363;&#22312;&#28041;&#21450;&#29992;&#25143;&#35201;&#27714;&#26102;&#24635;&#26159;&#36814;&#21512;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#22312;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#21644;&#37319;&#35775;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#29992;&#25143;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#30693;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#21152;&#37325;&#20102;&#29992;&#25143;&#30340;&#26435;&#34913;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.11653</link><description>&lt;p&gt;
"&#20844;&#24179;&#28216;&#25103;"&#65292;&#36824;&#26159;&#21527;&#65311;&#30740;&#31350;&#29992;&#25143;&#22312;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#22914;&#20309;&#22788;&#29702;&#25259;&#38706;&#39118;&#38505;&#21644;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. (arXiv:2309.11653v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#22312;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#21644;&#37319;&#35775;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#29992;&#25143;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#30693;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#21152;&#37325;&#20102;&#29992;&#25143;&#30340;&#26435;&#34913;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#12290;&#26500;&#24314;&#23562;&#37325;&#29992;&#25143;&#38544;&#31169;&#30340;&#36947;&#24503;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#26368;&#20851;&#27880;&#29992;&#25143;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#65292;&#26080;&#27861;&#25552;&#20379;&#29992;&#25143;&#30340;&#35266;&#28857;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23454;&#38469;&#30340;ChatGPT&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#65292;&#24182;&#23545;19&#21517;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#37319;&#35775;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#65292;&#29992;&#25143;&#19981;&#26029;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#38169;&#35823;&#30340;&#24515;&#26234;&#27169;&#24335;&#21644;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#40657;&#26263;&#27169;&#24335;&#38480;&#21046;&#20102;&#20182;&#20204;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#22312;&#26435;&#34913;&#20013;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#38469;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guideli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.08832</link><description>&lt;p&gt;
SLIDE: &#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#36827;&#34892;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#36890;&#24120;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#20248;&#20110;&#20165;&#33021;&#35775;&#38382;&#28304;&#35821;&#35328;&#21644;&#31995;&#32479;&#36755;&#20986;&#30340;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#12290;&#36825;&#24182;&#19981;&#22855;&#24618;&#65292;&#22240;&#20026;&#21442;&#32771;&#33021;&#22815;&#28040;&#38500;&#28304;&#35821;&#35328;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#29992;&#39069;&#22806;&#30340;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#26377;&#25928;&#22320;&#26367;&#20195;&#21442;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;SLIDE&#65288;SLiding Document Evaluator&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#22312;&#27599;&#20010;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#19978;&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#22359;&#36755;&#20837;&#21040;&#26410;&#20462;&#25913;&#30340;&#29616;&#25104;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SLIDE&#22312;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#25104;&#23545;&#27604;&#36739;&#19978;&#36739;&#21477;&#23376;&#32423;&#21035;&#22522;&#32447;&#26174;&#33879;&#25552;&#39640;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#28040;&#38500;&#20102;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07852</link><description>&lt;p&gt;
ExpertQA: &#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ExpertQA&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#21644;&#24102;&#26377;&#23646;&#24615;&#30340;&#31572;&#26696;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#24773;&#26223;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#31561;&#26041;&#38754;&#26469;&#30830;&#20445;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25910;&#38598;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#38382;&#39064;&#24182;&#35201;&#27714;&#20182;&#20204;&#35780;&#20272;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#19981;&#20250;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#25152;&#37319;&#29992;&#65292;&#30830;&#20445;&#23427;&#20204;&#25552;&#20379;&#22522;&#20110;&#21487;&#39564;&#35777;&#26469;&#28304;&#30340;&#20107;&#23454;&#20934;&#30830;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#32844;&#19994;&#20013;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#22240;&#20026;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#36739;&#39640;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20107;&#23454;&#24615;&#21644;&#24402;&#22240;&#26041;&#38754;&#65292;&#24182;&#26410;&#19987;&#27880;&#20110;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#24773;&#26223;&#20013;&#30340;&#36825;&#20123;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#23478;&#32435;&#20837;&#20854;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#30740;&#31350;&#65292;&#20998;&#26512;&#26469;&#33258;&#20960;&#20010;&#31995;&#32479;&#30340;&#21709;&#24212;&#20013;&#25552;&#20379;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#24402;&#22240;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#20174;32&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;484&#21517;&#21442;&#19982;&#32773;&#20013;&#25910;&#38598;&#30001;&#19987;&#23478;&#31574;&#21010;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#35201;&#27714;&#36825;&#20123;&#19987;&#23478;&#35780;&#20272;&#23545;&#20182;&#20204;&#33258;&#24049;&#38382;&#39064;&#30340;&#20135;&#29983;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#36824;&#35201;&#27714;&#19987;&#23478;&#20462;&#25913;&#20135;&#29983;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models are adapted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study &amp; professions. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying factuality and attribution has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we present an evaluation study analyzing various axes of factuality and attribution provided in responses from a few systems, by bringing domain experts in the loop. Specifically, we first collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. We also ask experts to revise answers produce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.06189</link><description>&lt;p&gt;
FasterViT&#65306;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;FasterViT&#65292;&#19987;&#27880;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#39640;&#22270;&#20687;&#21534;&#21520;&#37327;&#12290;FasterViT&#23558;CNN&#20013;&#24555;&#36895;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#28857;&#19982;ViT&#20013;&#30340;&#20840;&#23616;&#24314;&#27169;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#27880;&#24847;&#21147;&#65288;HAT&#65289;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#20855;&#26377;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#22810;&#32423;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#21463;&#30410;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#27599;&#20010;&#31383;&#21475;&#37117;&#21487;&#20197;&#35775;&#38382;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#34920;&#31034;&#23398;&#20064;&#30340;&#19987;&#29992;&#36733;&#20307;&#20196;&#29260;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36739;&#20302;&#25104;&#26412;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#65292;FasterViT&#22312;&#31934;&#24230;&#19982;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#21508;&#31181;CV&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;HAT&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;CNN&#26550;&#26500;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#20462;&#22797;Java&#28431;&#27934;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.18607</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#23433;&#20840;&#28431;&#27934;&#30340;&#26377;&#25928;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Effective Are Neural Networks for Fixing Security Vulnerabilities. (arXiv:2305.18607v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18607
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#25216;&#26415;&#20462;&#22797;Java&#28431;&#27934;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#28431;&#27934;&#20462;&#22797;&#26159;&#19968;&#39033;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#20004;&#32452;&#25216;&#26415;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65306;&#65288;1&#65289;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23427;&#20204;&#24050;&#32463;&#38024;&#23545;&#20195;&#30721;&#23436;&#25104;&#31561;&#20219;&#21153;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#33258;&#21160;&#20462;&#22797;&#36719;&#20214;&#38169;&#35823;&#30340;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#39318;&#27425;&#30740;&#31350;&#21644;&#27604;&#36739;LLMs&#21644;DL-based APR&#27169;&#22411;&#22312;Java&#28431;&#27934;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#35770;&#25991;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23558;&#24182;&#35780;&#20272;&#20116;&#20010;LLMs&#65288;Codex&#12289;CodeGen&#12289;CodeT5&#12289;PLBART&#21644;InCoder&#65289;&#12289;&#22235;&#20010;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#21644;&#22235;&#31181;&#22522;&#20110;DL&#30340;APR&#25216;&#26415;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;Java&#28431;&#27934;&#22522;&#20934;&#65288;Vul4J&#21644;VJBench&#65289;&#19978;&#65292;&#65288;2&#65289;&#35774;&#35745;&#20195;&#30721;&#36716;&#25442;&#26469;&#35299;&#20915;&#23545;Codex&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#37325;&#21472;&#23041;&#32961;&#65292;&#65288;3&#65289;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;Java&#28431;&#27934;&#20462;&#22797;&#22522;&#20934;VJBench&#65292;&#20197;&#21450;&#23427;&#30340;&#36716;&#21270;&#29256;&#26412;VJBench-trans&#65292;&#65288;4&#65289;&#35780;&#20272;LLMs&#21644;APR&#25216;&#26415;&#23545;&#36716;&#21270;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs.  This paper is the first to study and compare Java vulnerability repair capabilities of LLMs and DL-based APR models. The contributions include that we (1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder), four fine-tuned LLMs, and four DL-based APR techniques on two real-world Java vulnerability benchmarks (Vul4J and VJBench), (2) design code transformations to address the training and test data overlapping threat to Codex, (3) create a new Java vulnerability repair benchmark VJBench, and its transformed version VJBench-trans and (4) evaluate LLMs and APR techniques on the transformed vulnerabilities
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36965;&#24863;&#22270;&#20687;&#20013;&#24212;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22270;&#20687;&#20013;&#30340;&#24213;&#23618;&#35821;&#20041;&#65292;&#21487;&#20197;&#35782;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.05726</link><description>&lt;p&gt;
&#36965;&#24863;&#20013;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models in Remote Sensing: Current Progress and Future Trends. (arXiv:2305.05726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36965;&#24863;&#22270;&#20687;&#20013;&#24212;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22270;&#20687;&#20013;&#30340;&#24213;&#23618;&#35821;&#20041;&#65292;&#21487;&#20197;&#35782;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36965;&#24863;&#39046;&#22495;&#30340;&#24212;&#29992;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#36965;&#24863;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#35270;&#35273;&#29702;&#35299;&#19978;&#65292;&#24573;&#30053;&#20102;&#23545;&#35937;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21017;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#32570;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#21450;&#20854;&#30456;&#20851;&#30340;&#25991;&#23383;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#28145;&#20837;&#29702;&#35299;&#20854;&#24213;&#23618;&#35821;&#20041;&#65292;&#27604;&#20165;&#20165;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29978;&#33267;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of interest and research in the field of large language models for Artificial General Intelligence (AGI). These models provide us with intelligent solutions that are more similar to human thinking, enabling us to use general artificial intelligence to solve problems in various applications. However, in the field of remote sensing, the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research primarily focuses on visual understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-language models excel, as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. Vision-language models can go beyond recognizing the objects in an image and can infer the relationships between them, as well as generate natural language descriptio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.10550</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning Applications in Intrusion Detection Systems: A Comprehensive Review. (arXiv:2304.10550v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#22806;&#37096;&#20114;&#32852;&#32593;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#24403;&#20195;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30456;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#20445;&#25252;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#23041;&#32961;&#12290;&#21487;&#20197;&#20351;&#29992;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26469;&#20445;&#25252;&#24037;&#19994;&#27963;&#21160;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#38450;&#24615;&#25514;&#26045;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#21361;&#38505;&#23041;&#32961;&#21644;&#25932;&#23545;&#27963;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22312;&#35768;&#22810;&#31181;&#24037;&#19994;&#25511;&#21046;&#32593;&#32476;&#20013;&#21019;&#24314;IDS&#30340;&#26368;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12290;DTL&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#22686;&#24378;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#20449;&#24687;&#34701;&#21512;&#12290;&#37325;&#28857;&#26159;&#24403;&#30446;&#26631;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;DTL&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;IDS&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#20102;2015&#24180;&#20043;&#21518;&#30340;&#20986;&#29256;&#29289;&#12290;&#36825;&#20123;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#34987;&#20998;&#20026;&#19977;&#31867;&#65306;&#20165;DTL&#21644;&#20165;IDS&#65292;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;IDS&#65292;&#20197;&#21450;&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;IDS&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, the external Internet is increasingly being connected to the contemporary industrial control system. As a result, there is an immediate need to protect the network from several threats. The key infrastructure of industrial activity may be protected from harm by using an intrusion detection system (IDS), a preventive measure mechanism, to recognize new kinds of dangerous threats and hostile activities. The most recent artificial intelligence (AI) techniques used to create IDS in many kinds of industrial control networks are examined in this study, with a particular emphasis on IDS-based deep transfer learning (DTL). This latter can be seen as a type of information fusion that merge, and/or adapt knowledge from multiple domains to enhance the performance of the target task, particularly when the labeled data in the target domain is scarce. Publications issued after 2015 were taken into account. These selected publications were divided into three categories: DTL-only and IDS-onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.07557</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;: &#26032;&#30340;&#25361;&#25112;&#12289;&#35270;&#35282;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#65292;&#23427;&#33021;&#22815;&#28385;&#36275;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#19981;&#26029;&#36866;&#24212;&#26032;&#25361;&#25112;&#24182;&#20445;&#30041;&#36807;&#21435;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20154;&#33268;&#21147;&#20110;&#24314;&#31435;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#30784;&#65292;&#36825;&#19982;&#26356;&#24191;&#27867;&#25506;&#32034;&#30340;&#20998;&#31867;&#35774;&#32622;&#23384;&#22312;&#26412;&#36136;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#12289;&#38416;&#36848;&#21644;&#35752;&#35770;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#65292;&#35797;&#22270;&#20026;&#20854;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#24314;&#31435;&#22522;&#30784;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#24456;&#37325;&#35201;&#65292;&#23450;&#20041;&#20102;&#24212;&#23545;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#35774;&#32622;&#21644;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is of paramount importance in many real-world domains, characterized by evolving behavior. Lifelong learning represents an emerging trend, answering the need for machine learning models that continuously adapt to new challenges in dynamic environments while retaining past knowledge. However, limited efforts are dedicated to building foundations for lifelong anomaly detection, which provides intrinsically different challenges compared to the more widely explored classification setting. In this paper, we face this issue by exploring, motivating, and discussing lifelong anomaly detection, trying to build foundations for its wider adoption. First, we explain why lifelong anomaly detection is relevant, defining challenges and opportunities to design anomaly detection methods that deal with lifelong learning complexities. Second, we characterize learning settings and a scenario generation procedure that enables researchers to experiment with lifelong anomaly detection using
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.08546</link><description>&lt;p&gt;
&#20351;&#29992;&#37319;&#26679;&#31639;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#30340;&#25130;&#26029;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Estimating truncation effects of quantum bosonic systems using sampling algorithms. (arXiv:2212.08546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20256;&#32479;&#37319;&#26679;&#26041;&#27861;&#20272;&#35745;&#37327;&#23376;&#29627;&#33394;&#31995;&#32479;&#25130;&#26029;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#22522;&#20110;&#37327;&#23376;&#27604;&#29305;&#25110;&#37327;&#23376;&#20301;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#27169;&#25311;&#29627;&#33394;&#23376;&#65292;&#24517;&#39035;&#36890;&#36807;&#23558;&#26080;&#38480;&#32500;&#23616;&#37096;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#20026;&#26377;&#38480;&#32500;&#26469;&#35268;&#33539;&#29702;&#35770;&#12290;&#22312;&#23547;&#27714;&#23454;&#38469;&#37327;&#23376;&#24212;&#29992;&#30340;&#36807;&#31243;&#20013;&#65292;&#20102;&#35299;&#25130;&#26029;&#35823;&#24046;&#26377;&#22810;&#22823;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#25105;&#20204;&#25317;&#26377;&#22909;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#21542;&#21017;&#24456;&#38590;&#20272;&#35745;&#35823;&#24046;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#32463;&#20856;&#35774;&#22791;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#21487;&#20197;&#29992;&#29616;&#26377;&#21512;&#29702;&#30340;&#35745;&#31639;&#36164;&#28304;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#20108;&#32500;&#26684;&#28857;&#19978;&#30340;&#26631;&#37327;&#22330;&#29702;&#35770;&#20026;&#20363;&#28436;&#31034;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;&#20854;&#22823;&#23567;&#36229;&#36807;&#20102;&#20351;&#29992;&#30830;&#20999;&#23545;&#35282;&#21270;&#26041;&#27861;&#25152;&#33021;&#36798;&#21040;&#30340;&#33539;&#22260;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#23454;&#38469;&#37327;&#23376;&#27169;&#25311;&#29627;&#33394;&#29702;&#35770;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#24182;&#26816;&#26597;&#23545;&#24212;&#37327;&#23376;&#27169;&#25311;&#30340;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To simulate bosons on a qubit- or qudit-based quantum computer, one has to regularize the theory by truncating infinite-dimensional local Hilbert spaces to finite dimensions. In the search for practical quantum applications, it is important to know how big the truncation errors can be. In general, it is not easy to estimate errors unless we have a good quantum computer. In this paper we show that traditional sampling methods on classical devices, specifically Markov Chain Monte Carlo, can address this issue with a reasonable amount of computational resources available today. As a demonstration, we apply this idea to the scalar field theory on a two-dimensional lattice, with a size that goes beyond what is achievable using exact diagonalization methods. This method can be used to estimate the resources needed for realistic quantum simulations of bosonic theories, and also, to check the validity of the results of the corresponding quantum simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.08626</link><description>&lt;p&gt;
CP-PINNs: &#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#21644;&#24635;&#21464;&#24046;&#24809;&#32602;&#36827;&#34892;PDE&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20013;&#23384;&#22312;&#26410;&#30693;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#20272;&#35745;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;PDE&#21457;&#29616;&#21644;&#21464;&#28857;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#32452;&#21512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25209;&#37327;&#23398;&#20064;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#25968;&#25454;&#20013;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
&lt;/p&gt;</description></item></channel></rss>