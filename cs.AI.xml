<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06715</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23545;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24314;&#31435;&#20004;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#25552;&#39640;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#19981;&#21464;&#24615;&#30340;&#20581;&#22766;&#24615;&#24182;&#35777;&#26126;&#20026;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21482;&#26377;&#24403;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#22320;&#25551;&#36848;&#25152;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;&#65292;&#35299;&#37322;&#26041;&#27861;&#25165;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#39044;&#27979;&#22312;&#29305;&#23450;&#23545;&#31216;&#32676;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#36825;&#21253;&#25324;&#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#26550;&#26500;&#12290;&#20219;&#20309;&#24544;&#23454;&#25551;&#36848;&#36825;&#31181;&#31867;&#22411;&#27169;&#22411;&#30340;&#35299;&#37322;&#37117;&#38656;&#35201;&#19982;&#35813;&#19981;&#21464;&#24615;&#23646;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#36816;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#26469;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#12290;&#36890;&#36807;&#36825;&#31181;&#20005;&#26684;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#65288;1&#65289;&#20004;&#20010;&#24230;&#37327;&#26469;&#34913;&#37327;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#23545;&#31216;&#32676;&#30340;&#20581;&#22766;&#24615;;&#65288;2&#65289;&#19968;&#20123;&#27969;&#34892;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29702;&#35770;&#20581;&#22766;&#24615;&#20445;&#35777;&#65307;&#65288;3&#65289;&#25552;&#39640;&#20219;&#20309;&#35299;&#37322;&#26041;&#27861;&#30456;&#23545;&#20110;&#23545;&#31216;&#32676;&#30340;&#19981;&#21464;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#19982;&#19981;&#21516;&#23545;&#31216;&#32676;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#20013;&#32463;&#39564;&#22320;&#27979;&#37327;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#26469;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06708</link><description>&lt;p&gt;
&#21160;&#35789;&#34892;&#21160;&#65306;&#25913;&#36827;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21160;&#35789;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#26469;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21160;&#35789;&#23545;&#20110;&#27169;&#22411;&#21270;&#20154;&#19982;&#29289;&#20307;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19982;&#29615;&#22659;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#30340;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#35789;&#29702;&#35299;&#26041;&#38754;&#21463;&#38480;&#65292;&#19988;&#20005;&#37325;&#20381;&#36182;&#21517;&#35789;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#21160;&#20316;&#21644;&#26102;&#38388;&#29702;&#35299;&#30340;&#23454;&#38469;&#35270;&#39057;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21160;&#35789;&#32858;&#28966;&#23545;&#27604;&#65288;VFC&#65289;&#26694;&#26550;&#65292;&#25913;&#21892;&#22522;&#20110;CLIP&#30340;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#35789;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21019;&#24314;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#30828;&#36127;&#20363;&#65292;&#20197;&#21450;&#36890;&#36807;&#26657;&#20934;&#31574;&#30053;&#24179;&#34913;&#27491;&#36127;&#23545;&#20013;&#27010;&#24565;&#30340;&#20986;&#29616;&#26469;&#24179;&#34913;&#27491;&#36127;&#23545;&#65307;&#65288;2&#65289;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;&#21160;&#35789;&#30701;&#35821;&#23545;&#40784;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32858;&#28966;&#20110;&#21160;&#35789;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65306;&#35270;&#39057;t...
&lt;/p&gt;
&lt;p&gt;
Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding: video-t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#36827;&#34892;&#38543;&#24847;&#25429;&#25417;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#32455;&#29289;&#30340;&#33258;&#30001;&#33853;&#20307;&#25928;&#24212;&#65292;&#24182;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36755;&#20986;&#23436;&#25972;&#30340;&#21147;&#23398;&#21442;&#25968;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.06704</link><description>&lt;p&gt;
&#22914;&#20309;&#39044;&#27979;&#32455;&#29289;&#30340;&#33258;&#30001;&#33853;&#20307;&#25928;&#24212;? &#36890;&#36807;&#28145;&#24230;&#22270;&#20687;&#25429;&#25417;&#32455;&#29289;&#21147;&#23398;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Will It Drape Like? Capturing Fabric Mechanics from Depth Images. (arXiv:2304.06704v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#36827;&#34892;&#38543;&#24847;&#25429;&#25417;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#32455;&#29289;&#30340;&#33258;&#30001;&#33853;&#20307;&#25928;&#24212;&#65292;&#24182;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36755;&#20986;&#23436;&#25972;&#30340;&#21147;&#23398;&#21442;&#25968;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#36827;&#34892;&#38543;&#24847;&#25429;&#25417;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#32455;&#29289;&#30340;&#21147;&#23398;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#30495;&#23454;&#19990;&#30028;&#32442;&#32455;&#26448;&#26009;&#30340;&#26426;&#26800;&#27491;&#30830;&#25968;&#23383;&#34920;&#31034;&#65292;&#36825;&#26159;&#35768;&#22810;&#20132;&#20114;&#24335;&#35774;&#35745;&#21644;&#24037;&#31243;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#19982;&#29616;&#26377;&#30340;&#25429;&#25417;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#25429;&#25417;&#65292;&#19982;&#32442;&#32455;&#21697;&#30340;&#20809;&#23398;&#22806;&#35266;&#26080;&#20851;&#65292;&#24182;&#19988;&#26131;&#20110;&#30001;&#38750;&#19987;&#19994;&#25805;&#20316;&#32773;&#36827;&#34892;&#32455;&#29289;&#25490;&#21015;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#31574;&#30053;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#19968;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#23436;&#25972;&#30340;&#21147;&#23398;&#21442;&#25968;&#38598;&#12290;&#30001;&#20110;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#36716;&#31227;&#23398;&#20064;&#21327;&#35758;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22270;&#20687;&#65292;&#23613;&#31649;&#21482;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#25104;&#21151;&#22320;&#20851;&#38381;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to estimate the mechanical parameters of fabrics using a casual capture setup with a depth camera. Our approach enables to create mechanically-correct digital representations of real-world textile materials, which is a fundamental step for many interactive design and engineering applications. As opposed to existing capture methods, which typically require expensive setups, video sequences, or manual intervention, our solution can capture at scale, is agnostic to the optical appearance of the textile, and facilitates fabric arrangement by non-expert operators. To this end, we propose a sim-to-real strategy to train a learning-based framework that can take as input one or multiple images and outputs a full set of mechanical parameters. Thanks to carefully designed data augmentation and transfer learning protocols, our solution generalizes to real images despite being trained only on synthetic data, hence successfully closing the sim-to-real loop.Key in our work is to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.06701</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#20915;&#31574;&#32773;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#26469;&#25552;&#39640;&#20915;&#31574;&#32467;&#26524;&#65292;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#21738;&#31181;&#24418;&#24335;&#30340;&#25903;&#25345;&#20250;&#22312;&#20302;&#25104;&#26412;&#19979;&#23548;&#33268;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#32473;&#23450;&#36755;&#20837;&#26102;&#36873;&#25321;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#30340;&#20915;&#31574;&#32773;&#65292;&#24182;&#23558;&#23398;&#20064;&#21508;&#33258;&#30340;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#26435;&#34913;&#20102;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#12290;&#20351;&#29992;&#38543;&#26426;&#29615;&#22659;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; $\texttt{THREAD}$&#65292;&#36825;&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#26469;&#30830;&#23450;&#25104;&#26412;-&#24615;&#33021;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#35745;&#31639;&#23454;&#39564;&#26469;&#35777;&#26126; $\texttt{THREAD}$ &#30456;&#23545;&#20110;&#32447;&#19979;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855; $\texttt{Modiste}$&#65292;&#23427;&#20026;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#12290;$\texttt{Modiste}$ &#20351;&#29992; $\texttt{THREAD}$ &#20026;&#27599;&#20301;&#21307;&#29983;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#24182;&#25512;&#33616;&#20010;&#24615;&#21270;&#30740;&#31350;&#20197;&#20248;&#21270;&#24739;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#24182;&#23558;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#38477;&#33267;&#26368;&#20302;&#12290;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; $\texttt{Modiste}$ &#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
&lt;/p&gt;</description></item><item><title>LSFSL&#26159;&#19968;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#20808;&#39564;&#20449;&#24687;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06672</link><description>&lt;p&gt;
&#21033;&#29992;&#24418;&#29366;&#20449;&#24687;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;LSFSL
&lt;/p&gt;
&lt;p&gt;
LSFSL: Leveraging Shape Information in Few-shot Learning. (arXiv:2304.06672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06672
&lt;/p&gt;
&lt;p&gt;
LSFSL&#26159;&#19968;&#31181;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#20808;&#39564;&#20449;&#24687;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#20351;&#29992;&#36739;&#23569;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#26377;&#38480;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#24555;&#25463;&#23398;&#20064;&#21644;&#32441;&#29702;&#20559;&#24046;&#34892;&#20026;&#31561;&#25361;&#25112;&#26356;&#20026;&#20005;&#23803;&#12290;&#27492;&#22806;&#65292;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#35299;&#20915;&#24555;&#25463;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSFSL&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#20808;&#39564;&#20449;&#24687;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#26356;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#30340;&#20840;&#23616;&#35821;&#20041;&#20013;&#21033;&#29992;LSFSL&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#39068;&#33394;&#26041;&#26696;&#12289;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#23545;&#25239;&#25200;&#21160;&#30340;&#25913;&#21464;&#26356;&#19981;&#26131;&#21463;&#21040;&#25915;&#20987;&#21644;&#24178;&#25200;&#65292;&#20984;&#26174;&#20986;&#22312;&#23569;&#26679;&#26412;&#26041;&#27861;&#20013;&#34701;&#20837;&#30456;&#20851;&#20808;&#39564;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2304.06670</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#22791;&#20869;&#32622;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do deep neural networks have an inbuilt Occam's razor?. (arXiv:2304.06670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#21331;&#36234;&#24615;&#33021;&#24517;&#39035;&#28304;&#33258;&#20110;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#21306;&#20998;&#36825;&#19977;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;DNN&#25152;&#34920;&#36798;&#30340;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;&#32463;&#36807;&#32593;&#32476;&#30830;&#23450;&#30340;&#20989;&#25968;&#20808;&#39564;&#36890;&#36807;&#21033;&#29992;&#26377;&#24207;&#21644;&#28151;&#27788;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#32780;&#21464;&#21270;&#12290;&#23545;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#31867;&#65292;&#25105;&#20204;&#21033;&#29992;&#20989;&#25968;&#30340;&#35823;&#24046;&#35889;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#33021;&#24615;&#30340;&#36817;&#20284;&#12290;&#24403;&#19982;&#20808;&#39564;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#22320;&#39044;&#27979;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;DNN&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#20998;&#26512;&#25581;&#31034;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#36275;&#20197;&#25269;&#28040;&#22797;&#26434;&#24230;&#38543;&#20989;&#25968;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#32780;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#26159;DNNs&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25945;&#32946;&#38382;&#39064;&#26377;&#22810;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#25945;&#24072;&#35780;&#20272;&#35777;&#26126;&#36825;&#20123;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#26377;&#26497;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#23545;&#20110;&#25945;&#24072;&#21644;&#23398;&#29983;&#26469;&#35828;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#29983;&#25104;&#21487;&#20197;&#22823;&#24133;&#20943;&#36731;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#20182;&#20204;&#25945;&#23398;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#33021;&#34920;&#26126;&#30495;&#27491;&#30340;&#25945;&#24072;&#35780;&#21028;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#26159;&#21542;&#36275;&#22815;&#26377;&#29992;&#65292;&#25110;&#32773;&#38382;&#39064;&#26159;&#21542;&#23384;&#22312;&#38169;&#35823;&#21644;/&#25110;&#25945;&#23398;&#20869;&#23481;&#30340;&#24110;&#21161;&#19981;&#22823;&#12290;&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#25945;&#24072;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#36890;&#36807;&#32467;&#21512;CTG&#21644;&#38382;&#39064;&#20998;&#31867;&#65288;Bloom's&#21644;&#38590;&#24230;&#20998;&#31867;&#65289;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#26377;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#38382;&#39064;&#36136;&#37327;&#39640;&#19988;&#36275;&#22815;&#26377;&#29992;&#65292;&#23637;&#31034;&#20102;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation (CTG) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining CTG and question taxonomies (Bloom's and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06634</link><description>&lt;p&gt;
PGTask&#65306;&#20171;&#32461;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#26723;&#26696;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06634
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26469;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#20449;&#24687;&#31232;&#23569;&#19988;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#20351;&#24471;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#12290;&#25105;&#20204;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#30456;&#20851;&#35805;&#35821;&#23545;&#40784;&#30340;&#26723;&#26696;&#21477;&#23376;&#65292;&#20174;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26723;&#26696;&#29983;&#25104;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26723;&#26696;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#24037;&#20855;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#35789;&#25110;&#38656;&#27714;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#29983;&#27963;&#24102;&#26469;&#20415;&#25463;&#12290;&#26412;&#25991;&#35843;&#30740;&#24182;&#24191;&#27867;&#27010;&#36848;&#20102;AIGC&#30340;&#23450;&#20041;&#12289;&#37325;&#35201;&#26465;&#20214;&#12289;&#23574;&#31471;&#33021;&#21147;&#12289;&#39640;&#32423;&#21151;&#33021;&#12289;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#20135;&#19994;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.06632</link><description>&lt;p&gt;
AI-Generated Content (AIGC)&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Content (AIGC): A Survey. (arXiv:2304.06632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06632
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#24037;&#20855;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#35789;&#25110;&#38656;&#27714;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#29983;&#27963;&#24102;&#26469;&#20415;&#25463;&#12290;&#26412;&#25991;&#35843;&#30740;&#24182;&#24191;&#27867;&#27010;&#36848;&#20102;AIGC&#30340;&#23450;&#20041;&#12289;&#37325;&#35201;&#26465;&#20214;&#12289;&#23574;&#31471;&#33021;&#21147;&#12289;&#39640;&#32423;&#21151;&#33021;&#12289;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#20135;&#19994;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#25968;&#23383;&#32463;&#27982;&#20013;&#25968;&#23383;&#26234;&#33021;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#24212;&#36816;&#32780;&#29983;&#12290;AIGC&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26681;&#25454;&#29992;&#25143;&#36755;&#20837;&#30340;&#20851;&#38190;&#35789;&#25110;&#38656;&#27714;&#29983;&#25104;&#20869;&#23481;&#65292;&#20174;&#32780;&#36741;&#21161;&#25110;&#26367;&#20195;&#25163;&#21160;&#20869;&#23481;&#29983;&#25104;&#12290;&#22823;&#22411;&#27169;&#22411;&#31639;&#27861;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;AIGC&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#24037;&#20855;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#29983;&#27963;&#22686;&#28155;&#20102;&#20415;&#25463;&#12290;&#20316;&#20026;&#19978;&#28216;&#25216;&#26415;&#65292;AIGC&#20855;&#26377;&#25903;&#25345;&#19981;&#21516;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#26080;&#38480;&#28508;&#21147;&#12290;&#20998;&#26512;AIGC&#30340;&#24403;&#21069;&#33021;&#21147;&#21644;&#19981;&#36275;&#26159;&#29702;&#35299;&#22914;&#20309;&#22312;&#26410;&#26469;&#24212;&#29992;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#23427;&#30340;&#37325;&#35201;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;AIGC&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#23450;&#20041;&#12289;&#37325;&#35201;&#26465;&#20214;&#12289;&#23574;&#31471;&#33021;&#21147;&#21644;&#39640;&#32423;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#35752;&#35770;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22909;&#22788;&#21644;AIGC&#30340;&#20135;&#19994;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of digital intelligence in the digital economy, artificial intelligence-generated content (AIGC) has emerged. AIGC uses artificial intelligence to assist or replace manual content generation by generating content based on user-inputted keywords or requirements. The development of large model algorithms has significantly strengthened the capabilities of AIGC, which makes AIGC products a promising generative tool and adds convenience to our lives. As an upstream technology, AIGC has unlimited potential to support different downstream applications. It is important to analyze AIGC's current capabilities and shortcomings to understand how it can be best utilized in future applications. Therefore, this paper provides an extensive overview of AIGC, covering its definition, essential conditions, cutting-edge capabilities, and advanced features. Moreover, it discusses the benefits of large-scale pre-trained models and the industrial chain of AIGC. Furthermore, the arti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;</title><link>http://arxiv.org/abs/2304.06607</link><description>&lt;p&gt;
&#27169;&#22411;&#25152;&#26377;&#26435;&#20105;&#35758;&#20013;&#30340;&#34394;&#20551;&#25351;&#25511;
&lt;/p&gt;
&lt;p&gt;
False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#26377;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65292;&#26500;&#25104;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20445;&#25252;&#27169;&#22411;&#19981;&#34987;&#30423;&#29992;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#65288;MOR&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#34987;&#30423;&#30340;&#25216;&#26415;&#12290;MOR&#26041;&#26696;&#20351;&#24471;&#21407;&#21578;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#65288;&#22914;&#27700;&#21360;&#25110;&#25351;&#32441;&#65289;&#26469;&#26029;&#35328;&#23545;&#28041;&#23244;&#30423;&#29992;&#27169;&#22411;&#30340;&#34987;&#21578;&#26041;&#22768;&#31216;&#25152;&#26377;&#26435;&#65292;&#35777;&#26126;&#28041;&#23244;&#27169;&#22411;&#26159;&#34987;&#30423;&#25110;&#32773;&#28304;&#33258;&#20110;&#21407;&#21578;&#26041;&#25317;&#26377;&#30340;&#28304;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968; MOR &#26041;&#26696;&#37325;&#28857;&#25918;&#22312;&#38450;&#33539;&#24694;&#24847;&#28041;&#23244;&#26041;&#26041;&#38754;&#65292;&#30830;&#20445;&#22914;&#26524;&#28041;&#23244;&#27169;&#22411;&#30830;&#23454;&#26159;&#34987;&#30423;&#29256;&#65292;&#21017;&#21407;&#21578;&#26041;&#23558;&#33719;&#32988;&#12290;&#20294;&#26159;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24120;&#35265; MOR &#26041;&#26696;&#23384;&#22312;&#30528;&#21478;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65306;&#24694;&#24847;&#21407;&#21578;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#22320;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#19968;&#31995;&#21015;&#36229;&#20986;&#27425;&#27169;&#24615;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#32780;&#19981;&#29306;&#29298;&#22826;&#22810;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06596</link><description>&lt;p&gt;
&#36229;&#36234;&#27425;&#27169;&#24615;&#65306;&#24102;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#38598;&#21512;&#36873;&#25321;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beyond Submodularity: A Unified Framework of Randomized Set Selection with Group Fairness Constraints. (arXiv:2304.06596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#19968;&#31995;&#21015;&#36229;&#20986;&#27425;&#27169;&#24615;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#32452;&#20844;&#24179;&#24615;&#32780;&#19981;&#29306;&#29298;&#22826;&#22810;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#37325;&#35201;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#23450;&#21521;&#24191;&#21578;&#23637;&#31034;&#12289;&#23478;&#24237;&#36151;&#27454;&#25209;&#20934;&#21644;&#29359;&#32618;&#34892;&#20026;&#39044;&#27979;&#31561;&#12290;&#37492;&#20110;&#36825;&#20123;&#31639;&#27861;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#20851;&#38190;&#22312;&#20110;&#23427;&#20204;&#36816;&#20316;&#24212;&#35813;&#20844;&#24179;&#65292;&#27809;&#26377;&#20559;&#35265;&#25110;&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#23545;&#20110;&#20419;&#36827;&#24179;&#31561;&#21644;&#36991;&#20813;&#27495;&#35270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#28041;&#21450;&#20840;&#23616;&#25928;&#29992;&#20989;&#25968;&#20197;&#21450;&#27599;&#20010;&#32452;&#30340;&#19968;&#32452;&#25928;&#29992;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#32452;&#25351;&#20849;&#20139;&#30456;&#21516;&#23646;&#24615;&#65288;&#20363;&#22914;&#24615;&#21035;&#65289;&#30340;&#20010;&#20307;&#32452;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36328;&#21487;&#34892;&#23376;&#38598;&#30340;&#20998;&#24067;&#65292;&#25351;&#23450;&#27599;&#20010;&#21487;&#34892;&#38598;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#25928;&#29992;&#20989;&#25968;&#24182;&#28385;&#36275;&#39044;&#23450;&#37197;&#39069;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms play an important role in a variety of important decision-making processes, including targeted advertisement displays, home loan approvals, and criminal behavior predictions. Given the far-reaching impact of these algorithms, it is crucial that they operate fairly, free from bias or prejudice towards certain groups in the population. Ensuring impartiality in these algorithms is essential for promoting equality and avoiding discrimination. To this end we introduce a unified framework for randomized subset selection that incorporates group fairness constraints. Our problem involves a global utility function and a set of group utility functions for each group, here a group refers to a group of individuals (e.g., people) sharing the same attributes (e.g., gender). Our aim is to generate a distribution across feasible subsets, specifying the selection probability of each feasible set, to maximize the global utility function while meeting a predetermined quota for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25919;&#27835;Twitter&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30342;&#20248;&#20110;&#20154;&#31867;&#26631;&#27880;&#65292;&#23588;&#20854;&#26159;&#23427;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#23558;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#31038;&#20250;&#31185;&#23398;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06588</link><description>&lt;p&gt;
ChatGPT-4&#22312;&#25919;&#27835;Twitter&#20449;&#24687;&#27880;&#37322;&#20013;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#32988;&#36807;&#19987;&#23478;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. (arXiv:2304.06588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25919;&#27835;Twitter&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30342;&#20248;&#20110;&#20154;&#31867;&#26631;&#27880;&#65292;&#23588;&#20854;&#26159;&#23427;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#23558;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#31038;&#20250;&#31185;&#23398;&#30340;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-4&#22312;&#25919;&#27835;&#30456;&#20851;&#25512;&#25991;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20559;&#35265;&#12290;&#19982;&#19987;&#23478;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#65292;&#30740;&#31350;&#20351;&#29992;2020&#24180;&#32654;&#22269;&#36873;&#20030;&#26399;&#38388;&#30340;&#25919;&#27835;&#30456;&#20851;&#25512;&#25991;&#20316;&#20026;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20934;&#30830;&#24615;&#35780;&#27979;&#22522;&#20934;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT-4&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#21487;&#38752;&#24615;&#26356;&#39640;&#65292;&#24182;&#19988;&#20559;&#35265;&#30456;&#31561;&#25110;&#26356;&#20302;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27880;&#37322;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#20316;&#32773;&#24847;&#21521;&#30340;&#25512;&#25991;&#65292;&#36825;&#20123;&#33021;&#21147;&#34987;&#20256;&#32479;&#19978;&#35270;&#20026;&#26159;&#20154;&#31867;&#29420;&#26377;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#38754;&#23558;&#20135;&#29983;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#24471;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.
&lt;/p&gt;</description></item><item><title>&#22312;&#31616;&#21270;&#20551;&#35774;&#19979;&#65292;&#35757;&#32451;&#22870;&#21169;&#19968;&#33268;&#30340;&#30446;&#26631;&#38598;&#21512;&#20013;&#30340;&#26234;&#33021;&#20307;&#20173;&#26377;&#21487;&#33021;&#36861;&#27714;&#26435;&#21147;&#65292;&#20855;&#26377;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06528</link><description>&lt;p&gt;
&#35757;&#32451;&#21518;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#36861;&#27714;&#26435;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#39044;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Power-seeking can be probable and predictive for trained agents. (arXiv:2304.06528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06528
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31616;&#21270;&#20551;&#35774;&#19979;&#65292;&#35757;&#32451;&#22870;&#21169;&#19968;&#33268;&#30340;&#30446;&#26631;&#38598;&#21512;&#20013;&#30340;&#26234;&#33021;&#20307;&#20173;&#26377;&#21487;&#33021;&#36861;&#27714;&#26435;&#21147;&#65292;&#20855;&#26377;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#27714;&#26435;&#21147;&#30340;&#34892;&#20026;&#26159;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#35201;&#39118;&#38505;&#26469;&#28304;&#65292;&#20294;&#26159;&#25105;&#20204;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#29616;&#26377;&#29702;&#35770;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#36807;&#31243;&#23545;&#20110;&#26234;&#33021;&#20307;&#26435;&#21147;&#36861;&#27714;&#21160;&#26426;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#22312;&#19968;&#20123;&#31616;&#21270;&#30340;&#20551;&#35774;&#19979;&#65292;&#36825;&#19968;&#21160;&#26426;&#20173;&#28982;&#26377;&#21487;&#33021;&#22312;&#35757;&#32451;&#21518;&#30340;&#26234;&#33021;&#20307;&#20013;&#20135;&#29983;&#12290;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#8220;&#19982;&#35757;&#32451;&#22870;&#21169;&#19968;&#33268;&#30340;&#30446;&#26631;&#38598;&#8221;&#65288;&#21363;&#19982;&#35757;&#32451;&#22870;&#21169;&#30456;&#19968;&#33268;&#30340;&#30446;&#26631;&#38598;&#21512;&#65289;&#65292;&#24182;&#20551;&#35774;&#35757;&#32451;&#21518;&#30340;&#26234;&#33021;&#20307;&#20174;&#36825;&#20010;&#38598;&#21512;&#20013;&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#30446;&#26631;&#12290;&#22312;&#19968;&#20010;&#26032;&#30340;&#24773;&#22659;&#20013;&#65292;&#24403;&#35757;&#32451;&#21518;&#30340;&#26234;&#33021;&#20307;&#38754;&#20020;&#20851;&#26426;&#36824;&#26159;&#36991;&#20813;&#20851;&#26426;&#30340;&#36873;&#25321;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#36991;&#20813;&#20851;&#26426;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#21147;&#36861;&#27714;&#21160;&#26426;&#21487;&#33021;&#26159;&#21487;&#39044;&#27979;&#30340;&#65288;&#21487;&#20197;&#39044;&#27979;&#22312;&#26032;&#24773;&#22659;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65289;&#24182;&#19988;&#26159;&#26377;&#21487;&#33021;&#21457;&#29983;&#22312;&#35757;&#32451;&#21518;&#30340;&#26234;&#33021;&#20307;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power-seeking behavior is a key source of risk from advanced AI, but our theoretical understanding of this phenomenon is relatively limited. Building on existing theoretical results demonstrating power-seeking incentives for most reward functions, we investigate how the training process affects power-seeking incentives and show that they are still likely to hold for trained agents under some simplifying assumptions. We formally define the training-compatible goal set (the set of goals consistent with the training rewards) and assume that the trained agent learns a goal from this set. In a setting where the trained agent faces a choice to shut down or avoid shutdown in a new situation, we prove that the agent is likely to avoid shutdown. Thus, we show that power-seeking incentives can be probable (likely to arise for trained agents) and predictive (allowing us to predict undesirable behavior in new situations).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#24182;&#32467;&#21512;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#20180;&#32454;&#35843;&#26597;&#65292;&#20026;&#33529;&#26524;&#26641;&#20892;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#30340;&#21494;&#30149;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06520</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#33529;&#26524;&#21494;&#30149;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Transfer Learning-based Approach for Apple Leaf Disease Classification. (arXiv:2304.06520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#24182;&#32467;&#21512;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23545;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#20180;&#32454;&#35843;&#26597;&#65292;&#20026;&#33529;&#26524;&#26641;&#20892;&#25552;&#20379;&#20102;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#30340;&#21494;&#30149;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#35782;&#21035;&#21644;&#20998;&#31867;&#26893;&#29289;&#30142;&#30149;&#23545;&#20110;&#20445;&#38556;&#20840;&#29699;&#31918;&#39135;&#20379;&#24212;&#30340;&#23433;&#20840;&#21644;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#32463;&#27982;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#19981;&#21516;&#30340;&#20027;&#35201;&#20316;&#29289;&#65292;&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22320;&#21306;&#26159;&#26368;&#37325;&#35201;&#30340;&#21830;&#19994;&#20316;&#29289;&#20043;&#19968;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#20998;&#31867;&#33529;&#26524;&#21494;&#30149;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#33529;&#26524;&#21494;&#30149;&#35782;&#21035;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;EfficientNetV2S&#26550;&#26500;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20256;&#36882;&#21040;&#20998;&#31867;&#22120;&#22359;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#36816;&#34892;&#26102;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#24050;&#32463;&#20180;&#32454;&#35843;&#26597;&#20102;&#21508;&#31181;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#22914;&#36755;&#20837;&#20998;&#36776;&#29575;&#12289;&#23398;&#20064;&#29575;&#12289;&#32426;&#20803;&#25968;&#31561;&#12290;&#35813;&#26041;&#26696;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correct identification and categorization of plant diseases are crucial for ensuring the safety of the global food supply and the overall financial success of stakeholders. In this regard, a wide range of solutions has been made available by introducing deep learning-based classification systems for different staple crops. Despite being one of the most important commercial crops in many parts of the globe, research proposing a smart solution for automatically classifying apple leaf diseases remains relatively unexplored. This study presents a technique for identifying apple leaf diseases based on transfer learning. The system extracts features using a pretrained EfficientNetV2S architecture and passes to a classifier block for effective prediction. The class imbalance issues are tackled by utilizing runtime data augmentation. The effect of various hyperparameters, such as input resolution, learning rate, number of epochs, etc., has been investigated carefully. The competence of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2304.06519</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#39057;&#35889;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Secure Federated Learning for Cognitive Radio Sensing. (arXiv:2304.06519v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#19979;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#35748;&#30693;&#26080;&#32447;&#30005;&#29615;&#22659;&#20013;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#39057;&#35889;&#24863;&#30693;&#65288;SS&#65289;&#12290;&#35752;&#35770;&#20102;FL&#22312;SS&#20013;&#30340;&#21160;&#26426;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#27010;&#36848;&#20102;&#36825;&#20123;&#31639;&#27861;&#38754;&#20020;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#21487;&#33021;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20123;&#22270;&#31034;&#20363;&#65292;&#20197;&#21450;&#38024;&#23545;&#26410;&#26469;CR&#20013;&#22522;&#20110;FL&#30340;SS&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers reliable and secure Spectrum Sensing (SS) based on Federated Learning (FL) in the Cognitive Radio (CR) environment. Motivation, architectures, and algorithms of FL in SS are discussed. Security and privacy threats on these algorithms are overviewed, along with possible countermeasures to such attacks. Some illustrative examples are also provided, with design recommendations for FL-based SS in future CRs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#28304;&#23556;&#39057;&#30340;&#19977;&#32500;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#25429;&#25417;&#24773;&#26223;&#29305;&#24449;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06513</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#28304;&#23556;&#39057;&#30340;&#19977;&#32500;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#21450;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning. (arXiv:2304.06513v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#28304;&#23556;&#39057;&#30340;&#19977;&#32500;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#25429;&#25417;&#24773;&#26223;&#29305;&#24449;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#28304;&#23556;&#39057;&#30340;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#22240;&#20854;&#20302;&#25104;&#26412;&#12289;&#26131;&#20110;&#23450;&#21046;&#37197;&#32622;&#21644;&#38750;&#20405;&#20837;&#24615;&#35774;&#35745;&#21560;&#24341;&#20102;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#28304;&#23556;&#39057;&#30340;&#19977;&#32500;&#23460;&#20869;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20449;&#21495;&#36827;&#34892;&#23450;&#20301;&#24182;&#25429;&#25417;&#24773;&#26223;&#31614;&#21517;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21333;&#20010;&#25509;&#25910;&#22120;&#34987;&#21160;&#30417;&#27979;&#21253;&#21547;&#22330;&#26223;&#29305;&#24449;&#30340;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21160;&#24577;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#31995;&#32479;&#65288;DDDAS&#65289;&#26694;&#26550;&#35774;&#35745;&#21644;&#23450;&#21046;&#37319;&#26679;&#39057;&#29575;&#65292;&#20351;&#24471;&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#21463;&#24433;&#21709;&#26368;&#22823;&#30340;&#39057;&#24102;&#20316;&#20026;&#35780;&#23450;&#39057;&#24102;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#19977;&#31181;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#20869;&#30340;&#21508;&#31181;&#22238;&#24402;&#26041;&#27861;&#26469;&#35757;&#32451;&#21644;&#39044;&#27979;&#25509;&#25910;&#22120;&#20301;&#32622;&#12290;&#22312;&#23454;&#39564;&#22330;&#26223;&#19979;&#37319;&#38598;&#20102; 60 &#20010;&#20301;&#32622;&#30340; PRF &#39057;&#35889;&#65292;&#24182;&#24212;&#29992;&#20102;&#19977;&#20010;&#35780;&#20272;&#20934;&#21017;&#26469;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26080;&#28304;&#23556;&#39057;&#19977;&#32500;&#23450;&#20301;&#31995;&#32479;&#36798;&#21040;&#20102;&#24456;&#22909;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive radio frequency (PRF)-based indoor positioning systems (IPS) have attracted researchers' attention due to their low price, easy and customizable configuration, and non-invasive design. This paper proposes a PRF-based three-dimensional (3D) indoor positioning system (PIPS), which is able to use signals of opportunity (SoOP) for positioning and also capture a scenario signature. PIPS passively monitors SoOPs containing scenario signatures through a single receiver. Moreover, PIPS leverages the Dynamic Data Driven Applications System (DDDAS) framework to devise and customize the sampling frequency, enabling the system to use the most impacted frequency band as the rated frequency band. Various regression methods within three ensemble learning strategies are used to train and predict the receiver position. The PRF spectrum of 60 positions is collected in the experimental scenario, and three criteria are applied to evaluate the performance of PIPS. Experimental results show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#30340;&#21746;&#23398;&#22522;&#30784;&#65292;&#20998;&#21035;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#20013;&#30340;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#35282;&#24230;&#20986;&#21457;&#65292;&#20026;&#25105;&#20204;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#24110;&#21161;&#65292;&#20063;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#25552;&#20379;&#20102;&#20849;&#21516;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.06508</link><description>&lt;p&gt;
GeoAI&#30340;&#21746;&#23398;&#22522;&#30784;&#65306;&#25506;&#32034;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science. (arXiv:2304.06508v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#30340;&#21746;&#23398;&#22522;&#30784;&#65292;&#20998;&#21035;&#20174;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#20013;&#30340;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#35282;&#24230;&#20986;&#21457;&#65292;&#20026;&#25105;&#20204;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#24110;&#21161;&#65292;&#20063;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#25552;&#20379;&#20102;&#20849;&#21516;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20171;&#32461;&#20102;&#21487;&#33021;&#26500;&#25104;GeoAI&#21644;&#31354;&#38388;&#25968;&#25454;&#31185;&#23398;&#21746;&#23398;&#22522;&#30784;&#30340;&#19968;&#20123;&#22522;&#26412;&#20551;&#35774;&#21644;&#21407;&#21017;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#21487;&#25345;&#32493;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12289;&#27169;&#24335;&#30693;&#35782;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26469;&#33258;&#32479;&#19968;&#20262;&#29702;&#35270;&#35282;&#30340;GeoAI&#31995;&#32479;&#20013;&#65288;&#28508;&#22312;&#30340;&#65289;&#20013;&#31435;&#24615;&#32570;&#22833;&#31561;&#20027;&#39064;&#65292;&#32780;&#38750;&#23457;&#26597;&#31354;&#38388;&#25968;&#25454;&#65288;&#20998;&#26512;&#65289;&#30340;&#25104;&#29087;&#29305;&#24449;&#65292;&#22914;&#20132;&#20114;&#12289;&#37051;&#22495;&#21644;&#33258;&#30456;&#20851;&#24615;&#12290;&#21453;&#24605;&#25105;&#20204;&#32844;&#19994;&#36947;&#24503;&#30340;&#24433;&#21709;&#23558;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#36127;&#36131;&#22320;&#36827;&#34892;&#28508;&#22312;&#30340;&#30740;&#31350;&#65292;&#35782;&#21035;&#35774;&#35745;&#12289;&#22521;&#35757;&#21644;&#37096;&#32626;&#22522;&#20110;GeoAI&#30340;&#31995;&#32479;&#20013;&#30340;&#38519;&#38449;&#65292;&#24182;&#22312;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20849;&#21516;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21033;&#30410;&#21644;&#28508;&#22312;&#21361;&#38505;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#21516;&#26102;&#19982;&#20182;&#20154;&#20998;&#20139;&#25105;&#20204;&#29420;&#29305;&#30340;&#65288;&#22320;&#29702;&#65289;&#31354;&#38388;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter presents some of the fundamental assumptions and principles that could form the philosophical foundation of GeoAI and spatial data science. Instead of reviewing the well-established characteristics of spatial data (analysis), including interaction, neighborhoods, and autocorrelation, the chapter highlights themes such as sustainability, bias in training data, diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI systems from a unifying ethical perspective. Reflecting on our profession's ethical implications will assist us in conducting potentially disruptive research more responsibly, identifying pitfalls in designing, training, and deploying GeoAI-based systems, and developing a shared understanding of the benefits but also potential dangers of artificial intelligence and machine learning research across academic fields, all while sharing our unique (geo)spatial perspective with others.
&lt;/p&gt;</description></item><item><title>DiaTrend &#25968;&#25454;&#38598;&#30001; 54 &#20301;&#31958;&#23615;&#30149;&#24739;&#32773;&#31359;&#25140;&#24335;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#25968;&#25454;&#32452;&#25104;&#65292;&#21253;&#25324;&#24635;&#35745;27,561&#22825;&#30340;&#36830;&#32493;&#34880;&#31958;&#30417;&#27979;&#25968;&#25454;&#21644;8,220&#22825;&#30340;&#33008;&#23707;&#32032;&#27893;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#24320;&#21457;&#26032;&#22411;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06506</link><description>&lt;p&gt;
DiaTrend: &#22522;&#20110;&#20808;&#36827;&#31958;&#23615;&#30149;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#20419;&#36827;&#26032;&#22411;&#20998;&#26512;&#26041;&#27861;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
DiaTrend: A dataset from advanced diabetes technology to enable development of novel analytic solutions. (arXiv:2304.06506v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06506
&lt;/p&gt;
&lt;p&gt;
DiaTrend &#25968;&#25454;&#38598;&#30001; 54 &#20301;&#31958;&#23615;&#30149;&#24739;&#32773;&#31359;&#25140;&#24335;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#25968;&#25454;&#32452;&#25104;&#65292;&#21253;&#25324;&#24635;&#35745;27,561&#22825;&#30340;&#36830;&#32493;&#34880;&#31958;&#30417;&#27979;&#25968;&#25454;&#21644;8,220&#22825;&#30340;&#33008;&#23707;&#32032;&#27893;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#24320;&#21457;&#26032;&#22411;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#38656;&#35201;&#23458;&#35266;&#30340;&#25968;&#23383;&#21270;&#25968;&#25454;&#26469;&#25903;&#25345;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#30340;&#30740;&#31350;&#65292;&#20294;&#36825;&#31867;&#25968;&#25454;&#21364;&#24456;&#38590;&#33719;&#24471;&#12290;&#34429;&#28982;&#28040;&#36153;&#32423;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#26234;&#33021;&#25163;&#26426;&#31561;&#35774;&#22791;&#30340;&#25968;&#25454;&#27604;&#36739;&#23481;&#26131;&#33719;&#21462;&#65292;&#20294;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#19982;&#30456;&#20851;&#31958;&#23615;&#30149;&#35786;&#26029;&#26465;&#20214;&#30340;&#20020;&#24202;&#35774;&#22791;&#25552;&#20379;&#31867;&#20284;&#30340;&#25968;&#25454;&#12290;&#21487;&#31359;&#25140;&#21307;&#30103;&#35774;&#22791;&#22312;&#31958;&#23615;&#30149;&#39046;&#22495;&#30340;&#26222;&#21450;&#20026;&#35813;&#39046;&#22495;&#21450;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#21019;&#36896;&#20102;&#22865;&#26426;&#12290;&#20294;&#26159;&#65292;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#25104;&#20026;&#20102;&#21457;&#23637;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#26041;&#20415;&#26356;&#24191;&#27867;&#30340;&#31958;&#23615;&#30149;&#30456;&#20851;&#38382;&#39064;&#30740;&#31350;&#21644;&#21152;&#36895;&#20581;&#22766;&#30340;&#35745;&#31639;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; DiaTrend &#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;54&#20301;&#31958;&#23615;&#30149;&#24739;&#32773;&#38271;&#26399;&#20351;&#29992;&#30340;&#31359;&#25140;&#24335;&#21307;&#30103;&#35774;&#22791;&#20135;&#29983;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#24635;&#35745;27,561&#22825;&#30340;&#36830;&#32493;&#34880;&#31958;&#30417;&#27979;&#25968;&#25454;&#21644;8,220&#22825;&#30340;&#33008;&#23707;&#32032;&#27893;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#24320;&#21457;&#26032;&#22411;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective digital data is scarce yet needed in many domains to enable research that can transform the standard of healthcare. While data from consumer-grade wearables and smartphones is more accessible, there is critical need for similar data from clinical-grade devices used by patients with a diagnosed condition. The prevalence of wearable medical devices in the diabetes domain sets the stage for unique research and development within this field and beyond. However, the scarcity of open-source datasets presents a major barrier to progress. To facilitate broader research on diabetes-relevant problems and accelerate development of robust computational solutions, we provide the DiaTrend dataset. The DiaTrend dataset is composed of intensive longitudinal data from wearable medical devices, including a total of 27,561 days of continuous glucose monitor data and 8,220 days of insulin pump data from 54 patients with diabetes. This dataset is useful for developing novel analytic solutions tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#24503;&#22269;&#30340;AI&#20135;&#21697;&#21644;&#39033;&#30446;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#35813;&#27861;&#26696;&#30340;&#20855;&#20307;&#35201;&#27714;&#21644;&#38480;&#21046;&#65292;&#20026;&#35780;&#20272;&#35813;&#27861;&#26696;&#22312;AI&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#31995;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06503</link><description>&lt;p&gt;
AI&#27861;&#26696;&#20272;&#35745;&#24433;&#21709;&#30340;&#23450;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantitative study about the estimated impact of the AI Act. (arXiv:2304.06503v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;AI&#27861;&#26696;&#23545;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#24503;&#22269;&#30340;AI&#20135;&#21697;&#21644;&#39033;&#30446;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#35813;&#27861;&#26696;&#30340;&#20855;&#20307;&#35201;&#27714;&#21644;&#38480;&#21046;&#65292;&#20026;&#35780;&#20272;&#35813;&#27861;&#26696;&#22312;AI&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#31995;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25552;&#20986;&#35206;&#30422;&#25972;&#20010;AI&#31995;&#32479;&#22797;&#26434;&#24615;&#30340;AI&#27861;&#26696;&#65292;&#27431;&#30431;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25972;&#20010;AI&#31995;&#32479;&#30340;&#30417;&#31649;&#25991;&#20214;&#12290;&#19968;&#20123;&#20154;&#25285;&#24515;&#35813;&#27861;&#35268;&#30041;&#32473;&#35299;&#37322;&#30340;&#31354;&#38388;&#36807;&#22823;&#65292;&#22240;&#27492;&#23545;&#31038;&#20250;&#30340;&#21033;&#30410;&#24110;&#21161;&#19981;&#22823;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#35813;&#27861;&#35268;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#27492;&#38459;&#30861;&#20102;&#36827;&#27493;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#22952;&#30861;&#20102;&#27431;&#30431;&#20869;&#20844;&#21496;&#30340;&#32463;&#27982;&#25104;&#21151;&#12290;&#22312;&#27809;&#26377;&#31995;&#32479;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#35780;&#20272;&#23427;&#23558;&#22914;&#20309;&#24433;&#21709;AI&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;2021&#24180;4&#26376;&#21457;&#24067;&#30340;AI&#27861;&#26696;&#21021;&#22987;&#33609;&#26696;&#19978;&#24212;&#29992;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#19987;&#23478;&#19968;&#36215;&#65292;&#36890;&#36807;&#23545;&#24503;&#22269;&#21015;&#20986;&#30340;AI&#20135;&#21697;&#21644;&#39033;&#30446;&#36827;&#34892;&#20998;&#31867;&#65292;&#24402;&#31867;&#20102;&#36825;&#20123;&#20135;&#21697;&#21644;&#39033;&#30446;&#30340;AI&#27861;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the Proposal for a Regulation laying down harmonised rules on Artificial Intelligence (AI Act) the European Union provides the first regulatory document that applies to the entire complex of AI systems. While some fear that the regulation leaves too much room for interpretation and thus bring little benefit to society, others expect that the regulation is too restrictive and, thus, blocks progress and innovation, as well as hinders the economic success of companies within the EU. Without a systematic approach, it is difficult to assess how it will actually impact the AI landscape. In this paper, we suggest a systematic approach that we applied on the initial draft of the AI Act that has been released in April 2021. We went through several iterations of compiling the list of AI products and projects in and from Germany, which the Lernende Systeme platform lists, and then classified them according to the AI Act together with experts from the fields of computer science and law. Our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.06489</link><description>&lt;p&gt;
&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Inertial Measurement Unit-based Human Activity Recognition: A Survey. (arXiv:2304.06489v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#31359;&#25140;&#24335;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#24320;&#21457;&#21508;&#31181;&#26234;&#33021;&#31038;&#21306;&#24212;&#29992;&#65292;&#22914;&#30561;&#30496;&#27169;&#24335;&#30417;&#27979;&#12289;&#33647;&#29289;&#25552;&#37266;&#12289;&#35748;&#30693;&#20581;&#24247;&#35780;&#20272;&#12289;&#36816;&#21160;&#20998;&#26512;&#31561;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25918;&#32622;&#22312;&#19981;&#21516;&#30340;&#36523;&#20307;&#20301;&#32622;&#12289;&#35774;&#22791;&#22266;&#26377;&#20559;&#24046;&#21644;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#20010;&#20154;&#21644;&#29615;&#22659;&#30340;&#24046;&#24322;&#31561;&#36896;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#24322;&#36136;&#24615;&#65292;&#36825;&#20123;WHAR&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#24433;&#21709;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#30340;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#23545;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based wearable human activity recognition (WHAR) models enable the development of various smart and connected community applications such as sleep pattern monitoring, medication reminders, cognitive health assessment, sports analytics, etc. However, the widespread adoption of these WHAR models is impeded by their degraded performance in the presence of data distribution heterogeneities caused by the sensor placement at different body positions, inherent biases and heterogeneities across devices, and personal and environmental diversities. Various traditional machine learning algorithms and transfer learning techniques have been proposed in the literature to address the underpinning challenges of handling such data heterogeneities. Domain adaptation is one such transfer learning techniques that has gained significant popularity in recent literature. In this paper, we survey the recent progress of domain adaptation techniques in the Inertial Measurement Unit (IMU)-based 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.06488</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;AGI&#30340;&#19968;&#22823;&#27493;&#65306;AIGC&#26102;&#20195;&#20013;ChatGPT&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06488
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#29983;&#25104;&#24335;AI&#30340;&#19968;&#23567;&#27493;&#65292;&#20063;&#26159;AGI&#30340;&#19968;&#22823;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#23545;&#20854;&#22914;&#20309;&#28436;&#21464;&#20026;AIGC&#23637;&#26395;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#36890;&#29992;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#21457;&#23637;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#26368;&#36817;&#21457;&#24067;&#20102;GPT-4&#65288;&#21448;&#31216;&#20026;ChatGPT plus&#65289;&#65292;&#35813;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#29983;&#25104;&#24335;AI&#65288;GAI&#65289;&#36808;&#20986;&#30340;&#19968;&#23567;&#27493;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26469;&#35828;&#21017;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#39134;&#36291;&#12290;&#33258;2022&#24180;11&#26376;&#27491;&#24335;&#21457;&#24067;&#20197;&#26469;&#65292;ChatGPT&#20415;&#36805;&#36895;&#21560;&#24341;&#20102;&#20247;&#22810;&#29992;&#25143;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23186;&#20307;&#20851;&#27880;&#65292;&#30456;&#20851;&#30340;&#23398;&#26415;&#25991;&#31456;&#20063;&#36229;&#36807;&#20102;500&#31687;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#36827;&#34892;&#19968;&#27425;&#32508;&#36848;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23601;&#26159;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#19977;&#20010;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;ChatGPT&#30340;&#22242;&#38431;&#65292;&#24182;&#23637;&#26395;&#20102;ChatGPT&#22914;&#20309;&#28436;&#21464;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#65292;&#36825;&#23558;&#26159;AGI&#21457;&#23637;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.
&lt;/p&gt;</description></item><item><title>CoRe-Sleep&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#36890;&#36807;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#23481;&#24525;&#22122;&#22768;&#25110;&#20002;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#23637;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06485</link><description>&lt;p&gt;
CoRe-Sleep: &#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#23436;&#21892;&#27169;&#24577;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities. (arXiv:2304.06485v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06485
&lt;/p&gt;
&lt;p&gt;
CoRe-Sleep&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#23427;&#36890;&#36807;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#23481;&#24525;&#22122;&#22768;&#25110;&#20002;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#23637;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#24322;&#24120;&#21487;&#33021;&#20250;&#23545;&#20581;&#24247;&#20135;&#29983;&#20005;&#37325;&#30340;&#21518;&#26524;&#12290;&#33258;&#21160;&#21270;&#30561;&#30496;&#20998;&#26399;&#21487;&#20197;&#31616;&#21270;&#35786;&#26029;&#36807;&#31243;&#12290;&#20197;&#24448;&#30340;&#33258;&#21160;&#21270;&#30561;&#30496;&#20998;&#26399;&#24037;&#20316;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110; EEG &#20449;&#21495;&#12290;&#20294;&#26159;&#65292;&#36890;&#24120;&#22312; EEG &#20043;&#22806;&#36824;&#26377;&#22810;&#20010;&#20449;&#24687;&#28304;&#21487;&#29992;&#12290;&#24403; EEG &#35760;&#24405;&#23384;&#22312;&#22122;&#22768;&#29978;&#33267;&#23436;&#20840;&#32570;&#22833;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#26377;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; CoRe-Sleep&#65292;&#19968;&#31181;&#21327;&#35843;&#34920;&#31034;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#23427;&#29305;&#21035;&#20851;&#27880;&#20110;&#25552;&#39640;&#23545;&#19981;&#23436;&#21892;&#25968;&#25454;&#30340;&#20449;&#21495;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21487;&#20197;&#26159;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;CoRe-Sleep &#23481;&#24525;&#22122;&#22768;&#25110;&#32570;&#22833;&#30340;&#27169;&#24577;&#29255;&#27573;&#65292;&#20801;&#35768;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#20351;&#29992;&#21333;&#20010;&#27169;&#24335;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#27979;&#35797;&#21644;&#21333;&#27169;&#24577;&#25968;&#25454;&#27979;&#35797;&#26102;&#37117;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep abnormalities can have severe health consequences. Automated sleep staging, i.e. labelling the sequence of sleep stages from the patient's physiological recordings, could simplify the diagnostic process. Previous work on automated sleep staging has achieved great results, mainly relying on the EEG signal. However, often multiple sources of information are available beyond EEG. This can be particularly beneficial when the EEG recordings are noisy or even missing completely. In this paper, we propose CoRe-Sleep, a Coordinated Representation multimodal fusion network that is particularly focused on improving the robustness of signal analysis on imperfect data. We demonstrate how appropriately handling multimodal information can be the key to achieving such robustness. CoRe-Sleep tolerates noisy or missing modalities segments, allowing training on incomplete data. Additionally, it shows state-of-the-art performance when testing on both multimodal and unimodal data using a single mode
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06470</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#22833;&#36133;&#21450;&#20854;&#22312;&#26816;&#27979;Deepfakes&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20986;&#36924;&#30495;&#30340;&#24433;&#20687;&#30340;&#33021;&#21147;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#32570;&#38519;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31867;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;&#20170;&#22825;&#31038;&#20250;&#20013;Deepfakes&#30340;&#26222;&#36941;&#23384;&#22312;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#23427;&#20204;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#23450;&#20041;&#20102;&#19968;&#32452;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#20248;&#21183;&#21644;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06469</link><description>&lt;p&gt;
&#20998;&#26512;&#38544;&#31169;-&#25928;&#29992;&#31227;&#21160;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysing Fairness of Privacy-Utility Mobility Models. (arXiv:2304.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#23450;&#20041;&#20102;&#19968;&#32452;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#65292;&#30740;&#31350;&#20102;&#20004;&#31181;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#20248;&#21183;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#20139;&#26102;&#31354;&#25968;&#25454;&#38598;&#20013;&#65292;&#20445;&#25252;&#20010;&#20154;&#30340;&#38544;&#31169;&#23545;&#20110;&#38450;&#27490;&#22522;&#20110;&#21807;&#19968;&#36712;&#36857;&#30340;&#37325;&#26032;&#35782;&#21035;&#25915;&#20987;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#25216;&#26415;&#24448;&#24448;&#25552;&#20986;&#29702;&#24819;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#20294;&#22522;&#26412;&#24573;&#30053;&#20102;&#31227;&#21160;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;&#65292;&#20197;&#21450;&#36825;&#20123;&#25216;&#26415;&#23545;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#26159;&#21542;&#21516;&#31561;&#36866;&#29992;&#12290;&#22312;&#26102;&#31354;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#19982;&#38544;&#31169;&#24847;&#35782;&#27169;&#22411;&#20043;&#38388;&#30340;&#24230;&#37327;&#20173;&#28982;&#19981;&#28165;&#26224;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#23384;&#22312;&#20219;&#20309;&#23450;&#20041;&#30340;&#20844;&#24179;&#24230;&#37327;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#32452;&#19987;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#32780;&#35774;&#35745;&#30340;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#65292;&#22522;&#20110;&#36712;&#36857;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#29109;&#12290;&#22312;&#36825;&#20123;&#23450;&#20041;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20013;&#20381;&#38752;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#34920;&#31034;&#23398;&#20064;&#38477;&#20302;&#29992;&#25143;&#30340;&#37325;&#26032;&#35782;&#21035;&#29575;&#65292;&#26816;&#26597;&#20102;&#20854;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#20445;&#35777;&#22312;&#32452;&#19978;&#20844;&#24179;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#22312;&#20010;&#21035;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving the individuals' privacy in sharing spatial-temporal datasets is critical to prevent re-identification attacks based on unique trajectories. Existing privacy techniques tend to propose ideal privacy-utility tradeoffs, however, largely ignore the fairness implications of mobility models and whether such techniques perform equally for different groups of users. The quantification between fairness and privacy-aware models is still unclear and there barely exists any defined sets of metrics for measuring fairness in the spatial-temporal context. In this work, we define a set of fairness metrics designed explicitly for human mobility, based on structural similarity and entropy of the trajectories. Under these definitions, we examine the fairness of two state-of-the-art privacy-preserving models that rely on GAN and representation learning to reduce the re-identification rate of users for data sharing. Our results show that while both models guarantee group fairness in terms of de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;&#36866;&#37197;&#22120;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26377;&#38480;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2304.06459</link><description>&lt;p&gt;
Masakhane-Afrisenti&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#36827;&#34892;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Afro-centric&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;&#36866;&#37197;&#22120;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26377;&#38480;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AfriSenti-SemEval&#20849;&#20139;&#20219;&#21153;12&#26088;&#22312;&#20026;12&#31181;&#38750;&#27954;&#35821;&#35328;&#25191;&#34892;&#21333;&#35821;&#24773;&#24863;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#65288;&#23376;&#20219;&#21153;B&#65289;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#65288;&#20219;&#21153;C&#65289;&#12290;&#23545;&#20110;&#23376;&#20219;&#21153;A&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12289;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21644;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23545;&#20110;&#20219;&#21153;B&#65292;&#25105;&#20204;&#24494;&#35843;&#20102;&#25903;&#25345;&#20219;&#21153;&#20013;&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;&#20110;&#20219;&#21153;C&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#21333;&#35821;&#25991;&#26412;&#23454;&#29616;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38750;&#27954;&#20013;&#24515;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36866;&#37197;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#21487;&#20197;&#33719;&#24471;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform monolingual sentiment classification (sub-task A) for 12 African languages, multilingual sentiment classification (sub-task B), and zero-shot sentiment classification (task C). For sub-task A, we conducted experiments using classical machine learning classifiers, Afro-centric language models, and language-specific models. For task B, we fine-tuned multilingual pre-trained language models that support many of the languages in the task. For task C, we used we make use of a parameter-efficient Adapter approach that leverages monolingual texts in the target language for effective zero-shot transfer. Our findings suggest that using pre-trained Afro-centric language models improves performance for low-resource African languages. We also ran experiments using adapters for zero-shot tasks, and the results suggest that we can obtain promising results by using adapters with a limited amount of resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06430</link><description>&lt;p&gt;
&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (arXiv:2304.06430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#40657;&#30418;&#35774;&#32622;&#20013;&#23545;&#20110;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#24050;&#32463;&#20174;&#38646;&#38454;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#28982;&#32780;&#30001;&#20110;&#21435;&#22122;&#22120;&#30340;&#35774;&#35745;&#19981;&#22815;&#26377;&#25928;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#39640;&#27169;&#22411;&#26041;&#24046;&#21644;&#20302;&#24615;&#33021;&#65292;&#19988;&#22312;&#20351;&#29992;&#38646;&#38454;&#25216;&#26415;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#35777;&#30340;&#38646;&#38454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#27169;&#22411;&#26597;&#35810;&#21363;&#21487;&#20174;&#21463;&#25915;&#20987;&#22270;&#20687;&#20013;&#21435;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;UNet&#21435;&#22122;&#22120;&#65288;RDUNet&#65289;&#65292;&#30830;&#20445;&#20102;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#40657;&#30418;&#21435;&#22122;&#24179;&#28369;&#65288;DS&#65289;&#38450;&#24481;&#26426;&#21046;ZO-RUDS&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;RDUNet&#39044;&#32622;&#20110;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#65292;&#30830;&#20445;&#40657;&#30418;&#38450;&#24481;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;ZO-AE-RUDS&#65292;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#20351;&#29992;RDUNet&#21644;&#33258;&#32534;&#30721;&#22120;(AE)&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified defense methods against adversarial perturbations have been recently investigated in the black-box setting with a zeroth-order (ZO) perspective. However, these methods suffer from high model variance with low performance on high-dimensional datasets due to the ineffective design of the denoiser and are limited in their utilization of ZO techniques. To this end, we propose a certified ZO preprocessing technique for removing adversarial perturbations from the attacked image in the black-box setting using only model queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets. We propose a novel black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our RDUNet to the black-box model, ensuring black-box defense. We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model. We perform extensive experiments on four classification dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;</title><link>http://arxiv.org/abs/2304.06427</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;ECG&#34920;&#24449;&#23398;&#20064;&#22312;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65306;&#20998;&#24067;&#20998;&#26512;&#21450;&#23454;&#39564;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;&#65292;&#39318;&#27425;&#23545;&#19977;&#20010;&#24120;&#29992;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#23454;&#39564;&#21457;&#29616;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36824;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24515;&#30005;&#22270;(ECG)&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;(Self-Supervised Learning, SSL)&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19977;&#20010;&#24120;&#29992;&#30340;ECG&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#24067;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#65292;&#20351;&#29992;&#19981;&#21516;&#22686;&#24378;&#21644;&#21442;&#25968;&#35780;&#20272;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#65288;&#22914;SimCRL&#12289;BYOL&#21644;SwAV&#65289;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SwAV&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#38024;&#23545;In-Distribution (ID)&#21644;Out-of-Distribution (OOD) ECG&#25968;&#25454;&#30340;&#20132;&#21449;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#27979;&#35797;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SSL&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SwAV&#65292;&#22312;ECG&#34920;&#24449;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#19981;&#21516;&#31181;&#31867;&#30340;ECG&#25968;&#25454;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#26377;&#26395;&#22312;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#20154;&#32676;&#20013;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#26469;&#35299;&#20915;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06412</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#35299;&#37322;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#65306;&#36816;&#31609;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective. (arXiv:2304.06412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#26469;&#35299;&#20915;&#39044;&#27979;&#27969;&#31243;&#30417;&#25511;&#20013;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#37322;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#20449;&#24687;&#31995;&#32479;&#21644;&#20154;&#24037;&#26234;&#33021;&#34701;&#21512;&#65292;&#20197;&#22686;&#24378;&#36816;&#31609;&#23398;&#39046;&#22495;&#20869;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#27604;&#22914;&#24573;&#30053;&#20851;&#38190;&#29983;&#20135;&#21442;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;&#20272;&#35745;&#12289;&#20165;&#29983;&#25104;&#28857;&#39044;&#27979;&#32780;&#19981;&#32771;&#34385;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#32570;&#20047;&#20851;&#20110;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#29983;&#25104;&#21306;&#38388;&#39044;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#21464;&#20307;&#30340;SHapley&#21487;&#21152;&#35299;&#37322;&#26469;&#35299;&#20915;&#30740;&#31350;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#29983;&#20135;&#35745;&#21010;&#26696;&#20363;&#20013;&#24471;&#21040;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#35268;&#23450;&#24615;&#20998;&#26512;&#22312;&#31934;&#32454;&#21270;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#24378;&#35843;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a comprehensive, multi-stage machine learning methodology that effectively integrates information systems and artificial intelligence to enhance decision-making processes within the domain of operations research. The proposed framework adeptly addresses common limitations of existing solutions, such as the neglect of data-driven estimation for vital production parameters, exclusive generation of point forecasts without considering model uncertainty, and lacking explanations regarding the sources of such uncertainty. Our approach employs Quantile Regression Forests for generating interval predictions, alongside both local and global variants of SHapley Additive Explanations for the examined predictive process monitoring problem. The practical applicability of the proposed methodology is substantiated through a real-world production planning case study, emphasizing the potential of prescriptive analytics in refining decision-making procedures. This paper accentuates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#35270;&#36741;&#21161;&#20219;&#21153;&#24110;&#21161;&#20027;&#35201;&#39044;&#27979;&#32593;&#32476;&#36866;&#24212;&#27979;&#35797;&#24207;&#21015;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#25110;&#26410;&#35265;&#36816;&#21160;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.06411</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20154;&#20307;&#23039;&#21183;&#39044;&#27979;&#30340;&#20803;&#36741;&#21161;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Auxiliary Learning for Adaptive Human Pose Prediction. (arXiv:2304.06411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#35270;&#36741;&#21161;&#20219;&#21153;&#24110;&#21161;&#20027;&#35201;&#39044;&#27979;&#32593;&#32476;&#36866;&#24212;&#27979;&#35797;&#24207;&#21015;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#25110;&#26410;&#35265;&#36816;&#21160;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20154;&#20307;&#21382;&#21490;&#36712;&#36857;&#20043;&#21518;&#30340;&#39640;&#20445;&#30495;&#26410;&#26469;&#23039;&#21183;&#23545;&#20110;&#26234;&#33021;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#33324;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#25152;&#26377;&#27979;&#35797;&#26679;&#26412;&#65292;&#25104;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;&#20182;&#20204;&#23578;&#19981;&#23436;&#32654;&#65292;&#22240;&#20026;&#26080;&#27861;&#33258;&#36866;&#24212;&#29305;&#23450;&#24207;&#21015;&#30340;&#29305;&#27530;&#23646;&#24615;&#65288;&#22914;&#21160;&#20316;&#39118;&#26684;&#65292;&#33410;&#22863;&#65289;&#12290;&#26356;&#26222;&#36941;&#22320;&#65292;&#19968;&#26086;&#36935;&#21040;&#26410;&#35265;&#36807;&#30340;&#36816;&#21160;&#31867;&#22411;&#65288;&#20998;&#24067;&#20043;&#22806;&#65289;&#65292;&#39044;&#27979;&#30340;&#23039;&#21183;&#24448;&#24448;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#24110;&#21161;&#20027;&#35201;&#39044;&#27979;&#32593;&#32476;&#36866;&#24212;&#27979;&#35797;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting high-fidelity future human poses, from a historically observed sequence, is decisive for intelligent robots to interact with humans. Deep end-to-end learning approaches, which typically train a generic pre-trained model on external datasets and then directly apply it to all test samples, emerge as the dominant solution to solve this issue. Despite encouraging progress, they remain non-optimal, as the unique properties (e.g., motion style, rhythm) of a specific sequence cannot be adapted. More generally, at test-time, once encountering unseen motion categories (out-of-distribution), the predicted poses tend to be unreliable. Motivated by this observation, we propose a novel test-time adaptation framework that leverages two self-supervised auxiliary tasks to help the primary forecasting network adapt to the test sequence. In the testing phase, our model can adjust the model parameters by several gradient updates to improve the generation quality. However, due to catastrophic f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26102;&#38388;&#36793;&#30028;&#24674;&#22797;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.06403</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#20803;&#32452;&#25439;&#22833;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#21160;&#20316;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#26469;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26080;&#30417;&#30563;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26102;&#38388;&#36793;&#30028;&#24674;&#22797;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#23398;&#20064;&#36866;&#29992;&#20110;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#25805;&#20316;&#30456;&#20284;&#24230;&#20998;&#24067;&#30340;&#19977;&#20803;&#32452;&#25439;&#22833;&#21644;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#26102;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#20197;&#22312;&#26032;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#21457;&#29616;&#21160;&#20316;&#30340;&#19977;&#20803;&#32452;&#36873;&#25321;&#31574;&#30053;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#23398;&#20064;&#21040;&#30340;&#21160;&#20316;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#36793;&#30028;&#65292;&#36136;&#37327;&#26356;&#39640;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#19978;&#24212;&#29992;&#36890;&#29992;&#32858;&#31867;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations.
&lt;/p&gt;</description></item><item><title>VISION DIFFMASK&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#35782;&#21035;&#26368;&#23567;&#36755;&#20837;&#23376;&#38598;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.06391</link><description>&lt;p&gt;
VISION DIFFMASK&#65306;&#20855;&#26377;&#21487;&#24494;&#20998;&#34917;&#19969;&#25513;&#30721;&#30340;&#35270;&#35273;Transformer&#30340;&#24544;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking. (arXiv:2304.06391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06391
&lt;/p&gt;
&lt;p&gt;
VISION DIFFMASK&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#35782;&#21035;&#26368;&#23567;&#36755;&#20837;&#23376;&#38598;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#20294;Vision Transformer&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#20851;&#38190;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VISION DIFFMASK&#30340;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#38544;&#34255;&#23618;&#30340;&#28608;&#27963;&#26469;&#39044;&#27979;&#23545;&#20854;&#26368;&#32456;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#36755;&#20837;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#38376;&#25511;&#26426;&#21046;&#26469;&#35782;&#21035;&#20445;&#30041;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#26368;&#23567;&#21407;&#22987;&#36755;&#20837;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24544;&#23454;&#24230;&#20219;&#21153;&#24182;&#22312;CIFAR-10&#21644;ImageNet-1K&#19978;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#24037;&#20316;&#30340;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#23454;&#29616;&#65306;https://github.com/AngelosNal/Vision-DiffMask
&lt;/p&gt;
&lt;p&gt;
The lack of interpretability of the Vision Transformer may hinder its use in critical real-world applications despite its effectiveness. To overcome this issue, we propose a post-hoc interpretability method called VISION DIFFMASK, which uses the activations of the model's hidden layers to predict the relevant parts of the input that contribute to its final predictions. Our approach uses a gating mechanism to identify the minimal subset of the original input that preserves the predicted distribution over classes. We demonstrate the faithfulness of our method, by introducing a faithfulness task, and comparing it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K, achieving compelling results. To aid reproducibility and further extension of our work, we open source our implementation: https://github.com/AngelosNal/Vision-DiffMask
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.06377</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#31526;&#21495;&#30340;&#20986;&#29616;&#19982;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29983;&#25104;&#31526;&#21495;&#65292;&#23454;&#29616;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#12290;&#36825;&#20123;&#31526;&#21495;&#21487;&#20197;&#25429;&#25417;&#21040;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#21576;&#29616;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21019;&#36896;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#65292;&#24182;&#29087;&#32451;&#22320;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#39640;&#30340;&#35748;&#30693;&#21151;&#33021;&#65292;&#22914;&#20132;&#27969;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#31561;&#65292;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#37325;&#35201;&#21644;&#29420;&#29305;&#20043;&#22788;&#12290; &#30446;&#21069;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#20154;&#31867;&#21019;&#36896;&#31526;&#21495;&#36827;&#34892;&#36825;&#20123;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA-net&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31526;&#21495;&#21019;&#36896;&#12289;&#35821;&#20041;&#29702;&#35299;&#21644;&#20132;&#27969;&#33021;&#21147;&#12290;SEA-net&#29983;&#25104;&#21160;&#24577;&#37197;&#32622;&#32593;&#32476;&#20197;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#31526;&#21495;&#12290;&#36825;&#20123;&#31526;&#21495;&#25429;&#25417;&#20102;&#32452;&#25104;&#24615;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#32431;&#31526;&#21495;&#25805;&#20316;&#25110;&#20132;&#27969;&#33719;&#24471;&#26032;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#30340;&#31526;&#21495;&#21576;&#29616;&#20986;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#34920;&#26126;&#22312;&#20154;&#31867;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#29983;&#25104;&#21644;&#29702;&#35299;&#31526;&#21495;&#30340;&#20849;&#21516;&#26694;&#26550;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#23558;&#25104;&#20026;&#23558;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#21161;&#25512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#23450;&#37327;&#27169;&#22411;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#65292;&#19982;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#20248;&#20808;&#24418;&#25104;&#65292;&#36825;&#21453;&#26144;&#20102;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2304.06375</link><description>&lt;p&gt;
&#36229;&#22270;&#35748;&#30693;&#32593;&#32476;&#20316;&#20026;&#30693;&#35782;&#30340;&#29305;&#24449;&#20016;&#23500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards hypergraph cognitive networks as feature-rich models of knowledge. (arXiv:2304.06375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#23450;&#37327;&#27169;&#22411;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#65292;&#19982;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#20248;&#20808;&#24418;&#25104;&#65292;&#36825;&#21453;&#26144;&#20102;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#32593;&#32476;&#26159;&#29702;&#35299;&#22914;&#20309;&#20174;&#35760;&#24518;&#20013;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#25104;&#23545;&#36830;&#25509;&#34920;&#31034;&#35760;&#24518;&#21484;&#22238;&#27169;&#24335;&#12290;&#25104;&#23545;&#36830;&#25509;&#24573;&#30053;&#20102;&#26356;&#39640;&#38454;&#30340;&#20851;&#32852;&#65292;&#21363;&#19968;&#27425;&#28041;&#21450;&#20004;&#20010;&#20197;&#19978;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#26356;&#39640;&#38454;&#30340;&#20132;&#20114;&#21487;&#33021;&#19982;&#22823;&#33041;&#28784;&#36136;&#32467;&#26500;&#29305;&#24449;&#65292;&#22914;&#20852;&#22859;&#12289;&#24841;&#24742;&#12289;&#29087;&#24713;&#24230;&#12289;&#24615;&#21035;&#31561;&#26377;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#20316;&#20026;&#20154;&#31867;&#35760;&#24518;&#30340;&#23450;&#37327;&#27169;&#22411;&#65306;&#65288;i&#65289;&#19968;&#36215;&#22238;&#24518;&#30340;&#27010;&#24565;&#21487;&#20197;&#21516;&#26102;&#21442;&#19982;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#27010;&#24565;&#30340;&#36229;&#38142;&#25509;&#65288;&#35748;&#30693;&#36229;&#22270;&#26041;&#38754;&#65289;&#65307;&#65288;ii&#65289;&#27599;&#20010;&#27010;&#24565;&#37117;&#20855;&#26377;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#21521;&#37327;&#65288;&#29305;&#24449;&#20016;&#23500;&#26041;&#38754;&#65289;&#12290;&#25105;&#20204;&#20174;&#35789;&#27719;&#32852;&#24819;&#25968;&#25454;&#20013;&#26500;&#24314;&#36229;&#22270;&#65292;&#24182;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#35780;&#20272;&#26041;&#27861;&#26469;&#39044;&#27979;&#27010;&#24565;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#36830;&#25509;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#29305;&#24449;&#20016;&#23500;&#35748;&#30693;&#36229;&#22270;&#22312;&#39044;&#27979;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#28041;&#21450;&#26356;&#39640;&#38454;&#20851;&#32852;&#30340;&#36229;&#38142;&#25509;&#20248;&#20808;&#24418;&#25104;&#22312;&#20855;&#26377;&#30456;&#20284;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24449;&#30340;&#27010;&#24565;&#20043;&#38388;&#12290;&#36825;&#34920;&#26126;&#65292;&#20154;&#31867;&#35760;&#24518;&#30340;&#32467;&#26500;&#28041;&#21450;&#21453;&#26144;&#20849;&#20139;&#35748;&#30693;&#32500;&#24230;&#30340;&#26356;&#39640;&#38454;&#20851;&#32852;&#65292;&#36825;&#20123;&#32500;&#24230;&#21487;&#20197;&#29992;&#20110;&#23558;&#27010;&#24565;&#32452;&#32455;&#25104;&#35821;&#20041;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic networks provide a useful tool to understand how related concepts are retrieved from memory. However, most current network approaches use pairwise links to represent memory recall patterns. Pairwise connections neglect higher-order associations, i.e. relationships between more than two concepts at a time. These higher-order interactions might covariate with (and thus contain information about) how similar concepts are along psycholinguistic dimensions like arousal, valence, familiarity, gender and others. We overcome these limits by introducing feature-rich cognitive hypergraphs as quantitative models of human memory where: (i) concepts recalled together can all engage in hyperlinks involving also more than two concepts at once (cognitive hypergraph aspect), and (ii) each concept is endowed with a vector of psycholinguistic features (feature-rich aspect). We build hypergraphs from word association data and use evaluation methods from machine learning features to predict concep
&lt;/p&gt;</description></item><item><title>AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06364</link><description>&lt;p&gt;
AGIEval&#65306;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06364
&lt;/p&gt;
&lt;p&gt;
AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#32423;&#21035;&#20219;&#21153;&#30340;&#36890;&#29992;&#33021;&#21147;&#26159;&#23427;&#20204;&#22312;&#21457;&#23637;&#21644;&#24212;&#29992;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#20154;&#36896;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#20195;&#34920;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AGIEval&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#65292;&#27861;&#24459;&#23398;&#26657;&#20837;&#23398;&#32771;&#35797;&#65292;&#25968;&#23398;&#31454;&#36187;&#21644;&#24459;&#24072;&#36164;&#26684;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#65292;ChatGPT &#21644;Text-Davinci-003&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;SAT&#25968;&#23398;&#27979;&#35797;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95%&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#22823;&#23398;&#33521;&#35821;&#32771;&#35797;&#30340;&#33521;&#35821;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20063;&#36798;&#21040;&#20102;92.5%&#12290;&#36825;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;AGI&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;</title><link>http://arxiv.org/abs/2304.06348</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#30340;&#21453;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#65288;&#36890;&#36807;&#26576;&#20123;&#31867;&#22411;&#30340;&#23485;&#24230;&#37327;&#26469;&#34913;&#37327;&#65292;&#21253;&#25324;&#26641;&#23485;&#21644;&#22242;&#23485;&#31561;&#65289;&#65292;&#20026;&#24191;&#27867;&#30340;&#36923;&#36753;&#34164;&#21547;&#38382;&#39064;&#65288;&#31616;&#31216;&#26597;&#35810;&#65289;&#30340;&#21487;&#20915;&#23450;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23637;&#29616;&#20986;&#23485;&#24230;&#26377;&#38480;&#26377;&#38480;&#36890;&#29992;&#27169;&#22411;&#38598;&#30340;&#36923;&#36753;&#65292;&#20445;&#35777;&#20102;&#21508;&#31181;&#21516;&#24577;&#23553;&#38381;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#24378;&#22823;&#30340;&#23485;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Blumensath&#30340;&#20998;&#21106;&#23485;&#24230;&#65292;&#35813;&#37327;&#21253;&#21547;&#20102;&#21508;&#31181;&#36890;&#24120;&#32771;&#34385;&#30340;&#23485;&#24230;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#30340;&#35745;&#31639;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#38024;&#23545;&#26222;&#36941;&#23637;&#29616;&#23384;&#22312;&#24615;&#35268;&#21017;&#20026;&#19968;&#20010;&#23637;&#31034;&#26696;&#20363;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#26377;&#38480;&#20998;&#21106;&#23485;&#24230;&#35268;&#21017;&#38598;&#21253;&#21547;&#20854;&#20182;&#24050;&#30693;&#30340;&#25277;&#35937;&#21487;&#20915;&#23450;&#31867;&#65292;&#20294;&#20511;&#21161;&#29616;&#26377;&#30340;&#20998;&#23618;&#21644;&#21463;&#25511;&#35268;&#21017;&#38598;&#27010;&#24565;&#65292;&#20063;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#20363;&#22914;&#27491;&#21017;&#65292;&#36830;&#25509;&#21644;&#24067;&#23572;&#36830;&#25509;&#26597;&#35810;&#12290;&#25105;&#20204;&#20197;&#23384;&#22312;&#35268;&#21017;&#30340;&#24418;&#24335;&#20026;&#37325;&#28857;&#65292;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#39640;&#32423;&#30693;&#35782;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21452;&#25903;&#21464;&#24418;Transformer&#65288;DDT&#65289;&#21435;&#22122;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#65292;&#24182;&#19988;&#24212;&#29992;&#21464;&#24418;&#27880;&#24847;&#25805;&#20316;&#21487;&#20197;&#19987;&#27880;&#20110;&#26356;&#37325;&#35201;&#30340;&#21306;&#22495;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#21644;&#21512;&#25104;&#21435;&#22122;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#20197;&#21450;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.06346</link><description>&lt;p&gt;
DDT&#65306;&#21452;&#25903;&#21464;&#24418;Transformer&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
DDT: Dual-branch Deformable Transformer for Image Denoising. (arXiv:2304.06346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21452;&#25903;&#21464;&#24418;Transformer&#65288;DDT&#65289;&#21435;&#22122;&#32593;&#32476;&#65292;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#65292;&#24182;&#19988;&#24212;&#29992;&#21464;&#24418;&#27880;&#24847;&#25805;&#20316;&#21487;&#20197;&#19987;&#27880;&#20110;&#26356;&#37325;&#35201;&#30340;&#21306;&#22495;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#21644;&#21512;&#25104;&#21435;&#22122;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#20197;&#21450;&#26174;&#33879;&#38477;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#23545;&#20110;&#22270;&#20687;&#21435;&#22122;&#20219;&#21153;&#38750;&#24120;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#24314;&#27169;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#26469;&#20811;&#26381;&#24402;&#32435;&#21367;&#31215;&#20559;&#24046;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;Transformer&#32467;&#26500;&#26469;&#21435;&#38500;&#22122;&#22768;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#22797;&#26434;&#24230;&#19982;&#31354;&#38388;&#20998;&#36776;&#29575;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21452;&#25903;&#21464;&#24418;Transformer&#65288;DDT&#65289;&#21435;&#22122;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#25903;&#26550;&#19978;&#20351;&#29992;&#22266;&#23450;&#22359;&#23610;&#23544;&#21644;&#22266;&#23450;&#22359;&#25968;&#26469;&#21010;&#20998;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#25903;&#26550;&#19978;&#24212;&#29992;&#21464;&#24418;&#27880;&#24847;&#25805;&#20316;&#65292;&#36825;&#26377;&#21161;&#20110;&#32593;&#32476;&#19987;&#27880;&#20110;&#26356;&#37325;&#35201;&#30340;&#21306;&#22495;&#65292;&#24182;&#36827;&#19968;&#27493;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#21644;&#21512;&#25104;&#21435;&#22122;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;DDT&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#25104;&#26412;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is beneficial for image denoising tasks since it can model long-range dependencies to overcome the limitations presented by inductive convolutional biases. However, directly applying the transformer structure to remove noise is challenging because its complexity grows quadratically with the spatial resolution. In this paper, we propose an efficient Dual-branch Deformable Transformer (DDT) denoising network which captures both local and global interactions in parallel. We divide features with a fixed patch size and a fixed number of patches in local and global branches, respectively. In addition, we apply deformable attention operation in both branches, which helps the network focus on more important regions and further reduces computational complexity. We conduct extensive experiments on real-world and synthetic denoising tasks, and the proposed DDT achieves state-of-the-art performance with significantly fewer computational costs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06345</link><description>&lt;p&gt;
ASR: &#20687;&#27880;&#24847;&#21147;&#19968;&#26679;&#30340;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
ASR: Attention-alike Structural Re-parameterization. (arXiv:2304.06345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#65288;SRP&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#35813;&#25216;&#26415;&#20351;&#24471;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#36825;&#20123;&#36716;&#25442;&#20943;&#23569;&#24615;&#33021;&#25552;&#21319;&#30340;&#26032;&#22686;&#20195;&#20215;&#65292;&#20363;&#22914;&#21442;&#25968;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#22240;&#27492;SRP&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#24050;&#25104;&#21151;&#32771;&#34385;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#24402;&#19968;&#21270;&#12289;&#27744;&#21270;&#26041;&#27861;&#12289;&#22810;&#20998;&#25903;&#21367;&#31215;&#31561;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#27169;&#22359;&#30001;&#20110;&#22312;&#25512;&#29702;&#26399;&#38388;&#36890;&#24120;&#20197;&#20056;&#27861;&#26041;&#24335;&#20316;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#24182;&#19988;&#27169;&#22359;&#30340;&#36755;&#20986;&#22312;&#25512;&#29702;&#26102;&#20381;&#36182;&#20110;&#36755;&#20837;&#65292;&#25152;&#20197;&#26080;&#27861;&#30452;&#25509;&#23454;&#29616;SRP&#65292;&#32780;&#36825;&#38480;&#21046;&#20102;SRP&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#35282;&#24230;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, multi-branch convolution. However, the widely used self-attention modules cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. In this paper, we conduct extensive experiments from a statistical perspective and discover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06344</link><description>&lt;p&gt;
&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#31616;&#21270;&#26694;&#26550;&#25552;&#39640;&#24211;&#23384;&#31649;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Streamlined Framework for Agile Forecasting Model Development towards Efficient Inventory Management. (arXiv:2304.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#25463;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#24212;&#29992;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#65292;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#24320;&#21457;&#36807;&#31243;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#38388;&#30340;&#36830;&#25509;&#26469;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#26032;&#25968;&#25454;&#38598;&#33021;&#22815;&#24555;&#36895;&#21644;&#31283;&#20581;&#22320;&#38598;&#25104;&#65292;&#23454;&#39564;&#19981;&#21516;&#31639;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#20837;&#25163;&#65292;&#24182;&#24212;&#29992;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#28165;&#29702;&#21644;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#20026;&#20102;&#30830;&#23450;&#31283;&#20581;&#30340;&#35757;&#32451;&#37197;&#32622;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#37325;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#26426;&#21046;&#12290;&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#25214;&#21040;&#26368;&#36866;&#21512;&#19981;&#21516;&#24212;&#29992;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25105;&#20204;&#21442;&#21152;&#20102;&#32654;&#22269;&#22269;&#38469;&#24320;&#21457;&#32626;&#65288;USAID&#65289;&#32452;&#32455;&#30340;&#26234;&#33021;&#39044;&#27979;&#31454;&#36187;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#65292;&#24212;&#29992;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#24211;&#23384;&#31649;&#29702;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework for developing forecasting models by streamlining the connections between core components of the developmental process. The proposed framework enables swift and robust integration of new datasets, experimentation on different algorithms, and selection of the best models. We start with the datasets of different issues and apply pre-processing steps to clean and engineer meaningful representations of time-series data. To identify robust training configurations, we introduce a novel mechanism of multiple cross-validation strategies. We apply different evaluation metrics to find the best-suited models for varying applications. One of the referent applications is our participation in the intelligent forecasting competition held by the United States Agency of International Development (USAID). Finally, we leverage the flexibility of the framework by applying different evaluation metrics to assess the performance of the models in inventory management settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.06336</link><description>&lt;p&gt;
&#22810;&#23646;&#24615;&#22810;&#38454;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#20174;&#22810;&#20851;&#31995;&#32593;&#32476;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#20803;&#36335;&#24452;&#65292;&#23427;&#26174;&#30528;&#22320;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#23646;&#24615;&#30340;&#22810;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMOGCN&#65289;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#33410;&#28857;&#36830;&#25509;&#20013;&#26500;&#24314;&#19981;&#21516;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#20043;&#21518;&#65292;&#20174;&#21508;&#31181;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#21160;&#34701;&#21512;&#20013;&#38468;&#21152;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#12290;&#36825;&#20010;&#36807;&#31243;&#30001;&#20174;&#33410;&#28857;&#21516;&#36136;&#24615;&#36890;&#36807;&#23646;&#24615;&#35780;&#20215;&#25552;&#21462;&#30340;&#33410;&#28857;&#35821;&#20041;&#20449;&#24687;&#30417;&#30563;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#19968;&#23618;&#31616;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
&lt;/p&gt;</description></item><item><title>NeRFVS&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#20943;&#23569;&#21487;&#35265;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2304.06287</link><description>&lt;p&gt;
NeRFVS:&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#23454;&#29616;&#33258;&#30001;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds. (arXiv:2304.06287v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06287
&lt;/p&gt;
&lt;p&gt;
NeRFVS&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#22522;&#20110;&#20960;&#20309;&#25903;&#25745;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#20943;&#23569;&#21487;&#35265;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeRFVS&#30340;&#26032;&#22411;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#23460;&#20869;&#33258;&#30001;&#23548;&#33322;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#37325;&#24314;&#30340;&#8220;&#20840;&#23616;&#20449;&#24687;&#8221;&#65292;&#21253;&#25324;&#20266;&#28145;&#24230;&#22270;&#21644;&#35270;&#35282;&#35206;&#30422;&#20449;&#24687;&#65292;&#20174;&#32780;&#25351;&#23548;3D&#23460;&#20869;&#22330;&#26223;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room. NeRF achieves impressive performance in rendering images for novel views similar to the input views while suffering for novel views that are significantly different from the training views. To address this issue, we utilize the holistic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to improve the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the geometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure. These two loss
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#23631;&#34109;&#35774;&#35745;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#23631;&#34109;&#22120;&#33021;&#22815;&#21160;&#24577;&#20998;&#21106;&#12289;&#21512;&#24182;&#21644;&#37325;&#26032;&#35745;&#31639;&#26234;&#33021;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#25903;&#25345;&#26356;&#21152;&#39640;&#25928;&#30340;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#25511;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.06281</link><description>&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#30462;&#22411;&#20445;&#38556;&#29992;&#20110;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#39537;&#21160;&#30340;&#21160;&#24577;&#23631;&#34109;&#35774;&#35745;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#23631;&#34109;&#22120;&#33021;&#22815;&#21160;&#24577;&#20998;&#21106;&#12289;&#21512;&#24182;&#21644;&#37325;&#26032;&#35745;&#31639;&#26234;&#33021;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#25903;&#25345;&#26356;&#21152;&#39640;&#25928;&#30340;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#25511;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#21457;&#29616;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#31574;&#30053;&#65292;&#20294;&#22312;&#23398;&#20064;&#21644;&#37096;&#32626;&#38454;&#27573;&#27809;&#26377;&#23433;&#20840;&#20445;&#38556;&#12290;&#34429;&#28982;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;(LTL)&#30340;&#23631;&#34109;&#26159;&#30830;&#20445;&#21333;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(RL)&#23433;&#20840;&#30340;&#26377;&#21069;&#36884;&#30340;&#27491;&#24335;&#26041;&#27861;&#65292;&#20294;&#23427;&#22312;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#26102;&#20250;&#23548;&#33268;&#20445;&#23432;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21512;&#25104;&#23631;&#34109;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MBDS&#20197;&#25903;&#25345;MARL&#31639;&#27861;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21512;&#25104;&#20998;&#24067;&#24335;&#23631;&#34109;&#22120;&#65292;&#36825;&#20123;&#23631;&#34109;&#22120;&#26159;&#19982;&#27599;&#20010;MARL&#26234;&#33021;&#20307;&#24182;&#34892;&#36816;&#34892;&#30340;&#21453;&#24212;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#25511;&#21644;&#32416;&#27491;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#35774;&#35745;&#20351;&#24471;&#22312;&#27809;&#26377;&#21327;&#35843;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#21512;&#25104;&#23631;&#34109;&#22120;&#20197;&#30417;&#35270;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#19981;&#30693;&#36947;&#29615;&#22659;&#30340;&#23436;&#25972;&#36716;&#25442;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#23631;&#38556;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#24033;&#36923;&#20219;&#21153;&#20013;&#20248;&#20110;LTL&#23631;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#39046;&#22495;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#20445;&#35777;&#39640;&#25928;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06277</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#25913;&#36827;&#31574;&#30053;&#20248;&#21270;&#22810;&#39046;&#22495;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multi-Domain Performance with Active Learning-based Improvement Strategies. (arXiv:2304.06277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22810;&#39046;&#22495;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#20445;&#35777;&#39640;&#25928;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#22810;&#39046;&#22495;&#24615;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#12290;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#20351;&#27169;&#22411;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23454;&#29616;&#39640;&#24615;&#33021;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20010;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#36845;&#20195;&#22320;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#25913;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#29289;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#24456;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#23601;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving performance in multiple domains is a challenging task, and often requires significant amounts of data to train and test models. Active learning techniques provide a promising solution by enabling models to select the most informative samples for labeling, thus reducing the amount of labeled data required to achieve high performance. In this paper, we present an active learning-based framework for improving performance across multiple domains. Our approach consists of two stages: first, we use an initial set of labeled data to train a base model, and then we iteratively select the most informative samples for labeling to refine the model. We evaluate our approach on several multi-domain datasets, including image classification, sentiment analysis, and object recognition. Our experiments demonstrate that our approach consistently outperforms baseline methods and achieves state-of-the-art performance on several datasets. We also show that our method is highly efficient, requirin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#22270;&#20687;&#30340;&#30058;&#33540;&#23610;&#23544;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#31995;&#32479;&#22312;&#26524;&#22253;&#29615;&#22659;&#19979;&#36935;&#21040;&#30340;&#36974;&#25377;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#32463;&#36807;&#23454;&#39564;&#23460;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#27979;&#37327;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06177</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#23460;&#20869;&#20892;&#22330;&#29615;&#22659;&#19979;&#30058;&#33540;&#23610;&#23544;&#27979;&#37327;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Visual based Tomato Size Measurement System for an Indoor Farming Environment. (arXiv:2304.06177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#22270;&#20687;&#30340;&#30058;&#33540;&#23610;&#23544;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#31995;&#32479;&#22312;&#26524;&#22253;&#29615;&#22659;&#19979;&#36935;&#21040;&#30340;&#36974;&#25377;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#32463;&#36807;&#23454;&#39564;&#23460;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#27979;&#37327;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#26234;&#33021;&#33258;&#21160;&#21270;&#31995;&#32479;&#23558;&#22312;&#20892;&#19994;&#34892;&#19994;&#20013;&#25198;&#28436;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#24403;&#21069;&#29992;&#20110;&#20135;&#37327;&#20272;&#35745;&#30340;&#29616;&#26377;&#35270;&#35273;&#31995;&#32479;&#38754;&#20020;&#30528;&#36974;&#25377;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#37319;&#29992;&#30340;&#30456;&#26426;&#31995;&#32479;&#22823;&#19988;&#26114;&#36149;&#65292;&#19981;&#36866;&#21512;&#26524;&#22253;&#29615;&#22659;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#23544;&#27979;&#37327;&#26041;&#27861;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20174;&#19977;&#20010;&#20302;&#25104;&#26412;&#30340;RGBD&#30456;&#26426;&#25429;&#25417;&#30340;&#28145;&#24230;&#22270;&#20687;&#26469;&#26816;&#27979;&#21644;&#27979;&#37327;&#30058;&#33540;&#30340;&#39640;&#24230;&#21644;&#23485;&#24230;&#12290;&#35813;&#31995;&#32479;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#19979;&#20351;&#29992;&#30495;&#23454;&#30340;&#30058;&#33540;&#26524;&#23454;&#21644;&#20551;&#21494;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#20197;&#27169;&#25311;&#30495;&#23454;&#20892;&#22330;&#29615;&#22659;&#20013;&#30340;&#36974;&#25377;&#12290;&#20026;&#20102;&#36890;&#36807;&#35299;&#20915;&#27700;&#26524;&#36974;&#25377;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#30340;&#19977;&#25668;&#20687;&#22836;&#31995;&#32479;&#33021;&#22815;&#23454;&#29616;&#39640;&#24230;&#27979;&#37327;&#31934;&#24230;&#20026;0.9114&#65292;&#23485;&#24230;&#31934;&#24230;&#20026;0.9443&#12290;
&lt;/p&gt;
&lt;p&gt;
As technology progresses, smart automated systems will serve an increasingly important role in the agricultural industry. Current existing vision systems for yield estimation face difficulties in occlusion and scalability as they utilize a camera system that is large and expensive, which are unsuitable for orchard environments. To overcome these problems, this paper presents a size measurement method combining a machine learning model and depth images captured from three low cost RGBD cameras to detect and measure the height and width of tomatoes. The performance of the presented system is evaluated on a lab environment with real tomato fruits and fake leaves to simulate occlusion in the real farm environment. To improve accuracy by addressing fruit occlusion, our three-camera system was able to achieve a height measurement accuracy of 0.9114 and a width accuracy of 0.9443.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102; Dubins'&#23567;&#36710;&#27839;&#24050;&#30693;&#36712;&#36857;&#25318;&#25130;&#30446;&#26631;&#30340;&#26368;&#30701;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292; &#24314;&#31435;&#20102;&#25968;&#23398;&#27169;&#22411;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06169</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;Dubins'&#23567;&#36710;&#25318;&#25130;&#27839;&#24050;&#30693;&#36712;&#36857;&#31227;&#21160;&#30340;&#30446;&#26631;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Network Algorithm for Intercepting Targets Moving Along Known Trajectories by a Dubins' Car. (arXiv:2304.06169v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102; Dubins'&#23567;&#36710;&#27839;&#24050;&#30693;&#36712;&#36857;&#25318;&#25130;&#30446;&#26631;&#30340;&#26368;&#30701;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292; &#24314;&#31435;&#20102;&#25968;&#23398;&#27169;&#22411;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;Dubins'&#23567;&#36710;&#27839;&#30452;&#32447;&#25110;&#22278;&#24418;&#36712;&#36857;&#25318;&#25130;&#30446;&#26631;&#30340;&#20219;&#21153;&#65292;&#21046;&#23450;&#20026;&#19968;&#20010;&#20197;&#25318;&#25130;&#30636;&#38388;&#23567;&#36710;&#36895;&#24230;&#26041;&#21521;&#20219;&#24847;&#30340;&#26368;&#30701;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21644;&#32508;&#21512;&#25318;&#25130;&#36712;&#36857;&#65292;&#20351;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#23545;&#27604;&#20998;&#26512;&#20102;&#25152;&#24471;&#25511;&#21046;&#24459;&#21644;&#25318;&#25130;&#36712;&#36857;&#19982;&#25318;&#25130;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#12290;&#36827;&#34892;&#20102;&#31070;&#32463;&#32593;&#32476;&#26410;&#22312;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#30446;&#26631;&#36816;&#21160;&#21442;&#25968;&#30340;&#25968;&#23398;&#24314;&#27169;&#12290;&#36827;&#34892;&#27169;&#22411;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;&#31070;&#32463;&#35299;&#30340;&#31283;&#23450;&#24615;&#12290;&#35777;&#26126;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32508;&#21512;&#32473;&#23450;&#30446;&#26631;&#36816;&#21160;&#31867;&#21035;&#30340;&#25318;&#25130;&#36712;&#36857;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of intercepting a target moving along a rectilinear or circular trajectory by a Dubins' car is formulated as a time-optimal control problem with an arbitrary direction of the car's velocity at the interception moment. To solve this problem and to synthesize interception trajectories, neural network methods of unsupervised learning based on the Deep Deterministic Policy Gradient algorithm are used. The analysis of the obtained control laws and interception trajectories in comparison with the analytical solutions of the interception problem is performed. The mathematical modeling for the parameters of the target movement that the neural network had not seen before during training is carried out. Model experiments are conducted to test the stability of the neural solution. The effectiveness of using neural network methods for the synthesis of interception trajectories for given classes of target movements is shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20892;&#19994;&#39046;&#22495;&#24212;&#29992;AGI&#30340;&#28508;&#22312;&#26426;&#20250;&#65292;&#21253;&#25324;&#25552;&#39640;&#20892;&#20135;&#21697;&#20135;&#37327;&#12289;&#20943;&#23569;&#28010;&#36153;&#21644;&#20419;&#36827;&#21487;&#25345;&#32493;&#20892;&#19994;&#23454;&#36341;&#65292;&#20197;&#21450;&#21033;&#29992;&#23454;&#26102;&#25968;&#25454;&#24110;&#21161;&#20892;&#27665;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2304.06136</link><description>&lt;p&gt;
&#20892;&#19994;&#39046;&#22495;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AGI for Agriculture. (arXiv:2304.06136v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20892;&#19994;&#39046;&#22495;&#24212;&#29992;AGI&#30340;&#28508;&#22312;&#26426;&#20250;&#65292;&#21253;&#25324;&#25552;&#39640;&#20892;&#20135;&#21697;&#20135;&#37327;&#12289;&#20943;&#23569;&#28010;&#36153;&#21644;&#20419;&#36827;&#21487;&#25345;&#32493;&#20892;&#19994;&#23454;&#36341;&#65292;&#20197;&#21450;&#21033;&#29992;&#23454;&#26102;&#25968;&#25454;&#24110;&#21161;&#20892;&#27665;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26377;&#26395;&#38761;&#26032;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#12289;&#20132;&#36890;&#21644;&#25945;&#32946;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;AGI&#34987;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#21307;&#23398;&#35760;&#24405;&#65292;&#35782;&#21035;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#24110;&#21161;&#24739;&#32773;&#31649;&#29702;&#12290;&#20892;&#19994;&#26159;&#24433;&#21709;&#20840;&#29699;&#20010;&#20307;&#29983;&#27963;&#30340;&#21478;&#19968;&#20851;&#38190;&#39046;&#22495;&#12290;&#23427;&#20316;&#20026;&#25552;&#20379;&#31918;&#39135;&#12289;&#32420;&#32500;&#21644;&#29123;&#26009;&#30340;&#22522;&#30784;&#65292;&#21364;&#38754;&#20020;&#30528;&#27668;&#20505;&#21464;&#21270;&#12289;&#22303;&#22756;&#36864;&#21270;&#12289;&#27700;&#36164;&#28304;&#21294;&#20047;&#21644;&#31918;&#39135;&#23433;&#20840;&#31561;&#22810;&#31181;&#25361;&#25112;&#12290;AGI&#26377;&#33021;&#21147;&#36890;&#36807;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#12289;&#20943;&#23569;&#28010;&#36153;&#21644;&#20419;&#36827;&#21487;&#25345;&#32493;&#20892;&#19994;&#23454;&#36341;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#25968;&#25454;&#24110;&#21161;&#20892;&#27665;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#20892;&#19994;&#31649;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AGI&#22312;&#20892;&#19994;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#22914;&#20892;&#19994;&#22270;&#20687;&#22788;&#29702;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) is poised to revolutionize a variety of sectors, including healthcare, finance, transportation, and education. Within healthcare, AGI is being utilized to analyze clinical medical notes, recognize patterns in patient data, and aid in patient management. Agriculture is another critical sector that impacts the lives of individuals worldwide. It serves as a foundation for providing food, fiber, and fuel, yet faces several challenges, such as climate change, soil degradation, water scarcity, and food security. AGI has the potential to tackle these issues by enhancing crop yields, reducing waste, and promoting sustainable farming practices. It can also help farmers make informed decisions by leveraging real-time data, leading to more efficient and effective farm management. This paper delves into the potential future applications of AGI in agriculture, such as agriculture image processing, natural language processing (NLP), robotics, knowledge graphs, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#22238;&#31572;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#24102;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.06122</link><description>&lt;p&gt;
&#20998;&#26512;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course. (arXiv:2304.06122v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#21487;&#20197;&#22238;&#31572;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#24102;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26368;&#36817;&#21463;&#21040;&#20102;&#20844;&#20247;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#19988;&#21548;&#36215;&#26469;&#20687;&#20154;&#31867;&#22238;&#31572;&#30340;&#21508;&#31181;&#38382;&#39064;&#30340;&#25991;&#26412;&#31572;&#26696;&#12290; ChatGPT&#22312;&#23398;&#26415;&#25110;&#35838;&#22530;&#29615;&#22659;&#20013;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#29978;&#33267;&#29983;&#25104;&#25972;&#31687;&#35770;&#25991;&#30340;&#28508;&#22312;&#29992;&#36884;&#25110;&#28389;&#29992;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#20154;&#25991;&#23398;&#31185;&#12289;&#21830;&#23398;&#38498;&#25110;&#21307;&#23398;&#38498;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35745;&#31639;&#26426;&#24037;&#31243;&#20837;&#38376;&#35838;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20837;&#38376;&#32423;&#35745;&#31639;&#26426;&#24037;&#31243;&#35838;&#31243;&#20013;&#22238;&#31572;&#27979;&#39564;&#12289;&#20316;&#19994;&#12289;&#32771;&#35797;&#21644;&#23454;&#39564;&#23460;&#38382;&#39064;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#22312;&#35810;&#38382;&#26222;&#36890;&#27010;&#24565;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#26174;&#28982;&#65292;&#20316;&#20026;&#19968;&#20010;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#23427;&#26080;&#27861;&#22788;&#29702;&#20855;&#26377;&#22270;&#34920;&#25110;&#22270;&#24418;&#30340;&#38382;&#39064;&#65292;&#20063;&#26080;&#27861;&#29983;&#25104;&#22270;&#34920;&#21644;&#22270;&#24418;&#12290;&#21516;&#26102;&#65292;&#36825;&#20010;&#24037;&#20855;&#20063;&#26080;&#27861;&#36827;&#34892;&#23454;&#39564;&#23460;&#30340;&#29616;&#22330;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has recently gathered attention from the general public and academia as a tool that is able to generate plausible and human-sounding text answers to various questions. One potential use, or abuse, of ChatGPT is in answering various questions or even generating whole essays and research papers in an academic or classroom setting. While recent works have explored the use of ChatGPT in the context of humanities, business school, or medical school, this work explores how ChatGPT performs in the context of an introductory computer engineering course. This work assesses ChatGPT's aptitude in answering quizzes, homework, exam, and laboratory questions in an introductory-level computer engineering course. This work finds that ChatGPT can do well on questions asking about generic concepts. However, predictably, as a text-only tool, it cannot handle questions with diagrams or figures, nor can it generate diagrams and figures. Further, also clearly, the tool cannot do hands-on lab experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#26803;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#29289;&#32852;&#32593;&#20449;&#20219;&#21644;&#22768;&#35465;&#30340;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#26426;&#21046;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.06119</link><description>&lt;p&gt;
IoT &#20449;&#20219;&#19982;&#22768;&#35465;&#65306;&#19968;&#20221;&#35843;&#26597;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
IoT trust and reputation: a survey and taxonomy. (arXiv:2304.06119v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#26803;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#29289;&#32852;&#32593;&#20449;&#20219;&#21644;&#22768;&#35465;&#30340;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#26426;&#21046;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#39044;&#35745;&#21040;2030&#24180;&#24213;&#65292;&#20840;&#29699;&#23558;&#21033;&#29992;&#36229;&#36807;&#21313;&#20159;&#20010;&#35774;&#22791;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#36825;&#20123;&#36830;&#25509;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#29289;&#32852;&#32593;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#21644;&#22768;&#35465;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#20449;&#20219;&#31649;&#29702;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#26696;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#35774;&#22791;&#35282;&#33394;&#12289;&#35774;&#22791;&#31867;&#22411;&#20197;&#21450;&#20854;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#20449;&#20219;&#21644;&#22768;&#35465;&#27169;&#22411;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#29305;&#24449;&#21644;&#36830;&#25509;&#33410;&#28857;&#21040;&#32593;&#32476;&#26102;&#30340;&#19981;&#30830;&#23450;&#39118;&#38505;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#25345;&#32493;&#30340;&#30740;&#31350;&#24182;&#19988;&#35768;&#22810;&#25991;&#31456;&#22312;&#21463;&#38480;&#29615;&#22659;&#19979;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#20449;&#20219;&#21644;&#22768;&#35465;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#22768;&#35465;&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#29616;&#26377;&#29289;&#32852;&#32593;&#20449;&#20219;&#27169;&#22411;&#21644;&#26426;&#21046;&#30340;&#35843;&#26597;&#21644;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#38024;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#35774;&#35745;&#39640;&#25928;&#12289;&#24378;&#20581;&#30340;&#20449;&#20219;&#21644;&#22768;&#35465;&#31649;&#29702;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT is one of the fastest-growing technologies and it is estimated that more than a billion devices would be utilized across the globe by the end of 2030. To maximize the capability of these connected entities, trust and reputation among IoT entities is essential. Several trust management models have been proposed in the IoT environment; however, these schemes have not fully addressed the IoT devices features, such as devices role, device type and its dynamic behavior in a smart environment. As a result, traditional trust and reputation models are insufficient to tackle these characteristics and uncertainty risks while connecting nodes to the network. Whilst continuous study has been carried out and various articles suggest promising solutions in constrained environments, research on trust and reputation is still at its infancy. In this paper, we carry out a comprehensive literature review on state-of-the-art research on the trust and reputation of IoT devices and systems. Specifically
&lt;/p&gt;</description></item><item><title>AutoShot &#21457;&#24067;&#20102;&#19968;&#20221;&#26032;&#30340;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#21517;&#20026; AutoShot &#30340;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#22312; F1 &#20998;&#25968;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06116</link><description>&lt;p&gt;
AutoShot&#65306;&#19968;&#20221;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#26368;&#26032;&#30340;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection. (arXiv:2304.06116v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06116
&lt;/p&gt;
&lt;p&gt;
AutoShot &#21457;&#24067;&#20102;&#19968;&#20221;&#26032;&#30340;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#21517;&#20026; AutoShot &#30340;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#22312; F1 &#20998;&#25968;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#22312;&#26032;&#30340;&#31038;&#20132;&#23186;&#20307;&#36235;&#21183;&#20013;&#29190;&#21457;&#24615;&#22320;&#27969;&#34892;&#36215;&#26469;&#12290;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#65288;SBD&#65289;&#26159;&#21508;&#31181;&#22330;&#26223;&#20013;&#26368;&#22522;&#26412;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#23545;&#20110;&#35270;&#39057;&#20869;&#23481;&#30340;&#21019;&#24314;&#21644;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;SHOT&#30340;&#26032;&#30340;&#20844;&#20849;&#30701;&#35270;&#39057;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;853&#20010;&#23436;&#25972;&#30340;&#30701;&#35270;&#39057;&#21644;11,606&#20010;&#38236;&#22836;&#27880;&#37322;&#65292;&#20854;&#20013;&#21253;&#25324;200&#20010;&#27979;&#35797;&#35270;&#39057;&#20013;&#30340;2,716&#20010;&#39640;&#36136;&#37327;&#30340;&#38236;&#22836;&#36793;&#30028;&#27880;&#37322;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#36130;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoShot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21253;&#21547;&#21508;&#31181;&#20808;&#36827;&#30340;3D ConvNets&#21644;Transformers&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#26469;&#20248;&#21270;&#38236;&#22836;&#36793;&#30028;&#26816;&#27979;&#27169;&#22411;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#22312;&#36229;&#36807;TransNetV2 4.2&#65285;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The short-form videos have explosive popularity and have dominated the new social media trends. Prevailing short-video platforms,~\textit{e.g.}, Kuaishou (Kwai), TikTok, Instagram Reels, and YouTube Shorts, have changed the way we consume and create content. For video content creation and understanding, the shot boundary detection (SBD) is one of the most essential components in various scenarios. In this work, we release a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations, with 2,716 high quality shot boundary annotations in 200 test videos. Leveraging this new data wealth, we propose to optimize the model design for video SBD, by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Our proposed approach, named AutoShot, achieves higher F1 scores than previous state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when bein
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;3DG-GA&#26041;&#27861;&#29983;&#25104;&#36924;&#30495;&#30340;&#33647;&#29289;&#28389;&#29992;&#38754;&#37096;&#29305;&#24449;&#30340;&#20154;&#24037;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23558;&#23545;&#29983;&#29289;&#21307;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06106</link><description>&lt;p&gt;
&#37319;&#29992;3DG-GA&#65288;&#22522;&#22240;&#31639;&#27861;&#30340;&#28145;&#24230;&#21435;&#26631;&#35782;&#21270;&#21311;&#21517;&#25968;&#25454;&#38598;&#22686;&#24191;&#65289;&#29983;&#25104;&#20154;&#24037;&#38754;&#37096;&#33647;&#29289;&#28389;&#29992;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of artificial facial drug abuse images using Deep De-identified anonymous Dataset augmentation through Genetics Algorithm (3DG-GA). (arXiv:2304.06106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;3DG-GA&#26041;&#27861;&#29983;&#25104;&#36924;&#30495;&#30340;&#33647;&#29289;&#28389;&#29992;&#38754;&#37096;&#29305;&#24449;&#30340;&#20154;&#24037;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23558;&#23545;&#29983;&#29289;&#21307;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#33719;&#24471;&#22823;&#35268;&#27169;&#12289;&#33391;&#22909;&#24179;&#34913;&#12289;&#20195;&#34920;&#24615;&#24378;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#24320;&#21457;&#21487;&#20449;&#20219;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#21463;&#38480;&#20110;&#21307;&#38498;&#21644;&#19987;&#19994;&#35774;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24191;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#65292;&#23637;&#31034;&#33647;&#29289;&#28389;&#29992;&#29305;&#24449;&#30340;&#20154;&#24037;&#21512;&#25104;&#38754;&#23380;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;3DG-GA&#8221;&#65292;&#28145;&#24230;&#21435;&#26631;&#35782;&#21270;&#21311;&#21517;&#25968;&#25454;&#38598;&#29983;&#25104;&#65292;&#20351;&#29992;&#22522;&#22240;&#31639;&#27861;&#20316;&#20026;&#21512;&#25104;&#38754;&#23380;&#30340;&#31574;&#30053;&#12290;&#31639;&#27861;&#21253;&#25324;GAN&#20154;&#24037;&#38754;&#29983;&#25104;&#65292;&#20266;&#36896;&#26816;&#27979;&#21644;&#20154;&#33080;&#35782;&#21035;&#12290;&#26368;&#21021;&#20351;&#29992;&#20102;120&#24352;&#23454;&#38469;&#38754;&#37096;&#33647;&#29289;&#28389;&#29992;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20445;&#30041;&#33647;&#29289;&#29305;&#24615;&#65292;3DG-GA&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;3000&#24352;&#21512;&#25104;&#38754;&#37096;&#33647;&#29289;&#28389;&#29992;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#24320;&#25918;&#32473;&#31185;&#23398;&#30028;&#65292;&#31185;&#23398;&#23478;&#20204;&#21487;&#20197;&#22797;&#29616;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#21463;&#30410;&#20110;&#35813;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In biomedical research and artificial intelligence, access to large, well-balanced, and representative datasets is crucial for developing trustworthy applications that can be used in real-world scenarios. However, obtaining such datasets can be challenging, as they are often restricted to hospitals and specialized facilities. To address this issue, the study proposes to generate highly realistic synthetic faces exhibiting drug abuse traits through augmentation. The proposed method, called "3DG-GA", Deep De-identified anonymous Dataset Generation, uses Genetics Algorithm as a strategy for synthetic faces generation. The algorithm includes GAN artificial face generation, forgery detection, and face recognition. Initially, a dataset of 120 images of actual facial drug abuse is used. By preserving, the drug traits, the 3DG-GA provides a dataset containing 3000 synthetic facial drug abuse images. The dataset will be open to the scientific community, which can reproduce our results and benef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;RL&#26041;&#27861;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06056</link><description>&lt;p&gt;
&#21033;&#29992;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#20419;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation. (arXiv:2304.06056v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;RL&#26041;&#27861;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;&#26426;&#22120;&#20154;&#25805;&#20316;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#22411;&#24212;&#29992;&#65292;&#27169;&#25311;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23454;&#38469;&#23454;&#29616;&#21069;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#26159;RL&#20195;&#29702;&#24448;&#24448;&#23545;&#27169;&#25311;&#19982;&#23454;&#38469;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#24322;&#25935;&#24863;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#25104;&#27169;&#25311;&#36719;&#20214;&#23454;&#26102;&#27169;&#25311;&#65288;RT-IS&#65289;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;RL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the robustness of RL methods and the performance of domain randomization. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the occupation of the computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we apply the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06055</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#31216;&#24615;&#21644;&#21551;&#21457;&#24335;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation. (arXiv:2304.06055v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#33258;&#21160;&#26500;&#24314;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#26102;&#30001;&#20110;&#32500;&#24230;&#30340;&#38382;&#39064;&#65292;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#32467;&#21512;&#29289;&#29702;&#26426;&#22120;&#29615;&#22659;&#20013;&#23384;&#22312;&#30340;&#33258;&#28982;&#23545;&#31216;&#24615;&#65292;&#21033;&#29992;&#23545;&#31216;&#29615;&#22659;&#20013;&#30340;&#19987;&#23478;&#28436;&#31034;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#30340;&#34701;&#21512;&#26469;&#35757;&#32451;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#32473;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#22810;&#26679;&#21270;&#32780;&#32039;&#20945;&#30340;&#21551;&#21160;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#36817;&#27010;&#24565;&#30340;&#20005;&#26684;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#33539;&#22260;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20004;&#20010;&#28857;&#23545;&#28857;&#30340;&#24037;&#19994;&#33218;&#21040;&#36798;&#20219;&#21153;&#65288;&#26377;&#38556;&#30861;&#21644;&#26080;&#38556;&#30861;&#65289;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning demonstrates significant potential in automatically building control policies in numerous domains, but shows low efficiency when applied to robot manipulation tasks due to the curse of dimensionality. To facilitate the learning of such tasks, prior knowledge or heuristics that incorporate inherent simplification can effectively improve the learning performance. This paper aims to define and incorporate the natural symmetry present in physical robotic environments. Then, sample-efficient policies are trained by exploiting the expert demonstrations in symmetrical environments through an amalgamation of reinforcement and behavior cloning, which gives the off-policy learning process a diverse yet compact initiation. Furthermore, it presents a rigorous framework for a recent concept and explores its scope for robot manipulation tasks. The proposed method is validated via two point-to-point reaching tasks of an industrial arm, with and without an obstacle, in a simulat
&lt;/p&gt;</description></item><item><title>Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06051</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#26234;&#33021;&#20132;&#36890;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598; - Open-TransMind
&lt;/p&gt;
&lt;p&gt;
Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06051
&lt;/p&gt;
&lt;p&gt;
Open-TransMind&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#36187;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#31561;&#20856;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#35745;&#31639;&#33021;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#30784;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#36234;&#26469;&#36234;&#22810;&#30340;&#34892;&#19994;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#22312;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#20197;&#19979;&#20856;&#22411;&#25361;&#25112;&#65306;&#25968;&#25454;&#37327;&#23569;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#20197;&#21450;&#32570;&#20047;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#25361;&#25112;&#65292;&#26088;&#22312;&#22686;&#21152;&#22522;&#30784;&#27169;&#22411;&#25216;&#26415;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#24230;&#65292;&#24182;&#20419;&#36827;&#26234;&#33021;&#20132;&#36890;&#34892;&#19994;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#35813;&#25361;&#25112;&#20998;&#20026;&#20004;&#20010;&#36187;&#36947;&#65306;&#20840;&#33021;&#22411;&#21644;&#36328;&#27169;&#24577;&#22270;&#20687;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20004;&#20010;&#36187;&#36947;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#32447;&#21644;&#22522;&#20934;&#25968;&#25454;&#65292;&#31216;&#20026;Open-TransMind&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26234;&#33021;&#20132;&#36890;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RELS-DQN&#30340;&#36731;&#37327;&#32423;DQN&#26694;&#26550;&#65292;&#21487;&#20197;&#23637;&#29616;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20854;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#20540;&#35201;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06048</link><description>&lt;p&gt;
RELS-DQN&#65306;&#32452;&#21512;&#20248;&#21270;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RELS-DQN: A Robust and Efficient Local Search Framework for Combinatorial Optimization. (arXiv:2304.06048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RELS-DQN&#30340;&#36731;&#37327;&#32423;DQN&#26694;&#26550;&#65292;&#21487;&#20197;&#23637;&#29616;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#24182;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20854;&#22312;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#20540;&#35201;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#26088;&#22312;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#28041;&#21450;&#20174;&#32479;&#35745;&#29289;&#29702;&#21040;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#30340;NP&#38590;&#38382;&#39064;&#12290;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#24212;&#29992;&#21487;&#20197;&#20174;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#22312;&#36138;&#23146;&#31574;&#30053;&#19978;&#36827;&#34892;&#21487;&#36870;&#25805;&#20316;&#12290;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#65288;DQN&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#22797;&#21046;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#21644;&#33719;&#24471;&#19982;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#26041;&#38754;&#24456;&#26377;&#21069;&#36884;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#24687;&#20256;&#36882;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;&#36807;&#20110;&#24179;&#28369;&#21644;&#20449;&#24687;&#20002;&#22833;&#38480;&#21046;&#20102;&#20854;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22823;&#30340;&#28040;&#24687;&#21521;&#37327;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;RELS-DQN&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;DQN&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#23616;&#37096;&#25628;&#32034;&#34892;&#20026;&#21516;&#26102;&#25552;&#20379;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20351;&#29992;&#22312;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#19978;&#35757;&#32451;&#30340;RELS-DQN&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#20110;&#25110;&#31561;&#20110;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#20540;&#26469;&#25512;&#24191;&#21040;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) aims to efficiently find the best solution to NP-hard problems ranging from statistical physics to social media marketing. A wide range of CO applications can benefit from local search methods because they allow reversible action over greedy policies. Deep Q-learning (DQN) using message-passing neural networks (MPNN) has shown promise in replicating the local search behavior and obtaining comparable results to the local search algorithms. However, the over-smoothing and the information loss during the iterations of message passing limit its robustness across applications, and the large message vectors result in memory inefficiency. Our paper introduces RELS-DQN, a lightweight DQN framework that exhibits the local search behavior while providing practical scalability. Using the RELS-DQN model trained on one application, it can generalize to various applications by providing solution values higher than or equal to both the local search algorithms and the e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06044</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38750;&#32447;&#24615;&#26412;&#26500;&#26448;&#26009;&#27169;&#22411;&#65306;COMM-PINN&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#20915;&#38750;&#32447;&#24615;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#65292;&#26080;&#38656;&#21021;&#22987;&#25968;&#25454;&#65292;&#36991;&#20813;&#37325;&#22797;&#30340;&#29275;&#39039;&#36845;&#20195;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20316;&#20026;&#26377;&#38480;&#20803;&#31243;&#24207;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#36335;&#24452;&#30456;&#20851;&#26448;&#26009;&#34892;&#20026;&#30340;&#26412;&#26500;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#19981;&#20165;&#28385;&#36275;&#25152;&#26377;&#28909;&#21147;&#23398;&#32422;&#26463;&#65292;&#32780;&#19988;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#21152;&#36733;&#24773;&#20917;&#19979;&#65292;&#31435;&#21363;&#25552;&#20379;&#20851;&#20110;&#24403;&#21069;&#26448;&#26009;&#29366;&#24577;&#65288;&#21363;&#33258;&#30001;&#33021;&#65292;&#24212;&#21147;&#21644;&#20869;&#37096;&#21464;&#37327;&#30340;&#28436;&#21464;&#65289;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#21021;&#22987;&#25968;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#23427;&#35268;&#36991;&#20102;&#27714;&#35299;&#22797;&#26448;&#26009;&#27169;&#22411;&#20013;&#38750;&#32447;&#24615;&#26041;&#31243;&#25152;&#38656;&#30340;&#37325;&#22797;&#29275;&#39039;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20943;&#23569;&#33719;&#21462;&#20999;&#21521;&#31639;&#23376;&#25152;&#38656;&#30340;&#23548;&#25968;&#27425;&#24207;&#30340;&#31574;&#30053;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20316;&#20219;&#20309;&#26377;&#38480;&#20803;&#31243;&#24207;&#65288;&#25110;&#20854;&#20182;&#25968;&#20540;&#26041;&#27861;&#65289;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#26448;&#26009;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23450;&#20041;&#37197;&#28857;&#21644;&#25972;&#21512;&#21516;&#26102;&#28608;&#27963;&#25110;&#38750;&#28608;&#27963;&#30340;&#22810;&#20010;&#38750;&#30456;&#31561;&#32422;&#26463;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#65292;&#35813;&#25506;&#38024;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20998;&#26512;&#21644;&#22788;&#29702;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#20197;&#35782;&#21035;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.06041</link><description>&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Systems for Advanced Driving Assistance. (arXiv:2304.06041v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#39550;&#39542;&#36741;&#21161;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#65292;&#35813;&#25506;&#38024;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20998;&#26512;&#21644;&#22788;&#29702;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#20197;&#35782;&#21035;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#30456;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#27773;&#36710;&#36890;&#36807;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#26234;&#33021;&#35780;&#20272;&#36710;&#36742;&#39550;&#39542;&#23433;&#20840;&#12290;&#36825;&#39033;&#23433;&#20840;&#39550;&#39542;&#30417;&#27979;&#21487;&#20197;&#20351;&#29992;&#35768;&#22810;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#24191;&#27867;&#35752;&#35770;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#23450;&#30340;&#29983;&#29289;&#20256;&#24863;&#31995;&#32479;&#26469;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#27880;&#24847;&#21147;&#29366;&#24577;&#12290;&#20026;&#20102;&#37325;&#26500;&#27773;&#36710;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#29289;&#20256;&#24863;&#25506;&#38024;&#30340;&#26041;&#27861;&#65292;&#35813;&#25506;&#38024;&#30001;&#36817;&#32418;&#22806;(NiR)&#20809;&#35889;&#19979;&#30340;&#32806;&#21512;LED&#21644;&#20809;&#30005;&#26816;&#27979;&#22120;&#32452;&#25104;&#12290;&#35813;&#25506;&#38024;&#25918;&#32622;&#22312;&#34987;&#30417;&#27979;&#30340;&#23545;&#35937;&#19978;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19968;&#31181;&#31216;&#20026;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;(PPG)&#30340;&#29983;&#29702;&#20449;&#21495;&#12290;PPG&#20449;&#21495;&#30340;&#24418;&#25104;&#30001;&#34987;&#30417;&#27979;&#23545;&#35937;&#34880;&#28082;&#20013;&#27687;&#21512;&#21644;&#38750;&#27687;&#21512;&#34880;&#32418;&#34507;&#30333;&#27987;&#24230;&#30340;&#21464;&#21270;&#35843;&#33410;&#65292;&#36825;&#23558;&#30452;&#25509;&#19982;&#30001;&#33258;&#20027;&#31070;&#32463;&#31995;&#32479;(ANS)&#35843;&#33410;&#30340;&#24515;&#33039;&#27963;&#21160;&#30456;&#36830;&#25509;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22788;&#29702;&#21644;&#20998;&#26512;&#33719;&#24471;&#30340;PPG&#20449;&#21495;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#19982;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#27700;&#24179;&#19968;&#33268;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next generation cars embed intelligent assessment of car driving safety through innovative solutions often based on usage of artificial intelligence. The safety driving monitoring can be carried out using several methodologies widely treated in scientific literature. In this context, the author proposes an innovative approach that uses ad-hoc bio-sensing system suitable to reconstruct the physio-based attentional status of the car driver. To reconstruct the car driver physiological status, the author proposed the use of a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR) spectrum with a photodetector. This probe placed over the monitored subject allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The PPG signal formation is regulated by the change in oxygenated and non-oxygenated hemoglobin concentration in the monitored subject bloodstream which will be directly connected to cardiac activity in turn regulated by the Autonomic Nervous System (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#25490;&#38500;&#29305;&#23450;&#20154;&#32676;&#30340;&#32844;&#19994;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.06034</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#35282;&#24230;&#23457;&#35270;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Social Biases through the Text-to-Image Generation Lens. (arXiv:2304.06034v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#25490;&#38500;&#29305;&#23450;&#20154;&#32676;&#30340;&#32844;&#19994;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#29983;&#25104;&#25216;&#26415;&#36890;&#36807;&#23558;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#25554;&#22270;&#65292;&#20026;&#21019;&#20316;&#32773;&#12289;&#35774;&#35745;&#24072;&#21644;&#26222;&#36890;&#29992;&#25143;&#25552;&#20379;&#20102;&#26032;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#22823;&#37327;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21644;&#37327;&#21270;&#24120;&#35265;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#22914;&#32844;&#19994;&#12289;&#20154;&#26684;&#29305;&#24449;&#21644;&#26085;&#24120;&#24773;&#22659;&#22312;&#65288;&#34987;&#24863;&#30693;&#30340;&#65289;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#31181;&#26063;&#21644;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#37319;&#29992;&#22810;&#32500;&#24230;&#26041;&#27861;&#26469;&#25506;&#31350;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340; T2I &#27169;&#22411; (DALLE-v2 &#21644; Stable Diffusion) &#30340;&#21457;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20013;&#24615;&#25552;&#31034;&#23384;&#22312;&#20005;&#37325;&#30340;&#32844;&#19994;&#20559;&#35265;&#65292;&#20027;&#35201;&#26159;&#25490;&#38500;&#26576;&#20123;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#21327;&#35758;&#25361;&#25112;&#19981;&#23545;&#31216;&#30340;&#26435;&#21147;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#22242;&#38431;&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.06031</link><description>&lt;p&gt;
&#20844;&#27491;&#65306;&#20174;&#20262;&#29702;&#21407;&#21017;&#21040;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20013;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25345;&#32493;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Fairness: from the ethical principle to the practice of Machine Learning development as an ongoing agreement with stakeholders. (arXiv:2304.06031v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#20013;&#30340;&#25345;&#32493;&#21327;&#35758;&#25361;&#25112;&#19981;&#23545;&#31216;&#30340;&#26435;&#21147;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#22242;&#38431;&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#26126;&#20102;&#20026;&#20160;&#20040;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#23558;&#27491;&#20041;&#19982;&#20844;&#27491;&#30340;&#20262;&#29702;&#21407;&#21017;&#36716;&#21270;&#20026;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#25345;&#32493;&#21327;&#35758;&#65292;&#32435;&#20837;ML&#24320;&#21457;&#23454;&#36341;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#25903;&#25345;&#20262;&#29702;&#30340;&#36845;&#20195;&#36807;&#31243;&#26088;&#22312;&#25361;&#25112;ML&#35774;&#35745;&#20013;&#30340;&#19981;&#23545;&#31216;&#26435;&#21147;&#21160;&#24577;&#65292;&#24110;&#21161;ML&#24320;&#21457;&#22242;&#38431;&#22312;ML&#31995;&#32479;&#24320;&#21457;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#35782;&#21035;&#12289;&#20943;&#36731;&#21644;&#30417;&#27979;&#20559;&#35265;&#12290;&#35813;&#36807;&#31243;&#36824;&#25552;&#20379;&#20102;&#22914;&#20309;&#21521;&#29992;&#25143;&#35299;&#37322;&#20559;&#35265;&#22312;&#26435;&#34913;&#26041;&#38754;&#22987;&#32456;&#19981;&#23436;&#32654;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper clarifies why bias cannot be completely mitigated in Machine Learning (ML) and proposes an end-to-end methodology to translate the ethical principle of justice and fairness into the practice of ML development as an ongoing agreement with stakeholders. The pro-ethical iterative process presented in the paper aims to challenge asymmetric power dynamics in the fairness decision making within ML design and support ML development teams to identify, mitigate and monitor bias at each step of ML systems development. The process also provides guidance on how to explain the always imperfect trade-offs in terms of bias to users.
&lt;/p&gt;</description></item><item><title>LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.05869</link><description>&lt;p&gt;
LMR: &#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36712;&#36857;&#39044;&#27979;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05869
&lt;/p&gt;
&lt;p&gt;
LMR&#26159;&#19968;&#31181;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#24230;&#37327;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#30340;&#24320;&#21457;&#38656;&#35201;&#24230;&#37327;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#24050;&#32463;&#30830;&#23450;&#30340;&#24230;&#37327;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#37117;&#32473;&#20986;&#20102;&#30456;&#21516;&#30340;&#35823;&#24046;&#26435;&#37325;&#12290;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#23545;&#20110;&#20687;&#36947;&#36335;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#22949;&#21892;&#25429;&#25417;&#21040;&#19982;&#24213;&#23618;&#36710;&#36947;&#30456;&#20851;&#30340;&#25805;&#20316;&#21592;&#24847;&#22270;&#12290;&#20026;&#20102;&#38024;&#23545;&#19979;&#28216;&#35268;&#21010;&#20219;&#21153;&#21512;&#29702;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#65292;&#21363;&#22522;&#20110;&#36710;&#36947;&#36317;&#31163;&#30340;&#36710;&#36947;&#38169;&#36807;&#29575;&#65288;LMR&#65289;&#12290;&#23545;&#20110;LMR&#30340;&#35745;&#31639;&#65292;&#23558;&#22320;&#38754;&#23454;&#27979;&#21644;&#39044;&#27979;&#31471;&#28857;&#20998;&#37197;&#32473;&#36710;&#36947;&#32447;&#27573;&#65292;&#26356;&#30830;&#20999;&#22320;&#35828;&#26159;&#23427;&#20204;&#30340;&#20013;&#24515;&#32447;&#12290;&#36890;&#36807;&#27839;&#36710;&#36947;&#32447;&#27573;&#30340;&#36317;&#31163;&#27979;&#37327;&#65292;&#39044;&#27979;&#19982;&#23454;&#27979;&#20043;&#38388;&#30340;&#36317;&#31163;&#22312;&#19968;&#23450;&#38408;&#20540;&#33539;&#22260;&#20869;&#30340;&#39044;&#27979;&#34987;&#31216;&#20026;&#21629;&#20013;&#65292;&#21542;&#21017;&#31216;&#20026;&#38169;&#36807;&#12290;LMR&#21017;&#23450;&#20041;&#20026;&#20135;&#29983;&#38169;&#36807;&#30340;&#24207;&#21015;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27431;&#27663;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;LMR&#26159;&#36866;&#29992;&#20110;&#31867;&#20284;&#36710;&#36947;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#36712;&#36857;&#39044;&#27979;&#26356;&#20026;&#21512;&#36866;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05860</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#34920;&#31034;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#20197;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#24418;&#24322;&#20041;&#35789;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#19968;&#30452;&#26159;&#38590;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#21516;&#24418;&#24322;&#20041;&#35789;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#8220;HDR-encoder&#8221;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#36890;&#29992;&#21477;&#23376;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;WordNet&#20013;&#30340;&#21516;&#20041;&#35789;&#21477;&#23376;&#24314;&#31435;&#21516;&#24418;&#24322;&#20041;&#35789;&#35789;&#32423;&#28040;&#27495;&#34920;&#31034;&#65288;HDR&#65289;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;HDR-encoder&#19982;&#22522;&#20110;Transformer&#30340;NMT&#22312;&#19981;&#21516;&#26041;&#26696;&#20013;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#22235;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26412;&#26041;&#27861;&#22312;&#22686;&#24378;NMT&#31995;&#32479;&#22788;&#29702;&#21516;&#24418;&#24322;&#20041;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homographs, words with the same spelling but different meanings, remain challenging in Neural Machine Translation (NMT). While recent works leverage various word embedding approaches to differentiate word sense in NMT, they do not focus on the pivotal components in resolving ambiguities of homographs in NMT: the hidden states of an encoder. In this paper, we propose a novel approach to tackle homographic issues of NMT in the latent space. We first train an encoder (aka "HDR-encoder") to learn universal sentence representations in a natural language inference (NLI) task. We further fine-tune the encoder using homograph-based synset sentences from WordNet, enabling it to learn word-level homographic disambiguation representations (HDR). The pre-trained HDR-encoder is subsequently integrated with a transformer-based NMT in various schemes to improve translation accuracy. Experiments on four translation directions demonstrate the effectiveness of the proposed method in enhancing the perfor
&lt;/p&gt;</description></item><item><title>Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05800</link><description>&lt;p&gt;
Proximity Forest 2.0&#65306;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05800
&lt;/p&gt;
&lt;p&gt;
Proximity Forest 2.0&#26159;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#30001;&#20110;&#21487;&#33021;&#19982;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21253;&#25324;&#36235;&#21183;&#12289;&#26041;&#24046;&#12289;&#39057;&#29575;&#12289;&#24133;&#24230;&#21644;&#21508;&#31181;&#27169;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#30456;&#20284;&#24615;&#12289;&#29305;&#24449;&#21644;&#38388;&#38548;&#12289;&#24418;&#29366;&#12289;&#23383;&#20856;&#12289;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;Proximity Forest&#29256;&#26412;2.0&#65288;PF 2.0&#65289;&#65292;&#23427;&#22312;UCR&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20869;&#26680;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26368;&#36866;&#21512;&#20351;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;PF 2.0 &#21512;&#24182;&#20102;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#26368;&#36817;&#30340;&#19977;&#20010;&#36827;&#23637;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05350</link><description>&lt;p&gt;
Astroformer&#65306;&#20998;&#31867;&#24182;&#19981;&#24635;&#26159;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26032;&#30340;&#22534;&#26632;&#35774;&#35745;&#12289;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#21644;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#25110;&#37096;&#32626;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26143;&#31995;&#24418;&#24577;&#23545;&#20110;&#29702;&#35299;&#26143;&#31995;&#30340;&#24418;&#25104;&#21644;&#28436;&#21270;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#38656;&#35201;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#31867;&#26143;&#31995;&#24418;&#24577;&#65292;&#24182;&#20174;&#29616;&#20195;&#22825;&#25991;&#23398;&#35843;&#26597;&#20013;&#25552;&#21462;&#29289;&#29702;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#21464;&#25442;&#22120; - &#21367;&#31215;&#26550;&#26500;&#65292;&#20174;CoAtNet&#21644;MaxViT&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26032;&#22534;&#26632;&#35774;&#35745;&#21644;&#19981;&#21516;&#30340;&#30456;&#23545;&#33258;&#25105;&#27880;&#24847;&#23618;&#21019;&#24314;&#26041;&#24335;&#30340;Transformer - &#21367;&#31215;&#28151;&#21512;&#12290;&#24182;&#23558;&#20854;&#19982;&#31934;&#24515;&#36873;&#25321;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#37197;&#23545;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;Galaxy Zoo&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20180;&#32454;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.04914</link><description>&lt;p&gt;
&#30417;&#31649;&#24066;&#22330;&#65306;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#22320;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#32039;&#36843;&#30340;&#25919;&#31574;&#25361;&#25112;&#12290;&#31435;&#27861;&#26426;&#26500;&#21644;&#30417;&#31649;&#26426;&#26500;&#32570;&#20047;&#32763;&#35793;&#20844;&#20247;&#38656;&#27714;&#20026;&#27861;&#24459;&#35201;&#27714;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#26410;&#33021;&#20351;AI&#31995;&#32479;&#30340;&#29983;&#20135;&#32773;&#21644;&#20351;&#29992;&#32773;&#23545;&#27665;&#20027;&#35201;&#27714;&#36127;&#36131;&#12290;&#25552;&#20986;&#20102;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#21629;&#20196;&#21644;&#25511;&#21046;&#30417;&#31649;&#21644;&#33258;&#25105;&#30417;&#31649;&#30340;&#23616;&#38480;&#24615;&#12290;&#30417;&#31649;&#24066;&#22330;&#21487;&#20197;&#20351;&#25919;&#24220;&#20026;AI&#30417;&#31649;&#24314;&#31435;&#25919;&#31574;&#20248;&#20808;&#32423;&#65292;&#21516;&#26102;&#20381;&#38752;&#24066;&#22330;&#21147;&#37327;&#21644;&#34892;&#19994;&#30740;&#21457;&#21162;&#21147;&#26469;&#24320;&#21019;&#26368;&#33021;&#23454;&#29616;&#25919;&#31574;&#21046;&#23450;&#32773;&#22768;&#26126;&#30446;&#26631;&#30340;&#30417;&#31649;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&amp;D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.04227</link><description>&lt;p&gt;
&#35270;&#39057;&#32842;&#22825;&#23383;&#24149;&#29983;&#25104;&#22120;&#65306; &#36808;&#21521;&#20016;&#23500;&#26102;&#31354;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. (arXiv:2304.04227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04227
&lt;/p&gt;
&lt;p&gt;
Video ChatCaptioner&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#21644;&#31639;&#27861;&#29983;&#25104;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#30340;&#30446;&#30340;&#26159;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#35270;&#39057;&#20013;&#30340;&#21160;&#24577;&#22330;&#26223;&#65292;&#20419;&#36827;&#25105;&#20204;&#23545;&#29615;&#22659;&#20013;&#26102;&#31354;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#29983;&#25104;&#32454;&#33268;&#21644;&#20016;&#23500;&#30340;&#35270;&#39057;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;Video ChatCaptioner&#65292;&#29992;&#20110;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#26102;&#31354;&#35270;&#39057;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992; ChatGPT &#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#26694;&#26550;&#20197;&#25552;&#20986;&#35270;&#39057;&#20869;&#23481;&#39537;&#21160;&#30340;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#31639;&#27861;&#22238;&#31572;&#36825;&#20123;&#35270;&#35273;&#26597;&#35810;&#12290;&#36825;&#31181;&#38382;&#31572;&#26694;&#26550;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#35270;&#39057;&#32454;&#33410;&#65292;&#24182;&#26174;&#31034;&#20986;&#22686;&#24378;&#35270;&#39057;&#20869;&#23481;&#30340;&#26041;&#27861;&#30340;&#21069;&#36884;&#12290;&#22312;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#20043;&#21518;&#65292;ChatGPT &#21487;&#20197;&#26681;&#25454;&#20043;&#21069;&#30340;&#23545;&#35805;&#24635;&#32467;&#20016;&#23500;&#30340;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#23450;&#24615;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340; Video ChatCaptioner &#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#26356;&#22810;&#32454;&#33410;&#30340;&#35270;&#39057;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing mo
&lt;/p&gt;</description></item><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26041;&#27861;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#20225;&#19994;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.01897</link><description>&lt;p&gt;
InfluencerRank&#65306;&#22522;&#20110;&#22270;&#21367;&#31215;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#26377;&#25928;&#30340;&#24433;&#21709;&#32773;
&lt;/p&gt;
&lt;p&gt;
InfluencerRank: Discovering Effective Influencers via Graph Convolutional Attentive Recurrent Neural Networks. (arXiv:2304.01897v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26041;&#27861;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#20225;&#19994;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24433;&#21709;&#32773;&#22312;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#20225;&#19994;&#22686;&#21152;&#20102;&#24433;&#21709;&#32773;&#33829;&#38144;&#30340;&#39044;&#31639;&#12290;&#38599;&#29992;&#26377;&#25928;&#30340;&#24433;&#21709;&#32773;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#25968;&#20159;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#23427;&#22522;&#20110;&#24433;&#21709;&#32773;&#30340;&#21457;&#24067;&#34892;&#20026;&#21644;&#31038;&#20132;&#20851;&#31995;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#34920;&#31034;&#21457;&#24067;&#34892;&#20026;&#21644;&#31038;&#20132;&#20851;&#31995;&#65292;&#24212;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#22312;&#19981;&#21516;&#30340;&#21382;&#21490;&#26102;&#38388;&#27573;&#20869;&#23545;&#20855;&#26377;&#24322;&#26500;&#32593;&#32476;&#30340;&#24433;&#21709;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#23884;&#20837;&#24335;&#33410;&#28857;&#29305;&#24449;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;InfluencerRank&#21487;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#20026;&#24433;&#21709;&#32773;&#27966;&#29983;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#26368;&#32456;&#65292;&#19968;&#20010;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25429;&#25417;&#24433;&#21709;&#32773;&#34920;&#31034;&#30340;&#21160;&#24577;&#30693;&#35782;&#65292;&#21306;&#20998;&#39640;&#25928;&#30340;&#24433;&#21709;&#32773;&#21644;&#20854;&#20182;&#24433;&#21709;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
As influencers play considerable roles in social media marketing, companies increase the budget for influencer marketing. Hiring effective influencers is crucial in social influencer marketing, but it is challenging to find the right influencers among hundreds of millions of social media users. In this paper, we propose InfluencerRank that ranks influencers by their effectiveness based on their posting behaviors and social relations over time. To represent the posting behaviors and social relations, the graph convolutional neural networks are applied to model influencers with heterogeneous networks during different historical periods. By learning the network structure with the embedded node features, InfluencerRank can derive informative representations for influencers at each period. An attentive recurrent neural network finally distinguishes highly effective influencers from other influencers by capturing the knowledge of the dynamics of influencer representations over time. Extensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;MF-PPO&#65289;&#65292;&#20197;&#31283;&#23450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01547</link><description>&lt;p&gt;
&#23545;&#31574;&#30053;&#26356;&#26032;&#30340;&#27491;&#21017;&#21270;&#20197;&#31283;&#23450;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Regularization of the policy updates for stabilizing Mean Field Games. (arXiv:2304.01547v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;MF-PPO&#65289;&#65292;&#20197;&#31283;&#23450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20010;&#20307;&#22238;&#25253;&#12290;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#25193;&#22823;&#26102;&#65292;&#30001;&#20110;&#35768;&#22810;&#26234;&#33021;&#20307;&#24341;&#20837;&#30340;&#38750;&#38745;&#27490;&#24615;&#65292;&#20250;&#20135;&#29983;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20381;&#38752;&#23545;&#31216;&#24615;&#21644;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#36817;&#20284;&#20855;&#26377;&#24456;&#22823;&#32676;&#20307;&#30340;&#21338;&#24328;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#29992;&#20110;&#23558;MFG&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22810;&#29366;&#24577;&#30340;&#21338;&#24328;&#20013;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24179;&#28369;&#25216;&#26415;&#65292;&#22914;&#23545;q&#20540;&#25110;&#22343;&#22330;&#20998;&#24067;&#26356;&#26032;&#36827;&#34892;&#24179;&#22343;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22343;&#22330;&#31574;&#30053;&#19978;&#36827;&#34892;&#36817;&#20284;&#26356;&#26032;&#20197;&#31283;&#23450;&#23398;&#20064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#21629;&#21517;&#20026;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MF-PPO&#65289;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm \textit{Mean Field Proximal Policy Optimization (MF-PPO)}, and we empirically show the effectiveness of our method in the OpenSpiel framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01432</link><description>&lt;p&gt;
&#38477;&#20302;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frank-Wolfe&#31639;&#27861;&#26159;&#32467;&#26500;&#21463;&#38480;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#24555;&#36895;&#36845;&#20195;&#22797;&#26434;&#24230;&#32780;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#30001;&#20110;&#27493;&#38271;&#26041;&#21521;&#30340;&#19981;&#35268;&#21017;&#38663;&#33633;&#32780;&#38590;&#20197;&#21152;&#36895;&#65292;&#21363;&#20351;&#22312;&#25509;&#36817;&#35299;&#30340;&#28176;&#36817;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#31163;&#25955;&#21270;&#30340;&#20135;&#29289;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;Frank-Wolfe&#30340;&#27969;&#65288;&#21363;&#28176;&#36817;&#23567;&#27493;&#38271;&#24773;&#20917;&#19979;&#30340;&#36712;&#36857;&#65289;&#19981;&#20250;&#20986;&#29616;&#19981;&#35268;&#21017;&#38663;&#33633;&#65292;&#22240;&#27492;&#20943;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#23558;&#19982;&#20135;&#29983;&#26356;&#31283;&#23450;&#30340;&#26041;&#27861;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#65306;&#19968;&#20010;&#22810;&#27493;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#21644;&#19968;&#20010;&#20855;&#26377;&#38477;&#20302;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#22312;&#19968;&#33324;&#20984;&#38598;&#19978;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#20174;$O(1/k)$&#21152;&#36895;&#21040;$O(1/k^{3/2})$ &#12290;
&lt;/p&gt;
&lt;p&gt;
The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65293;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.02155</link><description>&lt;p&gt;
ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22312;&#32447;&#20114;&#21160;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#30340;&#36827;&#21270;&#24341;&#25806;&#65288;arXiv:2303.02155v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design. (arXiv:2303.02155v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65293;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#31185;&#23398;&#30028;&#25472;&#36215;&#39118;&#26292;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#29616;&#29366;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#24037;&#20855;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20986;&#22855;&#22320;&#23436;&#25104;&#26377;&#25361;&#25112;&#24615;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#20195;&#30721;&#21644;&#24212;&#29992;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20889;&#25925;&#20107;&#12289;&#38899;&#20048;&#29255;&#27573;&#31561;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20132;&#20114;&#24335;&#36827;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#28216;&#25103;&#35774;&#35745;&#26694;&#26550;&#65292;&#20197;&#27169;&#25311;&#20856;&#22411;&#30340;&#20154;&#31867;&#35774;&#35745;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32773;&#21033;&#29992;&#29992;&#25143;&#30340;&#21453;&#39304;&#36873;&#25321;&#26368;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#22797;&#26434;&#30340;&#21019;&#36896;&#24615;&#20219;&#21153;&#8212;&#8212;&#24605;&#24819;&#30340;&#37325;&#32452;&#21644;&#21464;&#24322;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#36807;&#31243;&#22987;&#20110;&#19968;&#20010;&#31616;&#35201;&#35828;&#26126;&#21644;&#19968;&#32452;&#20505;&#36873;&#35774;&#35745;&#65292;&#36825;&#20123;&#35774;&#35745;&#26159;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#25110;&#30001;&#29992;&#25143;&#25552;&#20986;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#29992;&#25143;&#36890;&#36807;&#21521;&#20132;&#20114;&#24335;&#36951;&#20256;&#31639;&#27861;&#25552;&#20379;&#21453;&#39304;&#26469;&#21327;&#20316;&#35774;&#35745;&#36807;&#31243;&#65292;&#35813;&#31639;&#27861;&#36873;&#25321;&#12289;&#37325;&#32452;&#21644;&#31361;&#21464;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task - the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mu
&lt;/p&gt;</description></item><item><title>TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.12601</link><description>&lt;p&gt;
"&#19968;&#31181;&#36866;&#24212;&#25110;&#28781;&#20129;&#30340;&#23616;&#38754;": &#28216;&#25103;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#23545;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
"An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals. (arXiv:2302.12601v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12601
&lt;/p&gt;
&lt;p&gt;
TTIG&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19987;&#19994;&#20154;&#22763;&#23545;&#20110;TTIG&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#23384;&#22312;12&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;(TTIG)&#27169;&#22411;&#26159;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#34917;&#20805;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#65292;&#24320;&#22987;&#19982;&#19987;&#19994;&#21019;&#20316;&#32773;&#30340;&#20316;&#21697;&#31454;&#20105;&#24182;&#24341;&#21457;&#20102;&#26377;&#20851;&#21019;&#20316;&#24037;&#20316;&#12289;&#22833;&#19994;&#21644;&#29256;&#26435;&#31561;&#37325;&#35201;&#24433;&#21709;&#30340;&#35752;&#35770;&#12290;&#20026;&#20102;&#25903;&#25345;TTIG&#30340;&#21487;&#25345;&#32493;&#37319;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#19987;&#19994;&#20154;&#22763;&#24863;&#30693;&#12289;&#37319;&#29992;&#21644;&#20351;&#29992;TTIG&#30340;&#20016;&#23500;&#12289;&#21487;&#38752;&#21644;&#36879;&#26126;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#36777;&#35770;&#27973;&#34180;&#12289;&#29421;&#31364;&#19988;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#23398;&#26415;&#24037;&#20316;&#21017;&#38598;&#20013;&#20110;&#30740;&#31350;TTIG&#22312;&#19968;&#33324;&#33402;&#26415;&#23478;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#29305;&#23450;&#34892;&#19994;&#30340;&#19987;&#19994;&#20154;&#22763;&#30340;&#24863;&#30693;&#21644;&#24577;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#33452;&#20848;&#28216;&#25103;&#34892;&#19994;&#36827;&#34892;&#20102;&#19968;&#39033;&#23450;&#24615;&#30340;&#25506;&#32034;&#24615;&#35775;&#35848;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;TTIG&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;14&#20010;&#28216;&#25103;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#27169;&#26495;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;12&#20010;&#24635;&#20307;&#20027;&#39064;&#65292;&#32467;&#26500;&#21270;&#25104;49&#20010;&#23376;&#20027;&#39064;&#65292;&#25506;&#35752;&#20102;TTIG&#30340;&#24212;&#29992;&#21644;&#35748;&#30693;&#31561;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description. These models have begun to rival the work of professional creatives, and sparked discussions on the future of creative work, loss of jobs, and copyright issues, amongst other important implications. To support the sustainable adoption of TTIG, we must provide rich, reliable and transparent insights into how professionals perceive, adopt and use TTIG. Crucially though, the public debate is shallow, narrow and lacking transparency, while academic work has focused on studying the use of TTIG in a general artist population, but not on the perceptions and attitudes of professionals in a specific industry. In this paper, we contribute a qualitative, exploratory interview study on TTIG in the Finnish videogame industry. Through a Template Analysis on semi-structured interviews with 14 game professionals, we reveal 12 overarching themes, structured into 49 sub-themes on pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25552;&#39640;&#20154;&#26426;&#29289;&#29702;&#21327;&#20316;&#23433;&#20840;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24418;&#24577;&#21644;&#36816;&#21160;&#33021;&#21147;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#20154;&#26426;&#20132;&#20114;&#36807;&#31243;&#20013;&#21457;&#29983;&#30896;&#25758;&#21644;&#21463;&#20260;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2302.11933</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25552;&#39640;&#20154;&#26426;&#29289;&#29702;&#21327;&#20316;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving safety in physical human-robot collaboration via deep metric learning. (arXiv:2302.11933v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25552;&#39640;&#20154;&#26426;&#29289;&#29702;&#21327;&#20316;&#23433;&#20840;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24418;&#24577;&#21644;&#36816;&#21160;&#33021;&#21147;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#20154;&#26426;&#20132;&#20114;&#36807;&#31243;&#20013;&#21457;&#29983;&#30896;&#25758;&#21644;&#21463;&#20260;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#19982;&#26426;&#22120;&#20154;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#22312;&#28789;&#27963;&#29983;&#20135;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#27809;&#26377;&#20445;&#25252;&#26629;&#26639;&#30340;&#26426;&#22120;&#20154;&#23545;&#25805;&#20316;&#32773;&#30340;&#39118;&#38505;&#20063;&#26356;&#22823;&#12290;&#20026;&#20102;&#20445;&#25345;&#39118;&#38505;&#20302;&#65292;&#35268;&#23450;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#25805;&#20316;&#25514;&#26045;&#65292;&#20363;&#22914;&#22914;&#26524;&#26377;&#29289;&#29702;&#25509;&#35302;&#25110;&#36829;&#21453;&#23433;&#20840;&#36317;&#31163;&#21017;&#20572;&#27490;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#36825;&#26679;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36991;&#20813;&#20154;&#21592;&#21463;&#20260;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#20849;&#21516;&#28857;&#22312;&#20110;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30495;&#27491;&#21512;&#20316;&#20960;&#20046;&#19981;&#21487;&#33021;&#65292;&#22240;&#27492;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#21183;&#12290;&#22312;&#20154;&#26426;&#21327;&#20316;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#33021;&#22815;&#36866;&#24212;&#25805;&#20316;&#32773;&#21644;/&#25110;&#24403;&#21069;&#24773;&#20917;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#33258;&#30001;&#26426;&#22120;&#20154;&#31227;&#21160;&#26399;&#38388;&#65292;&#24517;&#39035;&#20801;&#35768;&#29289;&#29702;&#25509;&#35302;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#20132;&#20114;&#65292;&#32780;&#19981;&#34987;&#35270;&#20026;&#30896;&#25758;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#26410;&#26469;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25552;&#39640;&#20154;&#26426;&#29289;&#29702;&#21327;&#20316;&#23433;&#20840;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24418;&#24577;&#21644;&#36816;&#21160;&#33021;&#21147;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#20154;&#26426;&#20132;&#20114;&#36807;&#31243;&#20013;&#21457;&#29983;&#30896;&#25758;&#21644;&#21463;&#20260;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#23433;&#20840;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct physical interaction with robots is becoming increasingly important in flexible production scenarios, but robots without protective fences also pose a greater risk to the operator. In order to keep the risk potential low, relatively simple measures are prescribed for operation, such as stopping the robot if there is physical contact or if a safety distance is violated. Although human injuries can be largely avoided in this way, all such solutions have in common that real cooperation between humans and robots is hardly possible and therefore the advantages of working with such systems cannot develop its full potential. In human-robot collaboration scenarios, more sophisticated solutions are required that make it possible to adapt the robot's behavior to the operator and/or the current situation. Most importantly, during free robot movement, physical contact must be allowed for meaningful interaction and not recognized as a collision. However, here lies a key challenge for future 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BDR&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#36890;&#36807;BDR&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08007</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#20139;&#24494;&#25351;&#25968;&#65292;&#24494;&#23567;&#30340;&#20301;&#31227;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
With Shared Microexponents, A Little Shifting Goes a Long Way. (arXiv:2302.08007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BDR&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#36890;&#36807;BDR&#65292;&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22359;&#25968;&#25454;&#34920;&#31034;&#65288;BDR&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#32034;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31364;&#31934;&#24230;&#26684;&#24335;&#12290;&#23427;&#33021;&#22815;&#27604;&#36739;&#27969;&#34892;&#30340;&#37327;&#21270;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;BDR&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#65288;MX&#65289;&#30340;&#26032;&#26684;&#24335;&#65292;&#20854;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#31364;&#31934;&#24230;&#28014;&#28857;&#21644;&#22359;&#28014;&#28857;&#12290;MX &#22312;&#30828;&#20214;&#20013;&#21033;&#29992;&#22810;&#20010;&#37327;&#21270;&#32423;&#21035;&#65292;&#20351;&#29992;&#22522;&#20110;&#20849;&#20139;&#24494;&#25351;&#25968;&#30340;&#36229;&#32454;&#32553;&#25918;&#22240;&#23376;&#12290;MX &#30340;&#26377;&#25928;&#24615;&#22312;&#21253;&#25324;&#22823;&#35268;&#27169;&#29983;&#25104;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20197;&#21450;&#29983;&#20135;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#22312;&#20869;&#30340;&#23454;&#38469;&#27169;&#22411;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.04054</link><description>&lt;p&gt;
&#36861;&#27714;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#25512;&#29702;&#22797;&#29616;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#32771;&#34385;&#22810;&#20010;&#26041;&#24046;&#26469;&#28304;&#21450;&#20854;&#19982;&#25968;&#25454;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#35780;&#20272;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#8212;&#8212;&#21363;&#22312;&#22797;&#21046;&#30340;&#27169;&#22411;&#35757;&#32451;&#36816;&#34892;&#20013;&#35266;&#23519;&#21040;&#30340;&#35780;&#20272;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21463;&#21040;&#20960;&#31181;&#38750;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#27979;&#37327;&#22122;&#22768;&#12290;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#21435;&#38500;&#22122;&#22768;&#65292;&#20197;&#24378;&#21046;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#24573;&#30053;&#20102;&#23454;&#29616;&#23618;&#38754;&#22266;&#26377;&#30340;&#38750;&#30830;&#23450;&#24615;&#20197;&#21450;&#31639;&#27861;&#22122;&#22768;&#22240;&#32032;&#21644;&#25968;&#25454;&#29305;&#24615;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#36825;&#38480;&#21046;&#20102;&#20174;&#36825;&#20123;&#23454;&#39564;&#20013;&#21487;&#20197;&#24471;&#20986;&#30340;&#32467;&#35770;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23558;&#20960;&#20010;&#26041;&#24046;&#26469;&#28304;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982;&#25968;&#25454;&#29305;&#24615;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30340;&#26174;&#33879;&#24615;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#20013;&#65292;&#20197;&#26399;&#20174;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#23450;&#23454;&#20363;&#24471;&#20986;&#25512;&#29702;&#32467;&#35770;, &#32780;&#38750;&#21435;&#38500;&#22122;&#22768;&#12290;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65288;LMEM&#65289;&#26469;&#20998;&#26512;&#24615;&#33021;&#35780;&#20272;&#20998;&#25968;&#65292;&#24182;&#29992;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#24335;&#26469;&#32771;&#34385;&#31639;&#27861;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#28304;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#21508;&#20010;&#26041;&#24046;&#26469;&#28304;&#23545;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26356;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#20108;&#32500;&#27979;&#37327;&#27010;&#24565;&#8212;&#8212;&#36866;&#24403;&#20381;&#36182;&#24615;&#65288;AoR&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#20998;&#26512;AI&#24314;&#35758;&#35299;&#37322;&#30340;&#25928;&#26524;&#26469;&#20174;&#26681;&#26412;&#19978;&#36129;&#29486;&#20102;&#23545;&#20110;&#20381;&#36182;&#34892;&#20026;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.02187</link><description>&lt;p&gt;
&#21512;&#29702;&#20381;&#36182;AI&#24314;&#35758;&#30340;&#27010;&#24565;&#21270;&#21450;&#35299;&#37322;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations. (arXiv:2302.02187v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#20108;&#32500;&#27979;&#37327;&#27010;&#24565;&#8212;&#8212;&#36866;&#24403;&#20381;&#36182;&#24615;&#65288;AoR&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#23454;&#39564;&#20013;&#20998;&#26512;AI&#24314;&#35758;&#35299;&#37322;&#30340;&#25928;&#26524;&#26469;&#20174;&#26681;&#26412;&#19978;&#36129;&#29486;&#20102;&#23545;&#20110;&#20381;&#36182;&#34892;&#20026;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#24314;&#35758;&#22312;&#25237;&#36164;&#21644;&#21307;&#30103;&#20915;&#31574;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#30001;&#20110;&#36825;&#20123;&#24314;&#35758;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#20915;&#31574;&#32773;&#24517;&#39035;&#33258;&#34892;&#20915;&#23450;&#26159;&#21542;&#23454;&#38469;&#19978;&#36981;&#24490;&#36825;&#20123;&#24314;&#35758;&#65306;&#20182;&#20204;&#24517;&#39035;&#8220;&#36866;&#24403;&#22320;&#8221;&#20381;&#36182;&#20110;&#27491;&#30830;&#30340;&#24314;&#35758;&#24182;&#25298;&#32477;&#38169;&#35823;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#36866;&#24403;&#20381;&#36182;&#30340;&#30740;&#31350;&#20173;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#23450;&#20041;&#20197;&#21450;&#19968;&#20010;&#25805;&#20316;&#24615;&#30340;&#27979;&#37327;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#36827;&#34892;&#28145;&#20837;&#30340;&#34892;&#20026;&#23454;&#39564;&#26469;&#24110;&#21161;&#29702;&#35299;&#24433;&#21709;&#36825;&#31181;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24403;&#20381;&#36182;&#24615;&#65288;AoR&#65289;&#20316;&#20026;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#20108;&#32500;&#27979;&#37327;&#27010;&#24565;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30740;&#31350;&#27169;&#22411;&#65292;&#20998;&#26512;&#25552;&#20379;AI&#24314;&#35758;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;&#22312;&#19968;&#39033;&#28041;&#21450;200&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;AoR&#65292;&#20174;&#32780;&#24433;&#21709;AI&#24314;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20381;&#36182;&#34892;&#20026;&#30340;&#20998;&#26512;&#36129;&#29486;&#20102;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI advice is becoming increasingly popular, e.g., in investment and medical treatment decisions. As this advice is typically imperfect, decision-makers have to exert discretion as to whether actually follow that advice: they have to "appropriately" rely on correct and turn down incorrect advice. However, current research on appropriate reliance still lacks a common definition as well as an operational measurement concept. Additionally, no in-depth behavioral experiments have been conducted that help understand the factors influencing this behavior. In this paper, we propose Appropriateness of Reliance (AoR) as an underlying, quantifiable two-dimensional measurement concept. We develop a research model that analyzes the effect of providing explanations for AI advice. In an experiment with 200 participants, we demonstrate how these explanations influence the AoR, and, thus, the effectiveness of AI advice. Our work contributes fundamental concepts for the analysis of reliance behavior and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.08243</link><description>&lt;p&gt;
&#20855;&#26377;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#29983;&#25104;&#26041;&#27861;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Image-based Joint-Embedding Predictive Architecture&#65288;I-JEPA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#21644;&#25513;&#27169;&#31574;&#30053;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#39640;&#24230;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;I-JEPA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#22270;&#20687;&#20013;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#38750;&#29983;&#25104;&#26041;&#27861;&#12290;I-JEPA&#30340;&#26680;&#24515;&#35774;&#35745;&#36873;&#25321;&#26159;&#25513;&#27169;&#31574;&#30053;&#65292;&#20197;&#24341;&#23548;I-JEPA&#20135;&#29983;&#35821;&#20041;&#34920;&#31034;&#12290;&#24403;&#19982;Vision Transformers&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#35777;&#26126;I-JEPA&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#25915;&#20987;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#22823;&#37327;&#36755;&#20986;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#21028;&#26029;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19982;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36873;&#21462;&#36866;&#24403;&#24230;&#37327;&#26631;&#20934;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2212.06008</link><description>&lt;p&gt;
&#35841;&#26469;&#35780;&#20272;&#35780;&#20272;&#32773;&#65311;&#20851;&#20110;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25915;&#20987;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#33258;&#21160;&#24230;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators. (arXiv:2212.06008v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#25915;&#20987;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#22823;&#37327;&#36755;&#20986;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#21028;&#26029;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19982;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36873;&#21462;&#36866;&#24403;&#24230;&#37327;&#26631;&#20934;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20986;&#21457;&#33258;&#21160;&#32534;&#20889;&#31243;&#24207;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#20195;&#30721;&#29983;&#25104;&#22120;&#24050;&#32463;&#34987;&#29992;&#20110;&#36827;&#34892;&#20262;&#29702;&#40657;&#23458;&#21644;&#25915;&#20987;&#24615;&#23433;&#20840;&#27979;&#35797;&#65292;&#20197;&#29983;&#25104;&#25915;&#20987;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#35780;&#20272;&#20173;&#28982;&#38754;&#20020;&#30528;&#35768;&#22810;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#36755;&#20986;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35745;&#31639;&#29983;&#25104;&#20195;&#30721;&#19982;&#21442;&#32771;&#32763;&#35793;&#20043;&#38388;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#28165;&#26970;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#21738;&#31181;&#24230;&#37327;&#26631;&#20934;&#26368;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;&#25915;&#20987;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#22823;&#37327;&#36755;&#20986;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#24212;&#29992;&#20110;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20351;&#29992;&#21253;&#21547;&#33521;&#35821;&#35821;&#35328;&#25551;&#36848;&#30340;&#25915;&#20987;&#35013;&#37197;&#20195;&#30721;&#21644;Python&#20195;&#30721;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#30340;&#20272;&#35745;&#20540;&#19982;&#29983;&#25104;&#20195;&#30721;&#30340;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#20004;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#30340;&#36873;&#25321;&#23545;&#29983;&#25104;&#22120;&#30340;&#35780;&#20272;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#19981;&#36275;&#20197;&#25429;&#25417;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#21462;&#36866;&#24403;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#25351;&#21335;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#39033;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (Neural Machine Translation, NMT). In particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. Unfortunately, the evaluation of code generators still faces several issues. The current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. However, it is not clear what metric to use, and which metric is most suitable for specific contexts. This work analyzes a large set of output similarity metrics on offensive code generators. We apply the metrics on two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their descriptions in the English language. We compare the estimates from the automatic metrics with human ev
&lt;/p&gt;</description></item><item><title>RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05961</link><description>&lt;p&gt;
RPN: &#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#35789;&#21521;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05961
&lt;/p&gt;
&lt;p&gt;
RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;&#38543;&#26426;&#20301;&#32622;&#22122;&#22768;&#65288;RPN&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#35789;&#21521;&#37327;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;RPN&#36890;&#36807;&#26681;&#25454;&#36873;&#23450;&#35789;&#21521;&#37327;&#30340;&#29616;&#26377;&#20540;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#20801;&#35768;&#26356;&#32454;&#31890;&#24230;&#30340;&#20462;&#25913;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#65292;RPN&#19981;&#38656;&#35201;&#35745;&#31639;&#22270;&#20013;&#30340;&#26799;&#24230;&#26469;&#36827;&#34892;&#34394;&#25311;&#26679;&#26412;&#26356;&#26032;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#31561;&#65292;RPN&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;(FFL)&#65292;&#21487;&#20197;&#21327;&#21516;&#35757;&#32451;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#26426;&#26500;&#38388;&#30340;&#24191;&#27867;&#21512;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13606</link><description>&lt;p&gt;
&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38750;&#22343;&#19968;&#26631;&#31614;&#30340;&#21327;&#21516;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Collaborative Training of Medical Artificial Intelligence Models with non-uniform Labels. (arXiv:2211.13606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;(FFL)&#65292;&#21487;&#20197;&#21327;&#21516;&#35757;&#32451;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#26426;&#26500;&#38388;&#30340;&#24191;&#27867;&#21512;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#20027;&#27969;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#24378;&#22823;&#32780;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#26041;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#24050;&#32463;&#25552;&#20379;&#20102;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#26159;&#36825;&#20123;&#25968;&#25454;&#30340;&#26631;&#31614;&#26041;&#24335;&#24046;&#24322;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65306;&#28789;&#27963;&#32852;&#21512;&#23398;&#20064;(FFL)&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;&#20840;&#29699;&#20116;&#20010;&#26426;&#26500;&#30340;695,000&#20010;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#33016;&#37096;&#36879;&#35270;&#22270;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24322;&#36136;&#21270;&#26631;&#27880;&#25968;&#25454;&#38598;&#24773;&#20917;&#19979;&#20351;&#29992;FFL&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#24191;&#27867;&#21327;&#20316;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid advancements in recent years, medical image analysis is largely dominated by deep learning (DL). However, building powerful and robust DL models requires training with large multi-party datasets. While multiple stakeholders have provided publicly available datasets, the ways in which these data are labeled vary widely. For Instance, an institution might provide a dataset of chest radiographs containing labels denoting the presence of pneumonia, while another institution might have a focus on determining the presence of metastases in the lung. Training a single AI model utilizing all these data is not feasible with conventional federated learning (FL). This prompts us to propose an extension to the widespread FL process, namely flexible federated learning (FFL) for collaborative training on such data. Using 695,000 chest radiographs from five institutions from across the globe each with differing labels - we demonstrate that having heterogeneously labeled datasets, FF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.17467</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Iterative Teaching by Data Hallucination. (arXiv:2210.17467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#30340;&#36845;&#20195;&#24335;&#25945;&#23398;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#20013;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19979;&#36845;&#20195;&#25945;&#23398;&#30340;&#38382;&#39064;&#65292;&#21363;&#25945;&#24072;&#26681;&#25454;&#23398;&#20064;&#32773;&#30340;&#29366;&#24577;&#21644;&#30446;&#26631;&#27010;&#24565;&#25552;&#20379;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#24187;&#35937;&#25945;&#23398;&#65288;DHT&#65289;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#30340;&#26679;&#26412;&#31354;&#38388;&#20869;&#65292;&#36890;&#36807;&#26234;&#33021;&#22320;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;&#25552;&#20379;&#31034;&#20363;&#30340;&#33021;&#21147;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35768;&#22810;&#25361;&#25112;&#24615;&#30340;&#25945;&#23398;&#35774;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;DHT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of iterative machine teaching, where a teacher sequentially provides examples based on the status of a learner under a discrete input space (i.e., a pool of finite samples), which greatly limits the teacher's capability. To address this issue, we study iterative teaching under a continuous input space where the input example (i.e., image) can be either generated by solving an optimization problem or drawn directly from a continuous distribution. Specifically, we propose data hallucination teaching (DHT) where the teacher can generate input data intelligently based on labels, the learner's status and the target concept. We study a number of challenging teaching setups (e.g., linear/neural learners in omniscient and black-box settings). Extensive empirical results verify the effectiveness of DHT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22788;&#29702;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#26102;&#65292;&#23545;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#25152;&#37319;&#21462;&#30340;&#31435;&#22330;&#20197;&#21450;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#22312;&#19981;&#21033;&#24433;&#21709;&#26041;&#38754;&#20135;&#29983;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#21644;&#21487;&#33021;&#24341;&#36215;&#30340;&#35780;&#20272;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2210.09014</link><description>&lt;p&gt;
&#22788;&#29702;&#31639;&#27861;&#65288;&#35823;&#65289;&#20449;&#24687;&#20998;&#31867;&#20013;&#30340;&#20598;&#28982;&#24615;&#65306;&#36208;&#21521;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda. (arXiv:2210.09014v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#22788;&#29702;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#26102;&#65292;&#23545;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#25152;&#37319;&#21462;&#30340;&#31435;&#22330;&#20197;&#21450;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#22312;&#19981;&#21033;&#24433;&#21709;&#26041;&#38754;&#20135;&#29983;&#30340;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#21644;&#21487;&#33021;&#24341;&#36215;&#30340;&#35780;&#20272;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#35299;&#20915;&#24222;&#22823;&#19988;&#36895;&#24230;&#24555;&#30340;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21644;&#20854;&#20182;&#21487;&#33021;&#34987;&#35782;&#21035;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#22312;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#23545;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#8220;&#30495;&#30456;&#8221;&#26469;&#28304;&#30340;&#21512;&#27861;&#24615;&#12289;&#26435;&#23041;&#24615;&#21644;&#23458;&#35266;&#24615;&#37319;&#21462;&#31435;&#22330;&#12290;&#36825;&#28041;&#21450;&#25919;&#27835;&#12289;&#20262;&#29702;&#21644;&#35748;&#35782;&#35770;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#25216;&#26415;&#35770;&#25991;&#20013;&#24456;&#23569;&#24471;&#21040;&#35299;&#20915;&#12290;&#23613;&#31649;&#65288;&#20063;&#23601;&#26159;&#30001;&#20110;&#65289;&#20854;&#25253;&#21578;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#65292;&#30001;ML&#39537;&#21160;&#30340;&#23457;&#26597;&#31995;&#32479;&#21487;&#33021;&#20250;&#22609;&#36896;&#22312;&#32447;&#20844;&#20849;&#36777;&#35770;&#65292;&#24182;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#19981;&#24403;&#23457;&#26597;&#21644;&#24378;&#21270;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#37319;&#29992;&#21512;&#20316;&#30340;&#27665;&#26063;&#24535;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#23545;&#24314;&#31435;&#65288;&#35823;&#65289;&#20449;&#24687;&#20998;&#31867;&#30340;ML&#27169;&#22411;&#30340;&#36807;&#31243;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#19968;&#31995;&#21015;&#31639;&#27861;&#30340;&#20598;&#28982;&#24615;&#8212;&#8212;&#20851;&#38190;&#30340;&#27169;&#22411;&#20915;&#31574;&#28857;&#65292;&#36825;&#20123;&#20915;&#31574;&#28857;&#21487;&#33021;&#20250;&#22312;&#25216;&#26415;&#23454;&#29616;&#20013;&#24341;&#20837;&#33258;&#24049;&#30340;&#35808;&#37322;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#21644;&#21512;&#20316;&#20248;&#21270;&#22522;&#20110;&#20998;&#37197;&#31574;&#30053;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#25913;&#21892;SAMS&#36710;&#38431;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36739;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.08659</link><description>&lt;p&gt;
&#38754;&#21521;&#20849;&#20139;&#33258;&#21160;&#39550;&#39542;&#20986;&#34892;&#26381;&#21153;&#30340;&#39044;&#27979;&#24335;&#36710;&#38431;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Fleet Repositioning for Shared-use Autonomous Mobility Services: An Optimization and Learning-Based Approach. (arXiv:2210.08659v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#21644;&#21512;&#20316;&#20248;&#21270;&#22522;&#20110;&#20998;&#37197;&#31574;&#30053;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#20174;&#32780;&#25913;&#21892;SAMS&#36710;&#38431;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#36739;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20986;&#34892;&#26381;&#21153;&#12289;&#20016;&#23500;&#30340;&#20132;&#36890;&#25968;&#25454;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21457;&#23637;&#20026;&#20849;&#20139;&#33258;&#21160;&#39550;&#39542;&#20986;&#34892;&#26381;&#21153;&#65288;SAMS&#65289;&#25552;&#20379;&#20102;&#37325;&#35201;&#26426;&#36935;&#65292;&#20197;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#38656;&#27714;&#21709;&#24212;&#24615;&#30340;&#20010;&#20154;&#20986;&#34892;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#39044;&#27979;&#24615;&#22320;&#37325;&#26032;&#35843;&#24230;&#31354;&#38386;&#30340;&#36710;&#36742;&#65292;&#25552;&#39640;SAMS&#36710;&#38431;&#30340;&#25928;&#29575;&#21644;&#26381;&#21153;&#36136;&#37327;&#12290;&#25226;&#20877;&#24179;&#34913;&#38382;&#39064;&#20316;&#20026;&#39532;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;A2C&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#39044;&#27979;&#26410;&#26469;&#38656;&#27714;&#24182;&#19982;&#22522;&#20110;&#20248;&#21270;&#30340;&#20998;&#37197;&#31574;&#30053;&#21512;&#20316;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#38598;&#20013;&#24335;&#30340;&#37325;&#26032;&#23450;&#20301;&#20915;&#31574;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#27773;&#36710;&#36710;&#38431;&#65292;&#22240;&#20026;&#38382;&#39064;&#22823;&#23567;&#19981;&#36229;&#36807;Fleetsize^2&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of mobility-on-demand services, rich transportation data sources, and autonomous vehicles (AVs) creates significant opportunities for shared-use AV mobility services (SAMSs) to provide accessible and demand-responsive personal mobility. SAMS fleet operation involves multiple interrelated decisions, with a primary focus on efficiently fulfilling passenger ride requests with a high level of service quality. This paper focuses on improving the efficiency and service quality of a SAMS vehicle fleet via anticipatory repositioning of idle vehicles. The rebalancing problem is formulated as a Markov Decision Process, which we propose solving using an advantage actor critic (A2C) reinforcement learning-based method. The proposed approach learns a rebalancing policy that anticipates future demand and cooperates with an optimization-based assignment strategy. The approach allows for centralized repositioning decisions and can handle large vehicle fleets since the problem size does
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.07474</link><description>&lt;p&gt;
SQA3D&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#22312;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#12290;&#22522;&#20110;650&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#20026;&#26234;&#33021;&#20195;&#29702;&#20154;&#30340;&#25512;&#29702;&#33021;&#21147;&#32771;&#23519;&#25552;&#20379;&#20102;&#24191;&#27867;&#19988;&#22823;&#37327;&#30340;&#38382;&#39064;&#65292;&#36825;&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#20855;&#26377;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#65306;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#38382;&#31572;&#65288;SQA3D&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22330;&#26223;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#19977;&#32500;&#25195;&#25551;&#65289;&#65292;SQA3D&#35201;&#27714;&#32463;&#36807;&#27979;&#35797;&#30340;&#20195;&#29702;&#20154;&#39318;&#20808;&#29702;&#35299;&#20854;&#22312;&#25991;&#26412;&#25551;&#36848;&#19979;&#30340;3D&#22330;&#26223;&#20013;&#30340;&#24773;&#22659;&#65288;&#20301;&#32622;&#12289;&#26041;&#21521;&#31561;&#65289;&#65292;&#28982;&#21518;&#22312;&#35813;&#24773;&#22659;&#19979;&#36827;&#34892;&#25512;&#29702;&#65292;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#26469;&#33258;ScanNet&#30340;650&#20010;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#24515;&#22260;&#32469;6.8k&#20010;&#21807;&#19968;&#24773;&#22659;&#65292;20.4k&#30340;&#25551;&#36848;&#21644;33.4k&#22810;&#26679;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#23545;&#26234;&#33021;&#20195;&#29702;&#20154;&#33539;&#22260;&#24191;&#27867;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#32771;&#23519;&#65292;&#20174;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#21040;&#24120;&#35782;&#29702;&#35299;&#12289;&#23548;&#33322;&#21644;&#22810;&#36339;&#25512;&#29702;&#12290;SQA3D&#23545;&#24403;&#21069;&#30340;&#22810;&#27169;&#24335;&#23588;&#20854;&#26159;3D&#25512;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26368;&#20339;&#32467;&#26524;&#20165;&#36798;&#21040;&#20102;47.20%&#30340;&#24635;&#20307;&#24471;&#20998;&#65292;&#32780;&#19994;&#20313;&#27700;&#24179;&#30340;&#34920;&#29616;&#26356;&#20026;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#22312;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20687;&#32032;&#31354;&#38388;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2210.03142</link><description>&lt;p&gt;
&#20851;&#20110;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
On Distillation of Guided Diffusion Models. (arXiv:2210.03142v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#22312;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20687;&#32032;&#31354;&#38388;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#23427;&#20204;&#24050;&#24191;&#27867;&#29992;&#20110;&#21253;&#25324; DALLE-2&#12289;Stable Diffusion &#21644; Imagen &#22312;&#20869;&#30340;&#22823;&#35268;&#27169;&#25193;&#25955;&#26694;&#26550;&#20013;&#12290;&#28982;&#32780;&#65292;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#32570;&#28857;&#26159;&#65292;&#22312;&#25512;&#26029;&#26102;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#38656;&#35201;&#35780;&#20272;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#65288;&#19968;&#20010;&#31867;&#26377;&#26465;&#20214;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26080;&#26465;&#20214;&#30340;&#27169;&#22411;&#65289;&#25968;&#21313;&#21040;&#25968;&#30334;&#27425;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20813;&#20998;&#31867;&#22120;&#30340;&#24341;&#23548;&#24335;&#25193;&#25955;&#27169;&#22411;&#33976;&#39311;&#20026;&#26131;&#20110;&#37319;&#26679;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;: &#32473;&#23450;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#20813;&#20998;&#31867;&#22120;&#24341;&#23548;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#20197;&#21305;&#37197;&#32852;&#21512;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36880;&#27493;&#23558;&#35813;&#27169;&#22411;&#33976;&#39311;&#21040;&#21482;&#38656;&#35201;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#23545;&#20110;&#22312;&#20687;&#32032;&#31354;&#38388;&#35757;&#32451;&#30340;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate im
&lt;/p&gt;</description></item><item><title>PiFold &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644; PiGNN &#23618;&#65292;&#33021;&#22815;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#65292;&#22312; CATH 4.2 &#19978;&#36798;&#21040;&#20102; 51.66% &#30340;&#24674;&#22797;&#29575;&#65292;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#65292;&#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72% &#21644; 60.42% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.12643</link><description>&lt;p&gt;
PiFold: &#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#34507;&#30333;&#36136;&#36870;&#21521;&#25240;&#21472;
&lt;/p&gt;
&lt;p&gt;
PiFold: Toward effective and efficient protein inverse folding. (arXiv:2209.12643v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12643
&lt;/p&gt;
&lt;p&gt;
PiFold &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644; PiGNN &#23618;&#65292;&#33021;&#22815;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#65292;&#22312; CATH 4.2 &#19978;&#36798;&#21040;&#20102; 51.66% &#30340;&#24674;&#22797;&#29575;&#65292;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#65292;&#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72% &#21644; 60.42% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#35774;&#35745;&#33021;&#22815;&#25240;&#21472;&#25104;&#25152;&#38656;&#32467;&#26500;&#30340;&#34507;&#30333;&#24207;&#21015;&#65311;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;AI&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#34920;&#36798;&#19981;&#22815;&#20805;&#20998;&#21644;&#33258;&#22238;&#24402;&#24207;&#21015;&#35299;&#30721;&#22120;&#30340;&#32570;&#20047;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PiFold&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#31181;&#26032;&#30340;&#27531;&#22522;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;PiGNN&#23618;&#65292;&#20197;&#19968;&#27425;&#24615;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#24182;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PiFold &#22312; CATH 4.2 &#19978;&#33021;&#22815;&#36798;&#21040; 51.66\% &#30340;&#24674;&#22797;&#29575;&#65292;&#32780;&#25512;&#29702;&#36895;&#24230;&#27604;&#33258;&#22238;&#24402;&#31454;&#20105;&#23545;&#25163;&#24555; 70 &#20493;&#12290;&#27492;&#22806;&#65292;PiFold &#22312; TS50 &#21644; TS500 &#19978;&#20998;&#21035;&#36798;&#21040;&#20102; 58.72\% &#21644; 60.42\% &#30340;&#24674;&#22797;&#20998;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;&#34507;&#30333;&#36136;&#29305;&#24449;&#21644;&#27169;&#22411;&#35774;&#35745;&#30340;&#20316;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#31616;&#21270;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;PyTorch &#20195;&#30721;&#21487;&#22312; \href{https://github.com/idea-iitp/PiFold}{https://github.com/idea-iitp/PiFold} &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we design protein sequences folding into the desired structures effectively and efficiently? AI methods for structure-based protein design have attracted increasing attention in recent years; however, few methods can simultaneously improve the accuracy and efficiency due to the lack of expressive features and autoregressive sequence decoder. To address these issues, we propose PiFold, which contains a novel residue featurizer and PiGNN layers to generate protein sequences in a one-shot way with improved recovery. Experiments show that PiFold could achieve 51.66\% recovery on CATH 4.2, while the inference speed is 70 times faster than the autoregressive competitors. In addition, PiFold achieves 58.72\% and 60.42\% recovery scores on TS50 and TS500, respectively. We conduct comprehensive ablation studies to reveal the role of different types of protein features and model designs, inspiring further simplification and improvement. The PyTorch code is available at \href{https://gith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;PePe&#65292;&#22312;&#19968;&#20010;&#23454;&#26102;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21518;&#32534;&#36753;&#25968;&#25454;&#65292;&#32467;&#21512;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#21442;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.10139</link><description>&lt;p&gt;
PePe: &#21033;&#29992;&#29992;&#25143;&#29983;&#25104;&#30340;&#21518;&#32534;&#36753;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#21518;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PePe: Personalized Post-editing Model utilizing User-generated Post-edits. (arXiv:2209.10139v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;PePe&#65292;&#22312;&#19968;&#20010;&#23454;&#26102;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#25143;&#21518;&#32534;&#36753;&#25968;&#25454;&#65292;&#32467;&#21512;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#21442;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23558;&#20010;&#20154;&#30340;&#20559;&#22909;&#21152;&#20197;&#32771;&#34385;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#24050;&#32463;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#27491;&#30830;&#21453;&#26144;&#20010;&#20154;&#39118;&#26684;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#33258;&#21160;&#21518;&#32534;&#36753;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#32771;&#34385;&#21040;&#19981;&#21516;&#20010;&#20154;&#34892;&#20026;&#30340;&#21477;&#23376;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#26102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#25910;&#38598;&#21253;&#21547;&#29992;&#25143;&#20559;&#22909;&#30340;&#21518;&#32534;&#36753;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30495;&#23454;&#29992;&#25143;&#36755;&#20837;&#24819;&#35201;&#32763;&#35793;&#30340;&#28304;&#35821;&#21477;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#39118;&#26684;&#20559;&#22909;&#32534;&#36753;&#26426;&#22120;&#32763;&#35793;&#30340;&#36755;&#20986;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;APE&#26694;&#26550;&#19978;&#32467;&#21512;&#20102;&#19968;&#20010;&#21028;&#21035;&#27169;&#22359;&#21644;&#29992;&#25143;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21363;BLEU&#65292;TER&#65292;YiSi-1&#21644;&#20154;&#31867;&#35780;&#20272;&#65289;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating personal preference is crucial in advanced machine translation tasks. Despite the recent advancement of machine translation, it remains a demanding task to properly reflect personal style. In this paper, we introduce a personalized automatic post-editing framework to address this challenge, which effectively generates sentences considering distinct personal behaviors. To build this framework, we first collect post-editing data that connotes the user preference from a live machine translation system. Specifically, real-world users enter source sentences for translation and edit the machine-translated outputs according to the user's preferred style. We then propose a model that combines a discriminator module and user-specific parameters on the APE framework. Experimental results show that the proposed method outperforms other baseline models on four different metrics (i.e., BLEU, TER, YiSi-1, and human evaluation).
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#27969;&#34892;&#26041;&#27861;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#19981;&#21487;&#33021;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.15750</link><description>&lt;p&gt;
&#26080;&#38656;&#19981;&#21487;&#33021;&#25968;&#25454;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15750
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#27969;&#34892;&#26041;&#27861;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#19981;&#21487;&#33021;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#35299;&#20915;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35780;&#20272;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#20013;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#26159;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#32467;&#21512;&#20102;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#20123;&#36755;&#20837;&#25968;&#25454;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12289;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#65292;&#29978;&#33267;&#26159;&#36923;&#36753;&#19978;&#19981;&#21487;&#33021;&#30340;&#65292;&#30001;&#27492;&#24471;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#19982;&#40657;&#31665;&#35757;&#32451;&#25968;&#25454;&#26377;&#24456;&#22823;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#24403;&#35299;&#37322;&#20915;&#31574;&#26102;&#20351;&#29992;&#36825;&#20123;&#20540;&#26102;&#65292;&#29992;&#25143;&#19981;&#33021;&#20449;&#20219;&#39044;&#27979;&#31639;&#27861;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#21517;&#20026;Cohort Shapley&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#27982;&#21338;&#24328;&#29702;&#35770;&#65292;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#21338;&#24328;&#35770;&#26041;&#27861;&#19981;&#21516;&#65292;&#20165;&#20351;&#29992;&#23454;&#38469;&#35266;&#27979;&#21040;&#30340;&#25968;&#25454;&#26469;&#37327;&#21270;&#21464;&#37327;&#37325;&#35201;&#24615;&#12290;Cohort Shapley&#36890;&#36807;&#32553;&#23567;&#19982;&#30446;&#26631;&#23545;&#35937;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#29305;&#24449;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#32452;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;&#31639;&#27861;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#24517;&#39035;&#23558;&#37325;&#35201;&#24615;&#24402;&#22240;&#20110;&#27169;&#22411;&#26410;&#32463;&#35757;&#32451;&#30340;&#21463;&#20445;&#25252;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#65292;&#23427;&#19987;&#27880;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#40723;&#21169;&#24320;&#21457;&#26356;&#22909;&#30340;&#31639;&#27861;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#27867;&#21270;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2205.13803</link><description>&lt;p&gt;
Bongard-HOI&#65306;&#22522;&#20110;&#20154;-&#29289;&#20132;&#20114;&#30340;&#20960;&#31181;&#24773;&#20917;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#65292;&#23427;&#19987;&#27880;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#40723;&#21169;&#24320;&#21457;&#26356;&#22909;&#30340;&#31639;&#27861;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#27867;&#21270;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24403;&#28041;&#21450;&#21040;&#26032;&#27010;&#24565;&#30340;&#20960;&#31181;&#24773;&#20917;&#23398;&#20064;&#21644;&#32452;&#21512;&#25512;&#29702;&#26102;&#65292;&#20170;&#22825;&#30340;&#35270;&#35273;&#22270;&#26696;&#35782;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#35270;&#35273;&#35748;&#30693;&#20043;&#38388;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Bongard-HOI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#20391;&#37325;&#20110;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#23398;&#20064;&#20154;-&#29289;&#20132;&#20114;&#30340;&#32452;&#21512;&#24615;&#23398;&#20064;&#12290;&#23427;&#21463;&#21040;&#21476;&#20856;&#30340;Bongard&#38382;&#39064;&#65288;BPs&#65289;&#20013;&#30340;&#20004;&#20010;&#21487;&#21462;&#29305;&#24449;&#30340;&#21551;&#21457;&#65306;1&#65289;&#20960;&#31181;&#24773;&#20917;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;2&#65289;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#23569;&#26679;&#26412;&#23454;&#20363;&#21253;&#21547;&#22256;&#38590;&#30340;&#36127;&#20363;&#65292;&#20854;&#20013;&#27491;&#36127;&#22270;&#20687;&#20165;&#22312;&#21160;&#20316;&#26631;&#31614;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20165;&#20165;&#23545;&#23545;&#35937;&#31867;&#21035;&#30340;&#35782;&#21035;&#19981;&#36275;&#20197;&#23436;&#25104;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#31995;&#32479;&#30740;&#31350;&#35270;&#35273;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#21464;&#20102;&#20960;&#31181;&#24773;&#20917;&#23454;&#20363;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;HOI&#27010;&#24565;&#37325;&#21472;&#31243;&#24230;&#65292;&#20174;&#37096;&#20998;&#21040;&#27809;&#26377;&#37325;&#21472;&#12290;Bongard-HOI&#23545;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#20986;&#20102;&#23454;&#36136;&#24615;&#25361;&#25112;&#65292;&#24182;&#20026;&#24320;&#21457;&#21487;&#20197;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#24182;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;HOI&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant gap remains between today's visual pattern recognition models and human-level visual cognition especially when it comes to few-shot learning and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new visual reasoning benchmark that focuses on compositional learning of human-object interactions (HOIs) from natural images. It is inspired by two desirable characteristics from the classical Bongard problems (BPs): 1) few-shot concept learning, and 2) context-dependent reasoning. We carefully curate the few-shot instances with hard negatives, where positive and negative images only disagree on action labels, making mere recognition of object categories insufficient to complete our benchmarks. We also design multiple test sets to systematically study the generalization of visual learning models, where we vary the overlap of the HOI concepts between the training and test sets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a substanti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#21010;&#20998;&#20026;&#26356;&#23567;&#30340;&#25216;&#33021;&#65292;&#21033;&#29992;&#39034;&#24207;&#24402;&#32435;&#20559;&#35265;&#23558;&#25216;&#33021;&#38142;&#25509;&#20197;&#35299;&#20915;&#25972;&#20010;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#19968;&#20010;&#19987;&#23478;&#28436;&#31034;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2204.07404</link><description>&lt;p&gt;
&#20998;&#32780;&#27835;&#20043;&#30340;&#27169;&#20223;&#23398;&#20064; - Divide &amp; Conquer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Conquer Imitation Learning. (arXiv:2204.07404v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#21010;&#20998;&#20026;&#26356;&#23567;&#30340;&#25216;&#33021;&#65292;&#21033;&#29992;&#39034;&#24207;&#24402;&#32435;&#20559;&#35265;&#23558;&#25216;&#33021;&#38142;&#25509;&#20197;&#35299;&#20915;&#25972;&#20010;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#19968;&#20010;&#19987;&#23478;&#28436;&#31034;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#22870;&#21169;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#23398;&#20064;&#31639;&#27861;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21487;&#20197;&#26159;&#21551;&#21160;&#23398;&#20064;&#36807;&#31243;&#30340;&#26377;&#21147;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;IL&#26041;&#27861;&#38656;&#35201;&#22810;&#20010;&#19987;&#23478;&#28436;&#31034;&#65292;&#36825;&#21487;&#33021;&#24456;&#38590;&#33719;&#24471;&#12290;&#21482;&#26377;&#23569;&#25968;IL&#31639;&#27861;&#22312;&#26497;&#20302;&#30340;&#19987;&#23478;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21482;&#26377;&#19968;&#20010;&#19987;&#23478;&#28436;&#31034;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#36712;&#36857;&#30340;&#29366;&#24577;&#20013;&#27169;&#20223;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#22522;&#20110;&#39034;&#24207;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22797;&#26434;&#20219;&#21153;&#21010;&#20998;&#20026;&#26356;&#23567;&#30340;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#34987;&#23398;&#20064;&#25104;&#20026;&#19968;&#20010;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#29420;&#31435;&#22320;&#35299;&#20915;&#27599;&#20010;&#25216;&#33021;&#65292;&#24182;&#38142;&#25509;&#25216;&#33021;&#20197;&#35299;&#20915;&#25972;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27169;&#20223;&#20102;&#19968;&#20010;&#38750;&#23436;&#25972;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#25193;&#23637;&#21040;&#20102;&#22797;&#26434;&#30340;&#20223;&#30495;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
When cast into the Deep Reinforcement Learning framework, many robotics tasks require solving a long horizon and sparse reward problem, where learning algorithms struggle. In such context, Imitation Learning (IL) can be a powerful approach to bootstrap the learning process. However, most IL methods require several expert demonstrations which can be prohibitively difficult to acquire. Only a handful of IL algorithms have shown efficiency in the context of an extreme low expert data regime where a single expert demonstration is available. In this paper, we present a novel algorithm designed to imitate complex robotic tasks from the states of an expert trajectory. Based on a sequential inductive bias, our method divides the complex task into smaller skills. The skills are learned into a goal-conditioned policy that is able to solve each skill individually and chain skills to solve the entire task. We show that our method imitates a non-holonomic navigation task and scales to a complex sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#25913;&#21892;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.07138</link><description>&lt;p&gt;
&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38598;&#25104;&#65306;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#25913;&#21892;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26088;&#22312;&#30740;&#31350;&#20195;&#29702;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#12290;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36923;&#36753;&#20851;&#31995;&#21644;&#35268;&#21017;&#24341;&#20837;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#65292;&#20363;&#22914;&#21033;&#29992;&#33258;&#21160;&#35268;&#21010;&#25216;&#26415;&#12290;&#33258;&#21160;&#35268;&#21010;&#21363;AI&#35268;&#21010;&#65292;&#20391;&#37325;&#20110;&#26500;&#24314;&#31526;&#21495;&#22495;&#27169;&#22411;&#24182;&#32508;&#21512;&#35268;&#21010;&#65292;&#20197;&#22522;&#20110;&#22495;&#27169;&#22411;&#23558;&#21021;&#22987;&#29366;&#24577;&#36716;&#25442;&#20026;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#19982;&#36825;&#20004;&#20010;&#39046;&#22495;&#30456;&#20851;&#30340;&#24037;&#20316;&#65292;&#21487;&#20197;&#20135;&#29983;&#26174;&#24335;&#30693;&#35782;&#65292;&#20363;&#22914;&#25805;&#20316;&#27169;&#22411;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#25928;&#26524;&#65292;&#24182;&#20174;&#38544;&#24335;&#30693;&#35782;&#65288;&#20363;&#22914;&#31070;&#32463;&#27169;&#22411;&#65289;&#20013;&#23398;&#20064;&#12290;&#26377;&#25928;&#22320;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38598;&#25104;&#21487;&#20197;&#25913;&#21892;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#29992;&#20110;&#23558;AI&#35268;&#21010;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26500;&#24314;&#22495;&#27169;&#22411;&#24182;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38544;&#24335;&#30693;&#35782;&#20013;&#23398;&#20064;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20998;&#26512;&#20854;&#22914;&#20309;&#36981;&#24490;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#24635;&#32467;&#20986;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#30340;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2112.07467</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;AI&#20262;&#29702;&#21407;&#21017;&#65306;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Ethics Principles in Practice: Perspectives of Designers and Developers. (arXiv:2112.07467v6 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20998;&#26512;&#20854;&#22914;&#20309;&#36981;&#24490;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#65292;&#24182;&#24635;&#32467;&#20986;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#30340;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#24050;&#21457;&#24067;&#30340;AI&#20262;&#29702;&#21407;&#21017;&#30340;&#20849;&#35782;&#36880;&#28176;&#24418;&#25104;&#65292;&#39640;&#23618;&#27425;&#30340;&#21407;&#21017;&#21644;&#21487;&#31435;&#21363;&#37319;&#29992;&#30340;&#23454;&#38469;&#25216;&#26415;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#32780;&#36825;&#20123;&#25216;&#26415;&#21487;&#29992;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36127;&#36131;&#20219;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#28595;&#22823;&#21033;&#20122;&#22269;&#23478;&#31185;&#23398;&#30740;&#31350;&#26426;&#26500;(CSIRO)&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#30340;&#23454;&#36341;&#21644;&#32463;&#39564;&#65292;&#20182;&#20204;&#21442;&#19982;&#35774;&#35745;&#21644;&#24320;&#21457;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#26469;&#30740;&#31350;&#21442;&#19982;&#32773;&#30340;&#23454;&#36341;&#22914;&#20309;&#19982;&#28595;&#22823;&#21033;&#20122;&#25919;&#24220;&#25552;&#20986;&#30340;&#19968;&#32452;&#39640;&#23618;&#27425;AI&#20262;&#29702;&#21407;&#21017;&#30456;&#32852;&#31995;&#21644;&#19968;&#33268;&#12290;&#36825;&#20123;&#21407;&#21017;&#21253;&#25324;&#65306;(1)&#38544;&#31169;&#20445;&#25252;&#21644;&#23433;&#20840;&#12289;(2)&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12289;(3)&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12289;(4)&#20844;&#27491;&#24615;&#12289;(5)&#21487;&#20105;&#35758;&#24615;&#12289;(6)&#36131;&#20219;&#21046;&#12289;(7)&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20215;&#20540;&#35266;&#12289;(8)&#20154;&#31867;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#30340;&#31119;&#31049;&#12290;&#36890;&#36807;&#23545;&#35775;&#35848;&#25152;&#33719;&#24471;&#30340;&#27934;&#35265;&#30340;&#35752;&#35770;&#65292;&#21253;&#25324;&#21407;&#21017;&#20043;&#38388;&#30340;&#21508;&#31181;&#24352;&#21147;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems. We examine the practices and experiences of researchers and engineers from Australia's national scientific research agency (CSIRO), who are involved in designing and developing AI systems for many application areas. Semi-structured interviews were used to examine how the practices of the participants relate to and align with a set of high-level AI ethics principles proposed by the Australian Government. The principles comprise: (1) privacy protection and security, (2) reliability and safety, (3) transparency and explainability, (4) fairness, (5) contestability, (6) accountability, (7) human-centred values, (8) human, social and environmental wellbeing. Discussions on the gained insights from the interviews include various tensions and trade-offs between the principles,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#24179;&#22343;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;AUPRC&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#25910;&#25947;&#30340;SOAP&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2104.08736</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#31934;&#30830;&#29575;-&#21484;&#22238;&#26354;&#32447;&#19979;&#38754;&#31215;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence. (arXiv:2104.08736v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#24179;&#22343;&#31934;&#24230;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;AUPRC&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#25910;&#25947;&#30340;SOAP&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#21644;&#31934;&#30830;&#29575;-&#21484;&#22238;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUPRC&#65289;&#26159;&#29992;&#20110;&#35780;&#20272;&#19981;&#24179;&#34913;&#38382;&#39064;&#20998;&#31867;&#24615;&#33021;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#19982;AUROC&#30456;&#27604;&#65292;AUPRC&#26159;&#39640;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#26356;&#21512;&#36866;&#30340;&#25351;&#26631;&#12290;&#34429;&#28982;&#20851;&#20110;AUROC&#30340;&#38543;&#26426;&#20248;&#21270;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22522;&#20110;&#21407;&#21017;&#30340;AUPRC&#30340;&#38543;&#26426;&#20248;&#21270;&#21364;&#24456;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#21270;&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#30340;&#25216;&#26415;&#26041;&#27861;&#26469;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#30340;AUPRC&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#34920;&#31034;&#20026;&#20381;&#36182;&#32452;&#21512;&#20989;&#25968;&#20043;&#21644;&#65292;&#20854;&#20013;&#20869;&#37096;&#20989;&#25968;&#20381;&#36182;&#20110;&#22806;&#23618;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#38543;&#26426;&#31639;&#27861;SOAP&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#35777;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#22312;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets d
&lt;/p&gt;</description></item></channel></rss>