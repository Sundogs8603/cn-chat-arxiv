<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#35838;&#22530;&#32771;&#21220;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#36827;&#34892;&#38754;&#37096;&#35782;&#21035;&#65292;&#20174;&#32780;&#22312;&#25945;&#32946;&#20013;&#25552;&#20379;&#23433;&#20840;&#24615;&#21644;&#33258;&#21160;&#21270;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.13317</link><description>&lt;p&gt;
&#25945;&#32946;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#35838;&#22530;&#32771;&#21220;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Class Attendance System in Education with Deep Learning Method. (arXiv:2309.13317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#35838;&#22530;&#32771;&#21220;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#22788;&#29702;&#36827;&#34892;&#38754;&#37096;&#35782;&#21035;&#65292;&#20174;&#32780;&#22312;&#25945;&#32946;&#20013;&#25552;&#20379;&#23433;&#20840;&#24615;&#21644;&#33258;&#21160;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#35745;&#31639;&#26426;&#30340;&#30828;&#20214;&#22686;&#30410;&#21644;&#22788;&#29702;&#22120;&#30340;&#22788;&#29702;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#20351;&#24471;&#21363;&#26102;&#21644;&#23454;&#26102;&#22270;&#20687;&#30340;&#22788;&#29702;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#38754;&#37096;&#35782;&#21035;&#36807;&#31243;&#20063;&#26159;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#38754;&#37096;&#35782;&#21035;&#36807;&#31243;&#32463;&#24120;&#22312;&#23433;&#20840;&#24212;&#29992;&#21644;&#21830;&#19994;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#29305;&#21035;&#26159;&#22312;&#36807;&#21435;&#30340;20&#24180;&#37324;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#30340;&#39640;&#24615;&#33021;&#23545;&#36825;&#20123;&#30740;&#31350;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#30340;&#25193;&#25955;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25945;&#32946;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;&#20351;&#29992;AI&#22312;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#23398;&#29983;&#12289;&#25945;&#24072;&#21644;&#26426;&#26500;&#12290;&#26426;&#26500;&#30740;&#31350;&#20043;&#19968;&#21487;&#33021;&#26159;&#25945;&#32946;&#29615;&#22659;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#33258;&#21160;&#21270;&#23545;&#25945;&#32946;&#21644;&#22521;&#35757;&#36807;&#31243;&#30340;&#36129;&#29486;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#26159;AI&#30340;&#19968;&#20010;&#23376;&#20998;&#25903;&#12290;&#23545;&#20110;&#20174;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#65292;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancing technology, the hardware gain of computers and the increase in the processing capacity of processors have facilitated the processing of instantaneous and real-time images. Face recognition processes are also studies in the field of image processing. Facial recognition processes are frequently used in security applications and commercial applications. Especially in the last 20 years, the high performances of artificial intelligence (AI) studies have contributed to the spread of these studies in many different fields. Education is one of them. The potential and advantages of using AI in education; can be grouped under three headings: student, teacher, and institution. One of the institutional studies may be the security of educational environments and the contribution of automation to education and training processes. From this point of view, deep learning methods, one of the sub-branches of AI, were used in this study. For object detection from images, a pioneering st
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.13289</link><description>&lt;p&gt;
USL-Net&#65306;&#29992;&#20110;&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13289
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20855;&#26377;&#22810;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#33410;&#32422;&#19987;&#23478;&#20154;&#21147;&#36164;&#28304;&#12289;&#20943;&#23569;&#20027;&#35266;&#20154;&#24037;&#26631;&#27880;&#24341;&#36215;&#30340;&#24046;&#24322;&#20197;&#21450;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#30382;&#32932;&#38236;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#65292;&#22914;&#27611;&#21457;&#22122;&#22768;&#12289;&#27700;&#30129;&#22122;&#22768;&#21644;&#32454;&#24494;&#36793;&#32536;&#24046;&#24322;&#31561;&#30382;&#32932;&#38236;&#22270;&#20687;&#20266;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;&#65288;USL-Net&#65289;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#12290;USL-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30149;&#21464;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20316;&#20026;&#26174;&#33879;&#22270;&#12290;&#19981;&#21516;&#30340;CAM&#20301;&#32622;&#23545;&#24212;&#20110;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#30149;&#21464;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#22320;&#22270;&#20013;&#30340;&#39640;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#65292;&#32780;&#20302;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#38750;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-sa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#23454;&#26045;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13285</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#20307;&#30340;&#30896;&#25758;&#22238;&#36991;&#21644;&#23548;&#33322;&#26041;&#27861;&#65306;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning. (arXiv:2309.13285v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13285
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#23454;&#26045;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26131;&#20110;&#37096;&#32626;&#12289;&#20219;&#21153;&#27867;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#33021;&#21147;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;DRL&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#22312;&#31616;&#21333;&#12289;&#26080;&#38556;&#30861;&#29615;&#22659;&#20013;&#35757;&#32451;&#21333;&#20010;&#26080;&#20154;&#26426;&#25110;&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#38556;&#30861;&#29289;&#23545;&#35757;&#32451;RL&#31574;&#30053;&#30340;&#22256;&#38590;&#24615;&#22686;&#21152;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#30340;&#31471;&#21040;&#31471;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#20010;&#35838;&#31243;&#65288;curriculum&#65289;&#21644;&#19968;&#20010;&#22238;&#25918;&#32531;&#20914;&#21306;&#65288;replay buffer&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#20805;&#28385;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#26045;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992; - &#36825;&#26159;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;&#20005;&#37325;&#35745;&#31639;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#30340;&#32676;&#20307;&#34892;&#20026;&#31574;&#30053;&#20013;&#24212;&#29992;&#27492;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#39044;&#27979;IoU&#24341;&#23548;&#30340;&#36136;&#37327;&#24471;&#20998;&#26469;&#24847;&#35782;&#21040;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#20998;&#31867;&#24471;&#20998;&#21644;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#38454;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13269</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#39044;&#27979;IoU&#24341;&#23548;&#30340;&#36136;&#37327;&#24471;&#20998;&#65292;&#24847;&#35782;&#21040;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Being Aware of Localization Accuracy By Generating Predicted-IoU-Guided Quality Scores. (arXiv:2309.13269v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#39044;&#27979;IoU&#24341;&#23548;&#30340;&#36136;&#37327;&#24471;&#20998;&#26469;&#24847;&#35782;&#21040;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#20998;&#31867;&#24471;&#20998;&#21644;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19968;&#38454;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26412;&#22320;&#21270;&#36136;&#37327;&#20272;&#35745;&#65288;LQE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#32771;&#34385;&#20998;&#31867;&#24471;&#20998;&#21644;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#65292;&#22312;&#21518;&#22788;&#29702;&#20013;&#26377;&#21161;&#20110;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#26412;&#22320;&#21270;&#20934;&#30830;&#24615;&#21644;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#65292;&#24182;&#25233;&#21046;&#37027;&#20123;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;LQE&#20998;&#25903;&#65292;&#20197;&#33719;&#21462;&#30001;&#39044;&#27979;&#30340;IoU&#24341;&#23548;&#30340;&#26412;&#22320;&#21270;&#36136;&#37327;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#22521;&#35757;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#36731;&#20998;&#31867;&#24471;&#20998;&#21644;&#26412;&#22320;&#21270;&#36136;&#37327;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21069;&#32773;&#21487;&#33021;&#20250;&#25439;&#23475;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;LQE&#20998;&#25903;&#23884;&#20837;&#20998;&#31867;&#20998;&#25903;&#20013;&#65292;&#20135;&#29983;&#19968;&#20010;&#32852;&#21512;&#30340;&#20998;&#31867;-&#26412;&#22320;&#21270;&#36136;&#37327;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLQ&#30340;&#26032;&#22411;&#19968;&#38454;&#26816;&#27979;&#22120;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;CLQ&#30340;&#24615;&#33021;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localization Quality Estimation (LQE) helps to improve detection performance as it benefits post processing through jointly considering classification score and localization accuracy. In this perspective, for further leveraging the close relationship between localization accuracy and IoU (Intersection-Over-Union), and for depressing those inconsistent predictions, we designed an elegant LQE branch to acquire localization quality score guided by predicted IoU. Distinctly, for alleviating the inconsistency of classification score and localization quality during training and inference, under which some predictions with low classification scores but high LQE scores will impair the performance, instead of separately and independently setting, we embedded LQE branch into classification branch, producing a joint classification-localization-quality representation. Then a novel one stage detector termed CLQ is proposed. Extensive experiments show that CLQ achieves state-of-the-arts' performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#24072;&#29983;&#33976;&#39311;&#26550;&#26500;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#22522;&#20934;&#32447;&#30340;&#40065;&#26834;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13266</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#27169;&#24577;&#34701;&#21512;&#21644;&#30693;&#35782;&#36801;&#31227;&#30340;&#40065;&#26834;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Robust Navigation with Cross-Modal Fusion and Knowledge Transfer. (arXiv:2309.13266v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#24072;&#29983;&#33976;&#39311;&#26550;&#26500;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#22522;&#20934;&#32447;&#30340;&#40065;&#26834;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24046;&#24378;&#20154;&#24847;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#27169;&#25311;-&#29616;&#23454;&#24046;&#36317;&#38480;&#21046;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#25552;&#39640;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#23454;&#29616;&#23548;&#33322;&#25216;&#33021;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#21644;&#19968;&#20010;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#24072;&#29983;&#33976;&#39311;&#26550;&#26500;&#23454;&#29616;&#12290;&#32769;&#24072;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#19968;&#20010;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#21644;&#36817;&#20046;&#23436;&#32654;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#27169;&#20223;&#32769;&#24072;&#30340;&#34892;&#20026;&#21644;&#34920;&#31034;&#65292;&#23398;&#29983;&#33021;&#22815;&#23545;&#26469;&#33258;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#23545;&#40784;&#29305;&#24449;&#65292;&#24182;&#20943;&#23569;&#21464;&#21270;&#23545;&#23548;&#33322;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#29616;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#20934;&#32447;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, learning-based approaches show promising results in navigation tasks. However, the poor generalization capability and the simulation-reality gap prevent a wide range of applications. We consider the problem of improving the generalization of mobile robots and achieving sim-to-real transfer for navigation skills. To that end, we propose a cross-modal fusion method and a knowledge transfer framework for better generalization. This is realized by a teacher-student distillation architecture. The teacher learns a discriminative representation and the near-perfect policy in an ideal environment. By imitating the behavior and representation of the teacher, the student is able to align the features from noisy multi-modal input and reduce the influence of variations on navigation policy. We evaluate our method in simulated and real-world environments. Experiments show that our method outperforms the baselines by a large margin and achieves robust navigation performance with varying wo
&lt;/p&gt;</description></item><item><title>WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13259</link><description>&lt;p&gt;
WikiMT++&#25968;&#25454;&#38598;&#21345;&#29255;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ Dataset Card. (arXiv:2309.13259v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13259
&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;&#19968;&#20010;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#30340;WikiMusicText&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#23427;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21487;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;CLaMP&#26469;&#32416;&#27491;&#23646;&#24615;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WikiMT++&#26159;WikiMusicText&#65288;WikiMT&#65289;&#30340;&#25193;&#23637;&#21644;&#31934;&#32454;&#29256;&#26412;&#65292;&#21253;&#21547;&#20102;1010&#20010;&#32463;&#36807;&#31574;&#21010;&#30340;ABC&#35760;&#35889;&#27861;&#30340;&#20027;&#39064;&#26354;&#12290;&#20026;&#20102;&#25193;&#23637;WikiMT&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#23458;&#35266;&#23646;&#24615;&#65288;&#19987;&#36753;&#12289;&#27468;&#35789;&#12289;&#35270;&#39057;&#65289;&#21644;&#20027;&#35266;&#24773;&#24863;&#23646;&#24615;&#65288;12&#20010;&#24773;&#24863;&#24418;&#23481;&#35789;&#65289;&#21644;&#24773;&#24863;4Q&#65288;Russell 4Q&#65289;&#65292;&#22686;&#24378;&#20102;&#20854;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#12289;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#12289;&#33258;&#21160;&#20316;&#26354;&#21644;&#24773;&#24863;&#20998;&#31867;&#31561;&#26041;&#38754;&#30340;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;CLaMP&#26469;&#32416;&#27491;&#20174;WikiMT&#32487;&#25215;&#30340;&#23646;&#24615;&#65292;&#20197;&#20943;&#23569;&#21407;&#22987;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38169;&#35823;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT), featuring 1010 curated lead sheets in ABC notation. To expand application scenarios of WikiMT, we add both objective (album, lyrics, video) and subjective emotion (12 emotion adjectives) and emo\_4q (Russell 4Q) attributes, enhancing its usability for music information retrieval, conditional music generation, automatic composition, and emotion classification, etc. Additionally, CLaMP is implemented to correct the attributes inherited from WikiMT to reduce errors introduced during original data collection and enhance the accuracy and completeness of our dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDP&#30340;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#26469;&#35782;&#21035;&#27745;&#26579;&#26679;&#26412;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;MDP&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#24418;&#25104;&#20102;&#36827;&#36864;&#20004;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.13256</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#21453;&#21521;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. (arXiv:2309.13256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDP&#30340;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#26469;&#35782;&#21035;&#27745;&#26579;&#26679;&#26412;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;MDP&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#24418;&#25104;&#20102;&#36827;&#36864;&#20004;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#23427;&#20204;&#30340;&#23433;&#20840;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#34920;&#26126;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;PLMs&#26497;&#26131;&#21463;&#21040;&#21453;&#21521;&#25915;&#20987;&#65292;&#32780;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#30001;&#20110;&#23569;&#26679;&#26412;&#24773;&#22659;&#30340;&#29420;&#29305;&#25361;&#25112;&#32780;&#19981;&#36275;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MDP&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#12289;&#21487;&#25554;&#25300;&#19988;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MDP&#21033;&#29992;&#20102;&#34987;&#27745;&#26579;&#26679;&#26412;&#21644;&#28165;&#27905;&#26679;&#26412;&#20043;&#38388;&#30340;&#25513;&#30721;&#25935;&#24863;&#24615;&#24046;&#36317;&#65306;&#21442;&#32771;&#26377;&#38480;&#30340;&#23569;&#26679;&#26412;&#25968;&#25454;&#20316;&#20026;&#20998;&#24067;&#38170;&#28857;&#65292;&#23427;&#27604;&#36739;&#19981;&#21516;&#25513;&#30721;&#19979;&#32473;&#23450;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#35782;&#21035;&#20986;&#20855;&#26377;&#26174;&#33879;&#21464;&#21270;&#30340;&#34987;&#27745;&#26579;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#34920;&#26126;&#65292;MDP&#23545;&#20110;&#25915;&#20987;&#32773;&#22312;&#25915;&#20987;&#25928;&#26524;&#21644;&#26816;&#27979;&#36867;&#36991;&#24615;&#20043;&#38388;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#36827;&#36864;&#20004;&#38590;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;be
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#27169;&#22411;&#30340;&#21333;&#35843;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13246</link><description>&lt;p&gt;
&#25105;&#21487;&#20197;&#30456;&#20449;&#35299;&#37322;&#21527;&#65311;&#30740;&#31350;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models. (arXiv:2309.13246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21333;&#35843;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#27169;&#22411;&#30340;&#21333;&#35843;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#24212;&#29992;&#20110;&#40657;&#30418;&#27169;&#22411;&#32780;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#30693;&#35782;&#12290;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#31185;&#23398;&#20026;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#35299;&#37322;&#24615;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#23558;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;&#31185;&#23398;&#30693;&#35782;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#33719;&#24471;&#19968;&#33268;&#30340;&#31185;&#23398;&#35299;&#37322;&#21527;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#23637;&#31034;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21333;&#35843;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#22238;&#31572;&#12290;&#20026;&#20102;&#23637;&#31034;&#21333;&#35843;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20844;&#29702;&#12290;&#30456;&#24212;&#22320;&#65292;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20165;&#28041;&#21450;&#20010;&#20307;&#21333;&#35843;&#24615;&#26102;&#65292;&#22522;&#20934;Shapley&#20540;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#35299;&#37322;&#65307;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#24378;&#22823;&#30340;&#25104;&#23545;&#21333;&#35843;&#24615;&#26102;&#65292;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#22312;&#24179;&#22343;&#19978;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, explainable machine learning methods have been very successful. Despite their success, most explainable machine learning methods are applied to black-box models without any domain knowledge. By incorporating domain knowledge, science-informed machine learning models have demonstrated better generalization and interpretation. But do we obtain consistent scientific explanations if we apply explainable machine learning methods to science-informed machine learning models? This question is addressed in the context of monotonic models that exhibit three different types of monotonicity. To demonstrate monotonicity, we propose three axioms. Accordingly, this study shows that when only individual monotonicity is involved, the baseline Shapley value provides good explanations; however, when strong pairwise monotonicity is involved, the Integrated gradients method provides reasonable explanations on average.
&lt;/p&gt;</description></item><item><title>UniHead&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21464;&#24418;&#24863;&#30693;&#12289;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#21644;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#20840;&#24863;&#30693;&#33021;&#21147;&#30340;&#32479;&#19968;&#12290;</title><link>http://arxiv.org/abs/2309.13242</link><description>&lt;p&gt;
UniHead: &#34701;&#21512;&#22810;&#24863;&#30693;&#30340;&#26816;&#27979;&#22836;
&lt;/p&gt;
&lt;p&gt;
UniHead: Unifying Multi-Perception for Detection Heads. (arXiv:2309.13242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13242
&lt;/p&gt;
&lt;p&gt;
UniHead&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#21464;&#24418;&#24863;&#30693;&#12289;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#21644;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#20840;&#24863;&#30693;&#33021;&#21147;&#30340;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22836;&#26159;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#36127;&#36131;&#25191;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#24182;&#34892;&#26816;&#27979;&#22836;&#24120;&#24120;&#32570;&#20047;&#20840;&#24863;&#30693;&#33021;&#21147;&#65292;&#22914;&#21464;&#24418;&#24863;&#30693;&#12289;&#20840;&#23616;&#24863;&#30693;&#21644;&#36328;&#20219;&#21153;&#24863;&#30693;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26041;&#27861;&#35797;&#22270;&#20174;&#21333;&#20010;&#26041;&#38754;&#25552;&#39640;&#36825;&#20123;&#33021;&#21147;&#65292;&#20294;&#23454;&#29616;&#20840;&#38754;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26816;&#27979;&#22836;&#65292;&#31216;&#20026;UniHead&#65292;&#21516;&#26102;&#32479;&#19968;&#20102;&#19977;&#31181;&#24863;&#30693;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#24341;&#20837;&#20102;&#21464;&#24418;&#24863;&#30693;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#37319;&#26679;&#23545;&#35937;&#29305;&#24449;&#65307;&#65288;2&#65289;&#25552;&#20986;&#20102;&#21452;&#36724;&#32858;&#21512;&#21464;&#25442;&#22120;&#65288;DAT&#65289;&#26469;&#28789;&#27963;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#23616;&#24863;&#30693;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#36328;&#20219;&#21153;&#20132;&#20114;&#21464;&#25442;&#22120;&#65288;CIT&#65289;&#65292;&#20419;&#36827;&#20998;&#31867;&#21644;&#23450;&#20301;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection head constitutes a pivotal component within object detectors, tasked with executing both classification and localization functions. Regrettably, the commonly used parallel head often lacks omni perceptual capabilities, such as deformation perception, global perception and cross-task perception. Despite numerous methods attempt to enhance these abilities from a single aspect, achieving a comprehensive and unified solution remains a significant challenge. In response to this challenge, we have developed an innovative detection head, termed UniHead, to unify three perceptual abilities simultaneously. More precisely, our approach (1) introduces deformation perception, enabling the model to adaptively sample object features; (2) proposes a Dual-axial Aggregation Transformer (DAT) to adeptly model long-range dependencies, thereby achieving global perception; and (3) devises a Cross-task Interaction Transformer (CIT) that facilitates interaction between the classification and lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25552;&#39640;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#20013;&#33410;&#28857;&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#24322;&#36136;&#29305;&#24449;&#34920;&#31034;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#24314;&#31435;&#20102;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#26469;&#37325;&#29616;&#29616;&#23454;&#29289;&#29702;&#25509;&#35302;&#32593;&#32476;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23545;&#28798;&#23475;&#38887;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.13229</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#30340;&#24322;&#36136;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Feature Representation for Digital Twin-Oriented Complex Networked Systems. (arXiv:2309.13229v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25552;&#39640;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#20013;&#33410;&#28857;&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#24322;&#36136;&#29305;&#24449;&#34920;&#31034;&#21407;&#21017;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#24314;&#31435;&#20102;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#26469;&#37325;&#29616;&#29616;&#23454;&#29289;&#29702;&#25509;&#35302;&#32593;&#32476;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23545;&#28798;&#23475;&#38887;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#29616;&#23454;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;(CNS)&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#20102;&#33021;&#22815;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#65292;&#24314;&#27169;&#38656;&#35201;&#32771;&#34385;&#19981;&#20165;&#20165;&#26159;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24378;&#24230;&#65292;&#36824;&#35201;&#32771;&#34385;&#31995;&#32479;&#20013;&#25152;&#26377;&#20803;&#32032;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#24322;&#36136;&#29305;&#24449;&#34920;&#31034;&#21407;&#21017;&#26469;&#25552;&#39640;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;(DT-CNSs)&#20013;&#33410;&#28857;&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36825;&#28041;&#21450;&#21040;&#20351;&#29992;&#28165;&#26224;&#30340;&#29305;&#24449;&#20540;&#21644;&#27169;&#31946;&#38598;&#21512;&#26469;&#34920;&#31034;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#25551;&#36848;&#20102;&#33410;&#28857;&#29305;&#24449;&#21644;&#29305;&#24449;&#24046;&#24322;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#24402;&#32435;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#22522;&#20110;&#21508;&#31181;&#34920;&#31034;&#21407;&#21017;&#21644;&#20248;&#21270;&#30340;&#29305;&#24449;&#20559;&#22909;&#65292;&#24314;&#31435;DT-CNSs&#26469;&#37325;&#29616;&#19981;&#21516;&#22269;&#23478;&#30340;&#29616;&#23454;&#29289;&#29702;&#25509;&#35302;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#23427;&#20204;&#23545;&#20174;&#27969;&#34892;&#30149;&#29190;&#21457;&#24320;&#22987;&#30340;&#28798;&#23475;&#38887;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building models of Complex Networked Systems (CNS) that can accurately represent reality forms an important research area. To be able to reflect real world systems, the modelling needs to consider not only the intensity of interactions between the entities but also features of all the elements of the system. This study aims to improve the expressive power of node features in Digital Twin-Oriented Complex Networked Systems (DT-CNSs) with heterogeneous feature representation principles. This involves representing features with crisp feature values and fuzzy sets, each describing the objective and the subjective inductions of the nodes' features and feature differences. Our empirical analysis builds DT-CNSs to recreate realistic physical contact networks in different countries from real node feature distributions based on various representation principles and an optimised feature preference. We also investigate their respective disaster resilience to an epidemic outbreak starting from the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13224</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#30340;&#25315;&#36873;&#35745;&#21010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pick Planning Strategies for Large-Scale Package Manipulation. (arXiv:2309.13224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;Robot Induction&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#36890;&#36807;&#20351;&#29992;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#20197;&#21450;&#35757;&#32451;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#36827;&#34892;&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#32773;&#30340;&#20215;&#26684;&#65292;&#21152;&#24555;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#24066;&#22330;&#27874;&#21160;&#30340;&#36866;&#24212;&#21147;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#20844;&#21496;&#30340;&#26426;&#22120;&#20154;&#24341;&#23548;&#65288;Robin&#65289;&#33328;&#38431;&#20013;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#32437;&#65292;&#29992;&#20110;&#27599;&#22825;&#25315;&#36873;&#21644;&#21333;&#29420;&#22788;&#29702;600&#19975;&#20010;&#21253;&#35065;&#65292;&#24182;&#19988;&#30446;&#21069;&#24050;&#32463;&#22788;&#29702;&#20102;20&#20159;&#20010;&#21253;&#35065;&#12290;&#23427;&#25551;&#36848;&#20102;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#24320;&#21457;&#30340;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#21450;&#20854;&#21518;&#32487;&#26041;&#27861;&#65292;&#21518;&#32487;&#26041;&#27861;&#21033;&#29992;&#20102;&#22312;&#30495;&#23454;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25315;&#36873;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#22312;&#30495;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#39318;&#27425;&#22823;&#35268;&#27169;&#37096;&#32626;&#23398;&#20064;&#30340;&#25315;&#36873;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to market fluctuations.  This extended abstract showcases a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is used for picking and singulating up to 6 million packages per day and so far has manipulated over 2 billion packages. It describes the various heuristic methods developed over time and their successor, which utilizes a pick success predictor trained on real production data.  To the best of the authors' knowledge, this work is the first large-scale deployment of learned pick quality estimation methods in a real production system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21360;&#24230;&#35821;Hindi&#21040;&#33521;&#25991;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22238;&#35793;&#21644;&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#27861;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.13222</link><description>&lt;p&gt;
Hindi to English: &#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Hindi to English: Transformer-Based Neural Machine Translation. (arXiv:2309.13222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21360;&#24230;&#35821;Hindi&#21040;&#33521;&#25991;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#22238;&#35793;&#21644;&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#27861;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#28041;&#21450;&#23558;&#25991;&#26412;&#20174;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#35821;&#35328;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#21547;&#20041;&#21644;&#27969;&#30021;&#24615;&#12290;&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;&#24050;&#32463;&#25345;&#32493;&#20102;&#25968;&#21313;&#24180;&#65292;&#20294;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;Transformer&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#21360;&#24230;&#35821;Hindi&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#25991;&#12290;Hindi&#20316;&#20026;&#19968;&#31181;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#29702;&#35299;&#35813;&#35821;&#35328;&#65292;&#20174;&#32780;&#23548;&#33268;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#30340;&#21457;&#23637;&#32531;&#24930;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#22238;&#35793;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;&#35789;&#32423;&#21644;&#23376;&#35789;&#32423;&#30340;&#20998;&#35789;&#26041;&#27861;&#21019;&#24314;&#20102;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) is one of the most prominent tasks in Natural Language Processing (NLP) which involves the automatic conversion of texts from one natural language to another while preserving its meaning and fluency. Although the research in machine translation has been going on since multiple decades, the newer approach of integrating deep learning techniques in natural language processing has led to significant improvements in the translation quality. In this paper, we have developed a Neural Machine Translation (NMT) system by training the Transformer model to translate texts from Indian Language Hindi to English. Hindi being a low resource language has made it difficult for neural networks to understand the language thereby leading to a slow growth in the development of neural machine translators. Thus, to address this gap, we implemented back-translation to augment the training data and for creating the vocabulary, we experimented with both word and subword level tokenizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SQAKD&#30340;&#33258;&#30417;&#30563;&#30340;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#31614;&#30417;&#30563;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;QAT&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#32479;&#19968;&#21508;&#31181;&#37327;&#21270;&#20989;&#25968;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;SQAKD&#20026;&#26368;&#20808;&#36827;&#30340;QAT&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#22522;&#32447;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.13220</link><description>&lt;p&gt;
&#28023;&#25253;&#65306;&#33258;&#30417;&#30563;&#30340;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Poster: Self-Supervised Quantization-Aware Knowledge Distillation. (arXiv:2309.13220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SQAKD&#30340;&#33258;&#30417;&#30563;&#30340;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#26631;&#31614;&#30417;&#30563;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;QAT&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#32479;&#19968;&#21508;&#31181;&#37327;&#21270;&#20989;&#25968;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;SQAKD&#20026;&#26368;&#20808;&#36827;&#30340;QAT&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#22522;&#32447;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#20174;&#39044;&#35757;&#32451;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#24320;&#22987;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20013;&#25191;&#34892;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#26631;&#31614;&#30340;&#30417;&#30563;&#65292;&#24182;&#19988;&#30001;&#20110;&#38477;&#20302;&#20102;&#31934;&#24230;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#30340;&#37327;&#21270;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;(SQAKD)&#12290;SQAKD&#39318;&#20808;&#32479;&#19968;&#20102;&#21508;&#31181;&#37327;&#21270;&#20989;&#25968;&#30340;&#21069;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#28982;&#21518;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;QAT&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#20849;&#21516;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;KL&#25439;&#22833;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;SQAKD&#26174;&#33879;&#25913;&#21892;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;QAT&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;SQAKD&#24314;&#31435;&#20102;&#26356;&#24378;&#30340;&#22522;&#32447;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#28508;&#22312;&#22320;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;QAT&#30740;&#31350;&#26356;&#26131;&#20110;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization-aware training (QAT) starts with a pre-trained full-precision model and performs quantization during retraining. However, existing QAT works require supervision from the labels and they suffer from accuracy loss due to reduced precision. To address these limitations, this paper proposes a novel Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD). SQAKD first unifies the forward and backward dynamics of various quantization functions and then reframes QAT as a co-optimization problem that simultaneously minimizes the KL-Loss and the discretization error, in a self-supervised manner. The evaluation shows that SQAKD significantly improves the performance of various state-of-the-art QAT works. SQAKD establishes stronger baselines and does not require extensive labeled training data, potentially making state-of-the-art QAT research more accessible.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.13218</link><description>&lt;p&gt;
AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#65306;&#19968;&#20010;&#26694;&#26550;&#21644;&#22312;&#29983;&#20135;&#35843;&#24230;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling. (arXiv:2309.13218v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13218
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20248;&#21270;&#26159;&#23547;&#25214;&#21644;&#23454;&#26045;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#36816;&#33829;&#26041;&#24335;&#65292;&#20197;&#20026;&#20225;&#19994;&#24102;&#26469;&#31454;&#20105;&#20248;&#21183;&#30340;&#36807;&#31243;&#12290;&#32508;&#21512;&#38382;&#39064;&#34920;&#36848;&#26159;&#20225;&#19994;&#20248;&#21270;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#22260;&#32469;&#30528;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#23637;&#24320;&#65292;&#22240;&#27492;&#24456;&#26377;&#21487;&#33021;&#25104;&#20026;&#29942;&#39048;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#28508;&#22312;&#22320;&#20943;&#23569;&#38382;&#39064;&#34920;&#36848;&#20013;&#25152;&#38656;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#29992;&#20110;&#38382;&#39064;&#34920;&#36848;&#30340;LLM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#12289;&#20196;&#29260;&#38480;&#21046;&#20197;&#21450;LLM&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;LLM&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by 
&lt;/p&gt;</description></item><item><title>MISFIT-V &#26159;&#19968;&#20010;&#20351;&#29992;&#26469;&#33258;&#28909;&#35270;&#35273;&#21644;&#21487;&#35265;&#20809;&#20449;&#24687;&#30340;&#19981;&#21305;&#37197;&#22270;&#20687;&#21512;&#25104;&#21644;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#23545;&#19981;&#21305;&#37197;&#21644;&#24694;&#21155;&#29615;&#22659;&#26465;&#20214;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13216</link><description>&lt;p&gt;
MISFIT-V: &#21033;&#29992;&#26469;&#33258;&#28909;&#35270;&#35273;&#21644;&#21487;&#35265;&#20809;&#30340;&#20449;&#24687;&#36827;&#34892;&#19981;&#21305;&#37197;&#22270;&#20687;&#21512;&#25104;&#21644;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual. (arXiv:2309.13216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13216
&lt;/p&gt;
&lt;p&gt;
MISFIT-V &#26159;&#19968;&#20010;&#20351;&#29992;&#26469;&#33258;&#28909;&#35270;&#35273;&#21644;&#21487;&#35265;&#20809;&#20449;&#24687;&#30340;&#19981;&#21305;&#37197;&#22270;&#20687;&#21512;&#25104;&#21644;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#23545;&#19981;&#21305;&#37197;&#21644;&#24694;&#21155;&#29615;&#22659;&#26465;&#20214;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#37326;&#22806;&#25628;&#25937;(WiSAR)&#22242;&#38431;&#26469;&#35828;&#65292;&#20174;&#31354;&#20013;&#21487;&#35265;&#20809;&#21644;&#28909;&#35270;&#35273;&#22270;&#20687;&#20013;&#26816;&#27979;&#20154;&#20307;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#24040;&#22823;&#21387;&#21147;&#19979;&#20934;&#30830;&#25191;&#34892;&#36825;&#20010;&#21151;&#33021;&#12290;&#34701;&#21512;&#36825;&#20004;&#31181;&#20256;&#24863;&#22120;&#27169;&#24335;&#30340;&#33021;&#21147;&#21487;&#33021;&#20250;&#20943;&#23569;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#35748;&#30693;&#36127;&#33655;&#21644;/&#25110;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;WiSAR&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#21644;&#26497;&#31471;&#30340;&#29615;&#22659;&#22240;&#32032;&#65292;&#34701;&#21512;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;Misaligned Image Synthesis and Fusion using Information from Thermal and Visual (MISFIT-V)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#36890;&#36947;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#27599;&#20010;&#27169;&#24577;&#20013;&#25429;&#25417;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;MISFIT-V&#22312;&#23545;&#19981;&#21305;&#37197;&#21644;&#20809;&#29031;/&#28909;&#29615;&#22659;&#26465;&#20214;&#24046;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting humans from airborne visual and thermal imagery is a fundamental challenge for Wilderness Search-and-Rescue (WiSAR) teams, who must perform this function accurately in the face of immense pressure. The ability to fuse these two sensor modalities can potentially reduce the cognitive load on human operators and/or improve the effectiveness of computer vision object detection models. However, the fusion task is particularly challenging in the context of WiSAR due to hardware limitations and extreme environmental factors. This work presents Misaligned Image Synthesis and Fusion using Information from Thermal and Visual (MISFIT-V), a novel two-pronged unsupervised deep learning approach that utilizes a Generative Adversarial Network (GAN) and a cross-attention mechanism to capture the most relevant features from each modality. Experimental results show MISFIT-V offers enhanced robustness against misalignment and poor lighting/thermal environmental conditions compared to existing v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#23545;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#30340;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#20004;&#21608;&#20869;11&#21517;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20010;&#24615;&#21464;&#37327;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#21512;&#29702;&#30456;&#20851;&#24615;&#65292;&#20363;&#22914;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#19982;&#22686;&#21152;&#30340;&#22256;&#24785;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20010;&#20154;&#28902;&#24700;&#19982;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.13214</link><description>&lt;p&gt;
&#35780;&#20272;&#20010;&#24615;&#23545;&#20110;&#35270;&#39057;&#28216;&#25103;&#20132;&#27969;&#20013;&#24773;&#24863;&#29366;&#24577;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Personality on Affective States from Video Game Communication. (arXiv:2309.13214v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#23545;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#30340;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#20004;&#21608;&#20869;11&#21517;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20010;&#24615;&#21464;&#37327;&#19982;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#21512;&#29702;&#30456;&#20851;&#24615;&#65292;&#20363;&#22914;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#19982;&#22686;&#21152;&#30340;&#22256;&#24785;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20010;&#20154;&#28902;&#24700;&#19982;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#30340;&#20010;&#20307;&#24046;&#24322;&#20915;&#23450;&#20102;&#25105;&#20204;&#30340;&#21916;&#22909;&#12289;&#29305;&#24449;&#21644;&#20215;&#20540;&#35266;&#65292;&#36825;&#21516;&#26679;&#36866;&#29992;&#20110;&#25105;&#20204;&#34920;&#36798;&#33258;&#24049;&#30340;&#26041;&#24335;&#12290;&#22312;&#24403;&#21069;&#25216;&#26415;&#21644;&#31038;&#20250;&#30340;&#36827;&#27493;&#21644;&#36716;&#21464;&#20013;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#27807;&#36890;&#21464;&#24471;&#26222;&#36941;&#65292;&#24182;&#19988;&#36890;&#24120;&#29978;&#33267;&#36229;&#36807;&#20102;&#33258;&#28982;&#30340;&#35821;&#38899;&#20132;&#27969;&#65292;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#22312;&#36825;&#39033;&#25506;&#32034;&#24615;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#24615;&#23545;&#22522;&#20110;&#22242;&#38431;&#21512;&#20316;&#30340;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#29609;&#23478;&#24773;&#24863;&#34920;&#36798;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#21608;&#20869;&#25910;&#38598;&#20102;&#21313;&#19968;&#20010;&#29609;&#23478;&#30340;&#32842;&#22825;&#35760;&#24405;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#24773;&#24863;&#29366;&#24577;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#19982;&#20116;&#20010;&#20154;&#26684;&#39046;&#22495;&#21644;&#26041;&#38754;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#24212;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#21512;&#29702;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#65288;&#32452;&#21512;&#65289;&#20010;&#24615;&#21464;&#37327;&#19982;&#34920;&#36798;&#30340;&#24773;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;--&#20363;&#22914;&#65292;&#36739;&#20302;&#30340;&#33258;&#25105;&#33021;&#21147;&#65288;C1&#65289;&#21487;&#20197;&#39044;&#27979;&#22686;&#21152;&#30340;&#22256;&#24785;&#65292;&#20010;&#20154;&#28902;&#24700;&#21487;&#20197;&#39044;&#27979;&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#24418;&#35937;&#38382;&#39064;&#30340;&#22686;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual differences in personality determine our preferences, traits and values, which should similarly hold for the way we express ourselves. With current advancements and transformations of technology and society, text-based communication has become ordinary and often even surpasses natural voice conversations -- with distinct challenges and opportunities. In this exploratory work, we investigate the impact of personality on the tendency how players of a team-based collaborative alternate reality game express themselves affectively. We collected chat logs from eleven players over two weeks, labeled them according to their affective state, and assessed the connection between them and the five-factor personality domains and facets. After applying multi-linear regression, we found a series of reasonable correlations between (combinations of) personality variables and expressed affect -- as increased confusion could be predicted by lower self-competence (C1), personal annoyance by vul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24847;&#22270;&#27807;&#36890;&#20316;&#20026;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39640;&#36895;&#21512;&#24182;&#24773;&#26223;&#20013;&#30740;&#31350;&#20102;&#24847;&#22270;&#20849;&#20139;&#22914;&#20309;&#24110;&#21161;&#25509;&#25910;&#36710;&#36742;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.13206</link><description>&lt;p&gt;
&#24847;&#22270;&#24863;&#30693;&#33258;&#21160;&#39550;&#39542;&#65306;&#39640;&#36895;&#21512;&#24182;&#24773;&#26223;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intent-Aware Autonomous Driving: A Case Study on Highway Merging Scenarios. (arXiv:2309.13206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24847;&#22270;&#27807;&#36890;&#20316;&#20026;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39640;&#36895;&#21512;&#24182;&#24773;&#26223;&#20013;&#30740;&#31350;&#20102;&#24847;&#22270;&#20849;&#20139;&#22914;&#20309;&#24110;&#21161;&#25509;&#25910;&#36710;&#36742;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24847;&#22270;&#30340;&#27807;&#36890;&#20316;&#20026;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#21512;&#20316;&#30340;&#25163;&#27573;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#24847;&#22270;&#21487;&#20197;&#26159;&#36710;&#36742;&#19982;&#21478;&#19968;&#36742;&#36710;&#27807;&#36890;&#30340;&#20851;&#20110;&#20854;&#26410;&#26469;&#34892;&#20026;&#30340;&#21487;&#38752;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;highway-env&#27169;&#25311;&#22120;&#30340;&#21512;&#24182;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#24847;&#22270;&#20849;&#20139;&#20219;&#21153;&#65292;&#35813;&#27169;&#25311;&#22120;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#31574;&#30053;&#30340;&#29615;&#22659;&#12290;&#22312;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#31616;&#21333;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#24847;&#22270;&#20849;&#20139;&#22914;&#20309;&#24110;&#21161;&#25509;&#25910;&#36710;&#36742;&#35843;&#25972;&#20854;&#22312;&#39640;&#36895;&#21512;&#24182;&#24773;&#26223;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we use the communication of intent as a means to facilitate cooperation between autonomous vehicle agents. Generally speaking, intents can be any reliable information about its future behavior that a vehicle communicates with another vehicle. We implement this as an intent-sharing task atop the merging environment in the simulator of highway-env, which provides a collection of environments for learning decision-making strategies for autonomous vehicles. Under a simple setting between two agents, we carefully investigate how intent-sharing can aid the receiving vehicle in adjusting its behavior in highway merging scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13205</link><description>&lt;p&gt;
&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#30340;&#23454;&#38469;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#35774;&#35745;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#37325;&#28857;&#35752;&#35770;&#20102;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#31561;&#22810;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#32771;&#34385;&#22810;&#31181;&#25351;&#26631;&#21644;&#32570;&#20047;&#21333;&#19968;&#26368;&#20339;&#25552;&#31034;&#31561;&#35780;&#20272;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#20805;&#20998;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#21512;&#22238;&#39038;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#65292;&#24182;&#25506;&#35752;&#23427;&#20204;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#28085;&#30422;&#20102;&#25552;&#31034;&#24037;&#31243;&#39046;&#22495;&#30340;&#20851;&#38190;&#30740;&#31350;&#65292;&#35752;&#35770;&#20102;&#20854;&#26041;&#27861;&#35770;&#21644;&#23545;&#35813;&#39046;&#22495;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#35780;&#20272;&#25552;&#31034;&#24615;&#33021;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#21333;&#19968;&#30340;"&#26368;&#20339;"&#25552;&#31034;&#21644;&#32771;&#34385;&#22810;&#20010;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#25552;&#31034;&#35774;&#35745;&#22312;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#24037;&#35774;&#35745;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35780;&#20215;&#26041;&#27861;&#32467;&#21512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single "best" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, opt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.13202</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts. (arXiv:2309.13202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25511;&#21046;&#26426;&#21046;&#25913;&#21892;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#24212;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;GPT&#27169;&#22411;&#30340;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#38590;&#20197;&#29702;&#35299;&#30340;&#19987;&#19994;&#26415;&#35821;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#22312;&#25552;&#39640;&#20844;&#20849;&#20581;&#24247;&#32032;&#20859;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#27492;&#31867;&#20219;&#21153;&#21487;&#20197;&#20351;&#38750;&#19987;&#19994;&#35835;&#32773;&#24555;&#36895;&#30452;&#25509;&#22320;&#33719;&#21462;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#30340;&#25968;&#25454;&#38598;&#65288;PLABA&#65289;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#39046;&#22495;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#22312;&#65306;1&#65289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;T5&#12289;SciFive&#21644;BART&#65289;&#19978;&#65292;2&#65289;&#20165;&#35299;&#30721;&#22120;&#30340;GPT&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#26469;&#33258;OpenAI&#21644;BioGPT&#65292;&#20197;&#21450;3&#65289;&#22522;&#20110;&#25511;&#21046;&#20196;&#29260;&#26426;&#21046;&#30340;&#22522;&#20110;BART&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;BLEU&#12289;ROUGE&#12289;SARI&#21644;BERTscore&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature often uses complex language and inaccessible professional terminologies. That is why simplification plays an important role in improving public health literacy. Applying Natural Language Processing (NLP) models to automate such tasks allows for quick and direct accessibility for lay readers. In this work, we investigate the ability of state-of-the-art large language models (LLMs) on the task of biomedical abstract simplification, using the publicly available dataset for plain language adaptation of biomedical abstracts (\textbf{PLABA}). The methods applied include domain fine-tuning and prompt-based learning (PBL) on: 1) Encoder-decoder models (T5, SciFive, and BART), 2) Decoder-only GPT models (GPT-3.5 and GPT-4) from OpenAI and BioGPT, and 3) Control-token mechanisms on BART-based models. We used a range of automatic evaluation metrics, including BLEU, ROUGE, SARI, and BERTscore, and also conducted human evaluations. BART-Large with Control Token (BART-L-w-CT) m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#26080;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20013;&#30340;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#19978;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#19981;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#21028;&#21035;&#22120;&#21644;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30001;&#20110;&#36974;&#30422;&#36807;&#31243;&#24341;&#36215;&#30340;&#20266;&#24433;&#12290;</title><link>http://arxiv.org/abs/2309.13188</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#29992;&#20110;&#20869;&#23481;&#19968;&#33268;&#24615;&#19981;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation. (arXiv:2309.13188v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#30422;&#30340;&#21028;&#21035;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#26080;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20013;&#30340;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#19978;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#19981;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#23616;&#37096;&#21028;&#21035;&#22120;&#21644;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#30001;&#20110;&#36974;&#30422;&#36807;&#31243;&#24341;&#36215;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#30340;&#19968;&#20010;&#20849;&#21516;&#30446;&#26631;&#26159;&#22312;&#27169;&#20223;&#30446;&#26631;&#22495;&#30340;&#39118;&#26684;&#30340;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#21644;&#36716;&#25442;&#21518;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;&#20004;&#20010;&#22495;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#65292;&#35768;&#22810;&#26041;&#27861;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#27809;&#26377;&#23545;&#21028;&#21035;&#22120;&#36827;&#34892;&#38480;&#21046;&#65292;&#23548;&#33268;&#35757;&#32451;&#35774;&#32622;&#26356;&#21152;&#26080;&#27861;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#19981;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#35009;&#21098;&#23610;&#23544;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20869;&#23481;&#30340;&#36974;&#32617;&#23545;&#20004;&#20010;&#22495;&#30340;&#20840;&#23616;&#21028;&#21035;&#22120;&#30340;&#36755;&#20837;&#36827;&#34892;&#36974;&#30422;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20869;&#23481;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#21487;&#20197;&#36861;&#28335;&#21040;&#36974;&#32617;&#36807;&#31243;&#30340;&#20266;&#24433;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#20266;&#24433;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#20351;&#29992;&#30456;&#20284;&#24615;&#37319;&#26679;&#31574;&#30053;&#36873;&#25321;&#30340;&#23567;&#35009;&#21098;&#23545;&#19978;&#25805;&#20316;&#30340;&#23616;&#37096;&#21028;&#21035;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#37319;&#26679;&#31574;&#30053;&#24212;&#29992;&#20110;&#20840;&#23616;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common goal of unpaired image-to-image translation is to preserve content consistency between source images and translated images while mimicking the style of the target domain. Due to biases between the datasets of both domains, many methods suffer from inconsistencies caused by the translation process. Most approaches introduced to mitigate these inconsistencies do not constrain the discriminator, leading to an even more ill-posed training setup. Moreover, none of these approaches is designed for larger crop sizes. In this work, we show that masking the inputs of a global discriminator for both domains with a content-based mask is sufficient to reduce content inconsistencies significantly. However, this strategy leads to artifacts that can be traced back to the masking process. To reduce these artifacts, we introduce a local discriminator that operates on pairs of small crops selected with a similarity sampling strategy. Furthermore, we apply this sampling strategy to sample global
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;(LCD)&#26469;&#20998;&#26512;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCD&#30340;&#39044;&#27979;&#21487;&#38752;&#19988;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.13181</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#21033;&#29992;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning. (arXiv:2309.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;(LCD)&#26469;&#20998;&#26512;&#35270;&#39057;&#28216;&#25103;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCD&#30340;&#39044;&#27979;&#21487;&#38752;&#19988;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#24182;&#24863;&#30693;&#34892;&#21160;&#32467;&#26524;&#26469;&#23398;&#20064;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#36825;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26159;&#19968;&#20010;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#21407;&#22240;&#26159;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21457;&#29616;&#26356;&#22909;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25110;&#32773;&#20004;&#32773;&#20860;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#25361;&#25112;&#35786;&#26029;&#22120;&#65288;LCD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#21333;&#29420;&#27979;&#37327;&#20219;&#21153;&#20013;&#24863;&#30693;&#21644;&#24378;&#21270;&#23398;&#20064;&#38656;&#27714;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;LCD&#22312;Procgen&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#20998;&#31867;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#26082;&#39640;&#24230;&#21487;&#38752;&#65292;&#21448;&#33021;&#25351;&#23548;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#26356;&#24191;&#27867;&#22320;&#35762;&#65292;LCD&#25581;&#31034;&#20102;&#22312;&#20687;P&#36825;&#26679;&#30340;&#25972;&#20010;&#35270;&#39057;&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#22810;&#31181;&#22833;&#36133;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#39118;&#38505;&#27010;&#36848;&#30340;&#26631;&#20934;&#65292;&#26088;&#22312;&#24110;&#21161;&#28040;&#36153;&#32773;&#29702;&#35299;&#19982;&#25259;&#38706;&#30456;&#20851;&#30340;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#21644;&#30417;&#31649;&#26694;&#26550;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.13176</link><description>&lt;p&gt;
AI&#39118;&#38505;&#27010;&#20917;&#65306;AI&#39118;&#38505;&#25259;&#38706;&#30340;&#26631;&#20934;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
AI Risk Profiles: A Standards Proposal for Pre-Deployment AI Risk Disclosures. (arXiv:2309.13176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#39118;&#38505;&#27010;&#36848;&#30340;&#26631;&#20934;&#65292;&#26088;&#22312;&#24110;&#21161;&#28040;&#36153;&#32773;&#29702;&#35299;&#19982;&#25259;&#38706;&#30456;&#20851;&#30340;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#21644;&#30417;&#31649;&#26694;&#26550;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#23545;&#20854;&#39118;&#38505;&#30340;&#35748;&#35782;&#20063;&#30456;&#24212;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;AI&#34892;&#19994;&#26356;&#21152;&#24378;&#35843;&#25259;&#38706;&#21644;&#36879;&#26126;&#24230;&#30340;&#21628;&#22768;&#36234;&#26469;&#36234;&#39640;&#65292;&#25552;&#35758;&#20174;&#26631;&#20934;&#21270;&#25216;&#26415;&#25259;&#38706;&#65288;&#22914;&#27169;&#22411;&#21345;&#29255;&#65289;&#21040;&#23578;&#26410;&#20855;&#20307;&#35828;&#26126;&#30340;&#35768;&#21487;&#21046;&#24230;&#12290;&#30001;&#20110;AI&#20215;&#20540;&#38142;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20195;&#34920;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#12289;&#35266;&#28857;&#21644;&#20215;&#20540;&#35266;&#30340;&#21442;&#19982;&#32773;&#65292;&#28040;&#36153;&#32773;&#33021;&#22815;&#29702;&#35299;&#19982;&#25259;&#38706;&#30456;&#20851;&#30340;AI&#31995;&#32479;&#30340;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39118;&#38505;&#27010;&#36848;&#26631;&#20934;&#65292;&#21487;&#20197;&#25351;&#23548;&#19979;&#28216;&#20915;&#31574;&#65292;&#21253;&#25324;&#39118;&#38505;&#35780;&#20272;&#30340;&#20998;&#27969;&#12289;&#37319;&#36141;&#21644;&#37096;&#32626;&#30340;&#20449;&#24687;&#21644;&#25351;&#23548;&#30417;&#31649;&#26694;&#26550;&#12290;&#36825;&#20010;&#26631;&#20934;&#26159;&#24314;&#31435;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;AI&#39118;&#38505;&#20998;&#31867;&#31995;&#32479;&#22522;&#30784;&#19978;&#30340;&#65292;&#21453;&#26144;&#20102;&#24191;&#27867;&#30340;&#39640;&#32423;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems' sophistication and proliferation have increased, awareness of the risks has grown proportionally (Sorkin et al. 2023). In response, calls have grown for stronger emphasis on disclosure and transparency in the AI industry (NTIA 2023; OpenAI 2023b), with proposals ranging from standardizing use of technical disclosures, like model cards (Mitchell et al. 2019), to yet-unspecified licensing regimes (Sindhu 2023). Since the AI value chain is complicated, with actors representing various expertise, perspectives, and values, it is crucial that consumers of a transparency disclosure be able to understand the risks of the AI system the disclosure concerns. In this paper we propose a risk profiling standard which can guide downstream decision-making, including triaging further risk assessment, informing procurement and deployment, and directing regulatory frameworks. The standard is built on our proposed taxonomy of AI risks, which reflects a high-level categorization of the wide 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;AES&#20391;&#20449;&#36947;&#25915;&#20987;&#20013;&#20351;&#29992;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;ANSI&#20391;&#20449;&#36947;&#25915;&#20987;&#25968;&#25454;&#24211;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13170</link><description>&lt;p&gt;
&#30740;&#31350;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;AES&#20391;&#20449;&#36947;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Efficient Deep Learning Architectures For Side-Channel Attacks on AES. (arXiv:2309.13170v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;AES&#20391;&#20449;&#36947;&#25915;&#20987;&#20013;&#20351;&#29992;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#22312;&#20110;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;ANSI&#20391;&#20449;&#36947;&#25915;&#20987;&#25968;&#25454;&#24211;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21033;&#29992;&#23884;&#20837;&#24335;&#23494;&#30721;&#24212;&#29992;&#20013;&#30340;&#20391;&#20449;&#36947;&#28431;&#27934;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#22312;&#26377;&#25928;&#23494;&#38053;&#24674;&#22797;&#25152;&#38656;&#30340;&#25915;&#20987;&#36712;&#36857;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#26041;&#38754;&#20943;&#23569;&#20854;&#25104;&#26412;&#26159;&#19968;&#20010;&#27704;&#24658;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#36861;&#27714;&#36825;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;ANSI&#20391;&#20449;&#36947;&#25915;&#20987;&#25968;&#25454;&#24211;&#65288;ASCAD&#65289;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;JAX&#30340;&#28145;&#24230;&#23398;&#20064;SCA&#26694;&#26550;&#65292;&#37325;&#29616;&#20102;&#19968;&#31995;&#21015;&#20808;&#21069;&#30340;&#32467;&#26524;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25913;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21508;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, deep learning has been getting progressively more popular for the exploitation of side-channel vulnerabilities in embedded cryptographic applications, as it offers advantages in terms of the amount of attack traces required for effective key recovery. A number of effective attacks using neural networks have already been published, but reducing their cost in terms of the amount of computing resources and data required is an ever-present goal, which we pursue in this work. We focus on the ANSSI Side-Channel Attack Database (ASCAD), and produce a JAX-based framework for deep-learning-based SCA, with which we reproduce a selection of previous results and build upon them in an attempt to improve their performance. We also investigate the effectiveness of various Transformer-based models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#22909;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#65292;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#12290;</title><link>http://arxiv.org/abs/2309.13165</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#33391;&#22909;&#30340;&#20856;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Also Good Prototypical Commonsense Reasoners. (arXiv:2309.13165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26356;&#22909;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20339;&#32467;&#26524;&#65292;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#20294;&#22312;&#28041;&#21450;&#27492;&#33021;&#21147;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#25345;&#32493;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#33021;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#65292;&#24182;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#21644;Claude&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;API&#35843;&#29992;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21322;&#33258;&#21160;&#22320;&#24320;&#21457;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25552;&#31034;&#65292;&#21253;&#25324;&#20219;&#21153;&#30456;&#20851;&#24615;&#12289;&#25903;&#25345;&#24615;&#35777;&#25454;&#29983;&#25104;&#65288;&#20363;&#22914;&#24605;&#36335;&#38142;&#21644;&#30693;&#35782;&#65289;&#12289;&#22810;&#26679;&#36335;&#24452;&#35299;&#30721;&#31561;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#12290;&#22312;ProtoQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26356;&#22909;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;ProtoQA&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#26032;&#30340;&#26368;&#20339;&#25104;&#32489;&#65292;&#23558;&#26368;&#22823;&#31572;&#26696;&#27491;&#30830;&#29575;&#25552;&#39640;&#20102;8&#65285;&#65292;&#26368;&#22823;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;4&#65285;&#65288;&#31361;&#30772;50&#65285;&#65289;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#19978;&#19979;&#25991;&#24773;&#24863;&#20272;&#35745;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23545;&#22270;&#20687;&#36827;&#34892;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#36827;&#34892;&#25512;&#29702;&#12290;&#30740;&#31350;&#30528;&#37325;&#20110;&#29702;&#35299;LLMs&#23545;&#20154;&#31867;&#24773;&#24863;&#30340;&#24863;&#30693;&#33021;&#21147;&#20197;&#21450;&#21738;&#20123;&#20449;&#24687;&#33021;&#24110;&#21161;&#20854;&#30830;&#23450;&#24773;&#24863;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#29983;&#25104;&#23383;&#24149;&#21644;&#24773;&#24863;&#27880;&#37322;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#20272;&#35745;&#21644;&#29702;&#35299;&#22330;&#26223;&#20013;&#20803;&#32032;&#23545;&#24773;&#24863;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.13136</link><description>&lt;p&gt;
&#22270;&#20687;&#26631;&#39064;&#20013;&#30340;&#19978;&#19979;&#25991;&#24773;&#24863;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Contextual Emotion Estimation from Image Captions. (arXiv:2309.13136v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#19978;&#19979;&#25991;&#24773;&#24863;&#20272;&#35745;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#23545;&#22270;&#20687;&#36827;&#34892;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#36827;&#34892;&#25512;&#29702;&#12290;&#30740;&#31350;&#30528;&#37325;&#20110;&#29702;&#35299;LLMs&#23545;&#20154;&#31867;&#24773;&#24863;&#30340;&#24863;&#30693;&#33021;&#21147;&#20197;&#21450;&#21738;&#20123;&#20449;&#24687;&#33021;&#24110;&#21161;&#20854;&#30830;&#23450;&#24773;&#24863;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#29983;&#25104;&#23383;&#24149;&#21644;&#24773;&#24863;&#27880;&#37322;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#20272;&#35745;&#21644;&#29702;&#35299;&#22330;&#26223;&#20013;&#20803;&#32032;&#23545;&#24773;&#24863;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#24773;&#24863;&#20272;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#36890;&#36807;&#38754;&#37096;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#30452;&#25509;&#20272;&#35745;&#20154;&#20204;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25903;&#25345;&#19978;&#19979;&#25991;&#24773;&#24863;&#20272;&#35745;&#20219;&#21153;&#65292;&#26041;&#27861;&#26159;&#39318;&#20808;&#23545;&#22270;&#20687;&#36827;&#34892;&#23383;&#24149;&#29983;&#25104;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#36827;&#34892;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#65306;LLMs&#23545;&#20154;&#31867;&#24773;&#24863;&#30340;&#24863;&#30693;&#33021;&#21147;&#22914;&#20309;&#65311;&#20197;&#21450;&#21738;&#20123;&#20449;&#24687;&#37096;&#20998;&#20351;&#23427;&#20204;&#33021;&#22815;&#30830;&#23450;&#24773;&#24863;&#65311;&#39318;&#20808;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#25551;&#36848;&#22330;&#26223;&#20013;&#30340;&#20154;&#29289;&#24182;&#21253;&#21547;&#19982;&#24773;&#24863;&#24863;&#30693;&#30456;&#20851;&#20449;&#24687;&#30340;&#26631;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#38754;&#37096;&#12289;&#36523;&#20307;&#12289;&#20114;&#21160;&#21644;&#29615;&#22659;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#23427;&#20204;&#20026;EMOTIC&#25968;&#25454;&#38598;&#20013;&#30340;331&#20010;&#22270;&#20687;&#25163;&#21160;&#29983;&#25104;&#23383;&#24149;&#21644;&#24773;&#24863;&#27880;&#37322;&#12290;&#36825;&#20123;&#23383;&#24149;&#20026;&#24773;&#24863;&#20272;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#29702;&#35299;&#22330;&#26223;&#20013;&#30340;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion estimation in images is a challenging task, typically using computer vision methods to directly estimate people's emotions using face, body pose and contextual cues. In this paper, we explore whether Large Language Models (LLMs) can support the contextual emotion estimation task, by first captioning images, then using an LLM for inference. First, we must understand: how well do LLMs perceive human emotions? And which parts of the information enable them to determine emotions? One initial challenge is to construct a caption that describes a person within a scene with information relevant for emotion perception. Towards this goal, we propose a set of natural language descriptors for faces, bodies, interactions, and environments. We use them to manually generate captions and emotion annotations for a subset of 331 images from the EMOTIC dataset. These captions offer an interpretable representation for emotion estimation, towards understanding how elements of a scene affect emotion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;OTTR&#20026;&#26680;&#24515;&#30340;&#26412;&#20307;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#21457;&#29616;&#12290;OTTR&#35821;&#35328;&#36890;&#36807;&#23454;&#20363;&#21270;&#27169;&#26495;&#26469;&#26500;&#24314;&#26412;&#20307;&#25110;&#30693;&#35782;&#24211;&#65292;&#36890;&#36807;&#38544;&#34255;&#26412;&#20307;&#34920;&#31034;&#35821;&#35328;&#30340;&#29305;&#23450;&#24615;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#20998;&#31163;&#20915;&#23450;&#24314;&#27169;&#20449;&#24687;&#21644;&#22914;&#20309;&#24314;&#27169;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.13130</link><description>&lt;p&gt;
&#20174;&#19968;&#20010;&#20197;OTTR&#20026;&#26680;&#24515;&#30340;&#26412;&#20307;&#24037;&#31243;&#26041;&#27861;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from an OTTR-centric Ontology Engineering Methodology. (arXiv:2309.13130v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;OTTR&#20026;&#26680;&#24515;&#30340;&#26412;&#20307;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#21457;&#29616;&#12290;OTTR&#35821;&#35328;&#36890;&#36807;&#23454;&#20363;&#21270;&#27169;&#26495;&#26469;&#26500;&#24314;&#26412;&#20307;&#25110;&#30693;&#35782;&#24211;&#65292;&#36890;&#36807;&#38544;&#34255;&#26412;&#20307;&#34920;&#31034;&#35821;&#35328;&#30340;&#29305;&#23450;&#24615;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#20998;&#31163;&#20915;&#23450;&#24314;&#27169;&#20449;&#24687;&#21644;&#22914;&#20309;&#24314;&#27169;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OTTR&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#26412;&#20307;&#24314;&#27169;&#27169;&#24335;&#30340;&#35821;&#35328;&#65292;&#23427;&#36890;&#36807;&#23454;&#20363;&#21270;&#27169;&#26495;&#26469;&#26500;&#24314;&#26412;&#20307;&#25110;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#26412;&#20307;&#34920;&#31034;&#35821;&#35328;&#30340;&#29305;&#23450;&#24615;&#34987;&#38544;&#34255;&#22312;&#39046;&#22495;&#19987;&#23478;&#20043;&#22806;&#65292;&#20351;&#24471;&#26412;&#20307;&#24037;&#31243;&#24072;&#22312;&#20915;&#23450;&#24314;&#27169;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#20449;&#24687;&#65288;&#20363;&#22914;&#20351;&#29992;&#21738;&#20123;&#35774;&#35745;&#27169;&#24335;&#65289;&#26102;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20998;&#31163;&#36825;&#20004;&#20010;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#26576;&#20123;&#20915;&#31574;&#21487;&#20197;&#25512;&#36831;&#65292;&#20197;&#20415;&#26356;&#21152;&#19987;&#27880;&#20110;&#20854;&#20013;&#19968;&#20010;&#36807;&#31243;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;&#24212;&#29992;&#26412;&#20307;&#27169;&#26495;&#30340;&#26412;&#20307;&#24037;&#31243;&#26041;&#38754;&#30340;&#25991;&#29486;&#20013;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#20316;&#21697;&#34987;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26412;&#20307;&#24037;&#31243;&#27963;&#21160;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#36825;&#20123;&#27963;&#21160;&#20013;&#65292;OTTR&#27169;&#26495;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26412;&#20307;&#24037;&#31243;&#36807;&#31243;&#26159;&#33258;&#19979;&#32780;&#19978;&#30340;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;&#25968;&#25454;&#24320;&#22987;&#24314;&#27169;&#27963;&#21160;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#26495;&#23558;&#25968;&#25454;&#36755;&#20837;&#21040;&#19968;&#20010;k
&lt;/p&gt;
&lt;p&gt;
OTTR is a language for representing ontology modeling patterns, which enables to build ontologies or knowledge bases by instantiating templates. Thereby, particularities of the ontological representation language are hidden from the domain experts, and it enables ontology engineers to, to some extent, separate the processes of deciding about what information to model from deciding about how to model the information, e.g., which design patterns to use. Certain decisions can thus be postponed for the benefit of focusing on one of these processes. To date, only few works on ontology engineering where ontology templates are applied are described in the literature.  In this paper, we outline our methodology and report findings from our ontology engineering activities in the domain of Material Science. In these activities, OTTR templates play a key role. Our ontology engineering process is bottom-up, as we begin modeling activities from existing data that is then, via templates, fed into a k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36827;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#23454;&#39564;&#25506;&#32034;&#20102;18&#19990;&#32426;&#29983;&#29289;&#23398;&#23478;Lamarck&#30340;&#36951;&#20256;&#29702;&#35770;&#65292;&#21457;&#29616;&#20010;&#20307;&#36890;&#36807;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#36951;&#20256;&#20256;&#36882;&#32473;&#21518;&#20195;&#65292;&#36825;&#23545;&#36827;&#21270;&#21160;&#21147;&#23398;&#21644;&#36951;&#20256;&#23398;&#26377;&#37325;&#35201;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.13099</link><description>&lt;p&gt;
Lamarck&#30340;&#22797;&#20167;&#65306;&#23398;&#20064;&#29305;&#24449;&#30340;&#36951;&#20256;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36827;&#21270;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Lamarck's Revenge: Inheritance of Learned Traits Can Make Robot Evolution Better. (arXiv:2309.13099v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36827;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#23454;&#39564;&#25506;&#32034;&#20102;18&#19990;&#32426;&#29983;&#29289;&#23398;&#23478;Lamarck&#30340;&#36951;&#20256;&#29702;&#35770;&#65292;&#21457;&#29616;&#20010;&#20307;&#36890;&#36807;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#36951;&#20256;&#20256;&#36882;&#32473;&#21518;&#20195;&#65292;&#36825;&#23545;&#36827;&#21270;&#21160;&#21147;&#23398;&#21644;&#36951;&#20256;&#23398;&#26377;&#37325;&#35201;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#20379;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#20248;&#21183;&#65306;&#36890;&#36807;&#36827;&#21270;&#20248;&#21270;&#26469;&#21457;&#23637;&#26426;&#22120;&#20154;&#30340;&#20808;&#36827;&#26041;&#27861;&#21644;&#29992;&#20110;&#36827;&#34892;&#20851;&#20110;&#36827;&#21270;&#38382;&#39064;&#30340;&#20551;&#35774;&#23454;&#39564;&#30340;&#29305;&#27530;&#30740;&#31350;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22788;&#20110;&#36825;&#20004;&#32773;&#30340;&#20132;&#21449;&#28857;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#21270;&#26426;&#22120;&#20154;&#26694;&#26550;&#30340;&#27169;&#25311;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22914;&#26524;18&#19990;&#32426;&#29983;&#29289;&#23398;&#23478;Lamarck&#24182;&#38750;&#23436;&#20840;&#38169;&#35823;&#65292;&#20010;&#20307;&#22312;&#19968;&#29983;&#20013;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36951;&#20256;&#20256;&#36882;&#32473;&#21518;&#20195;&#65311;&#8221;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20010;Lamarckian&#31995;&#32479;&#65288;&#23398;&#20064;&#21040;&#30340;&#22823;&#33041;&#37096;&#20998;&#21487;&#20197;&#36951;&#20256;&#65289;&#21644;&#19968;&#20010;Darwinian&#31995;&#32479;&#65288;&#23398;&#20064;&#21040;&#30340;&#22823;&#33041;&#37096;&#20998;&#19981;&#33021;&#36951;&#20256;&#65289;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;&#36825;&#20123;&#31995;&#32479;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#20851;&#20110;Lamarckian&#36827;&#21270;&#21160;&#21147;&#23398;&#20197;&#21450;&#36951;&#20256;&#21644;&#23398;&#20064;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary robot systems offer two principal advantages: an advanced way of developing robots through evolutionary optimization and a special research platform to conduct what-if experiments regarding questions about evolution. Our study sits at the intersection of these. We investigate the question ``What if the 18th-century biologist Lamarck was not completely wrong and individual traits learned during a lifetime could be passed on to offspring through inheritance?'' We research this issue through simulations with an evolutionary robot framework where morphologies (bodies) and controllers (brains) of robots are evolvable and robots also can improve their controllers through learning during their lifetime. Within this framework, we compare a Lamarckian system, where learned bits of the brain are inheritable, with a Darwinian system, where they are not. Analyzing simulations based on these systems, we obtain new insights about Lamarckian evolution dynamics and the interaction between
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#35745;&#31639;&#33258;&#28982;&#21746;&#23398;&#23558;&#23431;&#23449;&#27010;&#24565;&#21270;&#20026;&#20449;&#24687;&#21644;&#35745;&#31639;&#65292;&#25512;&#21160;&#20102;&#26234;&#33021;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#22914;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;ChatGPT&#12290;&#36825;&#31181;&#35745;&#31639;&#35270;&#35282;&#32467;&#21512;&#20102;&#22810;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#30340;&#28151;&#21512;&#35745;&#31639;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13094</link><description>&lt;p&gt;
&#35745;&#31639;&#33258;&#28982;&#21746;&#23398;: &#20174;&#21069;&#33487;&#26684;&#25289;&#24213;&#21040;&#22270;&#28789;&#21040;ChatGPT&#30340;&#32447;&#32034;
&lt;/p&gt;
&lt;p&gt;
Computational Natural Philosophy: A Thread from Presocratics through Turing to ChatGPT. (arXiv:2309.13094v1 [cs.GL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13094
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#33258;&#28982;&#21746;&#23398;&#23558;&#23431;&#23449;&#27010;&#24565;&#21270;&#20026;&#20449;&#24687;&#21644;&#35745;&#31639;&#65292;&#25512;&#21160;&#20102;&#26234;&#33021;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#22914;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;ChatGPT&#12290;&#36825;&#31181;&#35745;&#31639;&#35270;&#35282;&#32467;&#21512;&#20102;&#22810;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#30340;&#28151;&#21512;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#33258;&#28982;&#21746;&#23398;&#20197;&#20449;&#24687;&#21644;&#35745;&#31639;&#30340;&#27010;&#24565;&#21270;&#29289;&#36136;&#23431;&#23449;&#65292;&#24182;&#24314;&#31435;&#20102;&#35748;&#30693;&#21644;&#26234;&#33021;&#30740;&#31350;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#25209;&#35780;&#65292;&#20294;&#36825;&#31181;&#35745;&#31639;&#35270;&#35282;&#23545;&#25105;&#20204;&#23545;&#33258;&#28982;&#30028;&#30340;&#29702;&#35299;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;ChatGPT&#31561;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#24471;&#30410;&#20110;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#23558;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#25972;&#21512;&#36215;&#26469;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#20195;&#34920;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20513;&#23548;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#26032;&#19968;&#20195;&#30340;&#28151;&#21512;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computational natural philosophy conceptualizes the universe in terms of information and computation, establishing a framework for the study of cognition and intelligence. Despite some critiques, this computational perspective has significantly influenced our understanding of the natural world, leading to the development of AI systems like ChatGPT based on deep neural networks. Advancements in this domain have been facilitated by interdisciplinary research, integrating knowledge from multiple fields to simulate complex systems. Large Language Models (LLMs), such as ChatGPT, represent this approach's capabilities, utilizing reinforcement learning with human feedback (RLHF). Current research initiatives aim to integrate neural networks with symbolic computing, introducing a new generation of hybrid computational models.
&lt;/p&gt;</description></item><item><title>MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13079</link><description>&lt;p&gt;
MiChao-HuaFen 1.0&#65306;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#19987;&#29992;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0: A Specialized Pre-trained Corpus Dataset for Domain-specific Large Models. (arXiv:2309.13079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13079
&lt;/p&gt;
&lt;p&gt;
MiChao-HuaFen 1.0&#26159;&#19968;&#20010;&#19987;&#20026;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#23450;&#21046;&#30340;&#38754;&#21521;&#39046;&#22495;&#29305;&#23450;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#23427;&#19981;&#20165;&#33021;&#22815;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#38656;&#27714;&#65292;&#36824;&#26377;&#21161;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;GPT-4&#31561;&#36890;&#29992;&#22823;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35832;&#22914;&#21307;&#30103;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#23545;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#36755;&#20986;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;MiChao-HuaFen 1.0&#8221;&#39044;&#35757;&#32451;&#35821;&#26009;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29305;&#21035;&#38024;&#23545;&#26032;&#38395;&#21644;&#25919;&#24220;&#37096;&#38376;&#12290;&#35813;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;2022&#24180;&#20844;&#24320;&#21487;&#29992;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#65292;&#32463;&#36807;&#22810;&#36718;&#28165;&#27905;&#21644;&#22788;&#29702;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#22791;&#25345;&#32493;&#21644;&#31283;&#23450;&#30340;&#26356;&#26032;&#26426;&#21046;&#12290;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#25903;&#25345;&#38024;&#23545;&#20013;&#25991;&#22402;&#30452;&#39046;&#22495;&#30340;&#22823;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#36824;&#21161;&#21147;&#20110;&#25512;&#21160;&#30456;&#20851;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of deep learning technologies, general-purpose large models such as GPT-4 have demonstrated exceptional capabilities across various domains. Nevertheless, there remains a demand for high-quality, domain-specific outputs in areas like healthcare, law, and finance. This paper first evaluates the existing large models for specialized domains and discusses their limitations. To cater to the specific needs of certain domains, we introduce the ``MiChao-HuaFen 1.0'' pre-trained corpus dataset, tailored for the news and governmental sectors. The dataset, sourced from publicly available internet data from 2022, underwent multiple rounds of cleansing and processing to ensure high quality and reliable origins, with provisions for consistent and stable updates. This dataset not only supports the pre-training of large models for Chinese vertical domains but also aids in propelling deep learning research and applications in related fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.13078</link><description>&lt;p&gt;
LPML: &#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LPML&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;LLM&#25552;&#31034;&#26631;&#35760;&#35821;&#35328;&#12290;&#36890;&#36807;&#23558;Chain-of-Thought&#26041;&#27861;&#21644;Python REPL&#19982;&#35813;&#26631;&#35760;&#35821;&#35328;&#32467;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#21033;&#29992;Python&#35745;&#31639;&#32416;&#27491;&#38169;&#35823;&#21644;&#35299;&#20915;&#25361;&#25112;&#24615;&#25968;&#23398;&#38382;&#39064;&#65292;&#32780;&#21482;&#38656;&#35201;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#35299;&#20915;LLMs&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#38169;&#35823;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Chain-of-Thought&#65288;CoT&#65289;&#26041;&#27861;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;Python REPL&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#29983;&#25104;&#31867;&#20284;XML&#26631;&#35760;&#35821;&#35328;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;CoT&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#24182;&#25511;&#21046;LLMs&#30340;&#19981;&#33391;&#34892;&#20026;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;LLMs&#21487;&#20197;&#21033;&#29992;Python&#35745;&#31639;&#26469;&#32416;&#27491;CoT&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ChatGPT&#65288;GPT-3.5&#65289;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#26631;&#35760;&#35821;&#35328;&#23558;CoT&#21644;Python REPL&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#32534;&#20889;&#26631;&#35760;&#35821;&#35328;&#65292;&#24182;&#36827;&#34892;&#39640;&#32423;&#25968;&#23398;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#20013;&#34701;&#21512;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13077</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differentiable Framework for End-to-End Learning of Hybrid Structured Compression. (arXiv:2309.13077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#31471;&#21040;&#31471;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#20013;&#34701;&#21512;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28388;&#27874;&#22120;&#21098;&#26525;&#21644;&#20302;&#31209;&#20998;&#35299;&#26159;&#32467;&#26500;&#21270;&#21387;&#32553;&#30340;&#20004;&#20010;&#22522;&#26412;&#25216;&#26415;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20102;&#25972;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#20248;&#21183;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20294;&#24615;&#33021;&#25552;&#21319;&#19968;&#30452;&#24456;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Differentiable Framework (DF)&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#23558;&#28388;&#27874;&#22120;&#36873;&#25321;&#12289;&#31209;&#36873;&#25321;&#21644;&#39044;&#31639;&#32422;&#26463;&#34701;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20998;&#26512;&#20844;&#24335;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#28388;&#27874;&#22120;&#36873;&#25321;&#30340;DML-S&#65292;&#23558;&#35843;&#24230;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#25513;&#30721;&#23398;&#20064;&#25216;&#26415;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#31209;&#36873;&#25321;&#30340;DTL-S&#65292;&#21033;&#29992;&#22855;&#24322;&#20540;&#38408;&#20540;&#36816;&#31639;&#31526;&#12290;DF&#26694;&#26550;&#32467;&#21512;DML-S&#21644;DTL-S&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#32467;&#26500;&#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#26799;&#24230;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;DF&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#30740;&#31350;&#26041;&#21521;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Filter pruning and low-rank decomposition are two of the foundational techniques for structured compression. Although recent efforts have explored hybrid approaches aiming to integrate the advantages of both techniques, their performance gains have been modest at best. In this study, we develop a \textit{Differentiable Framework~(DF)} that can express filter selection, rank selection, and budget constraint into a single analytical formulation. Within the framework, we introduce DML-S for filter selection, integrating scheduling into existing mask learning techniques. Additionally, we present DTL-S for rank selection, utilizing a singular value thresholding operator. The framework with DML-S and DTL-S offers a hybrid structured compression methodology that facilitates end-to-end learning through gradient-base optimization. Experimental results demonstrate the efficacy of DF, surpassing state-of-the-art structured compression methods. Our work establishes a robust and versatile avenue fo
&lt;/p&gt;</description></item><item><title>SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.13075</link><description>&lt;p&gt;
SCREWS: &#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SCREWS: A Modular Framework for Reasoning with Revisions. (arXiv:2309.13075v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13075
&lt;/p&gt;
&lt;p&gt;
SCREWS&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#12290;&#23427;&#33021;&#22815;&#32479;&#19968;&#20808;&#21069;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#12290;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;ChatGPT&#21644;GPT-4&#65289;&#35780;&#20272;SCREWS&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#21644;&#20462;&#35746;&#20854;&#36755;&#20986;&#26469;&#25552;&#39640;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#20462;&#35746;&#21487;&#33021;&#20250;&#24341;&#20837;&#38169;&#35823;&#65292;&#22914;&#26524;&#26159;&#36825;&#26679;&#30340;&#35805;&#65292;&#26368;&#22909;&#22238;&#28378;&#21040;&#20808;&#21069;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20462;&#35746;&#36890;&#24120;&#26159;&#21516;&#36136;&#30340;&#65306;&#23427;&#20204;&#20351;&#29992;&#19982;&#20135;&#29983;&#21021;&#22987;&#31572;&#26696;&#30340;&#30456;&#21516;&#25512;&#29702;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SCREWS&#65292;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#20462;&#35746;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#12290;&#23427;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;: &#37319;&#26679;&#12289;&#26465;&#20214;&#37325;&#26032;&#37319;&#26679;&#21644;&#36873;&#25321;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21253;&#21547;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#25163;&#21160;&#36873;&#25321;&#30340;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; SCREWS &#19981;&#20165;&#23558;&#20960;&#20010;&#20808;&#21069;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26694;&#26550;&#20013;&#65292;&#36824;&#25581;&#31034;&#20102;&#20960;&#31181;&#29992;&#20110;&#35782;&#21035;&#25913;&#36827;&#30340;&#25512;&#29702;&#38142;&#30340;&#26032;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs &#65288;ChatGPT &#21644; GPT-4&#65289;&#22312;&#22810;&#26679;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#29992;&#30340;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can improve their accuracy on various tasks through iteratively refining and revising their output based on feedback. We observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. Further, revisions are typically homogeneous: they use the same reasoning method that produced the initial answer, which may not correct errors. To enable exploration in this space, we present SCREWS, a modular framework for reasoning with revisions. It is comprised of three main modules: Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. We show that SCREWS not only unifies several previous approaches under a common framework, but also reveals several novel strategies for identifying improved reasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning strategies fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.13072</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Reasoning by Neuro-Symbolic Approaches. (arXiv:2309.13072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#32467;&#21512;&#36215;&#26469;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#40657;&#30418;&#26426;&#22120;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#25105;&#20204;&#22312;NLP&#26041;&#38754;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#27966;&#65292;&#21363;&#31526;&#21495;&#20027;&#20041;&#21644;&#36830;&#25509;&#20027;&#20041;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#25105;&#20204;&#20250;&#35774;&#35745;&#19968;&#20010;&#24102;&#26377;&#31526;&#21495;&#28508;&#22312;&#32467;&#26500;&#30340;&#31070;&#32463;&#31995;&#32479;&#65292;&#29992;&#20110;NLP&#20219;&#21153;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20854;&#26494;&#24347;&#26041;&#27861;&#26469;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24369;&#30417;&#30563;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#34920;&#26684;&#26597;&#35810;&#25512;&#29702;&#12289;&#21477;&#27861;&#32467;&#26500;&#25512;&#29702;&#12289;&#20449;&#24687;&#25277;&#21462;&#25512;&#29702;&#21644;&#35268;&#21017;&#25512;&#29702;&#12290;&#23545;&#20110;&#27599;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#32972;&#26223;&#12289;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has largely improved the performance of various natural language processing (NLP) tasks. However, most deep learning models are black-box machinery, and lack explicit interpretation. In this chapter, we will introduce our recent progress on neuro-symbolic approaches to NLP, which combines different schools of AI, namely, symbolism and connectionism. Generally, we will design a neural system with symbolic latent structures for an NLP task, and apply reinforcement learning or its relaxation to perform weakly supervised reasoning in the downstream task. Our framework has been successfully applied to various tasks, including table query reasoning, syntactic structure reasoning, information extraction reasoning, and rule reasoning. For each application, we will introduce the background, our approach, and experimental results.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PCGML&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#26080;&#38656;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.13071</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#25968;&#25454;&#32423;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree-Based Reconstructive Partitioning: A Novel Low-Data Level Generation Approach. (arXiv:2309.13071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13071
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;PCGML&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#26080;&#38656;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21270;&#20869;&#23481;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#31639;&#27861;&#29983;&#25104;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#24212;&#29992;&#20110;&#28216;&#25103;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;PCG&#26041;&#27861;&#20986;&#29616;&#22312;&#24050;&#21457;&#34920;&#30340;&#28216;&#25103;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;PCG&#38656;&#35201;&#22312;&#35268;&#21017;&#25110;&#20989;&#25968;&#20013;&#34920;&#31034;&#35774;&#35745;&#24072;&#23545;&#36136;&#37327;&#30340;&#27010;&#24565;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;PCG&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22312;&#24320;&#21457;&#21021;&#26399;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#26641;&#30340;&#37325;&#24314;&#20998;&#21306;&#65288;TRP&#65289;&#30340;&#26032;&#39062;PCGML&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TRP&#29983;&#25104;&#30340;&#20851;&#21345;&#26356;&#20855;&#21487;&#29609;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#22312;&#20351;&#29992;&#36739;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;TRP&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;PCGML&#22312;&#28216;&#25103;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#24341;&#20837;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#25110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) is the algorithmic generation of content, often applied to games. PCG and PCG via Machine Learning (PCGML) have appeared in published games. However, it can prove difficult to apply these approaches in the early stages of an in-development game. PCG requires expertise in representing designer notions of quality in rules or functions, and PCGML typically requires significant training data, which may not be available early in development. In this paper, we introduce Tree-based Reconstructive Partitioning (TRP), a novel PCGML approach aimed to address this problem. Our results, across two domains, demonstrate that TRP produces levels that are more playable and coherent, and that the approach is more generalizable with less training data. We consider TRP to be a promising new approach that can afford the introduction of PCGML into the early stages of game development without requiring human expertise or significant training data.
&lt;/p&gt;</description></item><item><title>InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.13064</link><description>&lt;p&gt;
InvestLM&#65306;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13064
&lt;/p&gt;
&lt;p&gt;
InvestLM&#26159;&#19968;&#20010;&#36890;&#36807;&#23545;&#37329;&#34701;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#19978;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#37329;&#34701;&#19987;&#23478;&#35780;&#20215;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#21487;&#23218;&#32654;&#65292;&#24182;&#22312;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#37329;&#34701;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InvestLM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19982;&#37329;&#34701;&#25237;&#36164;&#30456;&#20851;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#23545;LLaMA-65B&#36827;&#34892;&#35843;&#20248;&#12290;&#21463;&#21040;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#26082;&#23567;&#21448;&#22810;&#26679;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#38382;&#39064;&#21040;SEC&#25991;&#20214;&#21644;Stackexchange&#37327;&#21270;&#37329;&#34701;&#35752;&#35770;&#30340;&#24191;&#27867;&#37329;&#34701;&#30456;&#20851;&#20027;&#39064;&#12290;InvestLM&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#29702;&#35299;&#37329;&#34701;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#25237;&#36164;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26377;&#24110;&#21161;&#30340;&#22238;&#31572;&#12290;&#21253;&#25324;&#23545;&#20914;&#22522;&#37329;&#32463;&#29702;&#21644;&#30740;&#31350;&#20998;&#26512;&#24072;&#22312;&#20869;&#30340;&#37329;&#34701;&#19987;&#23478;&#23558;InvestLM&#30340;&#22238;&#31572;&#35780;&#20215;&#20026;&#19982;&#26368;&#20808;&#36827;&#30340;&#21830;&#19994;&#27169;&#22411;&#65288;GPT-3.5&#12289;GPT-4&#21644;Claude-2&#65289;&#21487;&#23218;&#32654;&#12290;&#23545;&#19968;&#32452;&#37329;&#34701;NLP&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20174;&#30740;&#31350;&#35282;&#24230;&#26469;&#30475;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;LLM&#36827;&#34892;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13063</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#24212;&#29992;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies. (arXiv:2309.13063v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20998;&#26512;&#21644;&#39564;&#35777;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#26041;&#27861;&#22312;&#22823;&#22411;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#21487;&#20197;&#25581;&#31034;&#29992;&#25143;&#19982;&#32593;&#32476;&#25628;&#32034;&#26381;&#21153;&#30340;&#20132;&#20114;&#26041;&#24335;&#12289;&#29992;&#25143;&#30340;&#38656;&#27714;&#20197;&#21450;&#28385;&#24847;&#31243;&#24230;&#31561;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#30340;&#32593;&#32476;&#25628;&#32034;&#24418;&#24335;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#12290;&#20026;&#20102;&#29702;&#35299;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#29992;&#26377;&#24847;&#20041;&#30340;&#20998;&#31867;&#26041;&#24335;&#26631;&#35760;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#20854;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#25110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#19988;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#32780;&#35328;&#65292;&#35201;&#20040;&#20195;&#20215;&#39640;&#26114;&#35201;&#20040;&#19981;&#22815;&#28789;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20016;&#23500;&#19988;&#30456;&#20851;&#30340;&#27010;&#24565;&#12289;&#25551;&#36848;&#21644;&#31034;&#20363;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLM&#29983;&#25104;&#29992;&#25143;&#24847;&#22270;&#20998;&#31867;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26085;&#24535;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#20998;&#31867;&#24471;&#19981;&#21040;&#22806;&#37096;&#39564;&#35777;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#19981;&#33391;&#30340;&#21453;&#39304;&#22238;&#36335;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#19987;&#23478;&#21644;&#35780;&#20272;&#32773;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log data can reveal valuable information about how users interact with web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for new forms of web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or ML-based labeling, which are either expensive or inflexible for large and changing datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it to do log analysis can be problematic for two main reasons: such a taxonomy is not externally validated, and there may be an undesirable feedback loop. To overcome these issues, we propose a new methodology with human experts and assessors to verify th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#26045;&#20102;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#23398;&#20064;&#21407;&#29702;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2309.13060</link><description>&lt;p&gt;
&#29992;&#20010;&#20154;AI&#23548;&#24072;&#23454;&#26045;&#23398;&#20064;&#21407;&#29702;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Implementing Learning Principles with a Personal AI Tutor: A Case Study. (arXiv:2309.13060v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#26045;&#20102;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#23398;&#20064;&#21407;&#29702;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#24615;&#21270;&#12289;&#26816;&#32034;&#32451;&#20064;&#21644;&#38388;&#38548;&#37325;&#22797;&#31561;&#21407;&#21017;&#30340;&#26377;&#25928;&#23398;&#20064;&#31574;&#30053;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;AI&#23548;&#24072;&#19982;&#23398;&#20064;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#23398;&#20064;&#31185;&#23398;&#36827;&#34892;&#34917;&#20805;&#12290;&#22312;UniDistance Suisse&#36827;&#34892;&#20102;&#19968;&#20010;&#23398;&#26399;&#38271;&#30340;&#30740;&#31350;&#65292;&#23558;&#19968;&#20010;AI&#23548;&#24072;&#24212;&#29992;&#25552;&#20379;&#32473;&#20462;&#35835;&#31070;&#32463;&#31185;&#23398;&#35838;&#31243;&#30340;&#24515;&#29702;&#23398;&#23398;&#29983;&#65288;N=51&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3&#20174;&#29616;&#26377;&#35838;&#31243;&#26448;&#26009;&#33258;&#21160;&#29983;&#25104;&#24494;&#23398;&#20064;&#38382;&#39064;&#65292;AI&#23548;&#24072;&#24320;&#21457;&#20102;&#27599;&#20010;&#23398;&#29983;&#23545;&#20851;&#38190;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23398;&#29983;&#20010;&#20307;&#27700;&#24179;&#21644;&#33021;&#21147;&#20010;&#24615;&#21270;&#23454;&#26045;&#20998;&#24067;&#24335;&#26816;&#32034;&#32451;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#20351;&#29992;AI&#23548;&#24072;&#21442;&#19982;&#23398;&#20064;&#30340;&#23398;&#29983;&#33719;&#24471;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#31215;&#26497;&#21442;&#19982;&#23548;&#33268;&#24179;&#22343;&#25552;&#39640;&#20102;&#26368;&#22810;15&#20010;&#30334;&#20998;&#28857;&#65292;&#30456;&#27604;&#20110;&#24179;&#34892;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective learning strategies based on principles like personalization, retrieval practice, and spaced repetition are often challenging to implement due to practical constraints. Here we explore the integration of AI tutors to complement learning programs in accordance with learning sciences. A semester-long study was conducted at UniDistance Suisse, where an AI tutor app was provided to psychology students taking a neuroscience course (N=51). After automatically generating microlearning questions from existing course materials using GPT-3, the AI tutor developed a dynamic neural-network model of each student's grasp of key concepts. This enabled the implementation of distributed retrieval practice, personalized to each student's individual level and abilities. The results indicate that students who actively engaged with the AI tutor achieved significantly higher grades. Moreover, active engagement led to an average improvement of up to 15 percentile points compared to a parallel cours
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#30740;&#31350;&#29983;&#24037;&#31243;&#25945;&#32946;&#20013;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#24182;&#22312;&#35838;&#22530;&#19978;&#24102;&#26469;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.13059</link><description>&lt;p&gt;
&#36229;&#36234;&#20256;&#32479;&#25945;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#30740;&#31350;&#29983;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education. (arXiv:2309.13059v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13059
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#30740;&#31350;&#29983;&#24037;&#31243;&#25945;&#32946;&#20013;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#24182;&#22312;&#35838;&#22530;&#19978;&#24102;&#26469;&#28508;&#22312;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#29615;&#22659;&#20013;&#65292;&#25968;&#23383;&#25216;&#26415;&#19968;&#20877;&#25171;&#30772;&#20256;&#32479;&#25945;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#26032;&#30340;&#36825;&#20123;&#39072;&#35206;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#30740;&#31350;&#29983;&#24037;&#31243;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#34701;&#21512;&#12290;&#25105;&#20204;&#39318;&#20808;&#36861;&#28335;&#21382;&#21490;&#21644;&#25216;&#26415;&#30340;&#39072;&#35206;&#65292;&#20197;&#25552;&#20379;&#32972;&#26223;&#65292;&#28982;&#21518;&#20171;&#32461;&#20851;&#38190;&#26415;&#35821;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#20197;&#21450;&#26368;&#36817;&#36827;&#23637;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#21363;&#27880;&#24847;&#21147;/&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#23558;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#20110;&#30740;&#31350;&#29983;&#27969;&#20307;&#21147;&#23398;&#35838;&#31243;&#12290;&#25105;&#20204;&#20174;&#35838;&#31243;&#26448;&#26009;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#38382;&#39064;&#24211;&#65292;&#24182;&#35780;&#20272;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20934;&#30830;&#12289;&#26377;&#35265;&#22320;&#30340;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#19981;&#20165;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#36824;&#23637;&#31034;&#20102;&#35838;&#22530;&#19978;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;&#20363;&#22914;&#8230;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of education, digital technologies have repeatedly disrupted traditional pedagogical methods. This paper explores the latest of these disruptions: the potential integration of large language models (LLMs) and chatbots into graduate engineering education. We begin by tracing historical and technological disruptions to provide context and then introduce key terms such as machine learning and deep learning and the underlying mechanisms of recent advancements, namely attention/transformer models and graphics processing units. The heart of our investigation lies in the application of an LLM-based chatbot in a graduate fluid mechanics course. We developed a question bank from the course material and assessed the chatbot's ability to provide accurate, insightful responses. The results are encouraging, demonstrating not only the bot's ability to effectively answer complex questions but also the potential advantages of chatbot usage in the classroom, such as th
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21019;&#20316;&#30340;&#20316;&#21697;&#24341;&#21457;&#20102;&#20851;&#20110;&#20154;&#31867;&#21019;&#20316;&#26435;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13055</link><description>&lt;p&gt;
&#21407;&#21019;&#24615;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#29256;&#26435;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Originality and the Future of Copyright in an Age of Generative AI. (arXiv:2309.13055v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13055
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21019;&#20316;&#30340;&#20316;&#21697;&#24341;&#21457;&#20102;&#20851;&#20110;&#20154;&#31867;&#21019;&#20316;&#26435;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#20316;&#21697;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21019;&#24314;&#26102;&#65292;&#20154;&#31867;&#21019;&#20316;&#26435;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This papers explores the question of human authorship when works are created with generative AI tools.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#36731;&#26494;&#35775;&#38382;&#21644;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#25972;&#29702;&#30340;&#32791;&#26102;&#21644;&#32321;&#29712;&#38382;&#39064;&#65292;&#20351;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#21487;&#20197;&#26041;&#20415;&#22320;&#21512;&#24182;&#21644;&#20132;&#20114;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13054</link><description>&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Data Commons. (arXiv:2309.13054v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13054
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#36731;&#26494;&#35775;&#38382;&#21644;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#25972;&#29702;&#30340;&#32791;&#26102;&#21644;&#32321;&#29712;&#38382;&#39064;&#65292;&#20351;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#21487;&#20197;&#26041;&#20415;&#22320;&#21512;&#24182;&#21644;&#20132;&#20114;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#20844;&#24320;&#25968;&#25454;&#28304;&#65288;&#20363;&#22914;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#65288;&#20154;&#21475;&#26222;&#26597;&#65289;&#12289;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#12289;&#25919;&#24220;&#38388;&#27668;&#20505;&#21464;&#21270;&#19987;&#38376;&#22996;&#21592;&#20250;&#65288;IPCC&#65289;&#65289;&#30340;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#26159;&#25919;&#31574;&#21046;&#23450;&#32773;&#12289;&#23398;&#29983;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;&#23558;&#26469;&#33258;&#19981;&#21516;&#28304;&#22836;&#30340;&#25968;&#25454;&#32452;&#21512;&#36215;&#26469;&#38656;&#35201;&#29992;&#25143;&#35299;&#20915;&#27169;&#24335;&#12289;&#26684;&#24335;&#12289;&#20551;&#35774;&#31561;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#25968;&#25454;&#25972;&#29702;&#32791;&#26102;&#12289;&#32321;&#29712;&#65292;&#27599;&#20010;&#20351;&#29992;&#25968;&#25454;&#30340;&#29992;&#25143;&#37117;&#38656;&#35201;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;DC&#65289;&#24110;&#21161;&#23558;&#20844;&#20849;&#25968;&#25454;&#21464;&#24471;&#21487;&#35775;&#38382;&#19988;&#26377;&#29992;&#65292;&#20197;&#35299;&#20915;&#31038;&#20250;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#25105;&#20204;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#27169;&#24335;&#21644;&#20113;API&#24191;&#27867;&#25552;&#20379;&#24050;&#22788;&#29702;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#32593;&#32476;&#65292;&#36890;&#36807;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;API&#36827;&#34892;&#20114;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#20844;&#20849;&#27169;&#24335;&#21457;&#24067;&#25968;&#25454;&#30340;&#32593;&#31449;&#12290;&#19981;&#21516;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#25968;&#25454;&#21487;&#20197;&#36731;&#26494;&#21512;&#24182;&#12290;&#25152;&#26377;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#24635;&#21644;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#20805;&#20998;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#35299;&#20915;&#31038;&#20250;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available data from open sources (e.g., United States Census Bureau (Census), World Health Organization (WHO), Intergovernmental Panel on Climate Change (IPCC)) are vital resources for policy makers, students and researchers across different disciplines. Combining data from different sources requires the user to reconcile the differences in schemas, formats, assumptions, and more. This data wrangling is time consuming, tedious and needs to be repeated by every user of the data. Our goal with Data Commons (DC) is to help make public data accessible and useful to those who want to understand this data and use it to solve societal challenges and opportunities. We do the data processing and make the processed data widely available via standard schemas and Cloud APIs. Data Commons is a distributed network of sites that publish data in a common schema and interoperate using the Data Commons APIs. Data from different Data Commons can be joined easily. The aggregate of these Data Comm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#20013;&#23398;&#25945;&#32946;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#38656;&#30340;&#32039;&#36843;&#35838;&#31243;&#25913;&#38761;&#65292;&#24182;&#20998;&#26512;&#20102;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#35838;&#31243;&#32467;&#26500;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#22256;&#22659;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26102;&#38388;&#34920;&#12289;&#25945;&#26448;&#32534;&#20889;&#21644;&#25945;&#23398;&#26041;&#27861;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.13053</link><description>&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#29702;&#35770;&#25351;&#23548;&#23398;&#26657;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using Curriculum Theory to Inform Approaches to Generative AI in Schools. (arXiv:2309.13053v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#20013;&#23398;&#25945;&#32946;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#38656;&#30340;&#32039;&#36843;&#35838;&#31243;&#25913;&#38761;&#65292;&#24182;&#20998;&#26512;&#20102;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#35838;&#31243;&#32467;&#26500;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#22256;&#22659;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26102;&#38388;&#34920;&#12289;&#25945;&#26448;&#32534;&#20889;&#21644;&#25945;&#23398;&#26041;&#27861;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36805;&#36895;&#22686;&#38271;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20013;&#23398;&#25945;&#32946;&#25152;&#38656;&#30340;&#32039;&#36843;&#35838;&#31243;&#25913;&#38761;&#12290;&#20197;Madeline Grumet&#30340;&#35838;&#31243;&#30740;&#31350;&#19977;&#20803;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#30740;&#31350;&#30028;&#23450;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;Elliot Eisner&#30340;&#26174;&#24615;&#12289;&#38544;&#24615;&#21644;&#26080;&#23454;&#38469;&#30446;&#26631;&#35838;&#31243;&#27010;&#24565;&#20043;&#38388;&#30340;&#22810;&#32500;&#20851;&#31995;&#12290;&#30740;&#31350;&#23457;&#35270;&#20102;&#25945;&#32946;&#24037;&#20316;&#32773;&#22312;&#23558;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#21382;&#21490;&#24736;&#20037;&#30340;&#35838;&#31243;&#32467;&#26500;&#26102;&#25152;&#38754;&#20020;&#30340;&#29289;&#27969;&#21644;&#36947;&#24503;&#25361;&#25112;&#65292;&#20363;&#22914; AI &#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#25509;&#35302;Ted Aoki&#30340;&#8220;&#20013;&#38388;&#21306;&#22495;&#8221;&#29702;&#35770;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;&#25945;&#32946;&#24037;&#20316;&#32773;&#22312;&#21327;&#35843;&#35268;&#23450;&#24615;&#35838;&#31243;&#30446;&#26631;&#19982;&#25945;&#23460;&#29983;&#27963;&#30340;&#21464;&#21160;&#30495;&#23454;&#24615;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#21457;&#29983;&#22312;&#30001;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25345;&#32493;&#21464;&#21270;&#30340;&#25945;&#32946;&#29615;&#22659;&#20013;&#12290;&#35770;&#25991;&#26368;&#21518;&#36890;&#36807;&#30740;&#31350;&#20154;&#21592;&#30340;&#21453;&#24605;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#26102;&#38388;&#34920;&#12289;&#25945;&#26448;&#30340;&#32534;&#20889;&#21644;&#25945;&#23398;&#26041;&#27861;&#30340;&#33258;&#20027;&#36873;&#25321;&#31561;&#20027;&#35201;&#38382;&#39064;&#65292;&#20026;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#23398;&#26657;&#25552;&#20379;&#20102;&#19968;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an educational landscape dramatically altered by the swift proliferation of Large Language Models, this essay interrogates the urgent this essay interrogates the urgent pedagogical modifications required in secondary schooling. Anchored in Madeline Grumet's triadic framework of curriculum inquiry, the study delineates the multifaceted relationship between Generative AI and Elliot Eisner's explicit, implicit, and null curriculum concepts. It scrutinizes the logistical and ethical challenges, such as the reliability of AI detectors, that educators confront when attempting to assimilate this nascent technology into long-standing curricular structures. Engaging with Ted Aoki's theory of the "zone of between", the essay illuminates educators' dilemmas in reconciling prescriptive curricular aims with the fluid realities of classroom life, all within an educational milieu in constant flux due to Generative AI. The paper culminates in a reflective analysis by the researcher, identifying ave
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#20998;&#26512;&#20102;&#20234;&#26391;&#27861;&#24459;&#65292;&#20174;&#20013;&#35782;&#21035;&#20986;&#20102;&#21253;&#25324;&#32463;&#27982;&#12289;&#28023;&#20851;&#12289;&#20303;&#25151;&#19982;&#22478;&#24066;&#21457;&#23637;&#12289;&#20892;&#19994;&#12289;&#20445;&#38505;&#12289;&#27861;&#24459;&#21644;&#21496;&#27861;&#12289;&#25991;&#21270;&#12289;&#20449;&#24687;&#25216;&#26415;&#12289;&#25919;&#27835;&#21644;&#25919;&#24220;&#22312;&#20869;&#30340;10&#20010;&#20027;&#39064;&#65292;&#24182;&#21457;&#29616;&#32463;&#27982;&#26159;&#35268;&#23450;&#20013;&#26368;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13051</link><description>&lt;p&gt;
&#20234;&#26391;&#27861;&#24459;&#21644;&#35268;&#23450;&#30340;&#35821;&#22659;&#20027;&#39064;&#24314;&#27169;&#21644;&#20869;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Contextual Topic Modeling and Content Analysis of Iranian laws and Regulations. (arXiv:2309.13051v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#20998;&#26512;&#20102;&#20234;&#26391;&#27861;&#24459;&#65292;&#20174;&#20013;&#35782;&#21035;&#20986;&#20102;&#21253;&#25324;&#32463;&#27982;&#12289;&#28023;&#20851;&#12289;&#20303;&#25151;&#19982;&#22478;&#24066;&#21457;&#23637;&#12289;&#20892;&#19994;&#12289;&#20445;&#38505;&#12289;&#27861;&#24459;&#21644;&#21496;&#27861;&#12289;&#25991;&#21270;&#12289;&#20449;&#24687;&#25216;&#26415;&#12289;&#25919;&#27835;&#21644;&#25919;&#24220;&#22312;&#20869;&#30340;10&#20010;&#20027;&#39064;&#65292;&#24182;&#21457;&#29616;&#32463;&#27982;&#26159;&#35268;&#23450;&#20013;&#26368;&#37325;&#35201;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23466;&#27861;&#26159;&#19968;&#20010;&#22269;&#23478;&#26368;&#39640;&#30340;&#27861;&#24459;&#25991;&#20214;&#65292;&#23427;&#20316;&#20026;&#20854;&#20182;&#27861;&#24459;&#24314;&#31435;&#30340;&#25351;&#21335;&#12290;&#23466;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#22269;&#23478;&#25919;&#24220;&#30340;&#25919;&#27835;&#21407;&#21017;&#12289;&#32467;&#26500;&#12289;&#31561;&#32423;&#12289;&#32844;&#20301;&#21644;&#26435;&#21147;&#30340;&#38480;&#21046;&#12290;&#23427;&#30830;&#23450;&#24182;&#20445;&#38556;&#20844;&#27665;&#30340;&#26435;&#21033;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#20234;&#26391;&#27861;&#24459;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#20174;Dotic&#32593;&#31449;&#25910;&#38598;&#20102;11760&#26465;&#27861;&#24459;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;LDA&#23545;&#27861;&#35268;&#30340;&#26631;&#39064;&#21644;&#20869;&#23481;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#12290;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;10&#20010;&#20027;&#39064;&#65292;&#21253;&#25324;&#32463;&#27982;&#12289;&#28023;&#20851;&#12289;&#20303;&#25151;&#19982;&#22478;&#24066;&#21457;&#23637;&#12289;&#20892;&#19994;&#12289;&#20445;&#38505;&#12289;&#27861;&#24459;&#21644;&#21496;&#27861;&#12289;&#25991;&#21270;&#12289;&#20449;&#24687;&#25216;&#26415;&#12289;&#25919;&#27835;&#21644;&#25919;&#24220;&#12290;&#26368;&#22823;&#30340;&#20027;&#39064;&#26159;&#32463;&#27982;&#65292;&#21344;&#35268;&#23450;&#30340;29%&#65292;&#32780;&#26368;&#23567;&#30340;&#20027;&#39064;&#26159;&#25919;&#27835;&#21644;&#25919;&#24220;&#65292;&#21344;2%&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#26469;&#25506;&#32034;&#27861;&#24459;&#25991;&#26412;&#24182;&#35782;&#21035;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A constitution is the highest legal document of a country and serves as a guide for the establishment of other laws. The constitution defines the political principles, structure, hierarchy, position, and limits of the political power of a country's government. It determines and guarantees the rights of citizens. This study aimed at topic modeling of Iranian laws. As part of this research, 11760 laws were collected from the Dotic website. Then, topic modeling was conducted on the title and content of the regularizations using LDA. Data analysis with topic modeling led to the identification of 10 topics including Economic, Customs, Housing and Urban Development, Agriculture, Insurance, Legal and judicial, Cultural, Information Technology, Political, and Government. The largest topic, Economic, accounts for 29% of regulations, while the smallest are Political and Government, accounting for 2%. This research utilizes a topic modeling method in exploring law texts and identifying trends in 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20010;&#24615;&#21270;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#26159;&#19968;&#20010;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#39640;&#39118;&#38505;&#21306;&#22495;&#21644;&#25512;&#33616;&#31934;&#30830;&#30340;&#20943;&#37325;&#31574;&#30053;&#65292;&#26377;&#25928;&#39044;&#38450;&#31958;&#23615;&#30149;&#30456;&#20851;&#21069;&#36275;&#28291;&#30113;&#21644;&#24182;&#21457;&#30151;&#12290;</title><link>http://arxiv.org/abs/2309.13049</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20010;&#24615;&#21270;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#65306;&#39044;&#38450;&#31958;&#23615;&#30149;&#30456;&#20851;&#21069;&#36275;&#28291;&#30113;&#21644;&#24182;&#21457;&#30151;&#30340;&#23574;&#31471;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Personalised Offloading Device Prescriptions: A Cutting-Edge Approach to Preventing Diabetes-Related Plantar Forefoot Ulcers and Complications. (arXiv:2309.13049v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13049
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20010;&#24615;&#21270;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#26159;&#19968;&#20010;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#39640;&#39118;&#38505;&#21306;&#22495;&#21644;&#25512;&#33616;&#31934;&#30830;&#30340;&#20943;&#37325;&#31574;&#30053;&#65292;&#26377;&#25928;&#39044;&#38450;&#31958;&#23615;&#30149;&#30456;&#20851;&#21069;&#36275;&#28291;&#30113;&#21644;&#24182;&#21457;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#30456;&#20851;&#36275;&#28291;&#30113;&#21644;&#24182;&#21457;&#30151;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#22914;&#19979;&#32930;&#25130;&#32930;&#21644;&#29983;&#27963;&#36136;&#37327;&#38477;&#20302;&#12290;&#26412;&#31456;&#35752;&#35770;&#20102;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20010;&#24615;&#21270;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#20316;&#20026;&#39044;&#38450;&#36825;&#20123;&#29366;&#20917;&#30340;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#36825;&#19968;&#23574;&#31471;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#27599;&#20010;&#24739;&#32773;&#30340;&#29305;&#23450;&#38656;&#27714;&#26469;&#24320;&#20855;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#12290;&#21253;&#25324;&#24739;&#32773;&#23545;&#20943;&#37325;&#35013;&#32622;&#65288;&#22914;&#38795;&#31867;&#21644;&#36275;&#37096;&#30699;&#24418;&#22120;&#65289;&#30340;&#20559;&#22909;&#20197;&#21450;&#36866;&#21512;&#24739;&#32773;&#20351;&#29992;&#24847;&#22270;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30740;&#31350;&#12289;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#39640;&#39118;&#38505;&#21306;&#22495;&#65292;&#20174;&#32780;&#25512;&#33616;&#31934;&#30830;&#30340;&#20943;&#37325;&#31574;&#30053;&#65292;&#21253;&#25324;&#23450;&#21046;&#30699;&#24418;&#38795;&#22443;&#12289;&#38795;&#31867;&#25913;&#36896;&#25110;&#19987;&#38376;&#30340;&#38795;&#31867;&#12290;&#36890;&#36807;&#21253;&#25324;&#24739;&#32773;&#29305;&#23450;&#38656;&#27714;&#30340;&#20010;&#24615;&#21270;&#20943;&#37325;&#35013;&#32622;&#22788;&#26041;&#65292;&#21487;&#20197;&#26377;&#25928;&#39044;&#38450;&#31958;&#23615;&#30149;&#30456;&#20851;&#21069;&#36275;&#28291;&#30113;&#21644;&#24182;&#21457;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes-related foot ulcers and complications are a significant concern for individuals with diabetes, leading to severe health implications such as lower-limb amputation and reduced quality of life. This chapter discusses applying AI-driven personalised offloading device prescriptions as an advanced solution for preventing such conditions. By harnessing the capabilities of artificial intelligence, this cutting-edge approach enables the prescription of offloading devices tailored to each patient's specific requirements. This includes the patient's preferences on offloading devices such as footwear and foot orthotics and their adaptations that suit the patient's intention of use and lifestyle. Through a series of studies, real-world data analysis and machine learning algorithms, high-risk areas can be identified, facilitating the recommendation of precise offloading strategies, including custom orthotic insoles, shoe adaptations, or specialised footwear. By including patient-specific f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Python&#31639;&#27861;&#33258;&#21160;&#35299;&#20915;&#20102;&#39569;&#22763;&#19982;&#35809;&#35745;&#36923;&#36753;&#35868;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#38472;&#36848;&#24182;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26469;&#25512;&#26029;&#35282;&#33394;&#30340;&#30495;&#23454;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2309.13044</link><description>&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#26631;&#39064;&#26159;&#20160;&#20040;&#65311;&#20351;&#29992;&#31639;&#27861;&#35299;&#20915;&#36923;&#36753;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the Title of this Paper? Solving logic puzzles using algorithms. (arXiv:2309.13044v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Python&#31639;&#27861;&#33258;&#21160;&#35299;&#20915;&#20102;&#39569;&#22763;&#19982;&#35809;&#35745;&#36923;&#36753;&#35868;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#38472;&#36848;&#24182;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26469;&#25512;&#26029;&#35282;&#33394;&#30340;&#30495;&#23454;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36923;&#36753;&#35868;&#39064;&#39046;&#22495;&#65292;&#24182;&#19987;&#27880;&#20110;&#38647;&#33945;&#24503;&#183;&#26031;&#31302;&#21033;&#23433;&#22312;&#20182;&#30340;&#12298;&#36825;&#26412;&#20070;&#30340;&#21517;&#23383;&#26159;&#20160;&#20040;&#65311;&#12299;&#31995;&#21015;&#20013;&#25512;&#24191;&#30340;&#39569;&#22763;&#19982;&#35809;&#35745;&#38382;&#39064;&#12290;&#36825;&#20123;&#35868;&#39064;&#22260;&#32469;&#30528;&#34987;&#31216;&#20026;&#39569;&#22763;&#65288;&#35762;&#30495;&#35805;&#32773;&#65289;&#21644;&#35809;&#35745;&#65288;&#35828;&#35854;&#32773;&#65289;&#30340;&#35282;&#33394;&#23637;&#24320;&#65292;&#25361;&#25112;&#35299;&#39064;&#32773;&#26681;&#25454;&#20182;&#20204;&#30340;&#38472;&#36848;&#26469;&#30830;&#23450;&#27599;&#20010;&#20154;&#30340;&#30495;&#23454;&#36523;&#20221;&#12290;&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;Python&#31639;&#27861;&#33258;&#21160;&#21270;&#35299;&#20915;&#36825;&#20123;&#35868;&#39064;&#30340;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#25928;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#35299;&#26512;&#21644;&#20998;&#26512;&#39569;&#22763;&#19982;&#35809;&#35745;&#35868;&#39064;&#20013;&#25152;&#25552;&#20379;&#38472;&#36848;&#30340;Python&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#38598;&#25104;&#20102;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#26681;&#25454;&#38472;&#36848;&#25512;&#26029;&#35282;&#33394;&#30340;&#36523;&#20221;&#12290;&#31639;&#27861;&#22788;&#29702;&#36755;&#20837;&#30340;&#38472;&#36848;&#65292;&#21019;&#24314;&#30693;&#35782;&#24211;&#65292;&#24182;&#25353;&#29031;&#39569;&#22763;&#19982;&#35809;&#35745;&#36923;&#36753;&#30340;&#35268;&#21017;&#36827;&#34892;&#25512;&#29702;&#12290;&#24050;&#32463;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;
&lt;/p&gt;
&lt;p&gt;
This work delves into the realm of logic puzzles by focusing on the Knight and Knave problems popularized by Raymond Smullyan in his book series "What is the Name of This Book?". The puzzles revolve around characters known as Knights (truth-tellers) and Knaves (liars), challenging solvers to determine the true identity of each person based on their statements. This paper explores the utilization of Python algorithms to automate the process of solving these puzzles, offering a computational approach that enhances efficiency and accessibility. In this work, we aim to develop a Python algorithm capable of parsing and analyzing the statements provided in the Knight and Knave puzzles. A logical reasoning framework is integrated within the algorithm to deduce the identities of the characters based on their statements. The algorithm processes the input statements, create a knowledge base, and make deductions following the rules of Knight and Knave logic. The developed algorithm is thoroughly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12312</link><description>&lt;p&gt;
ForceSight: &#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#21147;&#23548;&#21521;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12312
&lt;/p&gt;
&lt;p&gt;
ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ForceSight&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#26469;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30340;&#30446;&#26631;&#12290;&#32473;&#23450;&#19968;&#24352;RGBD&#22270;&#29255;&#21644;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;ForceSight&#21487;&#20197;&#30830;&#23450;&#30456;&#26426;&#22352;&#26631;&#31995;&#19979;&#30340;&#30446;&#26631;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#23039;&#65288;&#36816;&#21160;&#30446;&#26631;&#65289;&#21644;&#30456;&#20851;&#30340;&#21147;&#37327;&#65288;&#21147;&#37327;&#30446;&#26631;&#65289;&#12290;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#20849;&#21516;&#24418;&#25104;&#20102;&#19968;&#20010;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36755;&#20986;&#20154;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#30446;&#26631;&#30340;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#24039;&#22937;&#25805;&#20316;&#12290;&#21147;&#37327;&#22312;&#25805;&#20316;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#36739;&#20302;&#23618;&#27425;&#30340;&#25191;&#34892;&#20013;&#12290;&#24403;&#24212;&#29992;&#20110;&#24102;&#26377;&#25163;&#33218;&#21644;&#30524;&#30555;&#30340;&#31227;&#21160;&#25805;&#20316;&#35013;&#32622;&#30340;ForceSight&#26102;&#65292;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#26174;&#33879;&#30340;&#26410;&#35265;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#20197;81%&#30340;&#25104;&#21151;&#29575;&#23436;&#25104;&#35832;&#22914;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#12290;&#22312;&#21478;&#19968;&#39033;&#29420;&#31435;&#23454;&#39564;&#20013;&#65292;ForceSight&#20165;&#20351;&#29992;&#35270;&#35273;&#20282;&#26381;&#65292;&#19981;&#32771;&#34385;&#21147;&#37327;&#20449;&#24687;&#65292;&#20294;&#20381;&#28982;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#25805;&#20316;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12295</link><description>&lt;p&gt;
&#23398;&#20064;&#39550;&#39542;&#21040;&#20219;&#20309;&#22320;&#26041;
&lt;/p&gt;
&lt;p&gt;
Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#39550;&#39542;&#20915;&#31574;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36947;&#36335;&#26465;&#20214;&#21644;&#20132;&#36890;&#35268;&#21017;&#65292;&#20363;&#22914;&#24038;&#39550;&#39542;&#21644;&#21491;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21482;&#33021;&#22312;&#38480;&#23450;&#30340;&#25805;&#20316;&#39046;&#22495;&#20869;&#37096;&#32626;&#65292;&#19981;&#33021;&#32771;&#34385;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#39550;&#39542;&#34892;&#20026;&#24046;&#24322;&#21644;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyD&#65292;&#19968;&#31181;&#21333;&#19968;&#30340;&#20855;&#26377;&#22320;&#29702;&#24863;&#30693;&#30340;&#26465;&#20214;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;CIL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20855;&#26377;&#21160;&#24577;&#29615;&#22659;&#12289;&#20132;&#36890;&#21644;&#31038;&#20250;&#29305;&#24449;&#30340;&#24322;&#26500;&#21644;&#20840;&#29699;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#39640;&#23481;&#37327;&#30340;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#26412;&#22320;&#32454;&#24494;&#24046;&#24322;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#24615;&#27169;&#20223;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21644;&#22320;&#29702;&#20301;&#32622;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#37325;&#24314;&#26550;&#26500;&#65288;OSNet&#21644;MNetO&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;LCT&#20013;&#23454;&#29616;&#31283;&#23450;&#20869;&#37096;&#37325;&#24314;&#21644;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#26059;&#36716;&#25805;&#20316;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11858</link><description>&lt;p&gt;
OSNet&#21644;MNetO&#65306;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#20004;&#31181;&#36890;&#29992;&#37325;&#24314;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios. (arXiv:2309.11858v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#37325;&#24314;&#26550;&#26500;&#65288;OSNet&#21644;MNetO&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;LCT&#20013;&#23454;&#29616;&#31283;&#23450;&#20869;&#37096;&#37325;&#24314;&#21644;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#26059;&#36716;&#25805;&#20316;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;LCT&#65289;&#31995;&#32479;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20943;&#24369;LCT&#20013;&#30340;&#25237;&#24433;&#25130;&#26029;&#24182;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#36827;&#34892;&#25104;&#20687;&#65292;&#21453;&#25237;&#24433;&#28388;&#27874;&#65288;BPF&#65289;&#31639;&#27861;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;LCT&#30340;BPF&#20013;&#65292;&#24456;&#38590;&#23454;&#29616;&#31283;&#23450;&#30340;&#20869;&#37096;&#37325;&#24314;&#65292;&#24182;&#19988;&#23545;&#20110;LCT&#30340;&#19981;&#21516;&#21453;&#25237;&#24433;&#65288;DBP&#65289;&#22270;&#20687;&#65292;&#22810;&#20010;&#26059;&#36716;&#26377;&#38480;&#21453;&#28436;&#30340;&#24076;&#23572;&#20271;&#29305;&#21464;&#25442;&#65288;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#65289;-&#21453;&#36716;&#25805;&#20316;&#23558;&#20351;&#22270;&#20687;&#27169;&#31946;&#12290;&#20026;&#20102;&#28385;&#36275;LCT&#30340;&#22810;&#31181;&#37325;&#24314;&#22330;&#26223;&#65292;&#21253;&#25324;&#20869;&#37096;ROI&#12289;&#23436;&#25972;&#23545;&#35937;&#21644;&#36229;&#20986;&#35270;&#37326;&#33539;&#22260;&#30340;&#22806;&#37096;&#21306;&#22495;&#65292;&#24182;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#30340;&#26059;&#36716;&#25805;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#37325;&#24314;&#26550;&#26500;&#12290;&#31532;&#19968;&#31181;&#26159;&#21472;&#21152;&#22810;&#20010;DBP&#22270;&#20687;&#20197;&#33719;&#24471;&#23436;&#25972;&#30340;DBP&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#21472;&#21152;&#30340;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#20989;&#25968;&#65292;&#31216;&#20026;&#21472;&#21152;&#21333;&#19968;&#32593;&#32476;&#65288;OSNet&#65289;&#12290;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#35757;&#32451;&#19981;&#21516;&#30340;&#21453;&#25237;&#24433;&#37325;&#26500;&#22120;&#32593;&#32476;&#65288;MNetO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, linear computed tomography (LCT) systems have actively attracted attention. To weaken projection truncation and image the region of interest (ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective solution. However, in BPF for LCT, it is difficult to achieve stable interior reconstruction, and for differentiated backprojection (DBP) images of LCT, multiple rotation-finite inversion of Hilbert transform (Hilbert filtering)-inverse rotation operations will blur the image. To satisfy multiple reconstruction scenarios for LCT, including interior ROI, complete object, and exterior region beyond field-of-view (FOV), and avoid the rotation operations of Hilbert filtering, we propose two types of reconstruction architectures. The first overlays multiple DBP images to obtain a complete DBP image, then uses a network to learn the overlying Hilbert filtering function, referred to as the Overlay-Single Network (OSNet). The second uses multiple networks to train diffe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.11495</link><description>&lt;p&gt;
&#38142;&#24335;&#39564;&#35777;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#29983;&#25104;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#20449;&#24687;&#65288;&#21363;&#24187;&#35273;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#22238;&#22797;&#26102;&#36827;&#34892;&#24605;&#32771;&#20197;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#65288;CoVe&#65289;&#26041;&#27861;&#65292;&#27169;&#22411;&#39318;&#20808;&#65288;i&#65289;&#36215;&#33609;&#21021;&#22987;&#22238;&#22797;&#65307;&#28982;&#21518;&#65288;ii&#65289;&#35745;&#21010;&#39564;&#35777;&#38382;&#39064;&#26469;&#20107;&#23454;&#26816;&#26597;&#33609;&#31295;&#65307;&#65288;iii&#65289;&#29420;&#31435;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#31572;&#26696;&#21463;&#20854;&#20182;&#22238;&#22797;&#30340;&#24433;&#21709;&#65307;&#26368;&#21518;&#65288;iv&#65289;&#29983;&#25104;&#26368;&#32456;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#22238;&#31572;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoVe&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;&#24187;&#35273;&#30340;&#24773;&#20917;&#65292;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#25968;&#25454;&#30340;&#21015;&#34920;&#38382;&#39064;&#12289;&#23553;&#38381;&#20070;&#31821;MultiSpanQA&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#27714;&#35299;SAT&#38382;&#39064;&#30340;&#8220;&#31070;&#35861;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11452</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20855;&#26377;&#24615;&#33021;&#36793;&#30028;&#30340;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Using deep learning to construct stochastic local search SAT solvers with performance bounds. (arXiv:2309.11452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#31181;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#27714;&#35299;SAT&#38382;&#39064;&#30340;&#8220;&#31070;&#35861;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#26159;&#26368;&#20856;&#22411;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#20855;&#26377;&#26497;&#22823;&#30340;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#27714;&#35299;&#22120;&#31867;&#21035;&#26159;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#65288;SLS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21644;&#38543;&#26426;&#26356;&#26032;&#20505;&#36873;&#35299;&#26469;&#27714;&#35299;&#12290;&#26368;&#36817;&#65292;&#22312;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24314;&#31435;&#20102;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;SLS&#27714;&#35299;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#27714;&#35299;SAT&#23454;&#20363;&#65292;&#21482;&#35201;&#23427;&#20204;&#21487;&#20197;&#35775;&#38382;&#21512;&#36866;&#30340;&#8220;&#31070;&#35861;&#8221;&#65292;&#20174;&#23454;&#20363;&#29305;&#23450;&#30340;&#20998;&#24067;&#20013;&#25552;&#20379;&#26679;&#26412;&#65292;&#21033;&#29992;&#23454;&#20363;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#21463;&#36825;&#20123;&#32467;&#26524;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#32467;&#26500;&#30340;&#33391;&#22909;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#31070;&#35861;&#65292;&#24182;&#22312;&#20004;&#20010;SLS&#27714;&#35299;&#22120;&#19978;&#23545;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#30340;&#38543;&#26426;SAT&#23454;&#20363;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35775;&#38382;&#22522;&#20110;GNN&#30340;&#31070;&#35861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20004;&#20010;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#20351;&#23427;&#20204;&#24179;&#22343;&#33021;&#22815;&#35299;&#20915;17&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.11331</link><description>&lt;p&gt;
Gold-YOLO: &#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;YOLO&#31995;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#12289;&#22686;&#21152;&#25968;&#25454;&#21644;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#22522;&#32447;&#25552;&#21319;&#21040;&#20102;&#26356;&#39640;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#34429;&#28982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#36335;&#24452;&#32858;&#21512;&#32593;&#32476;&#65288;PANet&#65289;&#24050;&#32463;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23454;&#29616;&#12290;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#27169;&#22411;&#21517;&#20026;Gold-YOLO&#65292;&#25552;&#21319;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#33021;&#21147;&#65292;&#24182;&#22312;&#25152;&#26377;&#27169;&#22411;&#23610;&#24230;&#19978;&#23454;&#29616;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#24819;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;YOLO&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;YOLO&#31995;&#21015;&#27169;&#22411;&#21487;&#20197;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;Gold-YOLO-N&#22312;COCO val2017&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;39.9%&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10283</link><description>&lt;p&gt;
FRAMU: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10283
&lt;/p&gt;
&lt;p&gt;
FRAMU&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#65292;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#65292;&#25903;&#25345;&#25345;&#32493;&#27169;&#22411;&#28436;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#36890;&#36807;&#20801;&#35768;&#20174;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#20013;&#21024;&#38500;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#65292;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20351;&#29992;&#36807;&#26102;&#30340;&#12289;&#31169;&#26377;&#30340;&#21644;&#26080;&#20851;&#30340;&#25968;&#25454;&#20250;&#24341;&#21457;&#19982;&#38544;&#31169;&#21644;&#27169;&#22411;&#25928;&#29575;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#19981;&#20165;&#24433;&#21709;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36951;&#24536;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#36824;&#20250;&#23545;&#25968;&#25454;&#38544;&#31169;&#36896;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#36951;&#24536;&#65288;FRAMU&#65289;&#12290;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#26426;&#21046;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#26159;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#65289;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;FRAMU&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#36866;&#24212;&#27874;&#21160;&#30340;&#25968;&#25454;&#29615;&#22659;&#12289;&#36951;&#24536;&#36807;&#26102;&#12289;&#31169;&#26377;&#25110;&#26080;&#20851;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25903;&#25345;&#27169;&#22411;&#25345;&#32493;&#28436;&#36827;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MLFF-Net &#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24687;&#32905;&#20998;&#21106;&#30340;&#25913;&#36827;&#12290;&#23545;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28388;&#27874;&#21644;&#21033;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10219</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24687;&#32905;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-level feature fusion network combining attention mechanisms for polyp segmentation. (arXiv:2309.10219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MLFF-Net &#30340;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24687;&#32905;&#20998;&#21106;&#30340;&#25913;&#36827;&#12290;&#23545;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28388;&#27874;&#21644;&#21033;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#19978;&#65292;&#33258;&#21160;&#21270;&#30340;&#24687;&#32905;&#20998;&#21106;&#25216;&#26415;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#21307;&#23398;&#35786;&#26029;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24739;&#32773;&#24739;&#32467;&#30452;&#32928;&#30284;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#26174;&#33879;&#24369;&#28857;&#21487;&#33021;&#24433;&#21709;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#26410;&#32463;&#20805;&#20998;&#28388;&#27874;&#21644;&#21033;&#29992;&#12290;&#20854;&#27425;&#65292;&#30001;&#29305;&#24449;&#34701;&#21512;&#24341;&#36215;&#30340;&#35821;&#20041;&#20914;&#31361;&#21644;&#20449;&#24687;&#20887;&#20313;&#27809;&#26377;&#24471;&#21040;&#20851;&#27880;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24687;&#32905;&#20998;&#21106;&#26041;&#27861;&#65292;&#21517;&#20026; MLFF-Net&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#27425;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MLFF-Net &#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;MAM&#65289;&#12289;&#39640;&#23618;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65288;HFEM&#65289;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;GAM&#65289;&#12290;&#20854;&#20013;&#65292;MAM &#29992;&#20110;&#20174;&#32534;&#30721;&#22120;&#30340;&#27973;&#23618;&#36755;&#20986;&#20013;&#25552;&#21462;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#24687;&#32905;&#32454;&#33410;&#12290;&#22312; HFEM &#20013;&#65292;&#32534;&#30721;&#22120;&#30340;&#28145;&#23618;&#29305;&#24449;&#21487;&#20197;&#34987;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinically, automated polyp segmentation techniques have the potential to significantly improve the efficiency and accuracy of medical diagnosis, thereby reducing the risk of colorectal cancer in patients. Unfortunately, existing methods suffer from two significant weaknesses that can impact the accuracy of segmentation. Firstly, features extracted by encoders are not adequately filtered and utilized. Secondly, semantic conflicts and information redundancy caused by feature fusion are not attended to. To overcome these limitations, we propose a novel approach for polyp segmentation, named MLFF-Net, which leverages multi-level feature fusion and attention mechanisms. Specifically, MLFF-Net comprises three modules: Multi-scale Attention Module (MAM), High-level Feature Enhancement Module (HFEM), and Global Attention Module (GAM). Among these, MAM is used to extract multi-scale information and polyp details from the shallow output of the encoder. In HFEM, the deep features of the encoders
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#36827;&#34892;&#23450;&#20215;&#65292;&#24182;&#19988;&#22312;&#26657;&#20934;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07843</link><description>&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Applying Deep Learning to Calibrate Stochastic Volatility Models. (arXiv:2309.07843v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#26657;&#20934;&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#36827;&#34892;&#23450;&#20215;&#65292;&#24182;&#19988;&#22312;&#26657;&#20934;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#27874;&#21160;&#24615;&#27169;&#22411;&#26159;&#19968;&#31181;&#27874;&#21160;&#29575;&#26159;&#38543;&#26426;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#38544;&#21547;&#27874;&#21160;&#29575;&#26354;&#38754;&#30340;&#22823;&#37096;&#20998;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#27874;&#21160;&#29575;&#31505;&#26354;&#32447;&#25110;&#20559;&#26012;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#26657;&#20934;&#26102;&#38388;&#36807;&#38271;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#30340;&#26367;&#20195;&#26657;&#20934;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#26500;&#24314;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26657;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;Huge&#21644;Savine&#24320;&#21457;&#20102;&#19968;&#31181;&#24046;&#20998;&#28145;&#24230;&#23398;&#20064;&#65288;DDL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#20013;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#26679;&#26412;&#19981;&#20165;&#21253;&#25324;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#23545;&#29305;&#24449;&#30340;&#24494;&#20998;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;DDL&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#22522;&#26412;&#27431;&#27954;&#26399;&#26435;&#65288;&#21363;&#26657;&#20934;&#24037;&#20855;&#65289;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#22522;&#20110;Heston&#27169;&#22411;&#30340;&#26631;&#30340;&#36164;&#20135;&#19978;&#23450;&#20215;&#30475;&#28072;&#26399;&#26435;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#23545;&#27169;&#22411;&#36827;&#34892;&#26657;&#20934;&#12290;DDL&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#35757;&#32451;&#21644;&#20934;&#30830;&#23450;&#20215;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#25103;&#21095;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
Stochastic volatility models, where the volatility is a stochastic process, can capture most of the essential stylized facts of implied volatility surfaces and give more realistic dynamics of the volatility smile or skew. However, they come with the significant issue that they take too long to calibrate.  Alternative calibration methods based on Deep Learning (DL) techniques have been recently used to build fast and accurate solutions to the calibration problem. Huge and Savine developed a Differential Deep Learning (DDL) approach, where Machine Learning models are trained on samples of not only features and labels but also differentials of labels to features. The present work aims to apply the DDL technique to price vanilla European options (i.e. the calibration instruments), more specifically, puts when the underlying asset follows a Heston model and then calibrate the model on the trained network. DDL allows for fast training and accurate pricing. The trained neural network dramatic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item><item><title>JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#20013;&#65292;JSMNet&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23460;&#20869;&#30446;&#26631;&#29305;&#24449;&#34920;&#36798;&#21644;&#35821;&#20041;&#12289;&#23454;&#20363;&#20998;&#21106;&#32467;&#26524;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07425</link><description>&lt;p&gt;
JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07425
&lt;/p&gt;
&lt;p&gt;
JSMNet&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#25913;&#36827;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#20013;&#65292;JSMNet&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#23460;&#20869;&#30446;&#26631;&#29305;&#24449;&#34920;&#36798;&#21644;&#35821;&#20041;&#12289;&#23454;&#20363;&#20998;&#21106;&#32467;&#26524;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;3D&#28857;&#20113;&#25968;&#25454;&#30340;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#19968;&#31995;&#21015;&#21518;&#32493;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#23460;&#20869;&#26381;&#21153;&#26426;&#22120;&#20154;&#12289;&#23548;&#33322;&#31995;&#32479;&#21644;&#25968;&#23383;&#23402;&#29983;&#24037;&#31243;&#12290;&#20840;&#23616;&#29305;&#24449;&#23545;&#20110;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#23460;&#20869;&#28857;&#20113;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JSMNet&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#23618;&#32593;&#32476;&#21644;&#20840;&#23616;&#29305;&#24449;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20849;&#21516;&#20998;&#21106;&#19977;&#32500;&#28857;&#20113;&#30340;&#35821;&#20041;&#21644;&#23454;&#20363;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#36798;&#23460;&#20869;&#30446;&#26631;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#29305;&#24449;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#65292;&#32771;&#34385;&#20102;&#30001;&#20110;&#25195;&#25551;&#20202;&#36317;&#31163;&#30446;&#26631;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#30340;&#28857;&#20113;&#23494;&#24230;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#35821;&#20041;&#21644;&#23454;&#20363;&#29305;&#24449;&#30340;&#32852;&#21512;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;&#65292;&#20197;&#36798;&#21040;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;S3DIS&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22823;&#22411;&#30340;&#23460;&#20869;&#22320;&#29289;&#28857;&#20113;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semantic understanding of indoor 3D point cloud data is crucial for a range of subsequent applications, including indoor service robots, navigation systems, and digital twin engineering. Global features are crucial for achieving high-quality semantic and instance segmentation of indoor point clouds, as they provide essential long-range context information. To this end, we propose JSMNet, which combines a multi-layer network with a global feature self-attention module to jointly segment three-dimensional point cloud semantics and instances. To better express the characteristics of indoor targets, we have designed a multi-resolution feature adaptive fusion module that takes into account the differences in point cloud density caused by varying scanner distances from the target. Additionally, we propose a framework for joint semantic and instance segmentation by integrating semantic and instance features to achieve superior results. We conduct experiments on S3DIS, which is a large thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06129</link><description>&lt;p&gt;
LEyes&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21152;&#24378;&#20102;&#20957;&#35270;&#20272;&#35745;&#25216;&#26415;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#30524;&#37096;&#22270;&#20687;&#30340;&#30828;&#20214;&#24341;&#36215;&#30340;&#21464;&#24322;&#20197;&#21450;&#35760;&#24405;&#30340;&#21442;&#19982;&#32773;&#20043;&#38388;&#22266;&#26377;&#30340;&#29983;&#29289;&#24046;&#24322;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34394;&#25311;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21019;&#24314;&#34394;&#25311;&#25968;&#25454;&#38598;&#26082;&#38656;&#35201;&#26102;&#38388;&#21448;&#38656;&#35201;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Light Eyes or "LEyes"&#30340;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#36924;&#30495;&#26041;&#27861;&#19981;&#21516;&#65292;LEyes&#20165;&#27169;&#25311;&#35270;&#39057;&#30524;&#21160;&#36319;&#36394;&#25152;&#38656;&#30340;&#20851;&#38190;&#22270;&#20687;&#29305;&#24449;&#12290;LEyes&#20415;&#20110;&#22312;&#22810;&#26679;&#21270;&#30340;&#20957;&#35270;&#20272;&#35745;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30524;&#30555;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addit
&lt;/p&gt;</description></item><item><title>SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2309.04077</link><description>&lt;p&gt;
SayNav&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26032;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04077
&lt;/p&gt;
&lt;p&gt;
SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25512;&#29702;&#21644;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#23545;&#20110;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#26159;&#20154;&#31867;&#25152;&#20855;&#22791;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SayNav&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23545;&#26410;&#30693;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;SayNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25509;&#22320;&#26426;&#21046;&#65292;&#36880;&#27493;&#26500;&#24314;&#19968;&#20010;&#25506;&#32034;&#29615;&#22659;&#30340;3D&#22330;&#26223;&#22270;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#19988;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#39640;&#23618;&#23548;&#33322;&#35745;&#21010;&#12290;&#28982;&#21518;&#65292;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#25191;&#34892;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#65292;&#23558;&#27599;&#20010;&#35745;&#21010;&#30340;&#27493;&#39588;&#35270;&#20026;&#30701;&#36317;&#31163;&#28857;&#30446;&#26631;&#23548;&#33322;&#23376;&#20219;&#21153;&#12290;SayNav&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#19968;&#27493;&#19968;&#27493;&#30340;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#26032;&#33719;&#21462;&#30340;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#26426;&#39564;&#35777;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;SayNav&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#31895;&#31961;&#38598;&#20013;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#24754;&#35266;&#21644;&#20048;&#35266;&#30340;&#21512;&#24182;&#65292;&#20197;&#21450;&#30740;&#31350;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03217</link><description>&lt;p&gt;
&#27867;&#31895;&#31961;&#38598;&#20013;&#30340;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#21450;&#25512;&#29702;&#20559;&#24046;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Algebraic Models for Qualified Aggregation in General Rough Sets, and Reasoning Bias Discovery. (arXiv:2309.03217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#31895;&#31961;&#38598;&#20013;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#24754;&#35266;&#21644;&#20048;&#35266;&#30340;&#21512;&#24182;&#65292;&#20197;&#21450;&#30740;&#31350;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27867;&#31895;&#31961;&#38598;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#20004;&#20010;&#20107;&#29289;&#32452;&#21512;&#25104;&#21478;&#19968;&#20010;&#24182;&#38750;&#30452;&#25509;&#12290;&#23545;&#20110;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#30340;&#20854;&#20182;&#29702;&#35770;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#31181;&#34892;&#20026;&#21487;&#20197;&#36171;&#20104;&#39069;&#22806;&#30340;&#24847;&#20041;&#65292;&#36229;&#36234;&#20102;&#32467;&#26500;&#19978;&#30340;&#21512;&#21462;&#21644;&#26512;&#21462;&#65292;&#23601;&#20687;$L$&#27169;&#31946;&#38598;&#19978;&#30340;$*$-&#33539;&#25968;&#29702;&#35770;&#21644;&#30456;&#20851;&#25512;&#23548;&#19968;&#26679;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#26126;&#20102;&#22312;&#20855;&#26377;&#36817;&#20284;&#31639;&#23376;&#65288;&#31216;&#20026;&#31895;&#31961;&#20415;&#21033;&#26684;&#65289;&#30340;&#26684;&#19978;&#23558;&#20107;&#29289;&#32452;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#12290;&#30740;&#31350;&#21463;&#21040;&#35201;&#24314;&#27169;&#24576;&#30097;&#35770;&#25110;&#24754;&#35266;&#35770;&#21644;&#20048;&#35266;&#35770;&#21512;&#24182;&#20110;&#20154;&#31867;&#25512;&#29702;&#65292;&#20197;&#21450;&#25805;&#20316;&#36873;&#25321;&#34987;&#35266;&#28857;&#25152;&#32422;&#26463;&#30340;&#21160;&#26426;&#30340;&#24378;&#28872;&#25512;&#21160;&#12290;&#35777;&#26126;&#20102;&#26368;&#23567;&#27169;&#22411;&#25552;&#20379;&#30340;&#24369;&#21542;&#23450;&#21644;&#25512;&#23548;&#30340;&#22522;&#26412;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of general rough sets, the act of combining two things to form another is not straightforward. The situation is similar for other theories that concern uncertainty and vagueness. Such acts can be endowed with additional meaning that go beyond structural conjunction and disjunction as in the theory of $*$-norms and associated implications over $L$-fuzzy sets. In the present research, algebraic models of acts of combining things in generalized rough sets over lattices with approximation operators (called rough convenience lattices) is invented. The investigation is strongly motivated by the desire to model skeptical or pessimistic, and optimistic or possibilistic aggregation in human reasoning, and the choice of operations is constrained by the perspective. Fundamental results on the weak negations and implications afforded by the minimal models are proved. In addition, the model is suitable for the study of discriminatory/toxic behavior in human reasoning, and of ML algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01219</link><description>&lt;p&gt;
AI&#28023;&#27915;&#20013;&#30340;&#22934;&#24618;&#20043;&#27468;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#34920;&#31034;&#25285;&#24551;&#65306;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19982;&#29992;&#25143;&#36755;&#20837;&#19981;&#31526;&#12289;&#19982;&#20808;&#21069;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#30683;&#30462;&#25110;&#19982;&#24050;&#24314;&#31435;&#30340;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#29616;&#35937;&#23545;LLMs&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#20851;&#20110;&#24187;&#35273;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26088;&#22312;&#32531;&#35299;LLM&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.16157</link><description>&lt;p&gt;
&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21457;&#26126;&#20102;&#30830;&#23450;&#33258;&#24049;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#23384;&#22312;&#24615;&#39063;&#31890;&#26159;&#37027;&#20123;&#26368;&#21021;&#30830;&#23450;&#33258;&#24049;&#65292;&#24182;&#38543;&#21518;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#39063;&#31890;&#12290;&#36825;&#20010;&#27010;&#24565;&#30340;&#31034;&#20363;&#65292;&#27604;&#22914;&#39063;&#31890;&#29699;&#65292;&#22312;&#20043;&#21069;&#20854;&#20182;&#20154;&#30340;&#20316;&#21697;&#20013;&#34429;&#28982;&#23450;&#20041;&#19981;&#23436;&#22791;&#12289;&#31639;&#27861;&#24314;&#31435;&#19981;&#20805;&#20998;&#12289;&#29702;&#35770;&#21270;&#19981;&#36275;&#65292;&#20294;&#24050;&#32463;&#22312;&#31895;&#31961;&#38598;&#21644;&#36719;&#35745;&#31639;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#36866;&#21512;&#20110;&#39063;&#31890;&#35745;&#31639;&#30340;&#22810;&#20010;&#29702;&#35770;&#26694;&#26550;&#65288;&#20844;&#29702;&#21270;&#12289;&#36866;&#24212;&#24615;&#31561;&#65289;&#12290;&#36825;&#31181;&#21051;&#30011;&#26088;&#22312;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#30340;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#30340;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Micro3Diff&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#20108;&#32500;&#21040;&#19977;&#32500;&#24494;&#32467;&#26500;&#30340;&#37325;&#24314;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#22810;&#24179;&#38754;&#21435;&#22122;&#25193;&#25955;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2308.14035</link><description>&lt;p&gt;
&#22810;&#24179;&#38754;&#21435;&#22122;&#25193;&#23637;&#32500;&#24230;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#24494;&#32467;&#26500;&#20108;&#32500;&#21040;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#19982;&#37319;&#26679;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Multi-plane denoising diffusion-based dimensionality expansion for 2D-to-3D reconstruction of microstructures with harmonized sampling. (arXiv:2308.14035v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Micro3Diff&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#20108;&#32500;&#21040;&#19977;&#32500;&#24494;&#32467;&#26500;&#30340;&#37325;&#24314;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#22810;&#24179;&#38754;&#21435;&#22122;&#25193;&#25955;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#21487;&#38752;&#30340;&#24494;&#32467;&#26500;&#25968;&#25454;&#38598;&#23545;&#20110;&#20351;&#29992;&#38598;&#25104;&#35745;&#31639;&#26448;&#26009;&#24037;&#31243;&#65288;ICME&#65289;&#26041;&#27861;&#36827;&#34892;&#26448;&#26009;&#31995;&#32479;&#35774;&#35745;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring reliable microstructure datasets is a pivotal step toward the systematic design of materials with the aid of integrated computational materials engineering (ICME) approaches. However, obtaining three-dimensional (3D) microstructure datasets is often challenging due to high experimental costs or technical limitations, while acquiring two-dimensional (2D) micrographs is comparatively easier. To deal with this issue, this study proposes a novel framework for 2D-to-3D reconstruction of microstructures called Micro3Diff using diffusion-based generative models (DGMs). Specifically, this approach solely requires pre-trained DGMs for the generation of 2D samples, and dimensionality expansion (2D-to-3D) takes place only during the generation process (i.e., reverse diffusion process). The proposed framework incorporates a new concept referred to as multi-plane denoising diffusion, which transforms noisy samples (i.e., latent variables) from different planes into the data structure whil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13546</link><description>&lt;p&gt;
Hyperscanning EEG&#30340;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#24773;&#32490;&#20256;&#26579;&#30340;&#32454;&#24494;&#24046;&#24322;&#21450;&#20854;&#23545;&#21452;&#20154;&#20114;&#21160;&#20013;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#32858;&#28966;&#20110;&#22899;&#24615;&#23545;&#30340;&#21512;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#32972;&#26223;&#12290;&#36890;&#36807;&#23545;&#24773;&#32490;&#20256;&#26579;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#28508;&#22312;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;&#21033;&#29992;&#22522;&#20110;EEG&#30340;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;fGCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25552;&#21462;&#20027;&#20307;&#19981;&#21464;&#30340;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#24212;&#29992;&#21160;&#24577;&#22270;&#20998;&#31867;&#65288;DGC&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#21078;&#26512;&#24773;&#32490;&#20256;&#26579;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#33041;&#37096;&#21516;&#27493;&#21644;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#24378;&#35843;&#24773;&#32490;&#20256;&#26579;&#22312;&#22609;&#36896;&#36712;&#36857;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11358</link><description>&lt;p&gt;
&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#20197;&#22238;&#31572;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22312;&#19977;&#20010;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#24314;&#27169;&#38271;&#26399;&#19978;&#19979;&#25991;&#23545;&#20110;&#35768;&#22810;&#32454;&#31890;&#24230;&#20219;&#21153;&#21253;&#25324;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;transformers&#21487;&#20197;&#23545;&#35270;&#39057;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#38271;&#35270;&#39057;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#30740;&#31350;&#32467;&#21512;&#20102;&#20351;&#29992;&#23616;&#37096;&#26102;&#38388;&#31383;&#21475;&#35745;&#31639;&#20986;&#30340;&#33258;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#26080;&#27861;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#24182;&#21033;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#35270;&#39057;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#65292;&#35797;&#22270;&#22238;&#31572;&#38656;&#35201;&#22810;&#23569;&#38271;&#26399;&#26102;&#38388;&#19978;&#19979;&#25991;&#25165;&#33021;&#36827;&#34892;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#30446;&#21069;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26102;&#38388;&#34892;&#21160;&#20998;&#21106;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;50Salads&#65292;Brea...
&lt;/p&gt;
&lt;p&gt;
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24314;&#27169;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#22797;&#26434;&#24615;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11034</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#30340;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Oriented Complex Networked Systems based on Heterogeneous node features and interaction rules. (arXiv:2308.11034v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#23383;&#23402;&#29983;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24314;&#27169;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#21644;&#20132;&#20114;&#35268;&#21017;&#12290;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#19981;&#21516;&#32423;&#21035;&#30340;&#22797;&#26434;&#24615;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#23548;&#21521;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#65288;DT-CNS&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#30495;&#23454;&#34920;&#31034;&#23454;&#38469;&#31995;&#32479;&#30340;&#32593;&#32476;&#12290;&#24314;&#27169;&#36807;&#31243;&#20851;&#27880;&#33410;&#28857;&#30340;&#29305;&#24449;&#21644;&#22522;&#20110;&#20010;&#20307;&#33410;&#28857;&#20559;&#22909;&#21019;&#24314;&#36830;&#25509;&#30340;&#20132;&#20114;&#35268;&#21017;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#27169;&#25311;&#30340;DT-CNS&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;&#21508;&#31181;&#29305;&#24449;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#19982;&#20256;&#26579;&#30149;&#22312;&#36825;&#20123;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#30456;&#20851;&#30340;&#19981;&#21516;&#20256;&#26579;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#29305;&#23450;&#26102;&#38388;&#21644;&#31038;&#20132;&#36317;&#31163;&#20869;&#30340;&#24863;&#26579;&#24773;&#20917;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#28798;&#23475;&#38887;&#24615;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#22797;&#26434;&#24615;&#30340;&#19981;&#21516;&#32423;&#21035;&#23545;&#32593;&#32476;&#22686;&#38271;&#21644;&#30123;&#24773;&#20256;&#25773;&#30340;&#24433;&#21709;&#65292;&#20998;&#21035;&#28041;&#21450;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#20132;&#20114;&#35268;&#21017;&#30340;&#28789;&#27963;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#26368;&#22823;&#30340;&#28798;&#23475;&#38887;&#24615;&#65292;&#38656;&#35201;&#22312;DT-CNS&#20013;&#24179;&#34913;&#36825;&#20123;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes an extendable modelling framework for Digital Twin-Oriented Complex Networked Systems (DT-CNSs) with a goal of generating networks that faithfully represent real systems. Modelling process focuses on (i) features of nodes and (ii) interaction rules for creating connections that are built based on individual node's preferences. We conduct experiments on simulation-based DT-CNSs that incorporate various features and rules about network growth and different transmissibilities related to an epidemic spread on these networks. We present a case study on disaster resilience of social networks given an epidemic outbreak by investigating the infection occurrence within specific time and social distance. The experimental results show how different levels of the structural and dynamics complexities, concerned with feature diversity and flexibility of interaction rules respectively, influence network growth and epidemic spread. The analysis revealed that, to achieve maximum dis
&lt;/p&gt;</description></item><item><title>GNNPipe&#26159;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#24335;&#20840;&#22270;&#28145;&#24230;GNN&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23618;&#32423;&#27169;&#22411;&#24182;&#34892;&#24615;&#23558;GNN&#23618;&#21010;&#20998;&#22312;&#19981;&#21516;&#30340;GPU&#19978;&#65292;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#37327;&#21644;&#22788;&#29702;&#29305;&#23450;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.10087</link><description>&lt;p&gt;
GNNPipe&#65306;&#20351;&#29992;&#27969;&#27700;&#32447;&#27169;&#22411;&#24182;&#34892;&#23454;&#29616;&#28145;&#24230;GNN&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GNNPipe: Scaling Deep GNN Training with Pipelined Model Parallelism. (arXiv:2308.10087v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10087
&lt;/p&gt;
&lt;p&gt;
GNNPipe&#26159;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#24335;&#20840;&#22270;&#28145;&#24230;GNN&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23618;&#32423;&#27169;&#22411;&#24182;&#34892;&#24615;&#23558;GNN&#23618;&#21010;&#20998;&#22312;&#19981;&#21516;&#30340;GPU&#19978;&#65292;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#37327;&#21644;&#22788;&#29702;&#29305;&#23450;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#35757;&#32451;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GNNPipe&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#20998;&#24067;&#24335;&#30340;&#20840;&#22270;&#28145;&#24230;GNN&#35757;&#32451;&#12290;&#20316;&#20026;&#39318;&#27425;&#20351;&#29992;&#23618;&#32423;&#27169;&#22411;&#24182;&#34892;&#24615;&#36827;&#34892;GNN&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;GNNPipe&#23558;GNN&#23618;&#21010;&#20998;&#22312;&#19981;&#21516;&#30340;GPU&#19978;&#65292;&#27599;&#20010;&#35774;&#22791;&#22312;&#25972;&#20010;&#22270;&#19978;&#20026;&#19968;&#32452;&#36830;&#32493;&#30340;GNN&#23618;&#25191;&#34892;&#35745;&#31639;&#12290;&#19982;&#27599;&#20010;GPU&#22788;&#29702;&#19968;&#20010;&#22270;&#21010;&#20998;&#30340;&#22270;&#24182;&#34892;&#24615;&#30456;&#27604;&#65292;GNNPipe&#23558;&#36890;&#20449;&#37327;&#20943;&#23569;&#20102;GNN&#23618;&#25968;&#30340;&#20493;&#25968;&#12290;GNNPipe&#20811;&#26381;&#20102;&#25972;&#20010;&#22270;&#19978;&#27969;&#27700;&#32447;&#23618;&#32423;&#27169;&#22411;&#24182;&#34892;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;&#22270;&#21010;&#20998;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#22359;&#65292;&#20801;&#35768;&#20351;&#29992;&#21382;&#21490;&#39030;&#28857;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#25216;&#26415;&#20197;&#30830;&#20445;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#65292;&#23558;GNNPipe&#19982;&#22270;&#24182;&#34892;&#32467;&#21512;&#20197;&#22788;&#29702;&#22823;&#22411;&#22270;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a key bottleneck for distributed graph neural network (GNN) training. This paper proposes GNNPipe, a new approach that scales the distributed full-graph deep GNN training. Being the first to use layer-level model parallelism for GNN training, GNNPipe partitions GNN layers among GPUs, each device performs the computation for a disjoint subset of consecutive GNN layers on the whole graph. Compared to graph parallelism with each GPU handling a graph partition, GNNPipe reduces the communication volume by a factor of the number of GNN layers. GNNPipe overcomes the unique challenges for pipelined layer-level model parallelism on the whole graph by partitioning it into dependent chunks, allowing the use of historical vertex embeddings, and applying specific training techniques to ensure convergence. We also propose a hybrid approach by combining GNNPipe with graph parallelism to handle large graphs, achieve better computer resource utilization and ensure model convergence. We
&lt;/p&gt;</description></item><item><title>AI Hilbert&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#25968;&#25454;&#19982;&#32972;&#26223;&#30693;&#35782;&#30340;&#31185;&#23398;&#21457;&#29616;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22238;&#24402;&#21644;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25628;&#32034;&#19982;&#32972;&#26223;&#29702;&#35770;&#19968;&#33268;&#30340;&#20844;&#24335;&#31354;&#38388;&#20013;&#25214;&#21040;&#26368;&#31526;&#21512;&#25968;&#25454;&#30340;&#20844;&#24335;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09474</link><description>&lt;p&gt;
AI Hilbert: &#25972;&#21512;&#25968;&#25454;&#19982;&#32972;&#26223;&#30693;&#35782;&#30340;&#31185;&#23398;&#21457;&#29616;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI Hilbert: A New Paradigm for Scientific Discovery by Unifying Data and Background Knowledge. (arXiv:2308.09474v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09474
&lt;/p&gt;
&lt;p&gt;
AI Hilbert&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#25968;&#25454;&#19982;&#32972;&#26223;&#30693;&#35782;&#30340;&#31185;&#23398;&#21457;&#29616;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22238;&#24402;&#21644;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#25628;&#32034;&#19982;&#32972;&#26223;&#29702;&#35770;&#19968;&#33268;&#30340;&#20844;&#24335;&#31354;&#38388;&#20013;&#25214;&#21040;&#26368;&#31526;&#21512;&#25968;&#25454;&#30340;&#20844;&#24335;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20013;&#65292;&#21457;&#29616;&#33021;&#31616;&#21333;&#35299;&#37322;&#33258;&#28982;&#29616;&#35937;&#24182;&#19982;&#29616;&#26377;&#32972;&#26223;&#29702;&#35770;&#19968;&#33268;&#30340;&#31185;&#23398;&#20844;&#24335;&#26159;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#36807;&#21435;&#65292;&#31185;&#23398;&#23478;&#36890;&#36807;&#25805;&#20316;&#22522;&#20110;&#29616;&#26377;&#30693;&#35782;&#30340;&#26041;&#31243;&#24335;&#12289;&#24418;&#25104;&#26032;&#26041;&#31243;&#24335;&#24182;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#26469;&#25512;&#23548;&#33258;&#28982;&#35268;&#24459;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#24050;&#25104;&#20026;&#22312;&#20855;&#26377;&#22823;&#37327;&#23454;&#39564;&#25968;&#25454;&#30340;&#24773;&#22659;&#20013;&#30340;&#19968;&#31181;&#21487;&#34892;&#31454;&#20105;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#22024;&#26434;&#25110;&#31232;&#32570;&#26102;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#21457;&#29616;&#26377;&#25928;&#30340;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#22238;&#24402;&#21644;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20197;&#21076;&#38500;&#19982;&#32972;&#26223;&#29702;&#35770;&#19981;&#19968;&#33268;&#30340;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#19982;&#32972;&#26223;&#29702;&#35770;&#19968;&#33268;&#30340;&#26041;&#31243;&#31354;&#38388;&#20013;&#25628;&#32034;&#20986;&#26368;&#31526;&#21512;&#25968;&#25454;&#30340;&#20844;&#24335;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#25152;&#26377;&#20844;&#29702;&#21644;&#31185;&#23398;&#23450;&#24459;&#37117;&#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#26469;&#34920;&#36798;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35748;&#20026;&#26159;&#24191;&#27867;&#36866;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of scientific formulae that parsimoniously explain natural phenomena and align with existing background theory is a key goal in science. Historically, scientists have derived natural laws by manipulating equations based on existing knowledge, forming new equations, and verifying them experimentally. In recent years, data-driven scientific discovery has emerged as a viable competitor in settings with large amounts of experimental data. Unfortunately, data-driven methods often fail to discover valid laws when data is noisy or scarce. Accordingly, recent works combine regression and reasoning to eliminate formulae inconsistent with background theory. However, the problem of searching over the space of formulae consistent with background theory to find one that fits the data best is not well-solved. We propose a solution to this problem when all axioms and scientific laws are expressible via polynomial equalities and inequalities and argue that our approach is widely applicab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#19987;&#27880;&#20110;&#31227;&#21160;&#24320;&#21457;&#30340;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#38454;&#27573;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#31561;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#24320;&#21457;&#20154;&#21592;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04736</link><description>&lt;p&gt;
&#26696;&#20363;&#30740;&#31350;&#65306;&#22312;&#31227;&#21160;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Case Study: Using AI-Assisted Code Generation In Mobile Teams. (arXiv:2308.04736v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#19987;&#27880;&#20110;&#31227;&#21160;&#24320;&#21457;&#30340;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#38454;&#27573;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#21512;&#20102;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#31561;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#24320;&#21457;&#20154;&#21592;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22312;&#19987;&#27880;&#20110;Kotlin&#21644;Swift&#31561;&#21407;&#29983;&#31227;&#21160;&#35821;&#35328;&#30340;&#23454;&#38469;&#31227;&#21160;&#24320;&#21457;&#22242;&#38431;&#20013;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;16&#21517;&#21442;&#19982;&#32773;&#21644;2&#21517;&#25216;&#26415;&#35780;&#23457;&#20154;&#21592;&#65292;&#26469;&#33258;&#19968;&#20010;&#36719;&#20214;&#24320;&#21457;&#37096;&#38376;&#65292;&#26088;&#22312;&#20102;&#35299;&#22312;&#22242;&#38431;&#30340;&#29305;&#23450;&#38454;&#27573;&#20013;&#20351;&#29992;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35757;&#32451;&#30340;LLMs&#30340;&#24433;&#21709;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#25216;&#26415;&#20837;&#32844;&#21644;&#25216;&#26415;&#22534;&#26632;&#20999;&#25442;&#12290;&#30740;&#31350;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#38454;&#27573;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;AI-Code&#29983;&#25104;&#22120;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;ReviewerScore&#36825;&#19968;&#29305;&#23450;&#20110;&#26412;&#35770;&#25991;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#20174;&#23454;&#38469;&#34892;&#19994;&#26631;&#20934;&#65288;&#21512;&#24182;&#35831;&#27714;&#30340;&#20195;&#30721;&#35780;&#23457;&#20154;&#21592;&#65289;&#20013;&#25552;&#21462;&#30340;&#24230;&#37327;&#26102;&#38388;&#12289;&#27491;&#30830;&#24615;&#21644;&#25216;&#26415;&#38598;&#25104;&#12290;&#36755;&#20986;&#19982;&#21442;&#19982;&#32773;&#30340;&#21453;&#39304;&#19968;&#36215;&#36716;&#25442;&#21644;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;AI&#36741;&#21161;&#32534;&#31243;&#24037;&#20855;&#26159;&#21542;&#23545;&#33719;&#24471;&#24320;&#21457;&#20154;&#21592;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this study is to evaluate the performance of AI-assisted programming in actual mobile development teams that are focused on native mobile languages like Kotlin and Swift. The extensive case study involves 16 participants and 2 technical reviewers, from a software development department designed to understand the impact of using LLMs trained for code generation in specific phases of the team, more specifically, technical onboarding and technical stack switch. The study uses technical problems dedicated to each phase and requests solutions from the participants with and without using AI-Code generators. It measures time, correctness, and technical integration using ReviewerScore, a metric specific to the paper and extracted from actual industry standards, the code reviewers of merge requests. The output is converted and analyzed together with feedback from the participants in an attempt to determine if using AI-assisted programming tools will have an impact on getting develope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.03882</link><description>&lt;p&gt;
&#36890;&#36807;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#22686;&#24378;&#21033;&#29992;&#24191;&#20041;&#21270;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#35265;&#29366;&#24577;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#21644;&#36807;&#28388;&#65292;&#23454;&#29616;&#20102;&#23545;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#29366;&#24577;&#30340;&#21033;&#29992;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#20445;&#23432;&#20215;&#20540;&#35780;&#20272;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#26080;&#27169;&#22411;&#26041;&#27861;&#20250;&#23545;&#25152;&#26377;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#32780;&#26377;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#27169;&#22411;&#23637;&#24320;&#23545;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#36827;&#34892;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25214;&#21040;&#31163;&#32447;&#25968;&#25454;&#20043;&#22806;&#30340;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#26102;&#23384;&#22312;&#22256;&#38590;&#65306;(a)&#30001;&#20110;&#32423;&#32852;&#27169;&#22411;&#35823;&#24046;&#65292;&#27169;&#22411;&#30340;&#23637;&#24320;&#33539;&#22260;&#38750;&#24120;&#30701;&#65292;(b)&#27169;&#22411;&#23637;&#24320;&#20165;&#20197;&#31163;&#32447;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#31532;&#20108;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26410;&#35265;&#36807;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#20801;&#35768;&#23398;&#24471;&#30340;&#27169;&#22411;&#21644;&#20215;&#20540;&#20272;&#35745;&#22312;&#26410;&#35265;&#29366;&#24577;&#20013;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#36827;&#34892;&#22522;&#20110;&#20215;&#20540;&#30340;&#25200;&#21160;&#26469;&#25214;&#21040;&#26410;&#35265;&#36807;&#30340;&#29366;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#36807;&#28388;&#20855;&#26377;&#36807;&#39640;&#30340;&#21551;&#21457;&#24615;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;&#39640;&#35823;&#24046;&#65289;&#25110;&#36807;&#20302;&#30340;&#65288;&#36807;&#20110;&#30456;&#20284;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.03527</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#30340;&#20849;&#24773;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT's Empathic Abilities. (arXiv:2308.03527v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#36890;&#24120;&#34987;&#29702;&#35299;&#20026;&#20998;&#20139;&#21644;&#29702;&#35299;&#20182;&#20154;&#30340;&#24515;&#24577;&#25110;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22686;&#21152;&#24212;&#29992;&#65292;&#20363;&#22914;&#20799;&#31461;&#23547;&#27714;&#20316;&#19994;&#24110;&#21161;&#12289;&#20010;&#20154;&#23547;&#27714;&#21307;&#30103;&#24314;&#35758;&#20197;&#21450;&#20154;&#20204;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#26085;&#24120;&#20276;&#20387;&#65292;&#20849;&#24773;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#22312;&#23637;&#29616;&#20849;&#24773;&#22238;&#24212;&#21644;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#65306;(1)&#29702;&#35299;&#21644;&#34920;&#36798;&#24773;&#24863;&#12289;(2)&#24182;&#34892;&#24773;&#24863;&#22238;&#24212;&#20197;&#21450;(3)&#20849;&#24773;&#20010;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#22312;&#21508;&#20010;&#20849;&#24773;&#26041;&#38754;&#35780;&#20272;&#20102;ChatGPT&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#20998;&#26512;&#32842;&#22825;&#26426;&#22120;&#20154;&#20849;&#24773;&#33021;&#21147;&#30340;&#21487;&#33021;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;91.7%&#30340;&#24773;&#20917;&#19979;&#65292;ChatGPT&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24773;&#24863;&#24182;&#20135;&#29983;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is often understood as the ability to share and understand another individual's state of mind or emotion. With the increasing use of chatbots in various domains, e.g., children seeking help with homework, individuals looking for medical advice, and people using the chatbot as a daily source of everyday companionship, the importance of empathy in human-computer interaction has become more apparent. Therefore, our study investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. We analyzed the following three aspects: (1) understanding and expressing emotions, (2) parallel emotional response, and (3) empathic personality. Thus, we not only evaluate ChatGPT on various empathy aspects and compare it with human behavior but also show a possible way to analyze the empathy of chatbots in general. Our results show, that in 91.7% of the cases, ChatGPT was able to correctly identify emotions and produces appropriate answers. In c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COCO-MMR&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#24320;&#25918;&#24615;&#38382;&#39064;&#12289;&#29702;&#30001;&#21644;&#31572;&#26696;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#21521;&#22810;&#27169;&#24577;&#25512;&#29702;&#39046;&#22495;&#36129;&#29486;&#30340;&#21019;&#26032;&#21644;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.12626</link><description>&lt;p&gt;
&#25552;&#21319;&#20154;&#31867;&#21270;&#22810;&#27169;&#24577;&#25512;&#29702;&#65306;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#21644;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework. (arXiv:2307.12626v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12626
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COCO-MMR&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#24320;&#25918;&#24615;&#38382;&#39064;&#12289;&#29702;&#30001;&#21644;&#31572;&#26696;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#21521;&#22810;&#27169;&#24577;&#25512;&#29702;&#39046;&#22495;&#36129;&#29486;&#30340;&#21019;&#26032;&#21644;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25512;&#29702;&#26159;&#36861;&#27714;&#23637;&#31034;&#20154;&#31867;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26102;&#12290;&#34429;&#28982;&#36830;&#32493;&#24605;&#32500;&#65288;Chain-of-Thought&#65292;CoT&#65289;&#25216;&#26415;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#29616;&#26377;&#30340;ScienceQA&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#31185;&#23398;&#38382;&#39064;&#21644;&#22522;&#20110;&#23567;&#23398;&#21644;&#39640;&#20013;&#25945;&#31185;&#20070;&#30340;&#35299;&#37322;&#65292;&#32570;&#20047;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20215;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COCO Multi-Modal Reasoning&#65288;COCO-MMR&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#24320;&#25918;&#24615;&#38382;&#39064;&#12289;&#29702;&#30001;&#21644;&#31572;&#26696;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#38382;&#39064;&#12289;&#29702;&#30001;&#21644;&#31572;&#26696;&#26159;&#20174;&#22823;&#22411;&#23545;&#35937;&#25968;&#25454;&#38598;COCO&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#22810;&#27169;&#24577;&#36830;&#32493;&#24605;&#32500;&#30340;&#32972;&#26223;&#19979;&#39318;&#27425;&#24341;&#20837;&#20102;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;CoT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#21521;&#35813;&#39046;&#22495;&#36129;&#29486;&#30340;&#21019;&#26032;&#21644;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal reasoning is a critical component in the pursuit of artificial intelligence systems that exhibit human-like intelligence, especially when tackling complex tasks. While the chain-of-thought (CoT) technique has gained considerable attention, the existing ScienceQA dataset, which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning(COCO-MMR) dataset, a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22788;&#29702;&#35270;&#39057;&#24103;&#21644;&#28145;&#24230;&#32454;&#33410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#21333;&#29420;&#20351;&#29992;&#35270;&#39057;&#24103;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.11058</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31574;&#30053;&#39044;&#27979;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Anticipating Driving Behavior through Deep Learning-Based Policy Prediction. (arXiv:2307.11058v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11058
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;&#35270;&#39057;&#24103;&#21644;&#28145;&#24230;&#32454;&#33410;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#26469;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#27604;&#21333;&#29420;&#20351;&#29992;&#35270;&#39057;&#24103;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#30001;&#26222;&#36890;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#35270;&#39057;&#24103;&#34893;&#29983;&#20986;&#30340;&#32508;&#21512;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#20174;&#28857;&#20113;&#25195;&#25551;&#20202;&#33719;&#24471;&#30340;&#28145;&#24230;&#32454;&#33410;&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#39044;&#27979;&#39550;&#39542;&#34892;&#20026;&#65292;&#21253;&#25324;&#36710;&#36742;&#36895;&#24230;&#21644;&#36716;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#30830;&#20445;&#20854;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#19982;&#29087;&#32451;&#30340;&#30495;&#23454;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#26082;&#23450;&#35268;&#33539;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#22312;&#33267;&#23569;&#19968;&#21322;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#65288;&#26681;&#25454;&#20855;&#20307;&#27169;&#22411;&#65292;&#22312;50-80%&#20043;&#38388;&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;&#32508;&#21512;&#29305;&#24449;&#30456;&#27604;&#20110;&#21482;&#20351;&#29992;&#35270;&#39057;&#24103;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this endeavor, we developed a comprehensive system that processes integrated visual features derived from video frames captured by a regular camera, along with depth details obtained from a point cloud scanner. This system is designed to anticipate driving actions, encompassing both vehicle speed and steering angle. To ensure its reliability, we conducted assessments where we juxtaposed the projected outcomes with the established norms adhered to by skilled real-world drivers. Our evaluation outcomes indicate that the forecasts achieve a noteworthy level of accuracy in a minimum of half the test scenarios (ranging around 50-80%, contingent on the specific model). Notably, the utilization of amalgamated features yielded superior performance in comparison to using video frames in isolation, as demonstrated by most of the cases.
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07420</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT&#20197;&#35782;&#21035;&#21487;&#27604;&#20844;&#21496;&#12290;&#20256;&#32479;&#30340;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#21462;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#20174;&#32780;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#65292;&#23454;&#29616;&#26356;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#20849;&#21644;&#31169;&#20154;&#20844;&#21496;&#65292;&#21487;&#27604;&#20844;&#21496;&#20998;&#26512;&#34987;&#24191;&#27867;&#29992;&#20316;&#20844;&#21496;&#20272;&#20540;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#30340;&#20272;&#20540;&#65292;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#21487;&#27604;&#20844;&#21496;&#26041;&#27861;&#30340;&#20960;&#31181;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23450;&#24615;&#26041;&#27861;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#21516;&#34892;&#20844;&#21496;&#65292;&#36825;&#24448;&#24448;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#34892;&#19994;&#20998;&#31867;&#26041;&#26696;&#21644;/&#25110;&#20998;&#26512;&#24072;&#30340;&#30452;&#35273;&#21644;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#21644;&#31169;&#21215;&#32929;&#26435;&#34892;&#19994;&#24320;&#22987;&#20351;&#29992;&#26356;&#22810;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23545;&#20110;NLP&#26041;&#27861;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20174;&#20844;&#21496;&#30340;&#32593;&#31449;&#25110;&#26469;&#33258;&#26576;&#20123;&#37329;&#34701;&#25968;&#25454;&#24211;&#31995;&#32479;&#30340;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#20135;&#21697;&#23454;&#20307;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#32500;&#22522;&#30334;&#31185;&#32593;&#31449;&#30340;&#20844;&#21496;&#25551;&#36848;/&#25688;&#35201;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;GPT
&lt;/p&gt;
&lt;p&gt;
For both public and private firms, comparable companies analysis is widely used as a method for company valuation. In particular, the method is of great value for valuation of private equity companies. The several approaches to the comparable companies method usually rely on a qualitative approach to identifying similar peer companies, which tends to use established industry classification schemes and/or analyst intuition and knowledge. However, more quantitative methods have started being used in the literature and in the private equity industry, in particular, machine learning clustering, and natural language processing (NLP). For NLP methods, the process consists of extracting product entities from e.g., the company's website or company descriptions from some financial database system and then to perform similarity analysis. Here, using companies descriptions/summaries from publicly available companies' Wikipedia websites, we show that using large language models (LLMs), such as GPT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#30340;&#21487;&#38752;&#22522;&#20934;&#8212;&#8212;&#20154;&#31867;&#20559;&#22909;&#20998;&#25968;v2&#12290;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;v2&#65288;HPD v2&#65289;&#21644;&#24494;&#35843;CLIP&#65292;&#30740;&#31350;&#32773;&#20204;&#25104;&#21151;&#33719;&#24471;&#20102;&#26356;&#33021;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#35780;&#20998;&#27169;&#22411;HPS v2&#65292;&#20854;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#24067;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25913;&#36827;&#20855;&#26377;&#21709;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09341</link><description>&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#20998;&#25968;v2&#65306;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#21487;&#38752;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis. (arXiv:2306.09341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#30340;&#21487;&#38752;&#22522;&#20934;&#8212;&#8212;&#20154;&#31867;&#20559;&#22909;&#20998;&#25968;v2&#12290;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;v2&#65288;HPD v2&#65289;&#21644;&#24494;&#35843;CLIP&#65292;&#30740;&#31350;&#32773;&#20204;&#25104;&#21151;&#33719;&#24471;&#20102;&#26356;&#33021;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#35780;&#20998;&#27169;&#22411;HPS v2&#65292;&#20854;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#24067;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25913;&#36827;&#20855;&#26377;&#21709;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#65292;&#20294;&#26159;&#36825;&#20123;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#35780;&#20272;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;v2&#65288;HPD v2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#22270;&#20687;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;HPD v2&#21253;&#25324;798,090&#20010;&#20154;&#31867;&#20559;&#22909;&#36873;&#25321;&#65292;&#28041;&#21450;433,760&#23545;&#22270;&#20687;&#65292;&#26159;&#21516;&#31867;&#25968;&#25454;&#38598;&#20013;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#25991;&#26412;&#25552;&#31034;&#21644;&#22270;&#20687;&#26159;&#32463;&#36807;&#31934;&#24515;&#25910;&#38598;&#30340;&#65292;&#20197;&#28040;&#38500;&#28508;&#22312;&#30340;&#20559;&#35265;&#65292;&#36825;&#26159;&#20808;&#21069;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;HPD v2&#19978;&#24494;&#35843;CLIP&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20154;&#31867;&#20559;&#22909;&#20998;&#25968;v2&#65288;HPS v2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#26356;&#20934;&#30830;&#39044;&#27979;&#29983;&#25104;&#22270;&#20687;&#20154;&#31867;&#20559;&#22909;&#30340;&#35780;&#20998;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HPS v2&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#24067;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25913;&#36827;&#20855;&#26377;&#21709;&#24212;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#21487;&#21462;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generative models can generate high-fidelity images from text inputs, but the quality of these generated images cannot be accurately evaluated by existing evaluation metrics. To address this issue, we introduce Human Preference Dataset v2 (HPD v2), a large-scale dataset that captures human preferences on images from a wide range of sources. HPD v2 comprises 798,090 human preference choices on 433,760 pairs of images, making it the largest dataset of its kind. The text prompts and images are deliberately collected to eliminate potential bias, which is a common issue in previous datasets. By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict human preferences on generated images. Our experiments demonstrate that HPS v2 generalizes better than previous metrics across various image distributions and is responsive to algorithmic improvements of text-to-image generative models, making it a preferable
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#25351;&#21335;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#20026;AI&#20174;&#19994;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.08323</link><description>&lt;p&gt;
&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65311;&#19968;&#20221;&#25351;&#21335;&#21644;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to estimate carbon footprint when training deep learning models? A guide and review. (arXiv:2306.08323v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#25351;&#21335;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#20272;&#35745;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22810;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#20026;AI&#20174;&#19994;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#21508;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;&#23384;&#22312;&#29615;&#22659;&#25104;&#26412;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#23545;&#27492;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#22312;&#32447;&#21644;&#36719;&#20214;&#24037;&#20855;&#26469;&#36319;&#36394;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#20840;&#38754;&#20171;&#32461;&#21644;&#27604;&#36739;&#65292;&#24182;&#38024;&#23545;&#24076;&#26395;&#24320;&#22987;&#20272;&#35745;&#20854;&#24037;&#20316;&#29615;&#22659;&#24433;&#21709;&#30340;AI&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#24037;&#20855;&#30340;&#29305;&#23450;&#35789;&#27719;&#21644;&#25216;&#26415;&#35201;&#27714;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20123;&#24037;&#20855;&#23545;&#20004;&#20010;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#21516;&#31867;&#22411;&#26381;&#21153;&#22120;&#30340;&#33021;&#28304;&#28040;&#32791;&#20272;&#35745;&#32467;&#26524;&#12290;&#26681;&#25454;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24314;&#35758;&#65292;&#20197;&#26356;&#22909;&#22320;&#36873;&#25321;&#21512;&#36866;&#30340;&#24037;&#20855;&#21644;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning models have become essential in the recent fast development of artificial intelligence in many sectors of the society. It is now widely acknowledge that the development of these models has an environmental cost that has been analyzed in many studies. Several online and software tools have been developed to track energy consumption while training machine learning models. In this paper, we propose a comprehensive introduction and comparison of these tools for AI practitioners wishing to start estimating the environmental impact of their work. We review the specific vocabulary, the technical requirements for each tool. We compare the energy consumption estimated by each tool on two deep neural networks for image processing and on different types of servers. From these experiments, we provide some advice for better choosing the right tool and infrastructure.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07215</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#19982;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#27169;&#22411;&#37096;&#32626;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26435;&#37325;&#21644;&#28608;&#27963;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#39640;&#33021;&#32791;&#12290;&#26680;&#24515;&#38598;&#36873;&#25321;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20887;&#20313;&#24615;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#25928;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#25552;&#39640;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22522;&#20110;QAT&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65306;&#35823;&#24046;&#21521;&#37327;&#20998;&#25968;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04959</link><description>&lt;p&gt;
FedMLSecurity&#65306;&#32852;&#37030;&#23398;&#20064;&#19982;LLMs&#20013;&#25915;&#20987;&#19982;&#38450;&#24481;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedMLSecurity&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#24212;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#24320;&#28304;&#24211;FedML&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;FedMLSecurity&#22686;&#24378;&#20102;FedML&#30340;&#23433;&#20840;&#35780;&#20272;&#33021;&#21147;&#12290;FedMLSecurity&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;FedMLAttacker&#27169;&#25311;&#22312;FL&#35757;&#32451;&#20013;&#27880;&#20837;&#30340;&#25915;&#20987;&#65292;&#32780;FedMLDefender&#21017;&#27169;&#25311;&#26088;&#22312;&#20943;&#36731;&#25915;&#20987;&#24433;&#21709;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;FedMLSecurity&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;ResNet&#65292;GAN&#31561;&#65289;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;FedAVG&#65292;FedOPT&#65292;FedNOVA&#31561;&#65289;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#35780;&#20272;&#36824;&#23637;&#31034;&#20102;&#23558;FedMLSecurity&#36731;&#26494;&#24212;&#29992;&#20110;LLMs&#30340;&#20415;&#21033;&#24615;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;&#20102;&#20854;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#20998;&#22270;&#20013;&#29992;&#25143;&#21644;&#20869;&#23481;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01913</link><description>&lt;p&gt;
PDT: &#38754;&#21521;&#26102;&#38388;&#24863;&#30693;&#30340;&#21452;&#21521;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#20108;&#20998;&#22270;
&lt;/p&gt;
&lt;p&gt;
PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs. (arXiv:2306.01913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20108;&#20998;&#22270;&#20013;&#29992;&#25143;&#21644;&#20869;&#23481;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#39044;&#20808;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#27491;&#22312;&#26222;&#21450;&#24182;&#28044;&#29616;&#65292;&#38543;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#12290;&#24050;&#32463;&#35748;&#35782;&#21040;&#65292;&#20174;&#25551;&#32472;&#29992;&#25143;&#20869;&#23481;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#39044;&#35757;&#32451;&#26041;&#27861;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#20294;&#20026;&#36825;&#31181;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#34920;&#31034;&#29992;&#25143;-&#20869;&#23481;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26041;&#38754;&#65292;&#21363;&#29992;&#25143;&#26041;&#38754;&#21644;&#20869;&#23481;&#26041;&#38754;&#12290;&#20026;&#20102;&#23398;&#20064;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#29992;&#25143;&#26041;&#38754;&#21644;&#20869;&#23481;&#26041;&#38754;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#30446;&#26631;&#21046;&#23450;&#20026;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#37325;Transformer&#26550;&#26500;&#26469;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large models is prevalent and emerging with the ever-growing user-generated content in many machine learning application categories. It has been recognized that learning contextual knowledge from the datasets depicting user-content interaction plays a vital role in downstream tasks. Despite several studies attempting to learn contextual knowledge via pre-training methods, finding an optimal training objective and strategy for this type of task remains a challenging problem. In this work, we contend that there are two distinct aspects of contextual knowledge, namely the user-side and the content-side, for datasets where user-content interaction can be represented as a bipartite graph. To learn contextual knowledge, we propose a pre-training method that learns a bi-directional mapping between the spaces of the user-side and the content-side. We formulate the training goal as a contrastive learning task and propose a dual-Transformer architecture to encode the contextual k
&lt;/p&gt;</description></item><item><title>&#21487;&#23398;&#20064;&#31354;&#38388;&#21487;&#20280;&#23637;&#21367;&#31215;&#65288;DCLS&#65289;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#38388;&#36317;&#21644;&#25554;&#20540;&#25216;&#24039;&#65292;&#36229;&#36234;&#20102;&#21452;&#32447;&#24615;&#25554;&#20540;&#65292;&#22312;&#19981;&#22686;&#21152;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#21367;&#31215;&#32467;&#26500;&#22312;ImageNet1k&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00817</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#31354;&#38388;&#21487;&#20280;&#23637;&#21367;&#31215;&#65306;&#36229;&#36234;&#21452;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Dilated Convolution with Learnable Spacings: beyond bilinear interpolation. (arXiv:2306.00817v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00817
&lt;/p&gt;
&lt;p&gt;
&#21487;&#23398;&#20064;&#31354;&#38388;&#21487;&#20280;&#23637;&#21367;&#31215;&#65288;DCLS&#65289;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#38388;&#36317;&#21644;&#25554;&#20540;&#25216;&#24039;&#65292;&#36229;&#36234;&#20102;&#21452;&#32447;&#24615;&#25554;&#20540;&#65292;&#22312;&#19981;&#22686;&#21152;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#21367;&#31215;&#32467;&#26500;&#22312;ImageNet1k&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23398;&#20064;&#31354;&#38388;&#21487;&#20280;&#23637;&#21367;&#31215;&#65288;Dilated Convolution with Learnable Spacings&#65292;DCLS&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#25913;&#36827;&#22411;&#33192;&#32960;&#21367;&#31215;&#65292;&#20854;&#20013;&#21367;&#31215;&#26680;&#20013;&#38750;&#38646;&#20803;&#32032;&#30340;&#38388;&#36317;&#25110;&#32773;&#31561;&#25928;&#22320;&#35828;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;&#38750;&#25972;&#25968;&#20301;&#32622;&#36890;&#36807;&#25554;&#20540;&#22788;&#29702;&#65292;&#36825;&#31181;&#25216;&#24039;&#20351;&#24471;&#20301;&#32622;&#20855;&#26377;&#26126;&#30830;&#30340;&#26799;&#24230;&#12290;&#21407;&#22987;&#30340;DCLS&#20351;&#29992;&#21452;&#32447;&#24615;&#25554;&#20540;&#65292;&#21482;&#32771;&#34385;&#20102;&#26368;&#36817;&#30340;&#22235;&#20010;&#20687;&#32032;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#26356;&#38271;&#31243;&#30340;&#25554;&#20540;&#65292;&#23588;&#20854;&#26159;&#39640;&#26031;&#25554;&#20540;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;ConvNeXt&#21644;Conv-Former&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#32467;&#26500;&#22312;ImageNet1k&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#30340;&#20195;&#30721;&#22522;&#20110;PyTorch&#65292;&#21487;&#20197;&#22312;https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dilated Convolution with Learnable Spacings (DCLS) is a recently proposed variation of the dilated convolution in which the spacings between the non-zero elements in the kernel, or equivalently their positions, are learnable. Non-integer positions are handled via interpolation. Thanks to this trick, positions have well-defined gradients. The original DCLS used bilinear interpolation, and thus only considered the four nearest pixels. Yet here we show that longer range interpolations, and in particular a Gaussian interpolation, allow improving performance on ImageNet1k classification on two state-of-the-art convolutional architectures (ConvNeXt and Conv\-Former), without increasing the number of parameters. The method code is based on PyTorch and is available at https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16437</link><description>&lt;p&gt;
KeyPosS: &#22522;&#20110; GPS &#28789;&#24863;&#30340;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#30340;&#21363;&#25554;&#21363;&#29992;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16437
&lt;/p&gt;
&lt;p&gt;
KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#37096;&#20998;&#26512;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#26631;&#35760;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20154;&#33080;&#35782;&#21035;&#21644;&#34920;&#24773;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28909;&#21147;&#22270;&#25110;&#22352;&#26631;&#22238;&#24402;&#25216;&#26415;&#32463;&#24120;&#38754;&#20020;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; KeyPoint Positioning System&#65288;KeyPosS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;KeyPosS&#39318;&#27425;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#65292;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;GPS&#31995;&#32479;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#36317;&#31163;&#22270;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#19982;&#22810;&#20010;&#38170;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#21033;&#29992;&#36825;&#20123;&#38170;&#28857;&#26469;&#19977;&#35282;&#27979;&#37327;POI&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#38754;&#37096;&#26631;&#35760;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#24314;&#31435;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#26082;&#26377;&#30410;&#22788;&#21448;&#26377;&#23475;&#22788;&#30340;&#31995;&#32479;&#65292;&#32780;&#36827;&#19968;&#27493;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23548;&#33268;&#26497;&#31471;&#39118;&#38505;&#65292;&#38656;&#35201;&#36890;&#36807;&#27169;&#22411;&#35780;&#20272;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15324</link><description>&lt;p&gt;
&#26497;&#31471;&#39118;&#38505;&#30340;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Model evaluation for extreme risks. (arXiv:2305.15324v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15324
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#24314;&#31435;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#26082;&#26377;&#30410;&#22788;&#21448;&#26377;&#23475;&#22788;&#30340;&#31995;&#32479;&#65292;&#32780;&#36827;&#19968;&#27493;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23548;&#33268;&#26497;&#31471;&#39118;&#38505;&#65292;&#38656;&#35201;&#36890;&#36807;&#27169;&#22411;&#35780;&#20272;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24314;&#31435;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#26082;&#26377;&#30410;&#22788;&#21448;&#26377;&#23475;&#22788;&#30340;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#21487;&#33021;&#20250;&#23548;&#33268;&#20855;&#26377;&#26497;&#31471;&#39118;&#38505;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#25915;&#20987;&#24615;&#30340;&#32593;&#32476;&#33021;&#21147;&#25110;&#24378;&#22823;&#30340;&#25805;&#32437;&#25216;&#33021;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#27169;&#22411;&#35780;&#20272;&#23545;&#20110;&#35299;&#20915;&#26497;&#31471;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#24320;&#21457;&#32773;&#24517;&#39035;&#33021;&#22815;&#35782;&#21035;&#21361;&#38505;&#30340;&#33021;&#21147;&#65288;&#36890;&#36807;&#8220;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#8221;&#65289;&#20197;&#21450;&#27169;&#22411;&#24212;&#29992;&#33021;&#21147;&#23545;&#36896;&#25104;&#21361;&#23475;&#30340;&#20542;&#21521;&#65288;&#36890;&#36807;&#8220;&#23545;&#40784;&#35780;&#20272;&#8221;&#65289;&#12290;&#36825;&#20123;&#35780;&#20272;&#23558;&#23545;&#20445;&#25345;&#20915;&#31574;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#23433;&#20840;&#24615;&#30340;&#36127;&#36131;&#20219;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through "dangerous capability evaluations") and the propensity of models to apply their capabilities for harm (through "alignment evaluations"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14243</link><description>&lt;p&gt;
&#20869;&#23481;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#35757;&#32451;&#19982; LoReTTa
&lt;/p&gt;
&lt;p&gt;
Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14243
&lt;/p&gt;
&lt;p&gt;
LoReTTa&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#20013;&#36716;&#25442;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#65292;&#25910;&#38598;&#20004;&#20010;&#21305;&#37197;&#30340;&#24418;&#24577;A&#21644;B&#25110;B&#21644;C&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#33719;&#24471;&#21253;&#21547;&#19977;&#20010;&#23545;&#40784;&#24418;&#24577;A&#12289;B&#21644;C&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoReTTa&#20197;&#24212;&#23545;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#24314;&#27169;&#21644;&#20132;&#25442;&#24459;&#21644;&#20256;&#36882;&#24615;&#30340;&#35268;&#21017;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21512;&#25104;&#26174;&#30528;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting a multimodal dataset with two paired modalities A and B or B and C is difficult in practice. Obtaining a dataset with three aligned modalities A, B, and C is even more challenging. For example, some public medical datasets have only genetic sequences and microscopic images for one patient, and only genetic sequences and radiological images for another - but no dataset includes both microscopic and radiological images for the same patient. This makes it difficult to integrate and combine all modalities into a large pre-trained neural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework combines causal masked modeling with the rules of commutativity and transitivity to transition within and between different modalities. Thus, it can model the relation A -&gt; C with A -&gt; B -&gt; C. Given a dataset containing only the disjoint combinations (A, B) and (B, C), we sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#65292;&#35777;&#26126;TDMs&#34920;&#29616;&#33391;&#22909;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.10912</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#29992;&#21160;&#21147;&#23398;&#27169;&#22411;&#29992;&#20110;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Generalist Dynamics Model for Control. (arXiv:2305.10912v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#65292;&#35777;&#26126;TDMs&#34920;&#29616;&#33391;&#22909;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;Transformer&#24207;&#21015;&#27169;&#22411;&#20316;&#20026;&#25511;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDMs)&#12290;&#25105;&#20204;&#22312;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#39318;&#20808;&#65292;&#22312;&#21333;&#29615;&#22659;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;TDMs&#34920;&#29616;&#33391;&#22909;&#12290;&#20854;&#27425;&#65292;TDMs&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;few-shot&#23398;&#20064;&#21644;zero-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20316;&#20026;&#31574;&#30053;&#36890;&#29992;&#26368;&#20248;&#34892;&#20026;&#65292;&#27867;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21487;&#20197;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;&#36825;&#20351;&#24471;TDMs&#25104;&#20026;&#25511;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the use of transformer sequence models as dynamics models (TDMs) for control. In a number of experiments in the DeepMind control suite, we find that first, TDMs perform well in a single-environment learning setting when compared to baseline models. Second, TDMs exhibit strong generalization capabilities to unseen environments, both in a few-shot setting, where a generalist model is fine-tuned with small amounts of data from the target environment, and in a zero-shot setting, where a generalist model is applied to an unseen environment without any further training. We further demonstrate that generalizing system dynamics can work much better than generalizing optimal behavior directly as a policy. This makes TDMs a promising ingredient for a foundation model of control.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;&#30340;4D&#20154;&#20307;&#25968;&#25454;&#38598;MM-Fi&#65292;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#20219;&#21153;&#30340;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#21517;&#21463;&#35797;&#32773;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#31181;&#27169;&#24577;&#65292;&#25903;&#25345;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#24320;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.10345</link><description>&lt;p&gt;
MM-Fi&#65306;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#30340;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;4D&#20154;&#20307;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing. (arXiv:2305.10345v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;&#30340;4D&#20154;&#20307;&#25968;&#25454;&#38598;MM-Fi&#65292;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#20219;&#21153;&#30340;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#21517;&#21463;&#35797;&#32773;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#31181;&#27169;&#24577;&#65292;&#25903;&#25345;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#33258;&#21160;&#21270;&#21644;&#20803;&#23431;&#23449;&#20154;&#29289;&#27169;&#25311;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;4D&#20154;&#20307;&#24863;&#30693;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#20110;&#25668;&#20687;&#22836;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#35201;&#20040;&#20405;&#29359;&#38544;&#31169;&#65292;&#35201;&#20040;&#20351;&#29992;&#19981;&#20415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26080;&#32447;&#20256;&#24863;&#25104;&#20026;&#28508;&#22312;&#30340;&#36873;&#25321;&#65292;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;&#12289;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;WiFi&#20449;&#21495;&#36827;&#34892;&#38750;&#35774;&#22791;&#24335;&#20154;&#20307;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MM-Fi&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;27&#31181;&#26085;&#24120;&#25110;&#24247;&#22797;&#21160;&#20316;&#31867;&#21035;&#30340;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;4D&#20154;&#20307;&#25968;&#25454;&#38598;&#65292;&#20197;&#24357;&#21512;&#26080;&#32447;&#20256;&#24863;&#21644;&#39640;&#32423;&#20154;&#20307;&#24863;&#30693;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;MM-Fi&#30001;40&#21517;&#20154;&#20307;&#20027;&#20307;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#20010;&#27169;&#24577;&#32452;&#25104;&#12290;&#25552;&#20379;&#20102;&#21508;&#31181;&#27880;&#37322;&#20197;&#25903;&#25345;&#28508;&#22312;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#20363;&#22914;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27169;&#24577;&#30340;&#24863;&#30693;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20123;&#25968;&#25454;&#21644;&#25152;&#25552;&#20986;&#30340;&#35780;&#20272;&#21327;&#35758;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#26032;&#30340;&#26080;&#32447;&#20256;&#24863;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#31181;&#20154;&#20307;&#24863;&#30693;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.13023</link><description>&lt;p&gt;
&#30524;&#35265;&#19981;&#19968;&#23450;&#20026;&#23454;&#65306;&#20154;&#31867;&#24863;&#30693;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#23450;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images. (arXiv:2304.13023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29031;&#29255;&#26159;&#20154;&#20204;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#32463;&#21382;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#25285;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#21487;&#33021;&#20135;&#29983;&#20266;&#36896;&#30340;&#29031;&#29255;&#65292;&#20174;&#32780;&#20135;&#29983;&#22256;&#24785;&#24182;&#38477;&#20302;&#23545;&#29031;&#29255;&#30340;&#20449;&#20219;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#27169;&#22411;&#33021;&#21542;&#25345;&#32493;&#22320;&#27450;&#39575;&#20154;&#31867;&#30340;&#30524;&#30555;&#65292;&#24182;&#20256;&#36798;&#38169;&#35823;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;50&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#23450;&#37327;&#30740;&#31350;&#65292;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#65292;&#20154;&#31867;&#26080;&#27861;&#26174;&#33879;&#21306;&#20998;&#30495;&#23454;&#29031;&#29255;&#21644;AI&#21019;&#24314;&#30340;&#20266;&#36896;&#29031;&#29255;&#65292;&#36798;&#21040;38.7%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20010;&#20154;&#30340;&#32972;&#26223;&#65292;&#22914;&#24615;&#21035;&#65292;&#24180;&#40836;&#21644;&#32463;&#39564;&#65292;&#23545;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#29031;&#29255;&#30340;&#33021;&#21147;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to answer the question of whether the current state-of-the-art AI-based visual content generation models can consistently deceive human eyes and convey false information. By conducting a high-quality quantitative study with fifty participants, we reveal, for the first time, that humans cannot distinguish between real photos and AI-created fake photos to a significant degree 38.7%. Our study also finds that an individual's background, such as their gender, age, and experience with AI-generated content (AIGC), does not significantly affect their ability to distinguish AI-generated images from real photographs. However, we do observe that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11263</link><description>&lt;p&gt;
&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#19979;&#20302;&#26679;&#26412;&#31283;&#20581;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#26356;&#22909;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#32463;&#21462;&#24471;&#20102;&#38024;&#23545;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#40065;&#26834;&#24615;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24494;&#35843;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#24403;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#39640;&#26102;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#21644;&#26368;&#20808;&#36827;&#30340;&#31283;&#20581;&#24615;&#24178;&#39044;&#30340;&#33258;&#28982;&#20998;&#24067;&#28418;&#31227;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#21333;&#19968;&#30340;&#36873;&#25321;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#31283;&#20581;&#65292;&#21363;&#20351;&#22312;&#23436;&#25972;&#26679;&#26412;&#19979;&#65292;&#29616;&#26377;&#30340;&#24178;&#39044;&#25514;&#26045;&#20063;&#21487;&#33021;&#26080;&#27861;&#25552;&#39640;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21169;&#31038;&#21306;&#20851;&#27880;&#36825;&#20010;&#23454;&#38469;&#37325;&#35201;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.06876</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#21453;&#24212;&#32508;&#21512;&#31639;&#27861;&#24212;&#29992;&#20110;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems. (arXiv:2304.06876v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#28151;&#21512;&#31995;&#32479;&#30340;&#28436;&#21270;&#35270;&#20026;&#19968;&#20010;&#21452;&#20154;&#28216;&#25103;&#65292;&#20854;&#20013;&#38750;&#30830;&#23450;&#24615;&#26159;&#19968;&#20010;&#23545;&#25163;&#29609;&#23478;&#65292;&#20854;&#30446;&#26631;&#26159;&#38459;&#27490;&#23454;&#29616;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#30446;&#26631;&#12290;&#26088;&#22312;&#21512;&#25104;&#19968;&#31181;&#33719;&#32988;&#31574;&#30053;&#8212;&#8212;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#23427;&#20445;&#35777;&#22312;&#23545;&#25163;&#29609;&#23478;&#30340;&#25152;&#26377;&#21487;&#33021;&#31227;&#21160;&#19979;&#28385;&#36275;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#26041;&#27861;&#19982;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#21644;&#25913;&#36827;&#37096;&#20998;&#31574;&#30053;&#30340;&#26032;&#22411;&#20056;&#23458;&#33329;&#26426;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#26465;&#20214;&#19979;&#65292;&#31639;&#27861;&#26159;&#27010;&#29575;&#19978;&#23436;&#22791;&#30340;&#65292;&#21363;&#65292;&#22914;&#26524;&#23384;&#22312;&#33719;&#32988;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25214;&#21040;&#23427;&#12290;&#26696;&#20363;&#30740;&#31350;&#21644;&#22522;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a sampling-based strategy synthesis algorithm for nondeterministic hybrid systems with complex continuous dynamics under temporal and reachability constraints. We view the evolution of the hybrid system as a two-player game, where the nondeterminism is an adversarial player whose objective is to prevent achieving temporal and reachability goals. The aim is to synthesize a winning strategy -- a reactive (robust) strategy that guarantees the satisfaction of the goals under all possible moves of the adversarial player. The approach is based on growing a (search) game-tree in the hybrid space by combining a sampling-based planning method with a novel bandit-based technique to select and improve on partial strategies. We provide conditions under which the algorithm is probabilistically complete, i.e., if a winning strategy exists, the algorithm will almost surely find it. The case studies and benchmark results show that the algorithm is general and consistently outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03593</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#22270;Crowd Navigation&#19982;&#24863;&#30693;&#39118;&#38505;&#25511;&#21046;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#36935;&#21040;&#8220;&#20923;&#32467;&#26426;&#22120;&#20154;&#38382;&#39064;&#8221;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#20294;&#26159;&#23384;&#22312;&#27867;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#26469;&#24110;&#21161;&#26426;&#22120;&#20154;&#23433;&#20840;&#36890;&#36807;&#20154;&#32676;&#30340;&#26041;&#27861;&#12290;&#23558;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#21253;&#25324;&#22312;&#35266;&#23519;&#31354;&#38388;&#20013;&#65292;&#32473;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#24863;&#30693;&#31227;&#21160;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#20250;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#31359;&#36807;&#20154;&#32676;&#65292;&#20294;&#22312;&#20154;&#32676;&#31227;&#21160;&#36807;&#20110;&#28608;&#28872;&#26102;&#20250;&#32469;&#36335;&#12290;&#36890;&#36807;&#20851;&#27880;&#26368;&#21361;&#38505;&#30340;&#38556;&#30861;&#29289;&#65292;&#26426;&#22120;&#20154;&#19981;&#20250;&#22312;&#20154;&#32676;&#23494;&#24230;&#36739;&#39640;&#26102;&#28151;&#28102;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24320;&#21457;&#65292;&#24182;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#36827;&#34892;&#20102;&#38750;&#21512;&#20316;&#20154;&#32676;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#30340;&#38556;&#30861;&#29289;&#20197;&#38543;&#26426;&#36895;&#24230;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16342</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#25351;&#23548;&#30340;&#19977;&#27169;&#24577;&#19968;&#33268;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#39057;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#22312;&#35270;&#39057;&#20013;&#23398;&#20064;&#25191;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#23545;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#23558;&#21457;&#22768;&#29289;&#20307;&#30340;&#35821;&#35328;&#25551;&#36848;&#19982;&#20854;&#35270;&#35273;&#29305;&#24449;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#32452;&#20214;&#32852;&#31995;&#36215;&#26469;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#35775;&#38382;&#27880;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20266;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#20419;&#36827;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#26356;&#24378;&#30340;&#23545;&#40784;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32473;&#23450;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#36755;&#20837;&#25110;&#20165;&#32473;&#23450;&#25991;&#26412;&#21644;&#38899;&#39057;&#36755;&#20837;&#26102;&#20998;&#31163;&#22768;&#38899;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38899;&#39057;-&#35270;&#39057;&#20998;&#31163;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;MUSIC&#12289;SOLOS&#21644;AudioSet&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#29616;&#26377;&#24378;&#30417;&#30563;&#26041;&#27861;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;AI&#27169;&#22411;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#20855;&#26377;&#31361;&#30772;&#24615;&#24212;&#29992;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#25968;&#25454;&#37327;&#30340;&#25361;&#25112;&#38656;&#35201;&#20811;&#26381;&#65292;&#26410;&#26469;&#20173;&#38656;&#28145;&#20837;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2303.11568</link><description>&lt;p&gt;
&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large AI Models in Health Informatics: Applications, Challenges, and the Future. (arXiv:2303.11568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11568
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;AI&#27169;&#22411;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#20855;&#26377;&#31361;&#30772;&#24615;&#24212;&#29992;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#25968;&#25454;&#37327;&#30340;&#25361;&#25112;&#38656;&#35201;&#20811;&#26381;&#65292;&#26410;&#26469;&#20173;&#38656;&#28145;&#20837;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;AI&#27169;&#22411;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#35268;&#27169;&#65292;&#20854;&#35268;&#27169;&#24448;&#24448;&#36229;&#36807;&#25968;&#21313;&#20159;&#12290;&#19968;&#26086;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#26041;&#27861;&#23398;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#24182;&#20026;&#20581;&#24247;&#30456;&#20851;&#39046;&#22495;&#30340;&#31361;&#30772;&#25552;&#20379;&#20102;&#25512;&#21160;&#21147;&#37327;&#12290;&#26412;&#25991;&#23545;&#22823;&#22411;AI&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#32972;&#26223;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large AI models, or foundation models, are models recently emerging with massive scales both parameter-wise and data-wise, the magnitudes of which often reach beyond billions. Once pretrained, large AI models demonstrate impressive performance in various downstream tasks. A concrete example is the recent debut of ChatGPT, whose capability has compelled people's imagination about the far-reaching influence that large AI models can have and their potential to transform different domains of our life. In health informatics, the advent of large AI models has brought new paradigms for the design of methodologies. The scale of multimodality data in the biomedical and health domain has been ever-expanding especially since the community embraced the era of deep learning, which provides the ground to develop, validate, and advance large AI models for breakthroughs in health-related areas. This article presents an up-to-date comprehensive review of large AI models, from background to their applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01692</link><description>&lt;p&gt;
&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#65306;&#20844;&#27491;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Travel Demand Forecasting: A Fair AI Approach. (arXiv:2303.01692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#12290;&#23613;&#31649;&#22522;&#20110;AI&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#33021;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#20135;&#29983;&#39044;&#27979;&#20559;&#24046;&#24182;&#24341;&#21457;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#20123;&#26377;&#20559;&#35265;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#21487;&#33021;&#20250;&#23548;&#33268;&#21152;&#21095;&#31038;&#20250;&#19981;&#24179;&#31561;&#30340;&#20132;&#36890;&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#12289;&#39640;&#24230;&#20934;&#30830;&#30340;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;AI&#27169;&#22411;&#23545;&#20110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#25910;&#20837;&#65289;&#30340;&#20844;&#24179;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#39033;&#65292;&#35813;&#39033;&#26126;&#30830;&#22320;&#35774;&#35745;&#29992;&#20110;&#34913;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#19982;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#21152;&#20837;&#21040;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) and machine learning have been increasingly adopted for travel demand forecasting. The AI-based travel demand forecasting models, though generate accurate predictions, may produce prediction biases and raise fairness issues. Using such biased models for decision-making may lead to transportation policies that exacerbate social inequalities. However, limited studies have been focused on addressing the fairness issues of these models. Therefore, in this study, we propose a novel methodology to develop fairness-aware, highly-accurate travel demand forecasting models. Particularly, the proposed methodology can enhance the fairness of AI models for multiple protected attributes (such as race and income) simultaneously. Specifically, we introduce a new fairness regularization term, which is explicitly designed to measure the correlation between prediction accuracy and multiple protected attributes, into the loss function of the travel demand forecasting model. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#25216;&#26415;&#30340;&#28789;&#27963;&#33021;&#28304;&#31038;&#21306;&#30446;&#26631;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#20026;&#21830;&#19994;&#21644;&#20303;&#23429;&#31038;&#21306;&#30340;&#33021;&#37327;&#20379;&#24212;&#32773;&#35774;&#35745;&#21644;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#35745;&#21010;&#30340;&#30446;&#30340;&#26159;&#25913;&#21464;&#24847;&#22823;&#21033;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#30340;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#12290;&#36825;&#31181;&#32858;&#21512;&#26088;&#22312;&#65306;a&#65289;&#26368;&#23567;&#21270;&#22312;&#20027;&#35201;&#21464;&#30005;&#31449;&#22788;&#20135;&#29983;&#30340;&#21453;&#21521;&#21151;&#29575;&#27969;&#65292;&#35813;&#21151;&#29575;&#27969;&#22312;&#24403;&#22320;&#30005;&#32593;&#20013;&#30340;&#22826;&#38451;&#33021;&#30005;&#27744;&#30340;&#21457;&#30005;&#37327;&#36229;&#36807;&#28040;&#32791;&#26102;&#20250;&#21457;&#29983;; b&#65289;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#65292;&#35813;&#38656;&#27714;&#36890;&#24120;&#21457;&#29983;&#22312;&#20621;&#26202;&#26102;&#20998;&#12290;&#22312;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#28909;&#38376;&#30340;&#30005;&#36127;&#33655;&#32858;&#31867;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;-&#21363;k-means&#65292;k-medoids&#21644;&#19968;&#31181;&#32858;&#21512;&#23618;&#27425;&#32858;&#31867;-alongside&#20004;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;-&#21363;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#21463;&#38480;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#39564;&#35777;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#39033;&#26032;&#39062;&#30340;&#25351;&#26631;-&#21363;&#23792;&#20540;&#24615;&#33021;&#35780;&#20998;&#65288;PPS&#65289;
&lt;/p&gt;
&lt;p&gt;
The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.14040</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#23545;&#26435;&#37325;&#36827;&#34892;&#32622;&#25442;&#23545;&#31216;&#24615;&#32534;&#30721;&#65292;&#23454;&#29616;&#23545;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#25110;&#26799;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#20026;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#26550;&#26500;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#22815;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#21151;&#33021;&#32593;&#32476;&#65288;NFN&#65289;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#20248;&#21270;&#12289;&#22788;&#29702;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#32593;&#32476;&#32534;&#36753;&#21644;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#35774;&#35745;&#22788;&#29702;&#20854;&#20182;&#32593;&#32476;&#26435;&#37325;&#30340;&#26377;&#25928;&#26550;&#26500;&#30340;&#32479;&#19968;&#21407;&#21017;&#24456;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31216;&#24615;&#30340;&#35270;&#35282;&#26469;&#35774;&#35745;&#31070;&#32463;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20851;&#27880;&#28145;&#24230;&#21069;&#39304;&#32593;&#32476;&#26435;&#37325;&#20013;&#20986;&#29616;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#22240;&#20026;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#27809;&#26377;&#22266;&#26377;&#39034;&#24207;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#32622;&#25442;&#31561;&#21464;&#31070;&#32463;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36825;&#20123;&#23545;&#31216;&#24615;&#32534;&#30721;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#21442;&#25968;&#26469;&#32422;&#26463;&#20026;&#32622;&#25442;&#31561;&#21464;&#30340;NF-Layers&#65288;&#31070;&#32463;&#21151;&#33021;&#23618;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#20998;&#26512;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#31561;&#22810;&#20010;&#27169;&#22411;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.12168</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#27604;&#36739;&#35780;&#20272;&#65306;&#30740;&#31350;&#20851;&#38190;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers. (arXiv:2302.12168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#20998;&#26512;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#31561;&#22810;&#20010;&#27169;&#22411;&#30340;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#65288;STLF&#65289;&#23545;&#30005;&#32593;&#21644;&#33021;&#28304;&#24066;&#22330;&#30340;&#26377;&#25928;&#21644;&#32463;&#27982;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30005;&#21147;&#38656;&#27714;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#20197;&#21450;&#20854;&#23545;&#21508;&#31181;&#22806;&#37096;&#22240;&#32032;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;STLF&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#25552;&#21069;&#19968;&#22825;&#30340;&#39044;&#27979;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#19987;&#27880;&#20110;&#33889;&#33796;&#29273;&#30340;&#22269;&#23478;&#20928;&#32858;&#21512;STLF&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#32771;&#34385;&#20102;&#19968;&#32452;&#26377;&#25351;&#31034;&#24615;&#30340;&#12289;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#31070;&#32463;&#22522;&#30784;&#25193;&#23637;&#31995;&#25968;&#20998;&#26512;&#65288;N-BEATS&#65289;&#12289;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#21644;&#26102;&#38388;&#34701;&#21512;&#21464;&#21387;&#22120;&#65288;TFT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#26174;&#33879;&#24433;&#21709;&#38656;&#27714;&#30340;&#22240;&#32032;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#27599;&#20010;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting (STLF) is vital for the effective and economic operation of power grids and energy markets. However, the non-linearity and non-stationarity of electricity demand as well as its dependency on various external factors renders STLF a challenging task. To that end, several deep learning models have been proposed in the literature for STLF, reporting promising results. In order to evaluate the accuracy of said models in day-ahead forecasting settings, in this paper we focus on the national net aggregated STLF of Portugal and conduct a comparative study considering a set of indicative, well-established deep autoregressive models, namely multi-layer perceptrons (MLP), long short-term memory networks (LSTM), neural basis expansion coefficient analysis (N-BEATS), temporal convolutional networks (TCN), and temporal fusion transformers (TFT). Moreover, we identify factors that significantly affect the demand and investigate their impact on the accuracy of each model. O
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;</title><link>http://arxiv.org/abs/2302.09512</link><description>&lt;p&gt;
SAT&#38656;&#35201;&#24443;&#24213;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;CSP&#21644;SAT&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#20123;&#20363;&#23376;&#26080;&#27861;&#22312;&#19981;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#36739;&#24369;&#30340;&#32467;&#35770;P $\neq$ NP&#12290;&#26412;&#25991;&#37319;&#29992;&#30340;&#26159;&#19968;&#31181;&#35777;&#26126;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#24314;&#35774;&#24615;&#26041;&#27861;&#65292;&#19982;&#30446;&#21069;&#35745;&#31639;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#19982;Kurt G\"{o}del&#22312;&#35777;&#26126;&#20182;&#33879;&#21517;&#30340;&#36923;&#36753;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#26102;&#20351;&#29992;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;&#27491;&#22914;G\"{o}del&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#24418;&#24335;&#19978;&#30340;&#19981;&#21487;&#35777;&#26126;&#24615;&#26159;&#21487;&#34892;&#30340;&#19968;&#26679;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#35745;&#31639;&#19978;&#30340;&#38590;&#24230;&#19981;&#26159;&#24456;&#38590;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;3-SAT&#65292;&#35777;&#26126;&#19979;&#30028;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#26377;&#21508;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#21487;&#29992;&#20110;&#36991;&#20813;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#38590;&#30340;&#20363;&#23376;&#20013;&#65292;&#24443;&#24213;&#25628;&#32034;&#21487;&#33021;&#26159;&#21807;&#19968;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#35777;&#26126;&#20854;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#22635;&#34917;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#24110;&#21161;&#31038;&#21306;&#34913;&#37327;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00521</link><description>&lt;p&gt;
Off-the-Grid MARL: &#24102;&#26377;&#22522;&#20934;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning. (arXiv:2302.00521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#22635;&#34917;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#24110;&#21161;&#31038;&#21306;&#34913;&#37327;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#24320;&#21457;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#22120;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#24320;&#21551;&#20102;&#24040;&#22823;&#30340;&#20215;&#20540;&#12290;&#35768;&#22810;&#37325;&#35201;&#30340;&#24037;&#19994;&#31995;&#32479;&#26159;&#22810;&#26234;&#33021;&#20307;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#20351;&#29992;&#23450;&#21046;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#19994;&#20013;&#65292;&#20998;&#24067;&#24335;&#36827;&#31243;&#32463;&#24120;&#21487;&#20197;&#22312;&#36816;&#34892;&#26399;&#38388;&#35760;&#24405;&#65292;&#24182;&#23384;&#20648;&#22823;&#37327;&#30340;&#28436;&#31034;&#25968;&#25454;&#12290;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#22686;&#24378;&#23398;&#20064;&#65288;MARL&#65289;&#20026;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24314;&#31435;&#26377;&#25928;&#30340;&#20998;&#25955;&#24335;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;MARL&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#22240;&#27492;&#32570;&#20047;&#22312;&#24378;&#21270;&#23398;&#20064;&#26356;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#20013;&#36890;&#24120;&#20250;&#25214;&#21040;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#12290;&#36825;&#20123;&#19981;&#36275;&#20351;&#24471;&#31038;&#21306;&#26080;&#27861;&#21512;&#29702;&#22320;&#34913;&#37327;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21457;&#24067;Off-the-Grid MARL&#65288;OG-MARL&#65289;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65306;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#21327;&#20316;&#31163;&#32447;MARL&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). These deficiencies make it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a growing repository of high-quality datasets with baselines for cooperative offline MARL
&lt;/p&gt;</description></item><item><title>Salesforce CausalAI&#24211;&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#26080;&#38656;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.10859</link><description>&lt;p&gt;
Salesforce CausalAI&#24211;: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#22240;&#26524;&#20998;&#26512;&#30340;&#24555;&#36895;&#21487;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis of Time Series and Tabular Data. (arXiv:2301.10859v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10859
&lt;/p&gt;
&lt;p&gt;
Salesforce CausalAI&#24211;&#26159;&#19968;&#20010;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#26080;&#38656;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Salesforce CausalAI&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24320;&#28304;&#24211;&#12290;&#23427;&#25903;&#25345;&#31163;&#25955;&#12289;&#36830;&#32493;&#21644;&#24322;&#36136;&#31867;&#22411;&#30340;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25512;&#26029;&#12290;&#35813;&#24211;&#21253;&#25324;&#22788;&#29702;&#21464;&#37327;&#20043;&#38388;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#22788;&#29702;&#36827;&#34892;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#30740;&#31350;&#21508;&#31181;&#31639;&#27861;&#30340;&#21516;&#26102;&#25511;&#21046;&#22522;&#30784;&#22240;&#26524;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26080;&#38656;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;&#35813;&#24211;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#24555;&#36895;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24615;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;Salesforce CausalAI API&#21450;&#20854;&#21151;&#33021;&#20197;&#21450;s&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Salesforce CausalAI Library, an open-source library for causal analysis using observational data. It supports causal discovery and causal inference for tabular and time series data, of discrete, continuous and heterogeneous types. This library includes algorithms that handle linear and non-linear causal relationships between variables, and uses multi-processing for speed-up. We also include a data generator capable of generating synthetic data with specified structural equation model for the aforementioned data formats and types, that helps users control the ground-truth causal process while investigating various algorithms. Finally, we provide a user interface (UI) that allows users to perform causal analysis on data without coding. The goal of this library is to provide a fast and flexible solution for a variety of problems in the domain of causality. This technical report describes the Salesforce CausalAI API along with its capabilities, the implementations of the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#20154;&#26426;&#22242;&#38431;&#20013;&#21160;&#24577;&#35282;&#33394;&#20998;&#37197;&#21644;&#21327;&#21516;&#20219;&#21153;&#35268;&#21010;&#30340;&#32479;&#19968;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#22522;&#20110;&#34892;&#20026;&#26641;&#30340;&#21453;&#24212;&#24335;&#35268;&#21010;&#26041;&#27861;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35299;&#20915;&#19981;&#21516;&#26041;&#38754;&#30340;&#21327;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08038</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#20154;&#26426;&#22242;&#38431;&#20013;&#21160;&#24577;&#35282;&#33394;&#20998;&#37197;&#21644;&#21327;&#21516;&#20219;&#21153;&#35268;&#21010;&#30340;&#32479;&#19968;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Unified Architecture for Dynamic Role Allocation and Collaborative Task Planning in Mixed Human-Robot Teams. (arXiv:2301.08038v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28151;&#21512;&#20154;&#26426;&#22242;&#38431;&#20013;&#21160;&#24577;&#35282;&#33394;&#20998;&#37197;&#21644;&#21327;&#21516;&#20219;&#21153;&#35268;&#21010;&#30340;&#32479;&#19968;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#22522;&#20110;&#34892;&#20026;&#26641;&#30340;&#21453;&#24212;&#24335;&#35268;&#21010;&#26041;&#27861;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#35299;&#20915;&#19981;&#21516;&#26041;&#38754;&#30340;&#21327;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#22788;&#29702;&#12289;&#28938;&#25509;&#21644;&#35013;&#37197;&#65292;&#20154;&#26426;&#21327;&#21516;&#36807;&#31243;&#30340;&#19981;&#26029;&#37096;&#32626;&#25512;&#21160;&#20102;&#31649;&#29702;&#22823;&#22411;&#24322;&#26500;&#22242;&#38431;&#21644;&#21516;&#26102;&#30417;&#25511;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#30340;&#36861;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#20154;&#26426;&#22242;&#38431;&#21160;&#24577;&#35282;&#33394;&#20998;&#37197;&#21644;&#21327;&#21516;&#20219;&#21153;&#35268;&#21010;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#32467;&#26500;&#21033;&#29992;&#22522;&#20110;&#34892;&#20026;&#26641;&#30340;&#38598;&#20013;&#24335;&#21453;&#24212;&#24335;&#21644;&#27169;&#22359;&#21270;&#20219;&#21153;&#26080;&#20851;&#35268;&#21010;&#26041;&#27861;&#36827;&#34892;&#21160;&#20316;&#35843;&#24230;&#65292;&#21516;&#26102;&#21033;&#29992;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#21046;&#23450;&#21160;&#24577;&#20998;&#37197;&#20010;&#20307;&#35282;&#33394;&#25110;&#21327;&#20316;&#30340;&#38382;&#39064;&#12290;&#37319;&#29992;&#19981;&#21516;&#30340;&#28151;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#25104;&#26412;&#24230;&#37327;&#25351;&#26631;&#20351;&#24471;&#35813;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#26356;&#22909;&#22320;&#20851;&#27880;&#21327;&#20316;&#30340;&#19981;&#21516;&#26041;&#38754;&#65288;&#20363;&#22914;&#23436;&#25104;&#26102;&#38388;&#12289;&#20154;&#20307;&#24037;&#31243;&#23398;&#12289;&#20154;&#31867;&#20559;&#22909;&#65289;&#12290;&#20154;&#31867;&#20559;&#22909;&#26159;&#36890;&#36807;&#35848;&#21028;&#38454;&#27573;&#26469;&#30830;&#23450;&#30340;&#65292;
&lt;/p&gt;
&lt;p&gt;
The growing deployment of human-robot collaborative processes in several industrial applications, such as handling, welding, and assembly, unfolds the pursuit of systems which are able to manage large heterogeneous teams and, at the same time, monitor the execution of complex tasks. In this paper, we present a novel architecture for dynamic role allocation and collaborative task planning in a mixed human-robot team of arbitrary size. The architecture capitalizes on a centralized reactive and modular task-agnostic planning method based on Behavior Trees (BTs), in charge of actions scheduling, while the allocation problem is formulated through a Mixed-Integer Linear Program (MILP), that assigns dynamically individual roles or collaborations to the agents of the team. Different metrics used as MILP cost allow the architecture to favor various aspects of the collaboration (e.g. makespan, ergonomics, human preferences). Human preference are identified through a negotiation phase, in which, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#25552;&#31034;&#30340;&#22240;&#32032;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#21644;&#21487;&#38752;&#22320;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#31034;&#30340;&#35774;&#35745;&#21644;&#23545;&#35805;&#20027;&#39064;&#26126;&#26174;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05843</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#29992;&#20110;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data. (arXiv:2301.05843v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#35774;&#35745;&#25552;&#31034;&#30340;&#22240;&#32032;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#21644;&#21487;&#38752;&#22320;&#25910;&#38598;&#29992;&#25143;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#31034;&#30340;&#35774;&#35745;&#21644;&#23545;&#35805;&#20027;&#39064;&#26126;&#26174;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#26469;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#36861;&#27714;&#32473;&#23450;&#30446;&#26631;&#65288;&#22914;&#20174;&#29992;&#25143;&#25910;&#38598;&#33258;&#25105;&#25253;&#21578;&#25968;&#25454;&#65289;&#30340;&#21516;&#26102;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21738;&#20123;&#25552;&#31034;&#30340;&#35774;&#35745;&#22240;&#32032;&#21487;&#20197;&#24110;&#21161;&#24341;&#23548;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#24182;&#21487;&#38752;&#22320;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#20154;&#35774;&#30340;&#25552;&#31034;&#24418;&#24335;&#12290;&#36890;&#36807;&#19968;&#39033;&#22312;&#32447;&#30740;&#31350;&#65288;N = 48&#65289;&#65292;&#21442;&#19982;&#32773;&#19982;&#30001;&#19981;&#21516;&#35774;&#35745;&#25552;&#31034;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35774;&#35745;&#25552;&#31034;&#21644;&#23545;&#35805;&#20027;&#39064;&#22914;&#20309;&#24433;&#21709;&#23545;&#35805;&#27969;&#31243;&#21644;&#29992;&#25143;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#12290;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35206;&#30422;&#20102;79%&#30340;&#25152;&#38656;&#20449;&#24687;&#27133;&#65292;&#24182;&#19988;&#25552;&#31034;&#21644;&#20027;&#39064;&#30340;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;&#23545;&#35805;&#27969;&#31243;&#21644;&#25968;&#25454;&#25910;&#38598;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#26500;&#24314;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide a new way to build chatbots by accepting natural language prompts. Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal, such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an online study (N = 48) where participants conversed with chatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics affected the conversation flows and users' perceptions of chatbots. Our chatbots covered 79% of the desired information slots during conversations, and the designs of prompts and topics significantly influenced the conversation flows and the data collection performance. We discuss the opportunities and challenges of building chatbots with LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;&#65288;R-MLTSK-FS&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.03283</link><description>&lt;p&gt;
&#19968;&#20010;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust Multilabel Method Integrating Rule-based Transparent Model, Soft Label Correlation Learning and Label Noise Resistance. (arXiv:2301.03283v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36879;&#26126;&#27169;&#22411;&#12289;&#36719;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#26631;&#31614;&#22122;&#22768;&#25239;&#24615;&#30340;&#40065;&#26834;&#22810;&#26631;&#35760;&#26041;&#27861;&#65288;R-MLTSK-FS&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#35760;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#36879;&#26126;&#24615;&#12289;&#26631;&#31614;&#30456;&#20851;&#24615;&#23398;&#20064;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#26041;&#27861;&#21516;&#26102;&#30740;&#31350;&#36825;&#19977;&#20010;&#29305;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22810;&#26631;&#35760;Takagi-Sugeno-Kang&#27169;&#31946;&#31995;&#32479;&#65288;R-MLTSK-FS&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36719;&#26631;&#31614;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#26126;&#30830;&#27979;&#37327;&#26631;&#31614;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#26469;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#20063;&#26159;&#20854;&#20182;&#20004;&#31181;&#26426;&#21046;&#30340;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;TSK FS&#20316;&#20026;&#22522;&#27169;&#22411;&#65292;&#20197;&#27604;&#35768;&#22810;&#29616;&#26377;&#30340;&#22810;&#26631;&#35760;&#27169;&#22411;&#26356;&#36879;&#26126;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24314;&#27169;&#29305;&#24449;&#21644;&#36719;&#26631;&#31614;&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#26631;&#35760;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22522;&#20110;&#36719;&#26631;&#31614;&#31354;&#38388;&#21644;&#27169;&#31946;&#29305;&#24449;&#31354;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#22686;&#24378;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model transparency, label correlation learning and the robust-ness to label noise are crucial for multilabel learning. However, few existing methods study these three characteristics simultaneously. To address this challenge, we propose the robust multilabel Takagi-Sugeno-Kang fuzzy system (R-MLTSK-FS) with three mechanisms. First, we design a soft label learning mechanism to reduce the effect of label noise by explicitly measuring the interactions between labels, which is also the basis of the other two mechanisms. Second, the rule-based TSK FS is used as the base model to efficiently model the inference relationship be-tween features and soft labels in a more transparent way than many existing multilabel models. Third, to further improve the performance of multilabel learning, we build a correlation enhancement learning mechanism based on the soft label space and the fuzzy feature space. Extensive experiments are conducted to demonstrate the superiority of the proposed method.
&lt;/p&gt;</description></item><item><title>StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2301.01947</link><description>&lt;p&gt;
StitchNet: &#20174;&#39044;&#35757;&#32451;&#29255;&#27573;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01947
&lt;/p&gt;
&lt;p&gt;
StitchNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#26041;&#24335;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#32593;&#32476;&#65292;&#26080;&#38656;&#20256;&#32479;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#36890;&#36807;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#25351;&#23548;&#29255;&#27573;&#30340;&#36873;&#25321;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;StitchNet&#36824;&#21487;&#20197;&#23454;&#29616;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#33539;&#24335;StitchNet&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#29255;&#27573;&#65288;&#19968;&#20010;&#25110;&#22810;&#20010;&#36830;&#32493;&#30340;&#32593;&#32476;&#23618;&#65289;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;StitchNet&#20801;&#35768;&#21019;&#24314;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#20256;&#32479;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#22823;&#37327;&#35745;&#31639;&#21644;&#25968;&#25454;&#35201;&#27714;&#12290;&#25105;&#20204;&#21033;&#29992;&#23621;&#20013;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20316;&#20026;&#19968;&#31181;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#22320;&#25351;&#23548;&#36873;&#25321;&#36825;&#20123;&#29255;&#27573;&#65292;&#20197;&#32452;&#21512;&#36866;&#21512;&#29305;&#23450;&#20934;&#30830;&#24615;&#38656;&#27714;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#30340;&#20219;&#21153;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29255;&#27573;&#21487;&#20197;&#34987;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#35201;&#27714;&#30340;&#19968;&#23567;&#37096;&#20998;&#19979;&#21019;&#24314;&#19982;&#20256;&#32479;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#20934;&#30830;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26032;&#33539;&#24335;&#25152;&#33021;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21363;&#26102;&#20010;&#24615;&#21270;&#27169;&#22411;&#21019;&#24314;&#21644;&#25512;&#26029;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#19982;&#22330;&#26223;&#34920;&#31034;&#32806;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00362</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#30340;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Guided Transformer-Enabled Reinforcement Learning for Efficient Autonomous Navigation. (arXiv:2301.00362v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20449;&#24687;&#19982;&#22330;&#26223;&#34920;&#31034;&#32806;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#33258;&#20027;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#26377;&#20123;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#30446;&#26631;&#20449;&#24687;&#19982;&#24863;&#30693;&#27169;&#22359;&#35299;&#32806;&#65292;&#24182;&#30452;&#25509;&#20316;&#20026;&#20915;&#31574;&#30340;&#26465;&#20214;&#24341;&#20837;&#65292;&#23548;&#33268;&#22330;&#26223;&#34920;&#31034;&#20013;&#19982;&#30446;&#26631;&#26080;&#20851;&#30340;&#29305;&#24449;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36215;&#21040;&#23545;&#25239;&#20316;&#29992;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;&#22686;&#24378;&#23398;&#20064;(GTRL)&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#30446;&#26631;&#29366;&#24577;&#20316;&#20026;&#22330;&#26223;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#26469;&#25351;&#23548;&#22330;&#26223;&#34920;&#31034;&#19982;&#30446;&#26631;&#20449;&#24687;&#30340;&#32806;&#21512;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#33258;&#20027;&#23548;&#33322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#21464;&#20307;&#20316;&#20026;&#24863;&#30693;&#31995;&#32479;&#30340;&#39592;&#24178;&#65292;&#21363;&#30446;&#26631;&#24341;&#23548;&#21464;&#21387;&#22120;(GoT)&#65292;&#24182;&#20351;&#29992;&#19987;&#23478;&#20808;&#39564;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite some successful applications of goal-driven navigation, existing deep reinforcement learning (DRL)-based approaches notoriously suffers from poor data efficiency issue. One of the reasons is that the goal information is decoupled from the perception module and directly introduced as a condition of decision-making, resulting in the goal-irrelevant features of the scene representation playing an adversary role during the learning process. In light of this, we present a novel Goal-guided Transformer-enabled reinforcement learning (GTRL) approach by considering the physical goal states as an input of the scene encoder for guiding the scene representation to couple with the goal information and realizing efficient autonomous navigation. More specifically, we propose a novel variant of the Vision Transformer as the backbone of the perception system, namely Goal-guided Transformer (GoT), and pre-train it with expert priors to boost the data efficiency. Subsequently, a reinforcement le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#36923;&#36753;&#35884;&#35823;&#12290;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#32972;&#26223;&#30693;&#35782;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#22823;&#37327;&#25968;&#25454;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.07425</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;
&lt;/p&gt;
&lt;p&gt;
Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments. (arXiv:2212.07425v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#35770;&#35777;&#20013;&#30340;&#36923;&#36753;&#35884;&#35823;&#12290;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#32972;&#26223;&#30693;&#35782;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#22823;&#37327;&#25968;&#25454;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#12289;&#23459;&#20256;&#21644;&#38169;&#35823;&#35770;&#35777;&#30340;&#20256;&#25773;&#29616;&#35937;&#24471;&#21040;&#20102;&#25918;&#22823;&#12290;&#32473;&#23450;&#25968;&#25454;&#37327;&#30340;&#24222;&#22823;&#21644;&#35782;&#21035;&#35770;&#35777;&#35268;&#33539;&#36829;&#35268;&#30340;&#24494;&#22937;&#24615;&#65292;&#20351;&#29992;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#26469;&#25903;&#25345;&#20449;&#24687;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#20869;&#23481;&#23457;&#26680;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#23558;&#20197;&#21069;&#20851;&#20110;&#36923;&#36753;&#35884;&#35823;&#30340;&#29702;&#35770;&#24037;&#20316;&#21046;&#23450;&#20026;&#26816;&#27979;&#12289;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#32508;&#21512;&#19977;&#38454;&#27573;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#35780;&#20272;&#30340;&#27599;&#20010;&#38454;&#27573;&#23545;&#29616;&#26377;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36866;&#24212;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#22411;&#25512;&#29702;&#12289;&#22522;&#20110;&#23454;&#20363;&#25512;&#29702;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#19977;&#20010;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26063;&#12290;&#36825;&#20123;&#26041;&#27861;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#12289;&#32972;&#26223;&#30693;&#35782;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#31574;&#30053;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#19977;&#38454;&#27573;&#26694;&#26550;&#33258;&#28982;&#22320;&#24041;&#22266;&#20102;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation, propaganda, and flawed argumentation has been amplified in the Internet era. Given the volume of data and the subtlety of identifying violations of argumentation norms, supporting information analytics tasks, like content moderation, with trustworthy methods that can identify logical fallacies is essential. In this paper, we formalize prior theoretical work on logical fallacies into a comprehensive three-stage evaluation framework of detection, coarse-grained, and fine-grained classification. We adapt existing evaluation datasets for each stage of the evaluation. We employ three families of robust and explainable methods based on prototype reasoning, instance-based reasoning, and knowledge injection. The methods combine language models with background knowledge and explainable mechanisms. Moreover, we address data sparsity with strategies for data augmentation and curriculum learning. Our three-stage framework natively consolidates prior datasets and metho
&lt;/p&gt;</description></item><item><title>3DHumanGAN &#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#22806;&#35266;&#30340;&#20840;&#36523;&#20154;&#20307;&#29031;&#29255;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;3D&#23039;&#21183;&#26144;&#23556;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#35270;&#35282;&#21644;&#23039;&#21183;&#30340;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;3D&#20154;&#20307;&#20808;&#39564;&#23454;&#29616;&#23039;&#24577;&#26465;&#20214;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.07378</link><description>&lt;p&gt;
3DHumanGAN: &#20855;&#26377;3D&#23039;&#21183;&#26144;&#23556;&#30340;3D&#24863;&#30693;&#20154;&#20307;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping. (arXiv:2212.07378v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07378
&lt;/p&gt;
&lt;p&gt;
3DHumanGAN &#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#22806;&#35266;&#30340;&#20840;&#36523;&#20154;&#20307;&#29031;&#29255;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;3D&#23039;&#21183;&#26144;&#23556;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#35270;&#35282;&#21644;&#23039;&#21183;&#30340;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;3D&#20154;&#20307;&#20808;&#39564;&#23454;&#29616;&#23039;&#24577;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;3DHumanGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19968;&#33268;&#22806;&#35266;&#30340;&#20840;&#36523;&#20154;&#20307;&#29031;&#29255;&#65292;&#19988;&#33021;&#36866;&#24212;&#19981;&#21516;&#35270;&#35282;&#21644;&#36523;&#20307;&#23039;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#21512;&#25104;&#20154;&#20307;&#32467;&#26500;&#26102;&#30340;&#34920;&#36798;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;2D&#21367;&#31215;&#39592;&#24178;&#30001;&#19968;&#20010;3D&#23039;&#24577;&#26144;&#23556;&#32593;&#32476;&#35843;&#21046;&#12290;3D&#23039;&#24577;&#26144;&#23556;&#32593;&#32476;&#20197;&#26377;&#23039;&#24577;&#30340;3D&#20154;&#20307;&#32593;&#26684;&#20026;&#26465;&#20214;&#65292;&#24418;&#25104;&#19968;&#20010;&#21487;&#21576;&#29616;&#30340;&#38544;&#24335;&#20989;&#25968;&#12290;&#36825;&#19968;&#35774;&#35745;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;i&#65289;&#23427;&#21033;&#29992;&#20102;2D GAN&#30340;&#20248;&#21183;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65307;ii&#65289;&#22312;&#19981;&#21516;&#30340;&#35270;&#35282;&#21644;&#23039;&#21183;&#19979;&#29983;&#25104;&#19968;&#33268;&#30340;&#22270;&#20687;&#65307;iii&#65289;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;3D&#20154;&#20307;&#20808;&#39564;&#24182;&#23454;&#29616;&#23039;&#24577;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 3DHumanGAN, a 3D-aware generative adversarial network that synthesizes photorealistic images of full-body humans with consistent appearances under different view-angles and body-poses. To tackle the representational and computational challenges in synthesizing the articulated structure of human bodies, we propose a novel generator architecture in which a 2D convolutional backbone is modulated by a 3D pose mapping network. The 3D pose mapping network is formulated as a renderable implicit function conditioned on a posed 3D human mesh. This design has several merits: i) it leverages the strength of 2D GANs to produce high-quality images; ii) it generates consistent images under varying view-angles and poses; iii) the model can incorporate the 3D human prior and enable pose conditioning. Project page: https://3dhumangan.github.io/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.00979</link><description>&lt;p&gt;
PASTA&#65306;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#29992;&#20110; Syn-to-Real &#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#24265;&#20215;&#19988;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#34920;&#29616;&#26174;&#33879;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Proportional Amplitude Spectrum Training Augmentation (PASTA)&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#25552;&#39640;&#21512;&#25104;&#21040;&#30495;&#23454;&#65288;Syn-to-Real&#65289;&#27867;&#21270;&#24615;&#33021;&#12290; PASTA &#22312; Fourier &#39046;&#22495;&#20013;&#25200;&#21160;&#21512;&#25104;&#22270;&#20687;&#30340;&#24133;&#24230;&#35889;&#20197;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992; PASTA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#25200;&#21160;&#31574;&#30053;&#65292;&#20854;&#20013;&#39640;&#39057;&#20998;&#37327;&#30456;&#23545;&#20110;&#20302;&#39057;&#20998;&#37327;&#26356;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65288;GTAV-to-Real&#65289;&#65292;&#30446;&#26631;&#26816;&#27979;&#65288;Sim10K-to-Real&#65289;&#21644;&#23545;&#35937;&#35782;&#21035;&#65288;VisDA-C Syn-to-Real&#65289;&#20219;&#21153;&#65292;&#22312;&#24635;&#20849;5&#20010; Syn-to-Real &#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; PASTA &#30340;&#24615;&#33021;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
&lt;/p&gt;</description></item><item><title>DyFOS&#26159;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#26469;&#23547;&#25214;&#26368;&#20339;&#20256;&#24863;&#22120;&#29366;&#24577;&#65292;&#20197;&#26368;&#23567;&#21270;&#24863;&#30693;&#21463;&#38480;&#28459;&#28216;&#22120;&#30340;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#38556;&#30861;&#21644;&#36974;&#25377;&#12290;</title><link>http://arxiv.org/abs/2211.16721</link><description>&lt;p&gt;
&#25105;&#29616;&#22312;&#22312;&#21738;&#37324;&#65311;&#21160;&#24577;&#23547;&#25214;&#26368;&#20339;&#20256;&#24863;&#22120;&#29366;&#24577;&#20197;&#26368;&#23567;&#21270;&#24863;&#30693;&#21463;&#38480;&#28459;&#28216;&#22120;&#30340;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Where Am I Now? Dynamically Finding Optimal Sensor States to Minimize Localization Uncertainty for a Perception-Denied Rover. (arXiv:2211.16721v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16721
&lt;/p&gt;
&lt;p&gt;
DyFOS&#26159;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#26469;&#23547;&#25214;&#26368;&#20339;&#20256;&#24863;&#22120;&#29366;&#24577;&#65292;&#20197;&#26368;&#23567;&#21270;&#24863;&#30693;&#21463;&#38480;&#28459;&#28216;&#22120;&#30340;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#38556;&#30861;&#21644;&#36974;&#25377;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DyFOS&#65292;&#19968;&#31181;&#21160;&#24577;&#23547;&#25214;&#26368;&#20339;&#29366;&#24577;&#20197;&#26368;&#23567;&#21270;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#24863;&#30693;&#26041;&#27861;&#65292;&#21516;&#26102;&#36991;&#20813;&#38556;&#30861;&#21644;&#36974;&#25377;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#24863;&#30693;&#21463;&#38480;&#30340;&#28459;&#28216;&#22120;&#20381;&#38752;&#26469;&#33258;&#35266;&#23519;&#32773;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#26469;&#23450;&#20301;&#33258;&#24049;&#27839;&#30528;&#20805;&#28385;&#38556;&#30861;&#29289;&#30340;&#36335;&#24452;&#30340;&#24773;&#26223;&#12290;&#35266;&#23519;&#32773;&#30340;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#26159;&#20256;&#24863;&#22120;&#26412;&#36523;&#12289;&#28459;&#28216;&#22120;&#21644;&#21608;&#22260;&#29615;&#22659;&#29366;&#24577;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#23567;&#21270;&#28459;&#28216;&#22120;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20339;&#20256;&#24863;&#22120;&#29366;&#24577;&#65292;DyFOS&#20351;&#29992;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#27969;&#27700;&#32447;&#36827;&#34892;&#20248;&#21270;&#25628;&#32034;&#12290;&#32473;&#23450;&#19978;&#36848;&#29366;&#24577;&#30340;&#22823;&#37327;&#26679;&#26412;&#65292;&#27969;&#27700;&#32447;&#20511;&#21161;&#35757;&#32451;&#26377;&#32032;&#30340;&#22797;&#26434;&#29366;&#24577;&#30456;&#20851;&#20256;&#24863;&#22120;&#27979;&#37327;&#27169;&#22411;&#65288;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65289;&#39044;&#27979;&#28459;&#28216;&#22120;&#30340;&#23450;&#20301;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#36824;&#39044;&#27979;&#36974;&#25377;&#21644;&#38556;&#30861;&#30896;&#25758;&#65292;&#20197;&#21435;&#38500;&#19981;&#21487;&#21462;&#30340;&#35266;&#23519;&#32773;&#29366;&#24577;&#24182;&#20943;&#23569;&#19981;&#24517;&#35201;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DyFOS, an active perception method that dynamically finds optimal states to minimize localization uncertainty while avoiding obstacles and occlusions. We consider the scenario where a perception-denied rover relies on position and uncertainty measurements from a viewer robot to localize itself along an obstacle-filled path. The position uncertainty from the viewer's sensor is a function of the states of the sensor itself, the rover, and the surrounding environment. To find an optimal sensor state that minimizes the rover's localization uncertainty, DyFOS uses a localization uncertainty prediction pipeline in an optimization search. Given numerous samples of the states mentioned above, the pipeline predicts the rover's localization uncertainty with the help of a trained, complex state-dependent sensor measurement model (a probabilistic neural network). Our pipeline also predicts occlusion and obstacle collision to remove undesirable viewer states and reduce unnecessary comput
&lt;/p&gt;</description></item><item><title>SnCQA&#26159;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12711</link><description>&lt;p&gt;
SnCQA&#65306;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SnCQA: A hardware-efficient equivariant quantum convolutional circuit architecture. (arXiv:2211.12711v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12711
&lt;/p&gt;
&lt;p&gt;
SnCQA&#26159;&#19968;&#31181;&#30828;&#20214;&#39640;&#25928;&#30340;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SnCQA&#65292;&#36825;&#26159;&#19968;&#32452;&#38024;&#23545;&#25490;&#21015;&#23545;&#31216;&#24615;&#21644;&#31354;&#38388;&#26230;&#26684;&#23545;&#31216;&#24615;&#30340;&#30828;&#20214;&#39640;&#25928;&#31561;&#21464;&#37327;&#23376;&#21367;&#31215;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#12290;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#65292;&#20363;&#22914;&#35768;&#22810;&#37327;&#23376;&#22810;&#20307;&#21644;&#37327;&#23376;&#21270;&#23398;&#38382;&#39064;&#20013;&#24120;&#35265;&#30340;&#26230;&#26684;&#21704;&#23494;&#39039;&#37327;&#65292;&#25105;&#20204;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36866;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#25490;&#21015;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#33410;&#30465;&#35745;&#31639;&#25104;&#26412;&#12290;&#38500;&#20102;&#29702;&#35770;&#30340;&#21019;&#26032;&#24615;&#22806;&#65292;&#22312;&#23454;&#38469;&#30340;&#37327;&#23376;&#35745;&#31639;&#21270;&#23398;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#25311;&#22312;&#23398;&#20064;&#22522;&#24577;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#25968;&#21442;&#25968;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;&#22914;&#32431;&#30828;&#20214;&#39640;&#25928;&#30340;&#22522;&#24577;&#20551;&#35774;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SnCQA&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22122;&#22768;&#38887;&#24615;&#65288;&#20855;&#26377;20&#20493;&#26356;&#22909;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SnCQA, a set of hardware-efficient variational circuits of equivariant quantum convolutional circuits respective to permutation symmetries and spatial lattice symmetries with the number of qubits $n$. By exploiting permutation symmetries of the system, such as lattice Hamiltonians common to many quantum many-body and quantum chemistry problems, Our quantum neural networks are suitable for solving machine learning problems where permutation symmetries are present, which could lead to significant savings of computational costs. Aside from its theoretical novelty, we find our simulations perform well in practical instances of learning ground states in quantum computational chemistry, where we could achieve comparable performances to traditional methods with few tens of parameters. Compared to other traditional variational quantum circuits, such as the pure hardware-efficient ansatz (pHEA), we show that SnCQA is more scalable, accurate, and noise resilient (with $20\times$ bette
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#21457;&#29616;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#25554;&#20540;&#20013;&#28608;&#27963;&#26041;&#24046;&#22349;&#32553;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;REPAIR&#26041;&#27861;&#26469;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26550;&#26500;&#20013;&#23558;REPAIR&#19982;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2211.08403</link><description>&lt;p&gt;
REPAIR: &#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
REPAIR: REnormalizing Permuted Activations for Interpolation Repair. (arXiv:2211.08403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08403
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#21457;&#29616;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#19981;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#25554;&#20540;&#20013;&#28608;&#27963;&#26041;&#24046;&#22349;&#32553;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;REPAIR&#26041;&#27861;&#26469;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#26550;&#26500;&#20013;&#23558;REPAIR&#19982;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Entezari&#31561;&#20154;&#65288;2021&#65289;&#30340;&#29468;&#24819;&#65292;&#21363;&#22914;&#26524;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#37027;&#20040;&#32447;&#24615;&#25554;&#20540;&#20043;&#38388;&#21487;&#33021;&#27809;&#26377;&#25439;&#22833;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20165;&#20351;&#29992;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#26080;&#27861;&#24314;&#31435;&#20302;&#38556;&#30861;&#32447;&#24615;&#36830;&#25509;&#30340;&#21407;&#22240;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#26041;&#24046;&#22349;&#32553;&#30340;&#29616;&#35937;&#65306;&#25554;&#20540;&#28145;&#23618;&#32593;&#32476;&#30340;&#28608;&#27963;&#26041;&#24046;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPAIR&#65288;&#20462;&#22797;&#25554;&#20540;&#30340;&#24402;&#19968;&#21270;&#32622;&#25442;&#28608;&#27963;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#36825;&#20123;&#25554;&#20540;&#32593;&#32476;&#30340;&#39044;&#28608;&#27963;&#26469;&#32531;&#35299;&#26041;&#24046;&#23849;&#28291;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#24402;&#19968;&#21270;&#23618;&#12289;&#32593;&#32476;&#23485;&#24230;&#21644;&#28145;&#24230;&#36873;&#25321;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#21508;&#31181;&#26550;&#26500;&#26063;&#20013;&#20351;&#29992;REPAIR&#20316;&#20026;&#31070;&#32463;&#20803;&#23545;&#40784;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23558;&#38556;&#30861;&#38477;&#20302;60%&#33267;100%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we look into the conjecture of Entezari et al. (2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance. Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks. We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#30340;&#28145;&#24230;&#27531;&#24046;GCN&#26041;&#27861;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#35268;&#21017;&#22270;&#19978;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11174</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#22312;&#28145;&#24230;&#27531;&#24046;GCN&#20013;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Overlapping Community Detection using Dynamic Dilated Aggregation in Deep Residual GCN. (arXiv:2210.11174v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#30340;&#28145;&#24230;&#27531;&#24046;GCN&#26041;&#27861;&#36827;&#34892;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#35268;&#21017;&#22270;&#19978;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#26159;&#22270;&#25366;&#25496;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#19968;&#20123;&#30740;&#31350;&#32771;&#34385;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#33324;&#19981;&#35268;&#21017;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#23558;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#21160;&#24577;&#25193;&#24352;&#32858;&#21512;&#26426;&#21046;&#21644;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21160;&#24577;&#27531;&#24046;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;DynaResGCN&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#37325;&#21472;&#31038;&#21306;&#12290;&#28145;&#24230;&#30340;DynaResGCN&#27169;&#22411;&#34987;&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#32780;&#25105;&#20204;&#23558;&#20271;&#21162;&#21033;-&#27850;&#26494;&#65288;BP&#65289;&#27169;&#22411;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#26694;&#26550;&#24212;&#29992;&#22312;&#19968;&#20010;&#27809;&#26377;&#22522;&#20934;&#20540;&#30340;&#30740;&#31350;&#20027;&#39064;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#25317;&#26377;&#21487;&#38752;&#65288;&#25163;&#24037;&#26631;&#35760;&#65289;&#22522;&#20934;&#20540;&#30340;Facebook&#32593;&#32476;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#32452;&#20855;&#26377;&#32463;&#39564;&#24615;&#65288;&#38750;&#25163;&#24037;&#26631;&#35760;&#65289;&#22522;&#20934;&#20540;&#30340;&#38750;&#24120;&#22823;&#30340;&#21512;&#33879;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overlapping community detection is a key problem in graph mining. Some research has considered applying graph convolutional networks (GCN) to tackle the problem. However, it is still challenging to incorporate deep graph convolutional networks in the case of general irregular graphs. In this study, we design a deep dynamic residual graph convolutional network (DynaResGCN) based on our novel dynamic dilated aggregation mechanisms and a unified end-to-end encoder-decoder-based framework to detect overlapping communities in networks. The deep DynaResGCN model is used as the encoder, whereas we incorporate the Bernoulli-Poisson (BP) model as the decoder. Consequently, we apply our overlapping community detection framework in a research topics dataset without having ground truth, a set of networks from Facebook having a reliable (hand-labeled) ground truth, and in a set of very large co-authorship networks having empirical (not hand-labeled) ground truth. Our experimentation on these datase
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.06366</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#20998;&#21106;&#20026;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#21644;&#23454;&#20363;ID&#26631;&#31614;&#12290;&#30001;&#20110;&#23454;&#20363;ID&#30340;&#25490;&#21015;&#20063;&#26159;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#23398;&#20064;&#39640;&#32500;&#24230;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#21046;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#19981;&#20381;&#36182;&#20219;&#21153;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#20449;&#21495;&#28155;&#21152;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#31867;&#20284;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
&lt;/p&gt;</description></item><item><title>DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09708</link><description>&lt;p&gt;
DenseShift: &#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09708
&lt;/p&gt;
&lt;p&gt;
DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26080;&#20056;&#27861;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;&#65292;&#20063;&#34987;&#31216;&#20026;Shift&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#21644;&#31616;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#20301;Shift&#32593;&#32476;&#19981;&#22914;&#20840;&#31934;&#24230;&#32593;&#32476;&#20934;&#30830;&#65292;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#26435;&#37325;&#33539;&#22260;&#32534;&#30721;&#26041;&#26696;&#21644;&#37327;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DenseShift&#32593;&#32476;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;Shift&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#35270;&#35273;&#21644;&#35821;&#38899;&#24212;&#29992;&#23454;&#29616;&#20102;&#19982;&#20840;&#31934;&#24230;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#30340;&#39640;&#25928;DenseShift&#32593;&#32476;&#37096;&#32626;&#26041;&#27861;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;1.6&#20493;&#21152;&#36895;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20302;&#20301;Shift&#32593;&#32476;&#20013;&#38646;&#26435;&#37325;&#20540;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30001;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RILL&#30456;&#27604;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#21644;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.06838</link><description>&lt;p&gt;
&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning. (arXiv:2208.06838v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20013;&#30001;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#24102;&#26469;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RILL&#30456;&#27604;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#21644;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#31639;&#23376;&#26469;&#36817;&#20284;&#36923;&#36753;&#25512;&#29702;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#26159;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#21487;&#24494;&#20998;&#31639;&#23376;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#21487;&#33021;&#24102;&#26469;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#24182;&#38477;&#20302;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#31181;&#20559;&#24046;&#65292;&#31216;&#20043;&#20026;&#8220;&#34164;&#21547;&#20559;&#24046;&#8221;&#65292;&#24120;&#35265;&#20110;&#20174;&#27169;&#31946;&#36923;&#36753;&#31639;&#23376;&#20013;&#27966;&#29983;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#20559;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36716;&#21270;&#20026;&#8220;&#20943;&#23569;&#34164;&#21547;&#20559;&#24046;&#36923;&#36753;&#25439;&#22833;&#65288;RILL&#65289;&#8221;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#26377;&#20559;&#24046;&#30340;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;RILL&#22312;&#30693;&#35782;&#24211;&#19981;&#23436;&#25972;&#26102;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22312;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#20445;&#25345;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems.  However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning.  In this paper, we reveal that this bias, named \textit{Implication Bias} is common in loss functions derived from fuzzy logic operators.  Furthermore, we propose a simple yet effective method to transform the biased loss functions into \textit{Reduced Implication-bias Logic Loss (RILL)} to address the above problem.  Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.09090</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#22522;&#22240;&#28418;&#21464;&#21040;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20998;&#24067;&#31639;&#27861;&#65288;EDAs&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20174;&#25628;&#32034;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#36731;&#26494;&#22320;&#37319;&#26679;&#20986;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;EDAs&#30340;&#20851;&#38190;&#21442;&#25968;&#26159;&#26679;&#26412;&#22823;&#23567;&#65288;&#31181;&#32676;&#22823;&#23567;&#65289;&#12290;&#22914;&#26524;&#31181;&#32676;&#22823;&#23567;&#22826;&#23567;&#65292;&#27010;&#29575;&#27169;&#22411;&#26356;&#26032;&#20165;&#22522;&#20110;&#23569;&#37327;&#26679;&#26412;&#65292;&#23548;&#33268;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#22522;&#22240;&#28418;&#21464;&#25928;&#24212;&#12290;&#31181;&#32676;&#22823;&#23567;&#36807;&#22823;&#20250;&#36991;&#20813;&#36951;&#20256;&#28418;&#21464;&#65292;&#20294;&#20250;&#20943;&#32531;&#36827;&#31243;&#12290;&#22522;&#20110;&#26368;&#36817;&#37327;&#21270;&#20998;&#26512;&#30340;&#31181;&#32676;&#22823;&#23567;&#22914;&#20309;&#23548;&#33268;&#22522;&#22240;&#28418;&#21464;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;EDAs&#30340;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#12290;&#24403;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#24456;&#39640;&#26102;&#20572;&#27490;&#36816;&#34892;&#65292;&#23427;&#20250;&#33258;&#21160;&#22312;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#36816;&#34892;EDA&#12290;&#36890;&#36807;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#36825;&#31181;&#26234;&#33021;&#37325;&#21551;&#26041;&#26696;&#35777;&#26126;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#24050;&#30693;&#26368;&#20339;&#30340;&#65288;&#38382;&#39064;&#29305;&#23450;&#30340;&#65289;&#21442;&#25968;&#20540;&#65292;&#37325;&#21551;&#26041;&#26696;&#20250;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process.  Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes.  Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Max Queue-Length (M-QL)&#21644;AttentionLight&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;M-QL&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;AttentionLight&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.00006</link><description>&lt;p&gt;
&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization. (arXiv:2201.00006v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;Max Queue-Length (M-QL)&#21644;AttentionLight&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;M-QL&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;AttentionLight&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#24448;&#24448;&#20027;&#35201;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#65292;&#32780;&#24573;&#35270;&#20102;&#36866;&#24403;&#30340;&#20132;&#36890;&#29366;&#24577;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19987;&#23478;&#35774;&#35745;&#30340;&#20132;&#36890;&#20449;&#21495;&#30456;&#20301;&#31454;&#20105;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38431;&#21015;&#38271;&#24230;&#20316;&#20026;&#39640;&#25928;&#29366;&#24577;&#34920;&#31034;&#30340;TSC&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65306;(1) &#22522;&#20110;&#38431;&#21015;&#38271;&#24230;&#23646;&#24615;&#35774;&#35745;&#30340;&#20248;&#21270;&#20256;&#32479;&#26041;&#27861;Max Queue-Length (M-QL)&#65307;(2) AttentionLight&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20449;&#21495;&#30456;&#20301;&#30456;&#20851;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30693;&#35782;&#30340;&#30456;&#20301;&#20851;&#31995;&#12290;&#23545;&#22810;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(1) M-QL&#26041;&#27861;&#20248;&#20110;&#26368;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65307;(2) &#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#65292;&#19988;&#30456;&#23545;&#20110;&#19987;&#23478;&#35774;&#35745;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) techniques for traffic signal control (TSC) have gained increasing popularity in recent years. However, most existing RL-based TSC methods tend to focus primarily on the RL model structure while neglecting the significance of proper traffic state representation. Furthermore, some RL-based methods heavily rely on expert-designed traffic signal phase competition. In this paper, we present a novel approach to TSC that utilizes queue length as an efficient state representation. We propose two new methods: (1) Max Queue-Length (M-QL), an optimization-based traditional method designed based on the property of queue length; and (2) AttentionLight, an RL model that employs the self-attention mechanism to capture the signal phase correlation without requiring human knowledge of phase relationships. Comprehensive experiments on multiple real-world datasets demonstrate the effectiveness of our approach: (1) the M-QL method outperforms the latest RL-based methods; (2) A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#32422;&#26463;&#32534;&#31243;&#21644;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;&#22312;&#20248;&#21270;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#19982;&#32500;&#25252;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#22312;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#30340;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#19978;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2109.07212</link><description>&lt;p&gt;
&#20248;&#21270;&#21253;&#25324;&#32500;&#25252;&#22312;&#20869;&#30340;&#26426;&#36710;&#32534;&#32452;&#35745;&#21010;&#30340;&#32422;&#26463;&#32534;&#31243;&#19982;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimising Rolling Stock Planning including Maintenance with Constraint Programming and Quantum Annealing. (arXiv:2109.07212v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#32422;&#26463;&#32534;&#31243;&#21644;&#37327;&#23376;&#36864;&#28779;&#26041;&#27861;&#22312;&#20248;&#21270;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#19982;&#32500;&#25252;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#22312;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#30340;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#19978;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;(CP)&#21644;&#37327;&#23376;&#36864;&#28779;(QA)&#26041;&#27861;&#26469;&#20248;&#21270;&#32771;&#34385;&#24517;&#35201;&#32500;&#25252;&#20219;&#21153;&#30340;&#26426;&#36710;&#32534;&#32452;&#20998;&#37197;&#12290;&#22312;CP&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;AllDifferent&#32422;&#26463;&#12289;Element&#32422;&#26463;&#30340;&#25193;&#23637;&#20197;&#21450;&#36923;&#36753;&#34164;&#21547;&#31561;&#26469;&#24314;&#27169;&#38382;&#39064;&#12290;&#23545;&#20110;QA&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;(QUBO)&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24503;&#22269;&#38081;&#36335;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;D-Wave&#30340;&#30495;&#23454;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;QA&#26041;&#27861;&#12290;&#32463;&#20856;&#35745;&#31639;&#26426;&#29992;&#20110;&#35780;&#20272;CP&#26041;&#27861;&#20197;&#21450;QUBO&#27169;&#22411;&#20013;&#30340;&#31105;&#24524;&#25628;&#32034;&#12290;&#22312;&#24403;&#21069;&#29289;&#29702;&#37327;&#23376;&#36864;&#28779;&#26426;&#22120;&#30340;&#24320;&#21457;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#26041;&#27861;&#24448;&#24448;&#20135;&#29983;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and compare Constraint Programming (CP) and Quantum Annealing (QA) approaches for rolling stock assignment optimisation considering necessary maintenance tasks. In the CP approach, we model the problem with an Alldifferent constraint, extensions of the Element constraint, and logical implications, among others. For the QA approach, we develop a quadratic unconstrained binary optimisation (QUBO) model. For evaluation, we use data sets based on real data from Deutsche Bahn and run the QA approach on real quantum computers from D-Wave. Classical computers are used to evaluate the CP approach as well as tabu search for the QUBO model. At the current development stage of the physical quantum annealers, we find that both approaches tend to produce comparable results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#35813;&#31574;&#30053;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31867;&#20284;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2008.11852</link><description>&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#39640;&#36895;&#20844;&#36335;&#20915;&#31574;&#65306;&#36830;&#32493;&#21160;&#20316;&#35270;&#35282;&#19979;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision-making for Autonomous Vehicles on Highway: Deep Reinforcement Learning with Continuous Action Horizon. (arXiv:2008.11852v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#20027;&#36710;&#36742;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#35813;&#31574;&#30053;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#31867;&#20284;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#22312;&#32447;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#25551;&#36848;&#20102;&#19968;&#31995;&#21015;&#30340;&#34892;&#39542;&#21160;&#20316;&#26469;&#23454;&#29616;&#29305;&#23450;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#39640;&#36895;&#20844;&#36335;&#19978;&#36830;&#32493;&#35270;&#35282;&#20915;&#31574;&#38382;&#39064;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#36710;&#36742;&#36816;&#21160;&#23398;&#21644;&#39640;&#36895;&#20844;&#36335;&#39550;&#39542;&#22330;&#26223;&#12290;&#33258;&#21160;&#36710;&#36742;&#30340;&#36816;&#34892;&#30446;&#26631;&#26159;&#20197;&#39640;&#25928;&#19988;&#24179;&#31283;&#30340;&#31574;&#30053;&#25191;&#34892;&#32780;&#19981;&#21457;&#29983;&#30896;&#25758;&#12290;&#28982;&#21518;&#20171;&#32461;&#20102;&#21517;&#20026;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#35757;&#32451;&#25928;&#29575;&#20302;&#21644;&#26679;&#26412;&#25928;&#29575;&#20302;&#30340;&#25361;&#25112;&#65292;&#36825;&#20010;&#24212;&#29992;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20248;&#31168;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#20174;&#26368;&#20248;&#24615;&#12289;&#23398;&#20064;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#31561;&#22810;&#20010;&#35282;&#24230;&#23545;&#22522;&#20110;PPO-DRL&#30340;&#20915;&#31574;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31867;&#20284;&#30340;&#39550;&#39542;&#22330;&#26223;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#32447;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making strategy for autonomous vehicles de-scribes a sequence of driving maneuvers to achieve a certain navigational mission. This paper utilizes the deep reinforcement learning (DRL) method to address the continuous-horizon decision-making problem on the highway. First, the vehicle kinematics and driving scenario on the freeway are introduced. The running objective of the ego automated vehicle is to execute an efficient and smooth policy without collision. Then, the particular algorithm named proximal policy optimization (PPO)-enhanced DRL is illustrated. To overcome the challenges in tardy training efficiency and sample inefficiency, this applied algorithm could realize high learning efficiency and excellent control performance. Finally, the PPO-DRL-based decision-making strategy is estimated from multiple perspectives, including the optimality, learning efficiency, and adaptability. Its potential for online application is discussed by applying it to similar driving scenario
&lt;/p&gt;</description></item></channel></rss>