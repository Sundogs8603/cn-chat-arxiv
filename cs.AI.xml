<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.18301</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#19982;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Motion Planning for Autonomous Vehicles with Joint Optimization. (arXiv:2310.18301v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#36710;&#36742;&#30340;&#34892;&#21160;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#21040;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#26679;&#30340;&#20132;&#20114;&#29615;&#22659;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#38656;&#35201;&#32771;&#34385;&#33258;&#36523;&#24847;&#22270;&#34892;&#21160;&#23545;&#21608;&#22260;&#36710;&#36742;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#30456;&#20851;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#25903;&#25345;&#20197;&#33258;&#36523;&#26465;&#20214;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#22312;&#19979;&#28216;&#35268;&#21010;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38480;&#21046;&#20102;&#35268;&#21010;&#22120;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#12290;&#23613;&#31649;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#33021;&#22815;&#29983;&#25104;&#31934;&#32454;&#30340;&#39640;&#36136;&#37327;&#36816;&#21160;&#36335;&#24452;&#65292;&#20294;&#22522;&#20110;&#26799;&#24230;&#30340;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#30001;&#20110;&#20854;&#36845;&#20195;&#24615;&#36136;&#21644;&#23545;&#26799;&#24230;&#30340;&#38656;&#27714;&#65292;&#24456;&#38590;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#65288;IJP&#65289;&#65292;&#23558;MPC&#19982;
&lt;/p&gt;
&lt;p&gt;
In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with 
&lt;/p&gt;</description></item><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.15386</link><description>&lt;p&gt;
&#20462;&#27491;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Course Correcting Koopman Representations. (arXiv:2310.15386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#34920;&#31034;&#26088;&#22312;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#31616;&#21270;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#24314;&#27169;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#19981;&#21516;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#25512;&#29702;&#26102;&#38388;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#38271;&#26399;&#21160;&#24577;&#30340;&#20934;&#30830;&#25429;&#25417;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13102</link><description>&lt;p&gt;
&#31890;&#23376;&#24341;&#23548;&#65306;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#22810;&#26679;&#24615;&#37319;&#26679;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#25104;&#21151;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21152;&#24555;&#20854;&#37319;&#26679;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#21462;&#22810;&#26679;&#24615;&#26679;&#26412;&#65292;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#36825;&#20250;&#36896;&#25104;&#19982;&#37319;&#26679;&#26102;&#38388;&#26080;&#20851;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#22788;&#29702;&#20102;&#22914;&#20309;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31890;&#23376;&#24341;&#23548;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#37319;&#26679;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#30340;&#32852;&#21512;&#31890;&#23376;&#26102;&#21464;&#20301;&#21183;&#24378;&#21046;&#23454;&#29616;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31890;&#23376;&#24341;&#23548;&#20135;&#29983;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#21450;&#23427;&#23545;&#20301;&#21183;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#19982;&#20854;&#20182;&#23398;&#31185;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#22810;&#26679;&#24615;&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#65292;&#24182;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#38477;&#20302;&#20102;&#24179;&#22343;13%&#30340;&#20808;&#36827;&#25216;&#26415;&#20013;&#20540;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.13007</link><description>&lt;p&gt;
XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#25209;&#21028;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#20856;&#22411;&#35770;&#36848;&#65292;&#20197;&#35299;&#24320;&#36825;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#22810;&#32500;&#20851;&#31995;&#12290;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#38543;&#21518;&#30340;&#23450;&#24615;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;175&#31687;&#35770;&#25991;&#20013;&#35782;&#21035;&#20986;&#20851;&#20110;XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#19971;&#20010;&#20856;&#22411;&#35770;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#20110;&#36825;&#20123;&#35770;&#26029;&#30340;&#37325;&#35201;&#35686;&#21578;&#65292;&#24182;&#20026;&#26410;&#26469;&#22260;&#32469;XAI&#22312;&#29305;&#23450;&#20844;&#24179;&#29702;&#24819;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#20837;&#28857;&#12290;&#34429;&#28982;&#25991;&#29486;&#36890;&#24120;&#35748;&#20026;XAI&#26159;&#23454;&#29616;&#22810;&#20010;&#20844;&#24179;&#29702;&#24819;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#29702;&#24819;&#19982;XAI&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#40723;&#21169;&#23558;XAI&#35270;&#20026;&#24212;&#23545;&#31639;&#27861;&#20844;&#24179;&#36825;&#19968;&#22810;&#32500;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#20247;&#22810;&#24037;&#20855;&#20043;&#19968;&#65292;&#24182;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;&#21738;&#31181;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#21738;&#20123;&#20154;&#35299;&#20915;&#21738;&#20123;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.05052</link><description>&lt;p&gt;
&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#23398;&#20064;&#32454;&#32990;&#20869;&#21644;&#32454;&#32990;&#38388;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#19981;&#21516;&#26465;&#20214;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#22312;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#30340;&#21516;&#26102;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#23545;&#30005;&#27744;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20381;&#36182;&#20110;&#29305;&#23450;&#30446;&#26631;&#30005;&#27744;&#30340;&#26089;&#26399;&#30005;&#20449;&#21495;&#26469;&#39044;&#27979;&#23427;&#20204;&#30340;&#23551;&#21629;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#19981;&#36275;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#29305;&#23450;&#32769;&#21270;&#26465;&#20214;&#24320;&#21457;&#30340;&#65292;&#36825;&#19981;&#20165;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27169;&#22411;&#33021;&#21147;&#65292;&#32780;&#19988;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#36864;&#21270;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20854;&#20182;&#26465;&#20214;&#19979;&#21487;&#29992;&#30340;&#20016;&#23500;&#21382;&#21490;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26126;&#30830;&#25429;&#25417;&#30446;&#26631;&#30005;&#27744;&#21644;&#21442;&#32771;&#30005;&#27744;&#20043;&#38388;&#30340;&#30005;&#20449;&#21495;&#24046;&#24322;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#26448;&#26009;&#21644;&#32769;&#21270;&#26465;&#20214;&#22914;&#20309;&#65292;&#26469;&#39044;&#27979;&#30446;&#26631;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;&#36890;&#36807;&#36825;&#31181;&#32454;&#32990;&#38388;&#24046;&#24322;&#65292;&#25105;&#20204;&#19981;&#20165;&#25193;&#23637;&#20102;&#29305;&#24449;&#31354;&#38388;&#65292;&#36824;&#20026;&#36890;&#29992;&#30340;&#30005;&#27744;&#23551;&#21629;&#39044;&#27979;&#26694;&#26550;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#31934;&#30830;&#39044;&#27979;&#30005;&#27744;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05028</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#36827;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#35777;&#26126;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#21363;&#20351;&#22312;&#38646;-shot&#35774;&#23450;&#19979;&#65292;&#19968;&#30452;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#35760;&#25110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#33258;&#21160;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23558;LLMs&#65292;&#22914;ChatGPT&#65292;&#20316;&#20026;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#22120;&#30340;&#30740;&#31350;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;RE&#25552;&#31034;&#30340;&#32570;&#28857;&#65292;&#24182;&#23581;&#35797;&#23558;&#26368;&#36817;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;CoT&#65292;&#32435;&#20837;&#20854;&#20013;&#20197;&#25552;&#39640;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24635;&#32467;&#21644;&#25552;&#38382;(\textsc{SumAsk})&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#36882;&#24402;&#20351;&#29992;LLMs&#23558;RE&#36755;&#20837;&#36716;&#25442;&#20026;&#26377;&#25928;&#30340;&#38382;&#31572;(QA)&#26684;&#24335;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#35774;&#32622;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;LLMs&#22312;&#38646;-shot&#20851;&#31995;&#25277;&#21462;&#19978;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26377;&#20197;&#19979;&#30340;followi
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.17169</link><description>&lt;p&gt;
&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;GPT&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#22312;&#32597;&#35265;&#30142;&#30149;&#24739;&#32773;&#30340;&#35786;&#26029;&#20197;&#21450;&#26500;&#24314;&#21327;&#35843;&#25252;&#29702;&#35745;&#21010;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#35813;&#36807;&#31243;&#20381;&#36182;&#20110;&#20351;&#29992;&#26412;&#20307;&#27010;&#24565;&#23545;&#24739;&#32773;&#26723;&#26696;&#36827;&#34892;&#24314;&#27169;&#21644;&#25972;&#29702;&#65292;&#36890;&#24120;&#20351;&#29992;&#20154;&#31867;&#34920;&#22411;&#26412;&#20307;&#36827;&#34892;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#25903;&#25345;&#36825;&#39033;&#34920;&#22411;&#27010;&#24565;&#35782;&#21035;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20020;&#24202;&#28145;&#24230;&#34920;&#22411;&#23398;&#20013;&#30340;&#24615;&#33021;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#30740;&#31350;&#30340;&#23454;&#39564;&#35774;&#32622;&#21253;&#25324;&#19971;&#20010;&#19981;&#21516;&#29305;&#24322;&#24615;&#32423;&#21035;&#30340;&#25552;&#31034;&#12289;&#20004;&#20010;GPT&#27169;&#22411;&#65288;gpt-3.5&#21644;gpt-4.0&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#34920;&#22411;&#35782;&#21035;&#40644;&#37329;&#26631;&#20934;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30446;&#21069;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#20339;&#36816;&#34892;&#32467;&#26524;&#37319;&#29992;&#20102;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#31579;&#36873;&#31995;&#32479;&#24615;&#23457;&#26597;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#30340;&#26597;&#35810;&#26469;&#28304;&#65292;&#22914;&#29992;&#20110;&#26816;&#32034;&#25991;&#26723;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31579;&#36873;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#25490;&#21517;&#37325;&#35201;&#25991;&#26723;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05238</link><description>&lt;p&gt;
&#20026;&#26356;&#26377;&#25928;&#30340;&#31995;&#32479;&#24615;&#23457;&#26597;&#31579;&#36873;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation. (arXiv:2309.05238v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#31579;&#36873;&#31995;&#32479;&#24615;&#23457;&#26597;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#30340;&#26597;&#35810;&#26469;&#28304;&#65292;&#22914;&#29992;&#20110;&#26816;&#32034;&#25991;&#26723;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31579;&#36873;&#36807;&#31243;&#20013;&#26356;&#20934;&#30830;&#22320;&#25490;&#21517;&#37325;&#35201;&#25991;&#26723;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#24615;&#23457;&#26597;&#20013;&#30340;&#31579;&#36873;&#20248;&#20808;&#32423;&#30446;&#26631;&#26159;&#36890;&#36807;&#22797;&#26434;&#30340;&#24067;&#23572;&#26597;&#35810;&#23545;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#38598;&#36827;&#34892;&#25490;&#21517;&#12290;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;&#25991;&#26723;&#21487;&#20197;&#30830;&#20445;&#21518;&#32493;&#23457;&#26597;&#27493;&#39588;&#33021;&#22815;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#23457;&#26597;&#30340;&#26368;&#32456;&#26631;&#39064;&#20316;&#20026;&#26597;&#35810;&#65292;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#25490;&#24207;&#22120;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;&#26368;&#32456;&#26631;&#39064;&#21482;&#22312;&#23457;&#26597;&#36807;&#31243;&#32467;&#26463;&#26102;&#24418;&#25104;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;ex post facto&#30340;&#20449;&#24687;&#12290;&#22312;&#31579;&#36873;&#30340;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#31895;&#30053;&#30340;&#24037;&#20316;&#26631;&#39064;&#21487;&#29992;&#65292;&#20351;&#29992;BERT-based&#25490;&#24207;&#22120;&#26102;&#25928;&#26524;&#26126;&#26174;&#19981;&#22914;&#26368;&#32456;&#26631;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;&#31579;&#36873;&#20248;&#20808;&#32423;&#30340;&#26597;&#35810;&#30340;&#26367;&#20195;&#26469;&#28304;&#65292;&#20363;&#22914;&#29992;&#20110;&#26816;&#32034;&#24453;&#31579;&#36873;&#25991;&#26723;&#30340;&#24067;&#23572;&#26597;&#35810;&#65292;&#20197;&#21450;&#30001;&#22522;&#20110;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Alpaca&#65289;&#29983;&#25104;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#19981;&#20165;&#20165;&#26159;
&lt;/p&gt;
&lt;p&gt;
Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12319</link><description>&lt;p&gt;
RemovalNet: DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;DNNs&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;DNN&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#20854;&#30693;&#35782;&#20135;&#26435;&#36890;&#36807;&#25152;&#26377;&#26435;&#39564;&#35777;&#25216;&#26415;&#65288;&#22914;DNN&#25351;&#32441;&#65289;&#24471;&#21040;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#21450;&#20854;&#28508;&#22312;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;DNN&#27169;&#22411;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#21487;&#20197;&#20998;&#20026;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#21644;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;min-max&#21452;&#23618;&#20248;&#21270;&#30340;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#8212;&#8212;RemovalNet&#65292;&#20197;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#19979;&#23618;&#20248;&#21270;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19978;&#23618;&#20248;&#21270;&#21017;&#22312;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance.
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.07871</link><description>&lt;p&gt;
&#31038;&#20250;AI&#23398;&#26657;&#65306;&#20174;&#21457;&#23637;&#24515;&#29702;&#23398;&#21040;&#20154;&#24037;&#31038;&#20250;&#25991;&#21270;&#20195;&#29702;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#21457;&#23637;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#20351;&#20195;&#29702;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#24037;&#20855;&#20197;&#20415;&#20110;&#36827;&#34892;&#30456;&#20851;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#38271;&#26399;&#20197;&#26469;&#24050;&#32463;&#30830;&#31435;&#20102;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#22312;&#20154;&#31867;&#26234;&#21147;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#20837;&#12289;&#21442;&#19982;&#21644;&#20174;&#20154;&#31867;&#25991;&#21270;&#20013;&#21463;&#30410;&#12290;&#31038;&#20250;&#20132;&#20114;&#20195;&#29702;&#30340;AI&#30740;&#31350;&#22823;&#22810;&#20851;&#27880;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25991;&#21270;&#30340;&#20986;&#29616;&#65288;&#36890;&#24120;&#27809;&#26377;&#24378;&#28872;&#30340;&#21457;&#23637;&#24515;&#29702;&#23398;&#22522;&#30784;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;AI&#30740;&#31350;&#24212;&#35813;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#65292;&#24182;&#30740;&#31350;&#33021;&#22815;&#36827;&#20837;&#25991;&#21270;&#30340;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Michael Tomasello&#21644;Jerome Bruner&#30340;&#29702;&#35770;&#65292;&#20171;&#32461;&#20102;&#20182;&#20204;&#30340;&#19968;&#20123;&#27010;&#24565;&#65292;&#24182;&#27010;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#21644;&#31038;&#20250;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;AI&#23398;&#26657;&#8212;&#8212;&#19968;&#20010;&#21253;&#25324;&#23450;&#21046;&#21442;&#25968;&#21270;&#29615;&#22659;&#30340;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;RL&#20195;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27492;&#31867;&#23454;&#39564;&#30340;&#31034;&#20363;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21560;&#24341;AI&#31038;&#21306;&#22260;&#32469;&#36825;&#20123;&#27010;&#24565;&#36827;&#34892;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have long-established the importance of socio-cognitive abilities in human intelligence. These abilities enable us to enter, participate and benefit from human culture. AI research on social interactive agents mostly concerns the emergence of culture in a multi-agent setting (often without a strong grounding in developmental psychology). We argue that AI research should be informed by psychology and study socio-cognitive abilities enabling to enter a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner to introduce some of their concepts to AI and outline key concepts and socio-cognitive abilities. We present The SocialAI school - a tool including a customizable parameterized uite of procedurally generated environments, which simplifies conducting experiments regarding those concepts. We show examples of such experiments with RL agents and Large Language Models. The main motivation of this work is to engage the AI community around the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.13686</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#65306; &#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#32508;&#21512;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#22810;&#26041;&#38754;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21518;&#26524;&#65292;&#21253;&#25324;&#38750;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12289;&#27495;&#35270;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#12289;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#20197;&#21450;&#32463;&#27982;&#23454;&#21147;&#30340;&#38598;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#20026;&#8220;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#8221;&#30340;&#29702;&#24565;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#25903;&#25345;&#12290;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65288;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#36825;&#20123;&#26631;&#20934;&#21644;&#25351;&#26631;&#22522;&#20110;&#25209;&#21028;&#24615;&#23457;&#26597;&#21644;&#19987;&#23478;&#30740;&#35752;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25972;&#20307;&#24615;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#26694;&#26550;&#65292;&#20026;AI&#31995;&#32479;&#30340;&#21518;&#32493;&#21457;&#23637;&#21644;&#35780;&#20272;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.10841</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25903;&#25345;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#21442;&#32771;&#26550;&#26500;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#21442;&#32771;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#23454;&#29616;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#24182;&#33021;&#26681;&#25454;&#38656;&#27714;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#26159;&#19968;&#20010;&#36866;&#29992;&#33539;&#22260;&#24191;&#27867;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#21442;&#32771;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23558;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#30340;&#12289;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#21644;&#29992;&#25143;&#25511;&#21046;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#25112;&#30053;&#24615;&#22320;&#37319;&#29992;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#26631;&#35782;&#31526;&#65288;DID&#65289;&#30340;&#36523;&#20221;&#39564;&#35777;&#31995;&#32479;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#20351;&#29992;&#20854;&#33258;&#20027; DID &#23433;&#20840;&#22320;&#35748;&#35777;&#24182;&#33719;&#24471;&#23545;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#35760;&#24405;&#22312;&#21306;&#22359;&#38142;&#19978;&#12290;&#36890;&#36807;&#25191;&#34892;&#26234;&#33021;&#21512;&#32422;&#26469;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; BCFL &#21442;&#32771;&#26550;&#26500;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38656;&#27714;&#21644;&#29992;&#20363;&#38598;&#25104;&#21508;&#31181;&#39069;&#22806;&#30340;&#20803;&#32032;&#65292;&#20351;&#20854;&#25104;&#20026;&#24191;&#27867;&#36866;&#29992;&#30340; BCFL &#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>Pick-a-Pic&#26159;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;CLIP&#35780;&#20998;&#20989;&#25968;PickScore&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#65292;&#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#27604;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.01569</link><description>&lt;p&gt;
Pick-a-Pic&#65306;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. (arXiv:2305.01569v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01569
&lt;/p&gt;
&lt;p&gt;
Pick-a-Pic&#26159;&#19968;&#20221;&#24320;&#25918;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;CLIP&#35780;&#20998;&#20989;&#25968;PickScore&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#65292;&#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#27604;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26356;&#33021;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#25991;&#26412;&#21040;&#22270;&#20687;&#29992;&#25143;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#21482;&#38480;&#20110;&#20225;&#19994;&#65292;&#20351;&#24471;&#36825;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#34987;&#20844;&#20247;&#33719;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29992;&#25143;&#21487;&#20197;&#29983;&#25104;&#22270;&#20687;&#24182;&#25351;&#23450;&#20854;&#20559;&#22909;&#12290;&#21033;&#29992;&#27492;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Pick-a-Pic&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#21644;&#30495;&#23454;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#20540;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#22522;&#20110;CLIP&#30340;&#35780;&#20998;&#20989;&#25968; PickScore&#65292;&#22312;&#39044;&#27979;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102; PickScore &#22312;&#27169;&#22411;&#35780;&#20272;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#19982;&#20854;&#20182;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30456;&#27604;&#65292;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#25490;&#21517;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992; PickScore &#35780;&#20272;&#26410;&#26469;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992; Pick-a-Pic &#25968;&#25454;&#38598;&#20316;&#20026;&#27604; MS-COCO &#26356;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807; PickScore &#25552;&#21319;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via 
&lt;/p&gt;</description></item><item><title>DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07061</link><description>&lt;p&gt;
DroidBot-GPT&#65306;&#22522;&#20110;GPT&#30340;Android UI&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07061
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DroidBot-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31867;&#20284;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#19982;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;&#32473;&#23450;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;DroidBot-GPT&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#23558;&#24212;&#29992;&#31243;&#24207;GUI&#29366;&#24577;&#20449;&#24687;&#21644;&#26234;&#33021;&#25163;&#26426;&#23631;&#24149;&#19978;&#21487;&#29992;&#30340;&#25805;&#20316;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35201;&#27714;LLM&#36873;&#25321;&#21160;&#20316;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#21463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#20316;&#20986;&#21512;&#29702;&#21160;&#20316;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;DroidBot-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;17&#20010;Android&#24212;&#29992;&#31243;&#24207;&#30340;33&#20010;&#20219;&#21153;&#12290;&#23427;&#21487;&#20197;&#25104;&#21151;&#23436;&#25104;39.39%&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24179;&#22343;&#37096;&#20998;&#23436;&#25104;&#36827;&#24230;&#32422;&#20026;66.76%&#12290;&#37492;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#26159;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;DroidBot-GPT&#22312;&#25913;&#21892;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.11744</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24863;&#30693;&#23454;&#29616;&#25163;&#25345;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Visual Dexterity: In-hand Dexterous Manipulation from Depth. (arXiv:2211.11744v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#25345;&#29289;&#20307;&#30340;&#37325;&#26032;&#23450;&#21521;&#23545;&#20110;&#25191;&#34892;&#35768;&#22810;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#38750;&#24120;&#24517;&#35201;&#65292;&#20363;&#22914;&#22312;&#24403;&#21069;&#26426;&#22120;&#20154;&#26080;&#27861;&#35302;&#21450;&#30340;&#32467;&#26500;&#19981;&#22826;&#23436;&#21892;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#37325;&#26032;&#23450;&#21521;&#31995;&#32479;&#65292;&#20551;&#35774;&#20197;&#19979;&#24773;&#20917;&#20043;&#19968;&#25110;&#22810;&#31181;&#24773;&#20917;&#21516;&#26102;&#23384;&#22312;&#65306;&#20165;&#37325;&#26032;&#23450;&#21521;&#20855;&#26377;&#31616;&#21333;&#24418;&#29366;&#30340;&#29305;&#23450;&#29289;&#20307;&#12289;&#37325;&#26032;&#23450;&#21521;&#33539;&#22260;&#26377;&#38480;&#12289;&#24930;&#36895;&#25110;&#20934;&#38745;&#24577;&#25805;&#20316;&#12289;&#20165;&#27169;&#25311;&#32467;&#26524;&#12289;&#38656;&#35201;&#19987;&#29992;&#19988;&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#22871;&#20214;&#20197;&#21450;&#20854;&#20182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20570;&#36825;&#20123;&#20551;&#35774;&#30340;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#12290;&#23427;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#26222;&#36890;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#35835;&#25968;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#36890;&#36807;&#20219;&#24847;&#26059;&#36716;&#21160;&#24577;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#19988;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#19978;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324; ...
&lt;/p&gt;
&lt;p&gt;
In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. The controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#39118;&#38505;&#21644;&#25552;&#20379;&#26410;&#26469;&#20998;&#26512;&#27169;&#26495;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#38598;&#25104;&#24230;&#30340;&#25285;&#24551;&#65292;&#24182;&#24378;&#35843;&#20102;&#22686;&#24378;&#31995;&#32479;&#33021;&#21147;&#21644;&#33258;&#27835;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#30340;&#24847;&#22806;&#25913;&#21464;&#25110;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2211.03157</link><description>&lt;p&gt;
&#26816;&#39564;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#24046;&#24322;&#39118;&#38505;&#21644;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control. (arXiv:2211.03157v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#39118;&#38505;&#21644;&#25552;&#20379;&#26410;&#26469;&#20998;&#26512;&#27169;&#26495;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;AI&#20915;&#31574;&#36879;&#26126;&#24230;&#21644;&#38598;&#25104;&#24230;&#30340;&#25285;&#24551;&#65292;&#24182;&#24378;&#35843;&#20102;&#22686;&#24378;&#31995;&#32479;&#33021;&#21147;&#21644;&#33258;&#27835;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#30340;&#24847;&#22806;&#25913;&#21464;&#25110;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;21&#19990;&#32426;&#26368;&#20855;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#26410;&#26469;AI&#33021;&#21147;&#30340;&#31243;&#24230;&#21644;&#33539;&#22260;&#20173;&#23384;&#22312;&#20851;&#38190;&#19981;&#30830;&#23450;&#24615;&#65292;&#21508;&#26041;&#23545;&#26102;&#38388;&#34920;&#21644;&#28508;&#22312;&#24433;&#21709;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#38543;&#30528;&#22269;&#23478;&#21644;&#25216;&#26415;&#20844;&#21496;&#31454;&#30456;&#36861;&#27714;&#26356;&#22797;&#26434;&#21644;&#33258;&#20027;&#30340;AI&#31995;&#32479;&#65292;&#20154;&#20204;&#25285;&#24515;&#21322;&#36879;&#26126;AI&#20915;&#31574;&#36807;&#31243;&#30340;&#38598;&#25104;&#21644;&#30417;&#30563;&#31243;&#24230;&#12290;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#20854;&#20013;&#31995;&#32479;&#23398;&#20064;&#20248;&#21270;&#30446;&#26631;&#32780;&#26080;&#38656;&#20154;&#31867;&#24110;&#21161;&#12290;&#30446;&#26631;&#21487;&#33021;&#26080;&#27861;&#23436;&#32654;&#22320;&#21046;&#23450;&#25110;&#20197;&#24847;&#22806;&#25110;&#28508;&#22312;&#26377;&#23475;&#30340;&#26041;&#24335;&#25191;&#34892;&#12290;&#38543;&#30528;&#31995;&#32479;&#22686;&#21152;&#21151;&#29575;&#21644;&#33258;&#20027;&#24615;&#65292;&#36825;&#21464;&#24471;&#26356;&#21152;&#20196;&#20154;&#25285;&#24551;&#65292;&#19968;&#20010;&#31361;&#28982;&#30340;&#33021;&#21147;&#36291;&#21319;&#21487;&#33021;&#23548;&#33268;&#26435;&#21147;&#21160;&#24577;&#24847;&#22806;&#36716;&#21464;&#29978;&#33267;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#22797;&#26434;&#31995;&#32479;&#26694;&#26550;&#26469;&#27169;&#25311;AI&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#26367;&#20195;&#26410;&#26469;&#20998;&#26512;&#30340;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is one of the most transformative technologies of the 21st century. The extent and scope of future AI capabilities remain a key uncertainty, with widespread disagreement on timelines and potential impacts. As nations and technology companies race toward greater complexity and autonomy in AI systems, there are concerns over the extent of integration and oversight of opaque AI decision processes. This is especially true in the subfield of machine learning (ML), where systems learn to optimize objectives without human assistance. Objectives can be imperfectly specified or executed in an unexpected or potentially harmful way. This becomes more concerning as systems increase in power and autonomy, where an abrupt capability jump could result in unexpected shifts in power dynamics or even catastrophic failures. This study presents a hierarchical complex systems framework to model AI risk and provide a template for alternative futures analysis. Survey data were co
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item></channel></rss>