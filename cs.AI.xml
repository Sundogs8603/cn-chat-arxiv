<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>SMATCH++&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22270;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#19977;&#20010;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#40784;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06993</link><description>&lt;p&gt;
SMATCH++: &#35821;&#20041;&#22270;&#30340;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. (arXiv:2305.06993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06993
&lt;/p&gt;
&lt;p&gt;
SMATCH++&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22270;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#19977;&#20010;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#40784;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Smatch&#24230;&#37327;&#26159;&#35780;&#20272;&#22270;&#24418;&#36317;&#31163;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20363;&#22914;&#35780;&#20272;&#35821;&#20041;&#22270;&#35299;&#26512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#19981;&#36879;&#26126;&#30340;&#39044;&#22788;&#29702;&#36873;&#25321;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#65292;&#24403;&#21069;&#30340;&#22270;&#24418;&#23545;&#40784;&#35299;&#31639;&#22120;&#27809;&#26377;&#25552;&#20379;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19978;&#30028;&#65292;&#20844;&#24179;&#30340;&#35780;&#20272;&#19981;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;Smatch&#30340;&#33258;&#36866;&#24212;&#25193;&#23637;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#32454;&#31890;&#24230;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#20998;&#25955;&#65292;&#24182;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26816;&#26597;&#65292;&#25105;&#20204;&#23558;&#24230;&#37327;&#20998;&#20026;&#19977;&#20010;&#27169;&#22359;&#65306;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#12290;&#26816;&#26597;&#27599;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#25351;&#23450;&#23427;&#30340;&#30446;&#26631;&#24182;&#35786;&#26029;&#28508;&#22312;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#35752;&#35770;&#24182;&#27979;&#35797;&#32531;&#35299;&#31574;&#30053;&#12290;&#23545;&#20110;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#23436;&#20840;&#31526;&#21512;&#20801;&#35768;&#32467;&#26500;&#19981;&#21516;&#20294;&#26377;&#25928;&#30340;&#22270;&#24418;&#27880;&#37322;&#25351;&#21335;&#12290;&#20026;&#20102;&#26356;&#23433;&#20840;&#21644;&#22686;&#24378;&#30340;&#23545;&#40784;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27969;&#24418;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Smatch metric is a popular method for evaluating graph distances, as is necessary, for instance, to assess the performance of semantic graph parsing systems. However, we observe some issues in the metric that jeopardize meaningful evaluation. E.g., opaque pre-processing choices can affect results, and current graph-alignment solvers do not provide us with upper-bounds. Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore, adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity) are spread out, and lack a unifying framework.  For better inspection, we divide the metric into three modules: pre-processing, alignment, and scoring. Examining each module, we specify its goals and diagnose potential issues, for which we discuss and test mitigation strategies. For pre-processing, we show how to fully conform to annotation guidelines that allow structurally deviating but valid graphs. For safer and enhanced alignment, we show the feasibility o
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#20803;&#24187;&#35273;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#38024;&#23545;&#23569;&#26679;&#26412;&#36328;&#27169;&#24577;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06978</link><description>&lt;p&gt;
&#20803;&#24187;&#35273;&#32773;&#65306;&#38024;&#23545;&#23569;&#26679;&#26412;&#36328;&#27169;&#24577;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation. (arXiv:2305.06978v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06978
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#20803;&#24187;&#35273;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#38024;&#23545;&#23569;&#26679;&#26412;&#36328;&#27169;&#24577;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36716;&#31227;&#21644;&#26631;&#31614;&#31232;&#32570;&#20005;&#37325;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#36890;&#36807;&#20174;&#26631;&#31614;&#20016;&#23500;&#30340;&#28304;&#39046;&#22495;&#21521;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#36716;&#31227;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#36328;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#20013;&#65292;&#20174;&#28304;&#39046;&#22495;&#25910;&#38598;&#27880;&#37322;&#20063;&#24456;&#22256;&#38590;&#65292;&#23548;&#33268;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;&#28304;&#39046;&#22495;&#29305;&#21035;&#26159;&#22312;&#21482;&#26377;&#23569;&#37327;&#28304;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#22815;&#23436;&#32654;&#12290;&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#36328;&#27169;&#24577;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#25442;&#19968;&#33268;&#20803;&#24187;&#35273;&#26694;&#26550;&#65292;&#21363;&#8220;&#20803;&#24187;&#35273;&#32773;&#8221;&#65292;&#26088;&#22312;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#24182;&#29983;&#25104;&#26377;&#29992;&#30340;&#31034;&#20363;&#20197;&#25552;&#39640;&#36328;&#27169;&#24577;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#32852;&#21512;&#35757;&#32451;&#20102;&#24187;&#35273;&#21644;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#20165;&#26377;&#23569;&#37327;&#26631;&#27880;&#26679;&#26412;&#30340;&#26032;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22823;&#33041;MRI&#21644;&#24515;&#33039;MRI&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift and label scarcity heavily limit deep learning applications to various medical image analysis tasks. Unsupervised domain adaptation (UDA) techniques have recently achieved promising cross-modality medical image segmentation by transferring knowledge from a label-rich source domain to an unlabeled target domain. However, it is also difficult to collect annotations from the source domain in many clinical applications, rendering most prior works suboptimal with the label-scarce source domain, particularly for few-shot scenarios, where only a few source labels are accessible. To achieve efficient few-shot cross-modality segmentation, we propose a novel transformation-consistent meta-hallucination framework, meta-hallucinator, with the goal of learning to diversify data distributions and generate useful examples for enhancing cross-modality performance. In our framework, hallucination and segmentation models are jointly trained with the gradient-based meta-learning strategy to 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.06972</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#26377;&#25928;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns. (arXiv:2305.06972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06972
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24050;&#32463;&#20135;&#29983;&#20102;&#21151;&#33021;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#21452;&#37325;&#29992;&#36884;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#36825;&#31181;&#27969;&#34892;&#30340;&#32593;&#32476;&#29359;&#32618;&#24418;&#24335;&#28041;&#21450;&#23558;&#30446;&#26631;&#20154;&#29289;&#35825;&#39575;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#20316;&#32773;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#22312;&#25104;&#21151;&#30340;&#38035;&#40060;&#25915;&#20987;&#30340;&#20390;&#23519;&#21644;&#20449;&#24687;&#29983;&#25104;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22312;&#36825;&#20123;&#38454;&#27573;&#26174;&#30528;&#25552;&#39640;&#32593;&#32476;&#32618;&#29359;&#30340;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#20316;&#32773;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#20026;&#36229;&#36807;600&#21517;&#33521;&#22269;&#35758;&#21592;&#21019;&#24314;&#20102;&#29420;&#29305;&#30340;&#38035;&#40060;&#37038;&#20214;&#30340;&#23454;&#35777;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#37038;&#20214;&#19981;&#20165;&#36924;&#30495;&#32780;&#19988;&#25104;&#26412;&#25928;&#30410;&#26174;&#33879;&#65292;&#27599;&#23553;&#30005;&#23376;&#37038;&#20214;&#20165;&#33457;&#36153;&#20960;&#20998;&#20043;&#19968;&#30340;&#32654;&#20998;&#21363;&#21487;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in artificial intelligence (AI), particularly in the domain of large language models (LLMs), has resulted in powerful and versatile dual-use systems. Indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. This study investigates how LLMs can be used for spear phishing, a prevalent form of cybercrime that involves manipulating targets into divulging sensitive information. I first explore LLMs' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where I find that advanced LLMs are capable of meaningfully improving cybercriminals' efficiency during these stages. Next, I conduct an empirical test by creating unique spear phishing messages for over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My findings reveal that these messages are not only realistic but also remarkably cost-effective, as each email cost only a fraction of a cent to generate. N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30528;&#30524;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#20986;&#21457;&#25552;&#20986;&#20102;&#35299;&#20915;&#20559;&#35265;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06967</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#27491;AI&#30340;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Data quality dimensions for fair AI. (arXiv:2305.06967v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#30524;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#20986;&#21457;&#25552;&#20986;&#20102;&#35299;&#20915;&#20559;&#35265;&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24182;&#38750;&#26412;&#36136;&#19978;&#20855;&#26377;&#20013;&#31435;&#24615;&#65292;&#22240;&#27492;&#20559;&#35265;&#20250;&#28183;&#36879;&#21040;&#20219;&#20309;&#31867;&#22411;&#30340;&#25216;&#26415;&#24037;&#20855;&#20013;&#12290;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20154;&#31867;&#26102;&#65292;AI&#31639;&#27861;&#20250;&#21453;&#26144;&#20986;&#28304;&#20110;&#38169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25216;&#26415;&#38169;&#35823;&#12290;&#30001;&#20110;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#21644;&#27495;&#35270;&#24615;&#30340;&#20998;&#31867;&#65292;&#24310;&#32493;&#20102;&#32467;&#26500;&#24615;&#31181;&#26063;&#20027;&#20041;&#21644;&#36793;&#32536;&#21270;&#29616;&#35937;&#65292;&#36825;&#20123;&#31995;&#32479;&#24182;&#26410;&#31995;&#32479;&#22320;&#38450;&#33539;&#20559;&#35265;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#36136;&#37327;&#32500;&#24230;&#30340;&#35282;&#24230;&#32771;&#34385;&#20102;AI&#31995;&#32479;&#20559;&#35265;&#38382;&#39064;&#65292;&#20197;&#20004;&#20010;&#36890;&#24120;&#36739;&#20026;&#22256;&#38590;&#30340;&#24773;&#22659;&#65292;&#21363;&#38750;&#20108;&#20803;&#20010;&#20307;&#30340;&#20998;&#31867;&#21644;&#36328;&#24615;&#21035;&#20010;&#20307;&#30340;&#20998;&#31867;&#20026;&#20363;&#65292;&#35828;&#26126;&#20102;&#20559;&#35265;&#32531;&#35299;&#24037;&#20855;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#30830;&#23450;&#35201;&#23454;&#26045;&#30340;&#25968;&#25454;&#36136;&#37327;&#32500;&#24230;&#20197;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#30446;&#30340;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#24314;&#35758;&#22312;&#23436;&#25972;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#21450;&#26102;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#26041;&#38754;&#32771;&#34385;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems are not intrinsically neutral and biases trickle in any type of technological tool. In particular when dealing with people, AI algorithms reflect technical errors originating with mislabeled data. As they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. In this article we consider the problem of bias in AI systems from the point of view of Information Quality dimensions. We illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. The identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. Hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.
&lt;/p&gt;</description></item><item><title>FastDiagP&#26159;&#19968;&#31181;&#24182;&#34892;&#21270;&#30340;&#30452;&#25509;&#35786;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#31181;&#39044;&#27979;&#24615;&#32534;&#31243;&#26426;&#21046;&#20197;&#25552;&#39640;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#36895;&#24230;&#65292;&#35299;&#20915;&#20102;FastDiag&#22312;&#20998;&#26512;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#26102;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06951</link><description>&lt;p&gt;
FastDiagP&#65306;&#19968;&#31181;&#24182;&#34892;&#21270;&#30452;&#25509;&#35786;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FastDiagP: An Algorithm for Parallelized Direct Diagnosis. (arXiv:2305.06951v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06951
&lt;/p&gt;
&lt;p&gt;
FastDiagP&#26159;&#19968;&#31181;&#24182;&#34892;&#21270;&#30340;&#30452;&#25509;&#35786;&#26029;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#31181;&#39044;&#27979;&#24615;&#32534;&#31243;&#26426;&#21046;&#20197;&#25552;&#39640;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#36895;&#24230;&#65292;&#35299;&#20915;&#20102;FastDiag&#22312;&#20998;&#26512;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#26102;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#30340;&#24212;&#29992;&#31243;&#24207;&#26088;&#22312;&#26597;&#25214;&#31526;&#21512;&#25152;&#26377;&#23450;&#20041;&#29992;&#25143;&#38656;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22914;&#26524;&#38656;&#27714;&#19982;&#24213;&#23618;&#32422;&#26463;&#38598;&#19981;&#19968;&#33268;&#65292;&#21017;&#24212;&#23454;&#29616;&#35745;&#31639;&#19981;&#19968;&#33268;&#32422;&#26463;&#30340;&#35786;&#26029;&#31639;&#27861;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#8220;&#25214;&#19981;&#21040;&#35299;&#20915;&#26041;&#26696;&#8221;&#30340;&#22256;&#22659;&#12290;FastDiag&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#30452;&#25509;&#35786;&#26029;&#31639;&#27861;&#65292;&#25903;&#25345;&#22312;&#26410;&#30830;&#23450;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35786;&#26029;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#30528;&#36816;&#34892;&#26102;&#24615;&#33021;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#26512;&#22797;&#26434;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FastDiagP&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#25512;&#27979;&#32534;&#31243;&#30340;&#24605;&#24819;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38598;&#25104;&#19968;&#31181;&#24182;&#34892;&#21270;&#26426;&#21046;&#26469;&#25193;&#23637;FastDiag&#65292;&#35813;&#26426;&#21046;&#39044;&#35745;&#24182;&#39044;&#35745;&#31639;FastDiag&#35831;&#27714;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#21161;&#20110;&#20026;&#19968;&#33268;&#24615;&#26816;&#26597;&#25552;&#20379;&#24555;&#36895;&#31572;&#26696;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#31639;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#26159;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FastDiagP&#30340;&#35786;&#26029;&#36816;&#34892;&#26102;&#24615;&#33021;&#27604;&#20854;&#21069;&#36523;FastDiag&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint-based applications attempt to identify a solution that meets all defined user requirements. If the requirements are inconsistent with the underlying constraint set, algorithms that compute diagnoses for inconsistent constraints should be implemented to help users resolve the "no solution could be found" dilemma. FastDiag is a typical direct diagnosis algorithm that supports diagnosis calculation without predetermining conflicts. However, this approach faces runtime performance issues, especially when analyzing complex and large-scale knowledge bases. In this paper, we propose a novel algorithm, so-called FastDiagP, which is based on the idea of speculative programming. This algorithm extends FastDiag by integrating a parallelization mechanism that anticipates and pre-calculates consistency checks requested by FastDiag. This mechanism helps to provide consistency checks with fast answers and boosts the algorithm's runtime performance. The performance improvements of our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.06934</link><description>&lt;p&gt;
&#20154;&#31867;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#65306;&#20197;IEEEXtreme&#32534;&#31243;&#31454;&#36187;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23427;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#24448;&#24448;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#20154;&#31867;&#34920;&#29616;&#20248;&#24322;&#30340;&#23454;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31435;&#30340;&#35266;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#23558;IEEExtreme&#32534;&#31243;&#25361;&#25112;&#36187;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#22797;&#26434;&#24230;&#38382;&#39064;&#30340;&#30693;&#21517;&#22269;&#38469;&#32534;&#31243;&#31454;&#36187;&#12290;&#20026;&#20102;&#36827;&#34892;&#24443;&#24213;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;102&#20010;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;IEEExtreme&#29256;&#26412;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;&#32534;&#31243;&#35821;&#35328;Python&#12289;Java&#21644;C++&#36827;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#35777;&#26126;&#19982;&#26222;&#36941;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#22312;&#32534;&#31243;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#30340;&#26576;&#20123;&#26041;&#38754;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we fou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#22312;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#23454;&#29616;&#20808;&#39564;&#30693;&#35782;&#20256;&#36882;&#20197;&#36827;&#34892;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation. (arXiv:2305.06924v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#23454;&#29616;&#20102;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#20272;&#35745;&#65292;&#38024;&#23545;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#20013;&#20808;&#39564;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#24066;&#22330;&#30340;&#25237;&#26631;&#28216;&#25103;&#20013;&#65292;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#20272;&#35745;&#26159;&#21457;&#30005;&#20844;&#21496;&#65288;GENCO&#65289;&#36827;&#34892;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#21644;&#29420;&#31435;&#31995;&#32479;&#36816;&#33829;&#21830;&#65288;ISO&#65289;&#36827;&#34892;&#24066;&#22330;&#30417;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NE&#20272;&#35745;&#26041;&#27861;&#22312;&#26032;&#20852;&#29616;&#20195;&#30005;&#21147;&#24066;&#22330;&#65288;FEM&#65289;&#20013;&#26159;&#19981;&#20934;&#30830;&#21644;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22312;&#20219;&#20309;&#29615;&#22659;&#21464;&#21270;&#20043;&#21069;&#65292;&#22914;&#36127;&#36733;&#38656;&#27714;&#21464;&#21270;&#12289;&#32593;&#32476;&#25317;&#22581;&#21644;&#24066;&#22330;&#35774;&#35745;&#30340;&#20462;&#25913;&#65292;&#25237;&#26631;&#31574;&#30053;&#30340;&#20808;&#39564;&#30693;&#35782;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#38024;&#23545;FEM&#24320;&#21457;&#20102;Bayes&#33258;&#36866;&#24212;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;BAMDP-FEM&#65289;&#65292;&#20197;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#26469;&#24314;&#27169;GENCO&#30340;&#25237;&#26631;&#31574;&#30053;&#20248;&#21270;&#12290;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65288;MAGAIL-FEM&#65289;&#65292;&#20351;GENCO&#33021;&#22815;&#21516;&#26102;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#19982;&#21464;&#21270;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#24471;&#21040;&#30340;NE&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#32435;&#20160;&#22343;&#34913;&#65288;BNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06921</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#26410;&#26469;&#30340;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#65311;&#31532;&#20108;&#37096;&#20998;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications. (arXiv:2305.06921v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26469;&#32852;&#21512;&#35774;&#35745;&#30005;&#21147;&#24066;&#22330;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#21644;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#20135;&#21697;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20026;&#20004;&#37096;&#20998;&#30340;&#35770;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#33539;&#24335;&#29702;&#35770;&#21644;&#35814;&#32454;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#25311;&#26469;&#32852;&#21512;&#30005;&#21147;&#24066;&#22330;&#35774;&#35745;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#36890;&#36807;&#38416;&#36848;&#35814;&#32454;&#30340;&#26041;&#27861;&#35774;&#35745;&#30005;&#21147;&#29616;&#36135;&#24066;&#22330;&#65288;ESM&#65289;&#12289;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#65288;ASM&#65289;&#20013;&#30340;&#20445;&#30041;&#33021;&#21147;&#20135;&#21697;&#65288;RC&#65289;&#21644;&#37329;&#34701;&#24066;&#22330;&#65288;FM&#65289;&#20013;&#30340;&#34394;&#25311;&#31454;&#26631;&#65288;VB&#65289;&#20135;&#21697;&#26469;&#36827;&#19968;&#27493;&#28436;&#31034;&#36825;&#19968;&#29702;&#35770;&#12290;&#26681;&#25454;&#31532;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#39318;&#20808;&#30830;&#23450;&#32852;&#21512;&#24066;&#22330;&#20013;&#30340;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;&#25509;&#30528;&#65292;&#24320;&#21457;&#20102;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#21644;&#19981;&#30830;&#23450;&#39118;&#38505;&#32435;&#20837;&#27169;&#22411;&#20844;&#24335;&#20013;&#12290;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#36817;&#31471;&#20248;&#21270;&#65288;MAPPO&#65289;&#31639;&#27861;&#65292;&#20316;&#20026;&#31532;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#24191;&#20041;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20123;&#24066;&#22330;&#36816;&#34892;&#32489;&#25928;&#25351;&#26631;&#65292;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#24066;&#22330;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators 
&lt;/p&gt;</description></item><item><title>AfriQA&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#38750;&#27954;&#35821;&#35328;&#25968;&#23383;&#21270;&#20869;&#23481;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06897</link><description>&lt;p&gt;
AfriQA&#65306;&#38024;&#23545;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#24320;&#25918;&#26816;&#32034;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages. (arXiv:2305.06897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06897
&lt;/p&gt;
&lt;p&gt;
AfriQA&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#38750;&#27954;&#35821;&#35328;&#25968;&#23383;&#21270;&#20869;&#23481;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#30340;&#38750;&#27954;&#35821;&#35328;&#20869;&#23481;&#36828;&#36828;&#19981;&#36275;&#65292;&#36825;&#20351;&#24471;&#38382;&#31572;&#31995;&#32479;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36328;&#35821;&#35328;&#24320;&#25918;&#26816;&#32034;&#38382;&#31572;&#65288;XOR QA&#65289;&#31995;&#32479;--&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#22312;&#20026;&#20154;&#20204;&#25552;&#20379;&#26412;&#22320;&#35821;&#35328;&#26381;&#21153;&#30340;&#21516;&#26102;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#33719;&#21462;&#31572;&#26696;&#20869;&#23481;--&#25552;&#20379;&#20102;&#19968;&#31181;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#30340;&#25163;&#27573;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AfriQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#12290;AfriQA&#21253;&#25324;10&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;12,000&#22810;&#20010;XOR QA&#31034;&#20363;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20851;&#27880;&#20132;&#21449;&#35821;&#35328;QA&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#30340;&#35821;&#35328;&#65292;&#20294;AfriQA&#20391;&#37325;&#20110;&#20132;&#21449;&#35821;&#35328;&#31572;&#26696;&#20869;&#23481;&#26159;&#21807;&#19968;&#39640;&#35206;&#30422;&#33539;&#22260;&#31572;&#26696;&#20869;&#23481;&#30340;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#38750;&#27954;&#35821;&#35328;&#26159;XOR QA&#20013;&#26368;&#37325;&#35201;&#21644;&#26368;&#29616;&#23454;&#30340;&#29992;&#20363;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems -- those that retrieve answer content from other languages while serving people in their native language -- offer a means of filling this gap. To this end, we create AfriQA, the first cross-lingual QA dataset with a focus on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African languages. While previous datasets have focused primarily on languages where cross-lingual QA augments coverage from the target language, AfriQA focuses on languages where cross-lingual answer content is the only high-coverage source of answer content. Because of this, we argue that African languages are one of the most important and realistic use cases for XOR QA. Our experiments demonstrate the poor performance of automatic translation and multilingual retri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65292;&#23545;N&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#30340;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#24182;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#25552;&#39640;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06884</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#26367;&#25442;&#21152;&#26435;&#25277;&#26679;&#36827;&#34892;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Risk-limiting Financial Audits via Weighted Sampling without Replacement. (arXiv:2305.06884v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65292;&#23545;N&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#30340;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21512;&#24182;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#25552;&#39640;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#38480;&#21046;&#36130;&#21153;&#23457;&#35745;&#65288;RLFA&#65289;&#30340;&#27010;&#24565;&#65306;&#22312;&#32473;&#23450;&#35823;&#24046;$\epsilon$&#21644;&#32622;&#20449;&#24230;$1-\delta$&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#21152;&#26435;&#25277;&#26679;&#20449;&#24515;&#24207;&#21015;&#65288;CSs&#65289;&#65292;&#23545;$N$&#20010;&#26410;&#30693;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#37325;&#35201;&#26435;&#37325;&#30340;&#24819;&#27861;&#26500;&#24314;&#27979;&#35797;&#38789;&#65292;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#26500;&#24314;&#20219;&#24847;&#25277;&#26679;&#31574;&#30053;&#30340;CSs&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#36890;&#36807;&#21512;&#24182;&#19982;&#27599;&#20010;&#39033;&#30446;&#20851;&#32852;&#30340;&#26410;&#30693;&#20540;&#30340;&#38468;&#21152;&#20449;&#24687;&#26469;&#25552;&#39640;CSs&#30340;&#36136;&#37327;&#12290;&#24403;&#38468;&#21152;&#20449;&#24687;&#36275;&#22815;&#20855;&#26377;&#39044;&#27979;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#23427;&#21487;&#20197;&#30452;&#25509;&#39537;&#21160;&#25277;&#26679;&#12290;&#23545;&#20110;&#31934;&#24230;&#26410;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#20351;&#29992;&#38468;&#21152;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26500;&#24314;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of a risk-limiting financial auditing (RLFA): given $N$ transactions, the goal is to estimate the total misstated monetary fraction~($m^*$) to a given accuracy $\epsilon$, with confidence $1-\delta$. We do this by constructing new confidence sequences (CSs) for the weighted average of $N$ unknown values, based on samples drawn without replacement according to a (randomized) weighted sampling scheme. Using the idea of importance weighting to construct test martingales, we first develop a framework to construct CSs for arbitrary sampling strategies. Next, we develop methods to improve the quality of CSs by incorporating side information about the unknown values associated with each item. We show that when the side information is sufficiently predictive, it can directly drive the sampling. Addressing the case where the accuracy is unknown a priori, we introduce a method that incorporates side information via control variates. Crucially, our construction is adaptive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#26469;&#22686;&#24378;Datalog&#25512;&#29702;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;Datalog&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#20943;&#23569;&#39069;&#22806;&#24320;&#38144;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2305.06854</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#22686;&#24378;Datalog&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Datalog Reasoning with Hypertree Decompositions. (arXiv:2305.06854v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#26469;&#22686;&#24378;Datalog&#25512;&#29702;&#25928;&#29575;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;Datalog&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#20943;&#23569;&#39069;&#22806;&#24320;&#38144;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21322;&#26420;&#32032;&#35780;&#20272;&#31574;&#30053;&#30340;Datalog&#25512;&#29702;&#20351;&#29992;&#20256;&#32479;&#30340;&#36830;&#25509;&#35745;&#21010;&#35780;&#20272;&#35268;&#21017;&#65292;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#23548;&#33268;&#20887;&#20313;&#21644;&#20302;&#25928;&#65292;&#23588;&#20854;&#26159;&#24403;&#35268;&#21017;&#24456;&#22797;&#26434;&#26102;&#12290;&#36229;&#26641;&#20998;&#35299;&#26377;&#21161;&#20110;&#30830;&#23450;&#26377;&#25928;&#30340;&#26597;&#35810;&#35745;&#21010;&#24182;&#20943;&#23569;&#26597;&#35810;&#21709;&#24212;&#20013;&#31867;&#20284;&#30340;&#20887;&#20313;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#36882;&#24402;Datalog&#31243;&#24207;&#30340;&#29289;&#21270;&#21644;&#22686;&#37327;&#25512;&#29702;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#36229;&#26641;&#20998;&#35299;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#22240;&#27492;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#24341;&#20837;&#20102;&#19981;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21033;&#29992;&#36229;&#26641;&#20998;&#35299;&#36827;&#34892;Datalog&#31243;&#24207;&#30340;&#29289;&#21270;&#21644;&#22686;&#37327;&#35780;&#20272;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#26631;&#20934;Datalog&#25512;&#29702;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#20943;&#23569;&#20998;&#35299;&#24341;&#36215;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#35268;&#21017;&#26102;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#21644;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datalog reasoning based on the semina\"ive evaluation strategy evaluates rules using traditional join plans, which often leads to redundancy and inefficiency in practice, especially when the rules are complex. Hypertree decompositions help identify efficient query plans and reduce similar redundancy in query answering. However, it is unclear how this can be applied to materialisation and incremental reasoning with recursive Datalog programs. Moreover, hypertree decompositions require additional data structures and thus introduce nonnegligible overhead in both runtime and memory consumption. In this paper, we provide algorithms that exploit hypertree decompositions for the materialisation and incremental evaluation of Datalog programs. Furthermore, we combine this approach with standard Datalog reasoning algorithms in a modular fashion so that the overhead caused by the decompositions is reduced. Our empirical evaluation shows that, when the program contains complex rules, the combined 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21463;&#25361;&#25112;&#32676;&#20307;&#38754;&#37096;&#34920;&#24773;&#21450;&#29031;&#26126;&#19981;&#21464;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06842</link><description>&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#22330;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#21463;&#25361;&#25112;&#32676;&#20307;&#30340;&#38754;&#37096;&#22806;&#35266;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network. (arXiv:2305.06842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#21463;&#25361;&#25112;&#32676;&#20307;&#38754;&#37096;&#34920;&#24773;&#21450;&#29031;&#26126;&#19981;&#21464;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20132;&#27969;&#26159;&#36890;&#36807;&#35821;&#38899;&#21644;&#38750;&#35821;&#35328;&#20449;&#21495;&#19982;&#20182;&#20154;&#20132;&#27969;&#12290;&#20154;&#30340;&#34920;&#24773;&#26159;&#22270;&#20687;&#21644;&#30417;&#25511;&#31995;&#32479;&#35760;&#24405;&#25968;&#25454;&#24211;&#20013;&#37325;&#35201;&#30340;&#29983;&#29289;&#35782;&#21035;&#23545;&#35937;&#12290;&#38754;&#37096;&#34920;&#24773;&#22312;&#29983;&#29289;&#35782;&#21035;&#26041;&#27861;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#23545;&#20110;&#21253;&#25324;&#35270;&#35273;&#23457;&#26597;&#21644;&#23433;&#20840;&#22312;&#20869;&#30340;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#12290;&#34920;&#24773;&#26159;&#19968;&#31181;&#38750;&#35821;&#35328;&#20132;&#27969;&#24418;&#24335;&#65307;&#35782;&#21035;&#23427;&#20204;&#26377;&#21161;&#20110;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;&#23454;&#29616;&#38754;&#37096;&#34920;&#24773;&#21644;&#29031;&#26126;&#19981;&#21464;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#20154;&#30340;&#38754;&#37096;&#12290;&#28982;&#21518;&#20351;&#29992;CNN&#20998;&#31867;&#22120;&#23558;&#33719;&#21462;&#30340;&#22270;&#29255;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#24773;&#24863;&#31867;&#21035;&#20013;&#12290;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#30340;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;&#20154;&#31867;&#34920;&#29616;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#19981;&#21516;&#30340;&#29031;&#26126;&#26465;&#20214;&#21487;&#33021;&#20250;&#24433;&#21709;&#25311;&#21512;&#36807;&#31243;&#24182;&#38477;&#20302;&#35782;&#21035;&#31934;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#38754;&#37096;&#20986;&#29616;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human communication is the vocal and non verbal signal to communicate with others. Human expression is a significant biometric object in picture and record databases of surveillance systems. Face appreciation has a serious role in biometric methods and is good-looking for plentiful applications, including visual scrutiny and security. Facial expressions are a form of nonverbal communication; recognizing them helps improve the human machine interaction. This paper proposes an idea for face and enlightenment invariant credit of facial expressions by the images. In order on, the person's face can be computed. Face expression is used in CNN classifier to categorize the acquired picture into different emotion categories. It is a deep, feed-forward artificial neural network. Outcome surpasses human presentation and shows poses alternate performance. Varying lighting conditions can influence the fitting process and reduce recognition precision. Results illustrate that dependable facial appear
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2305.06841</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#34913;&#37327;&#28040;&#38500;&#38382;&#31572;&#27169;&#22411;&#39044;&#27979;&#24555;&#25463;&#26041;&#24335;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06841
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#20027;&#23548;&#20102;&#22823;&#37096;&#20998;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24314;&#27169;&#20551;&#28151;&#28102;&#30340;&#25903;&#25345;&#19979;, &#22240;&#27492;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26576;&#20123;&#36825;&#20123;&#32467;&#26524;&#26159;&#30001;&#24314;&#27169;&#34394;&#20551;&#30456;&#20851;&#24615;&#23454;&#29616;&#30340;&#12290;&#20316;&#32773;&#24120;&#24120;&#36890;&#36807;&#35780;&#20272;&#21516;&#19968;&#20219;&#21153;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#23610;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#30340;&#25253;&#21578;OOD&#25910;&#30410;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#26377;&#20559;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;OOD&#27169;&#22411;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#20559;&#24046;&#29305;&#24449;&#65292;&#19982;ID&#27169;&#22411;&#30456;&#24403;&#65292;&#36827;&#32780;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#25512;&#21160;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35874;&#26519;&#21338;&#24328;&#20013;&#36830;&#32493;&#31867;&#22411;&#23545;&#20303;&#23429;&#20998;&#38548;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#36825;&#23545;&#22788;&#29702;&#38750;&#21516;&#36136;&#20154;&#32676;&#26102;&#30340;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06819</link><description>&lt;p&gt;
&#20855;&#26377;&#36830;&#32493;&#31867;&#22411;&#30340;&#35874;&#26519;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Schelling Games with Continuous Types. (arXiv:2305.06819v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35874;&#26519;&#21338;&#24328;&#20013;&#36830;&#32493;&#31867;&#22411;&#23545;&#20303;&#23429;&#20998;&#38548;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#26032;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#36825;&#23545;&#22788;&#29702;&#38750;&#21516;&#36136;&#20154;&#32676;&#26102;&#30340;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#20027;&#35201;&#22478;&#24066;&#21644;&#22478;&#21306;&#20013;&#65292;&#23621;&#27665;&#27839;&#30528;&#31181;&#26063;&#25110;&#31038;&#20250;&#32463;&#27982;&#30028;&#38480;&#24418;&#25104;&#21516;&#36136;&#21270;&#31038;&#21306;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20303;&#23429;&#20998;&#38548;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;50&#24180;&#21069;&#65292;&#35874;&#26519;&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#24335;&#30340;&#27169;&#22411;&#65292;&#20197;&#20248;&#38597;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#24335;&#35299;&#37322;&#20102;&#20303;&#23429;&#20998;&#38548;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#20351;&#29992;&#20102;&#21338;&#24328;&#35770;&#26041;&#27861;&#20998;&#26512;&#20102;&#35874;&#26519;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#30740;&#31350;&#37117;&#32771;&#34385;&#20102;&#20855;&#26377;&#32473;&#23450;&#25968;&#37327;&#30340;&#31163;&#25955;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#31867;&#22411;&#24314;&#27169;&#19981;&#21516;&#30340;&#26063;&#32676;&#12290;&#25105;&#20204;&#20391;&#37325;&#20110;&#38750;&#20998;&#31867;&#23646;&#24615;&#65288;&#20363;&#22914;&#23478;&#24237;&#25910;&#20837;&#25110;&#25919;&#27835;&#24038;&#21491;&#31435;&#22330;&#65289;&#24341;&#36215;&#30340;&#20998;&#31163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#20197;&#34920;&#31034;&#20026;&#23454;&#25968;&#30340;&#20195;&#29702;&#31867;&#22411;&#12290;&#36825;&#25171;&#24320;&#20102;&#22810;&#31181;&#21512;&#29702;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#38598;&#20013;&#22312;&#20102;&#20960;&#20010;&#33258;&#28982;&#20505;&#36873;&#32773;&#36523;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35748;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#24179;&#22343;&#31867;&#22411;&#24046;&#24322;&#25110;&#26368;&#22823;&#31867;&#22411;&#24046;&#24322;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#36830;&#32493;&#35774;&#32622;&#20013;&#65292;&#35874;&#26519;&#21407;&#22987;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#30456;&#21464;&#20063;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#33021;&#20986;&#29616;&#30340;&#26032;&#29616;&#35937;&#65292;&#35832;&#22914;&#35282;&#35299;&#30340;&#23384;&#22312;&#65292;&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#20154;&#32676;&#26102;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most major cities and urban areas, residents form homogeneous neighborhoods along ethnic or socioeconomic lines. This phenomenon is widely known as residential segregation and has been studied extensively. Fifty years ago, Schelling proposed a landmark model that explains residential segregation in an elegant agent-based way. A recent stream of papers analyzed Schelling's model using game-theoretic approaches. However, all these works considered models with a given number of discrete types modeling different ethnic groups.  We focus on segregation caused by non-categorical attributes, such as household income or position in a political left-right spectrum. For this, we consider agent types that can be represented as real numbers. This opens up a great variety of reasonable models and, as a proof of concept, we focus on several natural candidates. In particular, we consider agents that evaluate their location by the average type-difference or the maximum type-difference to their neig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.06807</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#20223;&#20154;&#31867;&#21644;&#21160;&#29289;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#23454;&#38469;&#29615;&#22659;&#20013;&#23384;&#22312;&#20854;&#20182;&#26377;&#33258;&#24049;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20250;&#36866;&#24212;&#22320;&#19982;&#33258;&#24049;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#38656;&#35201;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#20197;&#20351;&#23427;&#20204;&#30340;&#34892;&#20026;&#26356;&#26377;&#30410;&#12290;&#20449;&#24687;&#35774;&#35745;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#19968;&#32452;RL&#20195;&#29702;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.06784</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#28040;&#36153;&#32773;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;AFL&#65289;&#22240;&#36890;&#36807;&#32463;&#27982;&#25163;&#27573;&#28608;&#21169;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;FL&#32780;&#21463;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#22312;AFL&#24066;&#22330;&#19978;&#20165;&#23384;&#22312;&#19968;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21644;&#22810;&#20010;&#25968;&#25454;&#25317;&#26377;&#32773;&#65288;&#21363;&#22404;&#26029;&#24066;&#22330;&#65289;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#25317;&#26377;&#32773;&#31454;&#26631;&#21152;&#20837;&#25968;&#25454;&#28040;&#36153;&#32773;&#36827;&#34892;FL&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#30340;AFL&#24066;&#22330;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#33021;&#20250;&#31454;&#20105;&#20197;&#21560;&#24341;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;&#20182;&#20204;&#21508;&#33258;&#30340;FL&#20219;&#21153;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#23481;&#32435;&#19981;&#21516;&#24066;&#22330;&#21160;&#24577;&#30340;&#21508;&#31181;&#33719;&#32988;&#20989;&#25968;&#30340;&#25928;&#29992;&#20272;&#35745;&#33021;&#21147;&#12290;&#22522;&#20110;&#20845;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#65292;&#37319;&#29992;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#32032;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;RCO&#20462;&#27491;&#30340;ELM&#31639;&#27861;&#36827;&#34892;&#30701;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.06707</link><description>&lt;p&gt;
&#22522;&#20110;RIOHTrack&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23454;&#29616;&#27813;&#38738;&#36335;&#38754;&#36710;&#36761;&#28145;&#24230;&#30340;&#30701;&#26102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack. (arXiv:2305.06707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#65292;&#37319;&#29992;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#32032;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;RCO&#20462;&#27491;&#30340;ELM&#31639;&#27861;&#36827;&#34892;&#30701;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21508;&#31181;&#36947;&#36335;&#35774;&#35745;&#25351;&#21335;&#26469;&#35828;&#65292;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#26631;&#20934;&#12290;&#26412;&#25991;&#35797;&#22270;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#20272;&#35745;&#19981;&#21516;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#21098;&#36753;&#65292;&#28201;&#24230;&#20197;&#21450;&#36127;&#36733;&#36724;&#20316;&#20026;&#20027;&#35201;&#29305;&#24449;&#12290;&#23454;&#39564;&#25968;&#25454;&#26469;&#33258;&#26045;&#21152;&#19981;&#21516;&#21407;&#27833;&#26469;&#28304;&#30340;19&#20010;&#27813;&#38738;&#36335;&#38754;&#22312;&#36890;&#24030;&#24066;&#30340;2.038&#20844;&#37324;&#38271;&#30340;&#20840;&#23610;&#23544;&#21152;&#36895;&#36335;&#38754;&#35797;&#39564;&#32447;&#36335;(RIOHTrack, Road Track Institute)&#20013;&#25152;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#22797;&#26434;&#32593;&#32476;&#26041;&#27861;&#21644;Louvain&#31639;&#27861;&#23545;&#19981;&#21516;&#30340;&#36335;&#38754;&#36710;&#36761;&#28145;&#24230;&#26500;&#24314;&#22797;&#26434;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#32467;&#26500;&#20803;&#32032;&#65292;&#24182;&#25214;&#21040;&#30456;&#20284;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#37319;&#29992;&#24102;&#27531;&#24046;&#20462;&#27491;&#20248;&#21270;(Residual Corrective Optimization, RCO)&#30340;&#26497;&#38480;&#23398;&#20064;&#26426;&#31639;&#27861;(ELM)&#29992;&#20110;&#30701;&#26102;&#38388;&#20869;&#39044;&#27979;&#27813;&#38738;&#36335;&#38754;&#30340;&#36710;&#36761;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rutting of asphalt pavements is a crucial design criterion in various pavement design guides. A good road transportation base can provide security for the transportation of oil and gas in road transportation. This study attempts to develop a robust artificial intelligence model to estimate different asphalt pavements' rutting depth clips, temperature, and load axes as primary characteristics. The experiment data were obtained from 19 asphalt pavements with different crude oil sources on a 2.038 km long full-scale field accelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou, Beijing. In addition, this paper also proposes to build complex networks with different pavement rutting depths through complex network methods and the Louvain algorithm for community detection. The most critical structural elements can be selected from different asphalt pavement rutting data, and similar structural elements can be found. An extreme learning machine algorithm with residual cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.06703</link><description>&lt;p&gt;
&#31454;&#20105;&#39118;&#38505;&#30340;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Fine-Gray: Monotonic neural networks for competing risks. (arXiv:2305.06703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#22788;&#29702;&#24739;&#32773;&#22240;&#26410;&#32463;&#21382;&#24863;&#20852;&#36259;&#20107;&#20214;&#32780;&#20986;&#29616;&#30340;&#8220;censoring&#8221;&#30340;&#26102;&#38388;&#33267;&#20107;&#20214;&#27169;&#22411;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#27492;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#24448;&#24448;&#24573;&#30053;&#20102;&#31454;&#20105;&#39118;&#38505;&#23545;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23548;&#33268;&#29983;&#23384;&#29575;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#35843;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#27599;&#31181;&#31454;&#20105;&#29983;&#23384;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#26368;&#20248;&#21270;&#12290;&#36890;&#36807;&#25928;&#26524;&#23454;&#39564;&#23545;&#27604;&#23436;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#21644;&#19977;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#29983;&#23384;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#21518;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#21307;&#30103;&#23454;&#36341;&#39118;&#38505;&#35780;&#20272;&#25351;&#26631;&#26102;&#32771;&#34385;&#31454;&#20105;&#39118;&#38505;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-to-event modelling, known as survival analysis, differs from standard regression as it addresses censoring in patients who do not experience the event of interest. Despite competitive performances in tackling this problem, machine learning methods often ignore other competing risks that preclude the event of interest. This practice biases the survival estimation. Extensions to address this challenge often rely on parametric assumptions or numerical estimations leading to sub-optimal survival approximations. This paper leverages constrained monotonic neural networks to model each competing survival distribution. This modelling choice ensures the exact likelihood maximisation at a reduced computational cost by using automatic differentiation. The effectiveness of the solution is demonstrated on one synthetic and three medical datasets. Finally, we discuss the implications of considering competing risks when developing risk scores for medical practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.06695</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#29992;&#20110;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#26469;&#25552;&#39640;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#29645;&#31232;&#29289;&#31181;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#23545;&#40784;&#65292;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35270;&#35273;&#21644;&#36951;&#20256;&#29983;&#29289;&#27979;&#23450;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#29289;&#31181;&#21644;&#20010;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#19978;&#22686;&#24378;&#23569;&#37327;&#22270;&#20687;&#25968;&#25454;&#31232;&#26377;&#31867;&#21035;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#38754;&#65292;&#35813;&#39046;&#22495;&#23578;&#26410;&#36827;&#34892;&#23581;&#35797;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#40784;&#30340;&#35270;&#35273;-&#36951;&#20256;&#25512;&#29702;&#31354;&#38388;&#65292;&#26088;&#22312;&#38544;&#24335;&#32534;&#30721;&#36328;&#22495;&#20851;&#32852;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36825;&#31181;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#23884;&#20837;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#30452;&#25509;&#36866;&#29992;&#20110;&#25552;&#39640;&#31232;&#26377;&#29289;&#31181;&#30340;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20110;32&#20010;&#29289;&#31181;&#12289;&#36229;&#36807;30,000&#20010;&#28014;&#28216;&#26377;&#23380;&#34411;&#22771;&#30340;&#26174;&#24494;&#22270;&#20687;&#24182;&#19982;&#29420;&#31435;&#30340;&#36951;&#20256;&#25968;&#25454;&#26679;&#26412;&#19968;&#36215;&#20351;&#29992;&#26469;&#23454;&#39564;&#23460;&#23637;&#29616;&#20102;&#35813;&#27010;&#24565;&#30340;&#25928;&#21147;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#20174;&#19994;&#32773;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35270;&#35273;-&#36951;&#20256;&#23545;&#40784;&#21487;&#20197;&#26174;&#33879;&#26377;&#30410;&#20110;&#20165;&#22522;&#20110;&#35270;&#35273;&#30340;&#31232;&#26377;&#29289;&#31181;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR) particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.06683</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#65306;&#24037;&#20154;&#36873;&#25321;&#21644;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20154;&#36873;&#25321;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#27880;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#19982;&#20197;&#21069;&#38024;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#30456;&#20114;&#20381;&#36182;&#24615;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#35758;&#30340;&#31639;&#27861;&#20351;&#29992;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#12290;&#35299;&#20915;&#20102;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#38459;&#30861;&#20102;&#24037;&#20154;&#36873;&#25321;&#30340;&#31163;&#32447;&#27169;&#25311;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#65288;SES&#65289;&#30340;&#21019;&#26032;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;SES&#26041;&#27861;&#19987;&#38376;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#12290;&#22312;CoNLL 2003 NER&#21644;&#20013;&#25991;OEI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20005;&#26684;&#27979;&#35797;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;&#26412;&#25991;&#36824;&#21253;&#25324;&#19968;&#20010;&#29420;&#31435;&#20110;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25196;&#22768;&#22120;&#25391;&#21160;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#22312;&#32447;&#36866;&#24212;&#65292;&#20026;&#23567;&#22411;&#31227;&#21160;&#35774;&#22791;&#30340;&#38899;&#39057;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.06640</link><description>&lt;p&gt;
&#25196;&#22768;&#22120;&#25391;&#21160;&#39044;&#27979;&#65306;&#28145;&#24230;&#20851;&#27880;&#21644;&#22312;&#32447;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Speaker Diaphragm Excursion Prediction: deep attention and online adaptation. (arXiv:2305.06640v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25196;&#22768;&#22120;&#25391;&#21160;&#30340;&#20934;&#30830;&#39044;&#27979;&#21644;&#22312;&#32447;&#36866;&#24212;&#65292;&#20026;&#23567;&#22411;&#31227;&#21160;&#35774;&#22791;&#30340;&#38899;&#39057;&#20445;&#25252;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25196;&#22768;&#22120;&#20445;&#25252;&#31639;&#27861;&#26088;&#22312;&#21033;&#29992;&#25773;&#25918;&#20449;&#21495;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#22823;&#21709;&#24230;&#65292;&#38450;&#27490;&#36807;&#24230;&#25391;&#21160;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24102;&#26377;&#23567;&#22411;&#25196;&#22768;&#22120;&#30340;&#31227;&#21160;&#30005;&#35805;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#39044;&#27979;&#38750;&#32447;&#24615;&#25391;&#21160;&#65292;&#36825;&#23545;&#20110;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23454;&#39564;&#21644;&#39044;&#22788;&#29702;&#27969;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#21453;&#39304;&#30005;&#27969;&#21644;&#30005;&#21387;&#34987;&#37319;&#26679;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#28608;&#20809;&#26469;&#27979;&#37327;&#25391;&#21160;&#20316;&#20026;&#22522;&#30784;&#30495;&#30456;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;FFTNet&#27169;&#22411;&#26469;&#25506;&#32034;&#20027;&#35201;&#30340;&#20302;&#39057;&#21644;&#20854;&#20182;&#26410;&#30693;&#30340;&#35856;&#27874;&#65292;&#24182;&#19982;&#22522;&#32447;ConvNet&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;BN&#37325;&#26032;&#20272;&#35745;&#26469;&#25506;&#32034;&#22312;&#32447;&#36866;&#24212;&#65307;&#22522;&#20110;AI&#27169;&#22411;&#25928;&#29575;&#24037;&#20855;&#21253;&#65288;AIMET&#65289;&#30340;INT8&#37327;&#21270;&#34987;&#24212;&#29992;&#20110;&#36827;&#19968;&#27493;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#25196;&#22768;&#22120;&#21644;&#19977;&#20010;&#20856;&#22411;&#37096;&#32626;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker protection algorithm is to leverage the playback signal properties to prevent over excursion while maintaining maximum loudness, especially for the mobile phone with tiny loudspeakers. This paper proposes efficient DL solutions to accurately model and predict the nonlinear excursion, which is challenging for conventional solutions. Firstly, we build the experiment and pre-processing pipeline, where the feedback current and voltage are sampled as input, and laser is employed to measure the excursion as ground truth. Secondly, one FFTNet model is proposed to explore the dominant low-frequency and other unknown harmonics, and compares to a baseline ConvNet model. In addition, BN re-estimation is designed to explore the online adaptation; and INT8 quantization based on AI Model efficiency toolkit (AIMET\footnote{AIMET is a product of Qualcomm Innovation Center, Inc.}) is applied to further reduce the complexity. The proposed algorithm is verified in two speakers and 3 typical deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06590</link><description>&lt;p&gt;
FactKG: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06590
&lt;/p&gt;
&lt;p&gt;
FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#24212;&#29992;&#21644;&#23545;&#35805;&#20195;&#29702;&#65289;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#65292;KG&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#20316;&#20026;&#30693;&#35782;&#28304;&#12290;KG&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#39564;&#35777;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;KG&#30001;&#33410;&#28857;&#21644;&#36793;&#32452;&#25104;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20351;&#24471;&#26426;&#22120;&#21487;&#20197;&#25512;&#29702;&#20986;&#19968;&#31995;&#21015;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#36825;&#20123;&#26426;&#22120;&#21487;&#35835;&#30340;&#27010;&#24565;&#22914;&#20309;&#26144;&#23556;&#21040;&#25991;&#26412;&#20013;&#30340;&#20449;&#24687;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#31038;&#21306;&#26356;&#22909;&#22320;&#21033;&#29992;KG&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;FactKG:&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#23427;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#20197;&#21450;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65306;&#21333;&#36339;&#12289;&#21512;&#21462;&#12289;&#23384;&#22312;&#12289;&#22810;&#36339;&#21644;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;FactKG&#21253;&#21547;&#21508;&#31181;&#35821;&#35328;&#27169;&#24335;&#65292;&#21253;&#25324;&#21475;&#35821;&#39118;&#26684;&#30340;&#22768;&#26126;&#21644;&#20070;&#38754;&#39118;&#26684;&#30340;&#22768;&#26126;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06588</link><description>&lt;p&gt;
HAHE: &#22522;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#26159;&#20540;&#24471;&#23581;&#35797;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#8212;&#8212;HAHE&#65292;&#21253;&#25324;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34920;&#31034;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#37319;&#29992;&#36229;&#22270;&#21452;&#37325;&#27880;&#24847;&#21147;&#23618;&#65292;&#20840;&#23616;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21487;&#20197;&#24314;&#27169;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#65307;&#32780;&#37319;&#29992;&#24322;&#36136;&#24615;&#33258;&#27880;&#24847;&#23618;&#65292;&#23616;&#37096;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21017;&#21487;&#20197;&#23398;&#20064;H-Facts&#20869;&#37096;&#30340;&#39034;&#24207;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAHE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550; FGWEA &#65292;&#23427;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#30340;&#32508;&#21512;&#27604;&#36739;&#21644;&#23545;&#40784;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#21305;&#37197;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06574</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#26694;&#26550;&#30340;&#26080;&#30417;&#30563;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment. (arXiv:2305.06574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550; FGWEA &#65292;&#23427;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#30340;&#32508;&#21512;&#27604;&#36739;&#21644;&#23545;&#40784;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#21305;&#37197;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#30456;&#24212;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FGWEA&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#65288;FGW&#65289;&#36317;&#31163;&#65292;&#20801;&#35768;&#22312;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20869;&#20840;&#38754;&#27604;&#36739;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;FGW&#25152;&#28041;&#21450;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#12290;&#23427;&#20174;&#22522;&#26412;&#35821;&#20041;&#23884;&#20837;&#21305;&#37197;&#24320;&#22987;&#65292;&#26681;&#25454;&#39640;&#32622;&#20449;&#24230;&#23454;&#20307;&#38142;&#25509;&#30340;&#36845;&#20195;&#26356;&#26032;&#65292;&#36880;&#27493;&#36817;&#20284;&#36328;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#20851;&#31995;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#26368;&#32456;&#22312;&#30693;&#35782;&#22270;&#35889;&#38388;&#36827;&#34892;&#20840;&#23616;&#32467;&#26500;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;14&#20010;&#23454;&#20307;&#23545;&#40784;&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint optimization framework. To address the computational challenges associated with optimizing FGW, we devise a three-stage progressive optimization algorithm. It starts with a basic semantic embedding matching, proceeds to approximate cross-KG structural and relational similarity matching based on iterative updates of high-confidence entity links, and ultimately culminates in a global structural comparison between KGs. We perform extensive experiments on four entity alignment datasets covering 14 dist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06555</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#19979;&#30340;&#39046;&#22495;&#22686;&#37327;&#29983;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;NLP&#27169;&#22411;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#34987;&#25253;&#36947;&#20026;LL&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#39046;&#22495;&#22686;&#37327;LL&#22330;&#26223;&#24182;&#38750;&#26131;&#20107;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#22312;&#27979;&#35797;&#38454;&#27573;&#35775;&#38382;&#20219;&#21153;&#36523;&#20221;&#65292;&#35201;&#20040;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290; Diana&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \textbf{Diana}: a \underline{d}ynam\underline{i}c \underline{a}rchitecture-based lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w
&lt;/p&gt;</description></item><item><title>GeoGLUE&#26159;&#19968;&#20010;&#26032;&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#24182;&#19988;&#32463;&#36807;&#20102;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06545</link><description>&lt;p&gt;
GeoGLUE&#65306;&#19968;&#20010;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark. (arXiv:2305.06545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06545
&lt;/p&gt;
&lt;p&gt;
GeoGLUE&#26159;&#19968;&#20010;&#26032;&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#24182;&#19988;&#32463;&#36807;&#20102;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29702;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;&#27169;&#22411;&#26159;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32773;&#20851;&#27880;&#22320;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20063;&#20174;&#26410;&#24314;&#31435;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeoGLUE&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#20174;&#20844;&#24320;&#21457;&#24067;&#30340;&#22320;&#29702;&#36164;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#21253;&#25324;&#22522;&#20110;&#22238;&#35843;&#30340;&#22320;&#29702;&#25991;&#26412;&#30456;&#20284;&#24230;&#12289;&#22522;&#20110;&#37325;&#26032;&#25490;&#24207;&#30340;&#22320;&#29702;&#25991;&#26412;&#30456;&#20284;&#24230;&#12289;&#22320;&#29702;&#20803;&#32032;&#26631;&#35760;&#12289;&#22320;&#29702;&#32452;&#21512;&#20998;&#26512;&#12289;&#22320;&#29702;&#20309;&#22788;&#20998;&#31163;&#12289;&#22320;&#29702;&#23454;&#20307;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#23454;&#39564;&#21644;&#19968;&#33324;&#24615;&#22522;&#32447;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;GeoGLUE&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a fast developing pace of geographic applications, automatable and intelligent models are essential to be designed to handle the large volume of information. However, few researchers focus on geographic natural language processing, and there has never been a benchmark to build a unified standard. In this work, we propose a GeoGraphic Language Understanding Evaluation benchmark, named GeoGLUE. We collect data from open-released geographic resources and introduce six natural language understanding tasks, including geographic textual similarity on recall, geographic textual similarity on rerank, geographic elements tagging, geographic composition analysis, geographic where what cut, and geographic entity alignment. We also pro vide evaluation experiments and analysis of general baselines, indicating the effectiveness and significance of the GeoGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#22823;&#37327;&#28857;&#26102;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#24471;&#21040;&#20102;&#19968;&#23450;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2305.06541</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#65306;&#22312;&#20309;&#26102;&#26377;&#25928;&#65311;&#26469;&#33258;&#36830;&#32493;&#32858;&#31867;&#21644;&#23494;&#24230;Cheeger-Buser&#29702;&#35770;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser. (arXiv:2305.06541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#22823;&#37327;&#28857;&#26102;&#30340;&#35889;&#32858;&#31867;&#38382;&#39064;&#24471;&#21040;&#20102;&#19968;&#23450;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#24050;&#32463;&#32463;&#21463;&#20102;&#26102;&#38388;&#30340;&#32771;&#39564;&#12290;&#23427;&#26131;&#20110;&#25551;&#36848;&#65292;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#32447;&#24615;&#20195;&#25968;&#23454;&#29616;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#22914;K-means&#21644;K-centers&#25214;&#21040;&#26356;&#22909;&#30340;&#32858;&#31867;&#12290;&#30001;Shi&#21644;Malik&#24320;&#21457;&#30340;&#20004;&#21521;&#35889;&#32858;&#31867;&#22522;&#30784;&#31639;&#27861;&#20174;&#25968;&#25454;&#20013;&#21019;&#24314;&#20960;&#20309;&#22270;&#24418;&#65292;&#24182;&#25214;&#21040;&#22270;&#24418;&#30340;&#35889;&#20999;&#21106;&#12290;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#34987;&#24314;&#27169;&#20026;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#30340;&#22823;&#37327;&#28857;&#12290;&#23545;&#20110;&#36825;&#31181;&#26041;&#24335;&#22914;&#20309;&#36827;&#34892;&#35889;&#32858;&#31867;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#29992;&#22270;Cheeger&#19981;&#31561;&#24335;&#35777;&#26126;&#20102;&#35889;&#32858;&#31867;&#30340;&#27491;&#30830;&#24615;&#65288;&#21363;&#34920;&#31034;&#22270;&#35889;&#20999;&#21106;&#36924;&#36817;&#8220;&#24402;&#19968;&#21270;&#20999;&#21106;&#8221;&#65289;&#65292;&#20294;&#36825;&#31181;&#35777;&#26126;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#23849;&#28291;&#20102;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35889;&#32858;&#31867;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#25552;&#21462;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like $k$-means and $k$-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.06530</link><description>&lt;p&gt;
&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20419;&#20351;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#26410;&#30693;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#34987;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#21830;&#19994;API&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#34920;&#29616;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#20004;&#39033;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#20998;&#31867;&#65289;&#19978;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#24615;&#33021;&#30053;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#65292;&#36825;&#20063;&#26159;&#38750;&#27954;&#35821;&#31181;&#36880;&#28176;&#27969;&#34892;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550; RSMI&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32463;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.06522</link><description>&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#29992;&#20110;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (arXiv:2305.06522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550; RSMI&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32463;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#34987;&#30693;&#36947;&#23545;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#23384;&#22312;&#33030;&#24369;&#24615;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; RSMI&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#23427;&#23558;&#38543;&#26426;&#24179;&#28369;&#65288;RS&#65289;&#19982;&#25513;&#30721;&#25512;&#29702;&#65288;MI&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;RS&#23558;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#24179;&#28369;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#32780;MI&#24378;&#21046;&#27169;&#22411;&#21033;&#29992;&#36755;&#20837;&#24207;&#21015;&#20013;&#19968;&#20010;&#25513;&#34109;&#26631;&#35760;&#30340;&#21608;&#22260;&#19978;&#19979;&#25991;&#12290;RSMI&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#39564;&#35777; RSMI &#19981;&#21516;&#38454;&#27573;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#25506;&#31350;&#20854;&#26500;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126; RSMI &#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#25512;&#21521;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#21478;&#22806;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06485</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Contextualized Plan Prediction for Embodied Task Completion. (arXiv:2305.06485v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#21478;&#22806;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35268;&#21010;&#26159;&#20256;&#32479;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32452;&#21512;&#32454;&#31890;&#24230;&#25216;&#33021;&#26469;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#26500;&#24314;&#31995;&#32479;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#34892;&#21160;&#65292;&#20197;&#22312;&#27169;&#25311;&#30340;&#20855;&#36523;&#21270;&#20195;&#29702;&#20013;&#23436;&#25104;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30452;&#25509;&#39044;&#27979;&#21487;&#36890;&#36807;&#29289;&#29702;&#26426;&#22120;&#20154;&#30452;&#25509;&#25191;&#34892;&#30340;&#20302;&#32423;&#21035;&#34892;&#21160;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36716;&#32780;&#19987;&#27880;&#20110;&#39044;&#27979;&#36739;&#39640;&#23618;&#27425;&#30340;&#35745;&#21010;&#34920;&#31034;&#65292;&#29992;&#20110;TEACh&#36825;&#26679;&#30340;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20551;&#35774;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#33719;&#24471;&#39640;&#23618;&#27425;&#35745;&#21010;&#39044;&#27979;&#30340;&#25216;&#26415;&#39044;&#35745;&#23545;&#29289;&#29702;&#26426;&#22120;&#20154;&#31995;&#32479;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#22240;&#27492;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29702;&#24819;&#35745;&#21010;&#30340;&#25191;&#34892;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#35745;&#21010;&#39044;&#27979;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task planning is an important component of traditional robotics systems enabling robots to compose fine grained skills to perform more complex tasks. Recent work building systems for translating natural language to executable actions for task completion in simulated embodied agents is focused on directly predicting low level action sequences that would be expected to be directly executable by a physical robot. In this work, we instead focus on predicting a higher level plan representation for one such embodied task completion dataset - TEACh, under the assumption that techniques for high-level plan prediction from natural language are expected to be more transferable to physical robot systems. We demonstrate that better plans can be predicted using multimodal context, and that plan prediction and plan execution modules are likely dependent on each other and hence it may not be ideal to fully decouple them. Further, we benchmark execution of oracle plans to quantify the scope for improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#26041;&#26696;&#65292;&#20854;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#24182;&#36890;&#36807;LSTM&#27169;&#22411;&#35299;&#20915;&#27495;&#20041;&#65292;&#27979;&#35797;&#34920;&#26126;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06475</link><description>&lt;p&gt;
&#19968;&#31181;&#23558;&#21360;&#24230;&#35821;&#25991;&#26412;&#32763;&#35793;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Model for Translation of Text from Indian Languages to Bharti Braille Characters. (arXiv:2305.06475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#26041;&#26696;&#65292;&#20854;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#24182;&#36890;&#36807;LSTM&#27169;&#22411;&#35299;&#20915;&#27495;&#20041;&#65292;&#27979;&#35797;&#34920;&#26126;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#23398;&#20064;&#26102;&#38754;&#20020;&#24456;&#22810;&#22256;&#38590;&#65292;&#20854;&#20013;&#19968;&#22823;&#21407;&#22240;&#26159;&#32570;&#20047;Bharti&#30450;&#25991;&#33050;&#26412;&#30340;&#21487;&#29992;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;Bharti&#30450;&#25991;&#30340;&#26041;&#26696;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#21360;&#24230;&#35821;&#25991;&#26412;&#36755;&#20837;&#21040;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20013;&#65292;&#22914;&#26377;&#20219;&#20309;&#27495;&#20041;&#65292;&#21017;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#35299;&#20915;&#12290;&#24320;&#21457;&#30340;&#27169;&#22411;&#20063;&#32463;&#36807;&#27979;&#35797;&#65292;&#24182;&#34987;&#21457;&#29616;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
People who are visually impaired face a lot of difficulties while studying. One of the major causes to this is lack of available text in Bharti Braille script. In this paper, we have suggested a scheme to convert text in major Indian languages into Bharti Braille. The system uses a hybrid approach where at first the text in Indian language is given to a rule based system and in case if there is any ambiguity then it is resolved by applying a LSTM based model. The developed model has also been tested and found to have produced near accurate results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.06453</link><description>&lt;p&gt;
&#33258;&#20027;GIS&#65306;&#19979;&#19968;&#20195;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;GIS
&lt;/p&gt;
&lt;p&gt;
Autonomous GIS: the next-generation AI-powered GIS. (arXiv:2305.06453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06453
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;GIS&#26159;&#19968;&#31181;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#20855;&#26377;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65306;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#65292;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#25512;&#29702;&#12289;&#21019;&#36896;&#24615;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24212;&#29992;&#19982;&#25506;&#32034;&#12290;&#25105;&#20204;&#37319;&#29992;LLM&#20316;&#20026;&#25512;&#29702;&#26680;&#24515;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;&#8220;&#33258;&#20027;GIS&#8221;&#30340;AI&#21160;&#21147;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#65288;GIS&#65289;&#65292;&#20197;&#33258;&#21160;&#31354;&#38388;&#25968;&#25454;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#26469;&#35299;&#20915;&#31354;&#38388;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#33258;&#20027;GIS&#23558;&#38656;&#35201;&#23454;&#29616;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#21253;&#25324;&#33258;&#21160;&#29983;&#25104;&#12289;&#33258;&#32452;&#32455;&#12289;&#33258;&#39564;&#35777;&#12289;&#33258;&#25191;&#34892;&#21644;&#33258;&#29983;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#20027;GIS&#30340;&#35774;&#35745;&#21407;&#21017;&#26469;&#23454;&#29616;&#36825;&#20116;&#20010;&#33258;&#20027;&#30446;&#26631;&#65292;&#20174;&#20449;&#24687;&#20805;&#20998;&#24615;&#12289;LLM&#33021;&#21147;&#21644;&#20195;&#29702;&#26550;&#26500;&#19977;&#20010;&#26041;&#38754;&#36827;&#34892;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#31216;&#20026;LLM-Geo &#65292;&#23427;&#22312;Python&#29615;&#22659;&#20013;&#20351;&#29992;GPT-4 API&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval. By adopting LLM as the reasoning core, we propose Autonomous GIS, an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning and coding for addressing spatial problems with automatic spatial data collection, analysis and visualization. We envision that autonomous GIS will need to achieve five autonomous goals including self-generating, self-organizing, self-verifying, self-executing, and self-growing. We introduce the design principles of autonomous GIS to achieve these five autonomous goals from the aspects of information sufficiency, LLM ability, and agent architecture. We developed a prototype system called LLM-Geo using GPT-4 API in a Python environme
&lt;/p&gt;</description></item><item><title>AI&#23545;&#20110;&#20010;&#20154;&#29983;&#27963;&#21644;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#27599;&#20010;&#20154;&#37117;&#38656;&#35201;&#33021;&#21147;&#24688;&#24403;&#22320;&#20998;&#26512;&#12289;&#35752;&#35770;&#21644;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#12289;&#26426;&#36935;&#21644;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#23398;&#20064;&#30446;&#26631;&#35838;&#31243;&#20197;&#24110;&#21161;&#23398;&#29983;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#20013;&#24515;&#27010;&#24565;&#21644;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.06450</link><description>&lt;p&gt;
&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#30340;&#24314;&#35758;&#8212;&#8212;K-12&#35745;&#31639;&#26426;&#25945;&#32946;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
What Students Can Learn About Artificial Intelligence -- Recommendations for K-12 Computing Education. (arXiv:2305.06450v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06450
&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#20110;&#20010;&#20154;&#29983;&#27963;&#21644;&#25105;&#20204;&#30340;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#27599;&#20010;&#20154;&#37117;&#38656;&#35201;&#33021;&#21147;&#24688;&#24403;&#22320;&#20998;&#26512;&#12289;&#35752;&#35770;&#21644;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#12289;&#26426;&#36935;&#21644;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#23398;&#20064;&#30446;&#26631;&#35838;&#31243;&#20197;&#24110;&#21161;&#23398;&#29983;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#20013;&#24515;&#27010;&#24565;&#21644;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#36716;&#22411;&#32972;&#26223;&#19979;&#30340;&#25216;&#26415;&#36827;&#27493;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#22522;&#30784;&#12290;&#23613;&#31649;AI&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;CS&#65289;&#20013;&#19981;&#26159;&#19968;&#20010;&#26032;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#30340;&#21457;&#23637;&#23545;&#26085;&#24120;&#29983;&#27963;&#21644;&#31038;&#20250;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#20154;&#37117;&#38656;&#35201;&#33021;&#21147;&#33021;&#22815;&#36866;&#24403;&#12289;&#24688;&#24403;&#22320;&#20998;&#26512;&#12289;&#35752;&#35770;&#21644;&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#23545;&#20010;&#20154;&#29983;&#27963;&#21644;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#12289;&#26426;&#36935;&#21644;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;CS&#35838;&#31243;&#27491;&#34987;&#25193;&#23637;&#21040;&#21253;&#25324;AI&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;AI&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;CS&#35838;&#31243;&#20013;&#65292;&#38656;&#35201;&#26126;&#30830;&#22312;AI&#32972;&#26223;&#19979;&#23398;&#29983;&#21487;&#20197;&#21644;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#12290;&#32771;&#34385;&#21040;&#36804;&#20170;&#20026;&#27490;CS&#25945;&#32946;&#30740;&#31350;&#20013;&#30340;AI&#30340;&#20013;&#24515;&#27010;&#24565;&#21644;&#21407;&#21017;&#32570;&#20047;&#36275;&#22815;&#30340;&#38416;&#36848;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#29305;&#21035;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#30446;&#26631;&#35838;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technological advances in the context of digital transformation are the basis for rapid developments in the field of artificial intelligence (AI). Although AI is not a new topic in computer science (CS), recent developments are having an immense impact on everyday life and society. In consequence, everyone needs competencies to be able to adequately and competently analyze, discuss and help shape the impact, opportunities, and limits of artificial intelligence on their personal lives and our society. As a result, an increasing number of CS curricula are being extended to include the topic of AI. However, in order to integrate AI into existing CS curricula, what students can and should learn in the context of AI needs to be clarified. This has proven to be particularly difficult, considering that so far CS education research on central concepts and principles of AI lacks sufficient elaboration. Therefore, in this paper, we present a curriculum of learning objectives that addresses digit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28508;&#22312;&#26102;&#38388;&#23548;&#33322;&#65292;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#30340;&#21160;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21160;&#20316;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06437</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#65306;&#22522;&#20110;&#28508;&#22312;&#26102;&#38388;&#23548;&#33322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Video Representation Learning via Latent Time Navigation. (arXiv:2305.06437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28508;&#22312;&#26102;&#38388;&#23548;&#33322;&#65292;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#30340;&#21160;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#21160;&#20316;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#21516;&#19968;&#35270;&#39057;&#19981;&#21516;&#26102;&#38388;&#27573;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#24378;&#21046;&#29305;&#24449;&#22312;&#26102;&#38388;&#19978;&#30340;&#25345;&#32493;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#23548;&#33268;&#19982;&#26102;&#38388;&#20851;&#31995;&#26377;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#32780;&#20351;&#8220;&#36827;&#20837;&#8221;&#21644;&#8220;&#31163;&#24320;&#8221;&#31561;&#21160;&#20316;&#26080;&#27861;&#21306;&#20998;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Latent Time Navigation&#65288;LTN&#65289;&#30340;&#26102;&#38388;&#21442;&#25968;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#30340;&#21160;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28508;&#22312;&#34920;&#31034;&#20195;&#30721;&#30340;&#23376;&#31354;&#38388;&#20869;&#21253;&#21547;&#19968;&#20010;&#27491;&#20132;&#22522;&#26469;&#34920;&#31034;&#26102;&#38388;&#21464;&#21270;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#21516;&#19968;&#35270;&#39057;&#30340;&#19981;&#21516;&#35270;&#39057;&#29255;&#27573;&#20043;&#38388;&#30340;&#34920;&#31034;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26102;&#38388;&#24863;&#30693;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;LTN&#23398;&#20064;&#35270;&#39057;&#34920;&#31034;&#21487;&#20197;&#22312;&#32454;&#31890;&#24230;&#21644;&#20197;&#20154;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;Toyota Smarthome&#25968;&#25454;&#38598;&#19978;&#65289;&#20013;&#25552;&#39640;&#21160;&#20316;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as `enter' and `leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time-parameterized contrastive learning strategy that is streamlined to capture fine-grained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06436</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#21644;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Coordination and Layout Design for Automated Warehousing. (arXiv:2305.06436v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#65292;&#21487;&#20197;&#20943;&#23569;&#25317;&#22581;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25193;&#22823;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#23558;MAPF&#31639;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#33258;&#21160;&#21270;&#20179;&#24211;&#20013;&#65292;&#20197;&#21327;&#35843;&#25968;&#30334;&#20010;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#26356;&#22909;&#30340;MAPF&#31639;&#27861;&#26469;&#25552;&#39640;&#20179;&#24211;&#30340;&#21534;&#21520;&#37327;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#26469;&#25552;&#39640;&#20854;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;MAPF&#31639;&#27861;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#20154;&#24037;&#35774;&#35745;&#24067;&#23616;&#20063;&#21487;&#33021;&#23548;&#33268;&#20179;&#24211;&#30340;&#25317;&#22581;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#20197;&#20248;&#21270;&#20179;&#24211;&#24067;&#23616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#20179;&#24211;&#24067;&#23616;(1)&#20943;&#23569;&#20102;&#20132;&#36890;&#25317;&#22581;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#65292;(2)&#36890;&#36807;&#21152;&#20493;&#26426;&#22120;&#20154;&#25968;&#37327;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;(3)&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29992;&#25143;&#25351;&#23450;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#24067;&#23616;&#12290;&#28304;&#20195;&#30721;&#20301;&#20110;&#65306;\url{https://github.com/lun}
&lt;/p&gt;
&lt;p&gt;
With the rapid progress in Multi-Agent Path Finding (MAPF), researchers have studied how MAPF algorithms can be deployed to coordinate hundreds of robots in large automated warehouses. While most works try to improve the throughput of such warehouses by developing better MAPF algorithms, we focus on improving the throughput by optimizing the warehouse layout. We show that, even with state-of-the-art MAPF algorithms, commonly used human-designed layouts can lead to congestion for warehouses with large numbers of robots and thus have limited scalability. We extend existing automatic scenario generation methods to optimize warehouse layouts. Results show that our optimized warehouse layouts (1) reduce traffic congestion and thus improve throughput, (2) improve the scalability of the automated warehouses by doubling the number of robots in some cases, and (3) are capable of generating layouts with user-specified diversity measures. We include the source code at: \url{https://github.com/lun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.06435</link><description>&lt;p&gt;
&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase transitions in the mini-batch size for sparse and dense neural networks. (arXiv:2305.06435v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23567;&#25209;&#37327;&#22823;&#23567;&#23545;&#31232;&#30095;&#21644;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#20020;&#30028;&#20540;&#22788;&#20250;&#20986;&#29616;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#38416;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#29616;&#22312;&#38750;&#24120;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#23569;&#23450;&#37327;&#35299;&#37322;&#26368;&#20339;&#23567;&#25209;&#37327;&#22823;&#23567;&#24212;&#35813;&#26159;&#22810;&#22823;&#30340;&#29702;&#35770;&#12290;&#26412;&#25991;&#23581;&#35797;&#31995;&#32479;&#22320;&#29702;&#35299;&#23567;&#25209;&#37327;&#22823;&#23567;&#22312;&#35757;&#32451;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#12290;&#22312;&#25945;&#24072;-&#23398;&#29983;&#24773;&#22659;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#25945;&#24072;&#65292;&#24182;&#32858;&#28966;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;m&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23398;&#29983;&#30340;&#27867;&#21270;&#24615;&#33021;&#24378;&#28872;&#20381;&#36182;&#20110;m&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#20020;&#30028;&#20540;mc&#22788;&#32463;&#21382;&#23574;&#38160;&#30340;&#30456;&#21464;&#65292;&#36825;&#26679;&#24403;m&lt; mc&#26102;&#65292;&#35757;&#32451;&#36807;&#31243;&#22833;&#36133;&#65292;&#32780;&#24403;m&gt; mc&#26102;&#65292;&#23398;&#29983;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;&#25110;&#24456;&#22909;&#22320;&#27867;&#21270;&#25945;&#24072;&#12290;&#30456;&#21464;&#26159;&#30001;&#32479;&#35745;&#21147;&#23398;&#39318;&#27425;&#21457;&#29616;&#30340;&#38598;&#20307;&#29616;&#35937;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#35266;&#23519;&#21040;&#12290;&#25214;&#21040;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25913;&#21464;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#30456;&#21464;&#65292;&#21487;&#20197;&#38416;&#26126;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#26412;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of mini-batches of data in training artificial neural networks is nowadays very common. Despite its broad usage, theories explaining quantitatively how large or small the optimal mini-batch size should be are missing. This work presents a systematic attempt at understanding the role of the mini-batch size in training two-layer neural networks. Working in the teacher-student scenario, with a sparse teacher, and focusing on tasks of different complexity, we quantify the effects of changing the mini-batch size $m$. We find that often the generalization performances of the student strongly depend on $m$ and may undergo sharp phase transitions at a critical value $m_c$, such that for $m&lt;m_c$ the training process fails, while for $m&gt;m_c$ the student learns perfectly or generalizes very well the teacher. Phase transitions are induced by collective phenomena firstly discovered in statistical mechanics and later observed in many fields of science. Finding a phase transition varying the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#26469;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#31038;&#21306;&#21355;&#29983;&#24037;&#20316;&#32773;&#35745;&#21010;&#65292;&#20197;&#22312;&#31038;&#21306;&#27700;&#24179;&#19978;&#26368;&#22823;&#21270;&#34880;&#31958;&#25511;&#21046;&#65292;&#32771;&#34385;&#31579;&#26597;&#12289;&#31649;&#29702;&#21644;&#24739;&#32773;&#25307;&#21215;&#20915;&#31574;&#65292;&#20197;&#21450;&#24739;&#32773;&#30340;&#21160;&#26426;&#29366;&#24577;&#21644;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06426</link><description>&lt;p&gt;
&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#20013;&#21033;&#29992;&#20248;&#21270;&#35268;&#21010;&#31038;&#21306;&#31958;&#23615;&#30149;&#25252;&#29702;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning a Community Approach to Diabetes Care in Low- and Middle-Income Countries Using Optimization. (arXiv:2305.06426v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26694;&#26550;&#26469;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#31038;&#21306;&#21355;&#29983;&#24037;&#20316;&#32773;&#35745;&#21010;&#65292;&#20197;&#22312;&#31038;&#21306;&#27700;&#24179;&#19978;&#26368;&#22823;&#21270;&#34880;&#31958;&#25511;&#21046;&#65292;&#32771;&#34385;&#31579;&#26597;&#12289;&#31649;&#29702;&#21644;&#24739;&#32773;&#25307;&#21215;&#20915;&#31574;&#65292;&#20197;&#21450;&#24739;&#32773;&#30340;&#21160;&#26426;&#29366;&#24577;&#21644;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#20581;&#24247;&#20248;&#20808;&#20107;&#39033;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#65292;&#36229;&#36807;50%&#30340;&#26089;&#36893;&#24402;&#22240;&#20110;&#39640;&#34880;&#31958;&#12290;&#22810;&#39033;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20351;&#29992;&#31038;&#21306;&#21355;&#29983;&#24037;&#20316;&#32773;&#65288;CHW&#65289;&#35745;&#21010;&#25552;&#20379;&#26089;&#26399;&#21457;&#29616;&#21644;&#31649;&#29702;&#31958;&#23615;&#30149;&#30340;&#21487;&#25215;&#21463;&#21644;&#25991;&#21270;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#25552;&#20986;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#35774;&#35745;&#21644;&#23454;&#26045;CHW&#35745;&#21010;&#65292;&#21516;&#26102;&#32771;&#34385;&#31579;&#26597;&#12289;&#31649;&#29702;&#21644;&#24739;&#32773;&#25307;&#21215;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#26469;&#30830;&#23450;&#20010;&#24615;&#21270;&#30340;CHW&#35775;&#38382;&#65292;&#20197;&#22312;&#31038;&#21306;&#27700;&#24179;&#19978;&#26368;&#22823;&#21270;&#34880;&#31958;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26126;&#30830;&#24314;&#27169;&#20102;&#31579;&#26597;&#26032;&#24739;&#32773;&#21644;&#20026;&#24050;&#32463;&#20837;&#27835;&#30103;&#30340;&#20010;&#20307;&#25552;&#20379;&#31649;&#29702;&#35775;&#38382;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#32771;&#34385;&#24739;&#32773;&#30340;&#21160;&#26426;&#29366;&#24577;&#65292;&#36825;&#24433;&#21709;&#20182;&#20204;&#20915;&#23450;&#20837;&#27835;&#30103;&#25110;&#36864;&#20986;&#30340;&#20915;&#23450;&#65292;&#22240;&#27492;&#24433;&#21709;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#20123;&#22240;&#32032;&#26469;&#23454;&#29616;&#26234;&#33021;&#20010;&#24615;&#21270;&#30340;CHW&#35775;&#38382;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes is a global health priority, especially in low- and-middle-income countries, where over 50% of premature deaths are attributed to high blood glucose. Several studies have demonstrated the feasibility of using Community Health Worker (CHW) programs to provide affordable and culturally tailored solutions for early detection and management of diabetes. Yet, scalable models to design and implement CHW programs while accounting for screening, management, and patient enrollment decisions have not been proposed. We introduce an optimization framework to determine personalized CHW visits that maximize glycemic control at a community-level. Our framework explicitly models the trade-off between screening new patients and providing management visits to individuals who are already enrolled in treatment. We account for patients' motivational states, which affect their decisions to enroll or drop out of treatment and, therefore, the effectiveness of the intervention. We incorporate these de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#23558;&#36328;&#27169;&#24577;&#31354;&#38388;&#38480;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#30340;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#26082;&#20445;&#30041;&#20102;&#35270;&#35273;&#29305;&#24449;&#65292;&#20063;&#33021;&#22815;&#20174;&#20016;&#23500;&#30340;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#20013;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.06407</link><description>&lt;p&gt;
&#32467;&#21512;&#24605;&#32771;&#21644;&#35266;&#23519;&#30340;&#22806;&#37096;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Combo of Thinking and Observing for Outside-Knowledge VQA. (arXiv:2305.06407v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#23558;&#36328;&#27169;&#24577;&#31354;&#38388;&#38480;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#30340;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#26082;&#20445;&#30041;&#20102;&#35270;&#35273;&#29305;&#24449;&#65292;&#20063;&#33021;&#22815;&#20174;&#20016;&#23500;&#30340;&#33258;&#28982;&#35821;&#35328;&#30693;&#35782;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#33719;&#21462;&#21644;&#20351;&#29992;&#24320;&#25918;&#24335;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#22806;&#37096;&#30693;&#35782;&#24341;&#20837;&#36328;&#27169;&#24577;&#31354;&#38388;&#20013;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#26356;&#24191;&#27867;&#30340;&#25991;&#26412;&#30693;&#35782;&#65292;&#32780;&#21478;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#23558;&#22270;&#20687;&#36716;&#21270;&#20026;&#25991;&#26412;&#65292;&#36827;&#19968;&#27493;&#34701;&#21512;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#25991;&#26412;&#30693;&#35782;&#65292;&#24182;&#23436;&#20840;&#25918;&#24323;&#20102;&#23545;&#35270;&#35273;&#29305;&#24449;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#30340;&#21551;&#21457;&#26159;&#25226;&#36328;&#27169;&#24577;&#31354;&#38388;&#38480;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#30340;&#21516;&#19968;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#35270;&#35273;&#29305;&#24449;&#30452;&#25509;&#22320;&#20445;&#30041;&#19979;&#26469;&#65292;&#24182;&#19988;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#20013;&#33719;&#30410;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#12289;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#31572;&#26696;&#35299;&#30721;&#22120;&#12290;&#36825;&#26679;&#30340;&#32467;&#26500;&#20801;&#35768;&#25105;&#20204;&#24341;&#20837;&#26356;&#22810;&#31867;&#22411;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#26174;&#24335;&#21644;&#38544;&#24335;&#22810;&#27169;&#24577;&#21644;&#25991;&#26412;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text that further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;LACoS-BLOOM&#65292;&#37319;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#12289;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#21644;Siamese&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.06404</link><description>&lt;p&gt;
LACoS-BLOOM&#65306;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM. (arXiv:2305.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;LACoS-BLOOM&#65292;&#37319;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#12289;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#21644;Siamese&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#26159;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#29992;&#29305;&#24449;&#65292;&#20363;&#22914;&#21477;&#23376;&#30456;&#20284;&#24615;&#12289;&#25991;&#26412;&#32858;&#31867;&#21644;&#35821;&#20041;&#25628;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;8&#20301;Siamese-BLOOM&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#20197;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings are useful features for several NLP applications, such as sentence similarity, text clustering, and semantic search. In this paper, we present a Low-rank Adaptation with a Contrastive objective on top of 8-bit Siamese-BLOOM, a multilingual large language model optimized to produce semantically meaningful word embeddings. The innovation is threefold. First, we cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification. Third, we apply a Siamese architecture on BLOOM model with a contrastive objective to ease the multi-lingual labeled data scarcity. The experiment results show the quality of learned embeddings from LACoS-BLOOM is proportional to the number of model parameters and the amount of unlabeled training data. With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1 billion parameters end-to-end on a single GPU machine with 32GB memory. Compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;"Text-To-Concept"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#36716;&#25442;&#20026;&#21487;&#19982;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#36739;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#65292;&#24182;&#20813;&#36153;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.06386</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#23545;&#40784;&#23454;&#29616;&#25991;&#26412;&#21040;&#27010;&#24565;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"Text-To-Concept"&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#36716;&#25442;&#20026;&#21487;&#19982;&#25991;&#26412;&#32534;&#30721;&#22120;&#27604;&#36739;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#65292;&#24182;&#20813;&#36153;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#21363;&#20351;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#65292;&#22270;&#20687;&#34920;&#31034;&#30340;&#26144;&#23556;&#20063;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#32447;&#24615;&#23618;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;"Text-To-Concept"&#65292;&#20854;&#20013;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#29305;&#24449;&#19982;CLIP&#31354;&#38388;&#36827;&#34892;&#32447;&#24615;&#23545;&#40784;&#65292;&#20351;&#24471;&#26469;&#33258;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#25991;&#26412;&#23884;&#20837;&#21487;&#30452;&#25509;&#19982;&#23545;&#40784;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;Text-To-Concept&#36716;&#25442;&#65292;&#21487;&#20813;&#36153;&#23558;&#22266;&#23450;&#30340;&#29616;&#25104;&#35270;&#35273;&#32534;&#30721;&#22120;&#36716;&#25442;&#20026;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;CLIP&#30340;&#31934;&#24230;&#65292;&#21363;&#20351;&#36825;&#20123;&#32534;&#30721;&#22120;&#27604;CLIP&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;CLIP&#65292;&#35757;&#32451;&#25968;&#25454;&#20165;&#21344;&#24456;&#23567;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;Text-To-Concept&#30340;&#20854;&#20182;&#30452;&#25509;&#24212;&#29992;&#65306;&#22914;&#26500;&#24314;&#19981;&#38656;&#35201;&#27010;&#24565;&#30417;&#30563;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#31867;&#27010;&#24565;&#35786;&#26029;&#20998;&#24067;&#31227;&#20301;&#65292;&#24182;&#26816;&#32034;&#28385;&#36275;&#19968;&#32452;&#22522;&#20110;&#25991;&#26412;&#30340;&#32422;&#26463;&#26465;&#20214;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#23545;&#20934;&#30830;&#23454;&#29616;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose $\textit{text-to-concept}$, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;UVLN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#25552;&#39640;&#20854;&#26131;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06358</link><description>&lt;p&gt;
&#8220;&#21487;&#35775;&#38382;&#30340;&#25351;&#20196;&#36319;&#38543;&#26426;&#22120;&#20154;&#8221;
&lt;/p&gt;
&lt;p&gt;
Accessible Instruction-Following Agent. (arXiv:2305.06358v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;UVLN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#25552;&#39640;&#20854;&#26131;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#26681;&#25454;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#20449;&#21495;&#21644;&#25351;&#20196;&#21512;&#20316;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#26426;&#22120;&#20154;&#24456;&#38590;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#20351;&#24471;&#20808;&#21069;&#30340;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#20559;&#21521;&#33521;&#35821;&#65292;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#29978;&#33267;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#26159;&#22312;&#20551;&#35774;&#29992;&#25143;&#21487;&#20197;&#35266;&#23519;&#21040;&#29615;&#22659;&#30340;&#27169;&#24335;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24182;&#25913;&#21892;&#20854;&#19981;&#21487;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UVLN (&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;), &#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#22914; GPT3) &#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can collaborate and complete tasks based on visual signals and instruction from the environment. Training such a robot is difficult especially due to the understanding of the instruction and the complicated environment. Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages. Nevertheless, the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility. In this work, we're trying to generalize the success of instruction-following agents to non-English languages with little corpus resources, and improve its intractability and accessibility. We introduce UVLN (Universal Vision-Language Navigation), a novel machine-translation instructional augmented framework for cross-lingual vision-language navigation, with a novel composition of state-of-the-art large language model (GPT3) with t
&lt;/p&gt;</description></item><item><title>CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05711</link><description>&lt;p&gt;
CodeIE: &#22823;&#22411;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05711
&lt;/p&gt;
&lt;p&gt;
CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26041;&#38754;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23558;&#20219;&#21153;&#37325;&#26500;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#20197;&#20415;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#24335;LLMs&#65288;&#22914;GPT-3&#65289;&#21487;&#20197;&#34987;&#25552;&#31034;&#35299;&#20915;&#23427;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;NL-LLMs&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#26159;&#19981;&#26131;&#30340;&#65292;&#22240;&#20026;IE&#20219;&#21153;&#30340;&#36755;&#20986;&#36890;&#24120;&#26159;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36716;&#25442;&#25104;&#32431;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20195;&#30721;&#24418;&#24335;&#32780;&#38750;&#33258;&#28982;&#35821;&#35328;&#26469;&#34920;&#36798;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#29983;&#25104;LLMs&#65288;&#22914;Codex&#65289;&#26469;&#25191;&#34892;IE&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#19982;NL-LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35774;&#35745;&#20195;&#30721;&#39118;&#26684;&#30340;&#25552;&#31034;&#21644;&#23558;&#36825;&#20123;IE&#20219;&#21153;&#26356;&#25913;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;Code-LLMs&#21487;&#20197;&#19982;&#36825;&#20123;IE&#20219;&#21153;&#24456;&#22909;&#22320;&#23545;&#40784;&#12290;&#22312;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#19968;&#30452;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2305.03963</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#22411;&#65306;Android&#24212;&#29992;&#20013;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps. (arXiv:2305.03963v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26234;&#33021;&#25163;&#26426;&#20302;&#24310;&#36831;&#21644;&#33410;&#30465;&#24102;&#23485;&#31561;&#20248;&#28857;&#25512;&#21160;&#20102;&#26234;&#33021;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#20063;&#31216;&#20026;&#20256;&#32479;&#24212;&#29992;&#65292;&#20294;&#36825;&#31181;&#25216;&#26415;&#36827;&#23637;&#20063;&#24341;&#21457;&#20102;&#35768;&#22810;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#27169;&#22411;&#31363;&#21462;&#21644;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#25915;&#20987;&#21644;&#38024;&#23545;&#35774;&#22791;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#31574;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26412;&#36523;&#65292;&#32780;&#24456;&#23569;&#20851;&#27880;&#25968;&#25454;&#22788;&#29702;&#23545;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#30693;&#35782;&#24046;&#36317;&#20984;&#26174;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#21644;&#35299;&#20915;&#19982;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36827;&#34892;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#33021;&#22815;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.03888</link><description>&lt;p&gt;
&#36890;&#36807;Sponge&#27602;&#21270;&#23545;&#35774;&#22791;&#19978;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33021;&#37327;&#24310;&#36831;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning. (arXiv:2305.03888v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#22240;&#20854;&#24320;&#21457;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#32463;&#27982;&#23454;&#24800;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#21463;&#21040;&#26377;&#38480;&#30340;&#33021;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#32422;&#26463;&#12290;&#21516;&#26102;&#65292;&#19968;&#31181;&#21517;&#20026;Sponge&#27602;&#21270;&#30340;&#25915;&#20987;&#26041;&#24335;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#24335;&#28041;&#21450;&#21040;&#25552;&#20379;&#27602;&#23475;&#26679;&#26412;&#32473;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25512;&#26029;&#26399;&#38388;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#30001;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26381;&#21153;&#22120;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Sponge&#27602;&#21270;&#25915;&#20987;&#25193;&#23637;&#21040;&#35774;&#22791;&#19978;&#30340;&#24773;&#20917;&#65292;&#20197;&#35780;&#20272;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#20197;&#27169;&#25311;&#27969;&#24335;&#21644;&#19968;&#33268;&#30340;&#25512;&#26029;&#22330;&#26223;&#65292;&#20197;&#22635;&#34917;&#35774;&#22791;&#19978;&#35774;&#32622;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#22788;&#29702;&#22120;&#21644;&#35774;&#22791;&#32593;&#32476;&#26041;&#38754;&#36827;&#34892;&#20102;&#29420;&#23478;&#23454;&#39564;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;Sponge&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#26377;&#25928;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, on-device deep learning has gained attention as a means of developing affordable deep learning applications for mobile devices. However, on-device models are constrained by limited energy and computation resources. In the mean time, a poisoning attack known as sponge poisoning has been developed.This attack involves feeding the model with poisoned examples to increase the energy consumption during inference. As previous work is focusing on server hardware accelerators, in this work, we extend the sponge poisoning attack to an on-device scenario to evaluate the vulnerability of mobile device processors. We present an on-device sponge poisoning attack pipeline to simulate the streaming and consistent inference scenario to bridge the knowledge gap in the on-device setting. Our exclusive experimental analysis with processors and on-device networks shows that sponge poisoning attacks can effectively pollute the modern processor with its built-in accelerator. We analyze the 
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.03411</link><description>&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#20449;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Trustworthiness of Autonomous Systems. (arXiv:2305.03411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03411
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#23433;&#20840;&#21644;&#19982;&#23427;&#20204;&#30340;&#20132;&#20114;&#36234;&#26469;&#36234;&#39057;&#32321;&#65292;&#20351;&#23427;&#20204;&#20449;&#24471;&#36807;&#26159;&#24517;&#35201;&#30340;&#12290;&#35780;&#20272;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#23545;&#20110;&#39564;&#35777;&#21644;&#24320;&#21457;&#31038;&#21306;&#26469;&#35828;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#25361;&#25112;&#12290;&#36825;&#23558;&#38656;&#35201;&#36866;&#24403;&#30340;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#22312;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#20869;&#23458;&#35266;&#22320;&#21644;&#27604;&#36739;&#22320;&#21028;&#26029;AS&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#25991;&#31456;&#20013;&#65292;&#32771;&#34385;&#21040;&#25991;&#29486;&#20013;&#26500;&#25104;&#36825;&#20010;&#35789;&#30340;&#30456;&#20851;&#29305;&#36136;&#65292;&#23545;&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#20013;&#30340;&#8220;&#21487;&#20449;&#24230;&#8221;&#19968;&#35789;&#36827;&#34892;&#20102;&#23457;&#26597;&#12290;&#22238;&#39038;&#20102;&#25903;&#25345;&#33258;&#20027;&#31995;&#32479;&#20445;&#35777;&#30340;&#26631;&#20934;&#21644;&#26694;&#26550;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#31038;&#21306;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Autonomous Systems (AS) become more ubiquitous in society, more responsible for our safety and our interaction with them more frequent, it is essential that they are trustworthy. Assessing the trustworthiness of AS is a mandatory challenge for the verification and development community. This will require appropriate standards and suitable metrics that may serve to objectively and comparatively judge trustworthiness of AS across the broad range of current and future applications. The meta-expression `trustworthiness' is examined in the context of AS capturing the relevant qualities that comprise this term in the literature. Recent developments in standards and frameworks that support assurance of autonomous systems are reviewed. A list of key challenges are identified for the community and we present an outline of a process that can be used as a trustworthiness assessment framework for AS.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#36710;&#36742;&#30340;&#36712;&#36857;&#29983;&#25104;&#36335;&#24452;&#65292;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#24773;&#20917;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01622</link><description>&lt;p&gt;
FlowMap&#65306;&#20351;&#29992;&#20132;&#36890;&#27969;&#29983;&#25104;&#24320;&#25918;&#31354;&#38388;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow. (arXiv:2305.01622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#36710;&#36742;&#30340;&#36712;&#36857;&#29983;&#25104;&#36335;&#24452;&#65292;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#24773;&#20917;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#29486;&#25506;&#35752;&#20102;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#20256;&#24863;&#22120;&#36755;&#20837;&#65288;&#22914;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#21644;&#30456;&#26426;&#22270;&#20687;&#65289;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24863;&#30693;&#36947;&#36335;&#32467;&#26500;&#12290;&#21033;&#29992;&#26368;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65288;&#22914;&#21464;&#21387;&#22120;&#65289;&#21644;&#40479;&#30640;&#22270;&#65288;BEV&#65289;&#34920;&#31034;&#27861;&#65292;&#36947;&#36335;&#35748;&#30693;&#31934;&#24230;&#19981;&#26029;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35748;&#30693;&#8220;&#36947;&#36335;&#8221;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#23450;&#20041;&#28165;&#26224;&#30340;&#8220;&#36947;&#36335;&#8221;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#36890;&#27969;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;FlowMap&#12290;FlowMap&#36890;&#36807;&#25193;&#23637;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;RoadMap&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is extensive literature on perceiving road structures by fusing various sensor inputs such as lidar point clouds and camera images using deep neural nets. Leveraging the latest advance of neural architects (such as transformers) and bird-eye-view (BEV) representation, the road cognition accuracy keeps improving. However, how to cognize the ``road'' for automated vehicles where there is no well-defined ``roads'' remains an open problem. For example, how to find paths inside intersections without HD maps is hard since there is neither an explicit definition for ``roads'' nor explicit features such as lane markings. The idea of this paper comes from a proverb: it becomes a way when people walk on it. Although there are no ``roads'' from sensor readings, there are ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap, a path generation framework for automated vehicles based on traffic flows. FlowMap is built by extending our previous work RoadMap, a light-weight 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SkeAttnCLR&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#19982;&#20840;&#23616;&#29305;&#24449;&#25972;&#21512;&#21040;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;&#20013;&#12290;&#20854;&#20013;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#23545;&#27604;&#37197;&#23545;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00666</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20214;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Part Aware Contrastive Learning for Self-Supervised Action Recognition. (arXiv:2305.00666v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SkeAttnCLR&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#19982;&#20840;&#23616;&#29305;&#24449;&#25972;&#21512;&#21040;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;&#20013;&#12290;&#20854;&#20013;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#23545;&#27604;&#37197;&#23545;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#19982;&#39592;&#39612;&#24207;&#21015;&#22312;&#33258;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#33879;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;SkeAttnCLR&#65292;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#21644;&#20840;&#23616;&#29305;&#24449;&#38598;&#25104;&#21040;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#20013;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#27169;&#22359;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#21387;&#21046;&#38750;&#26174;&#33879;&#37096;&#20301;&#29305;&#24449;&#21516;&#26102;&#31361;&#20986;&#26174;&#33879;&#37096;&#20301;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#26174;&#33879;&#21644;&#38750;&#26174;&#33879;&#29305;&#24449;&#30340;&#23545;&#27604;&#37197;&#23545;&#65292;&#33719;&#24471;&#20102;&#20805;&#36275;&#30340;&#23545;&#27604;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, remarkable results have been achieved in self-supervised action recognition using skeleton sequences with contrastive learning. It has been observed that the semantic distinction of human action features is often represented by local body parts, such as legs or hands, which are advantageous for skeleton-based action recognition. This paper proposes an attention-based contrastive learning framework for skeleton representation learning, called SkeAttnCLR, which integrates local similarity and global features for skeleton-based action representations. To achieve this, a multi-head attention mask module is employed to learn the soft attention mask features from the skeletons, suppressing non-salient local features while accentuating local salient features, thereby bringing similar local features closer in the feature space. Additionally, ample contrastive pairs are generated by expanding contrastive pairs based on salient and non-salient features with global features, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00304</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#26465;&#20214;&#36923;&#36753;&#20013;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20248;&#20808;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality. (arXiv:2305.00304v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#34920;&#31034;&#20013;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#19982;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32771;&#34385;&#20102;&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#31616;&#21333;&#25551;&#36848;&#36923;&#36753;&#30340;&#21152;&#26435;&#30693;&#35782;&#24211;&#65292;&#22312;&#8220;&#27010;&#24565;&#23618;&#38754;&#8221;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#19979;&#36827;&#34892;&#12290;&#35813;&#35821;&#20041;&#34987;&#29992;&#26469;&#25552;&#20379;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#30340;&#20248;&#20808;&#35299;&#37322;&#12290;&#21033;&#29992;&#27169;&#22411;&#26816;&#26597;&#21644;&#34164;&#21547;&#20851;&#31995;&#30340;&#26041;&#27861;&#39564;&#35777;MLPs&#30340;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the relationships between a multipreferential semantics for defeasible reasoning in knowledge representation and a multilayer neural network model. Weighted knowledge bases for a simple description logic with typicality are considered under a (many-valued) ``concept-wise" multipreference semantics. The semantics is used to provide a preferential interpretation of MultiLayer Perceptrons (MLPs). A model checking and an entailment based approach are exploited in the verification of conditional properties of MLPs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#36923;&#36753;&#8212;&#8212;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#23427;&#65292;&#36825;&#20010;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14323</link><description>&lt;p&gt;
&#25512;&#21160;&#21487;&#34892;&#30340;&#22810;&#35282;&#24230;&#25512;&#29702;&#36793;&#30028;&#65306;&#19968;&#31181;&#38754;&#21521;STANDPOINT EL + &#30340;&#28436;&#32462;&#28436;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+. (arXiv:2304.14323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#36923;&#36753;&#8212;&#8212;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#23427;&#65292;&#36825;&#20010;&#31639;&#27861;&#38656;&#35201;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Standpoint EL&#26159;&#27969;&#34892;&#30340;&#25551;&#36848;&#36923;&#36753;EL&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#20801;&#35768;&#30456;&#23545;&#20110;&#19981;&#21516;&#30340;&#35266;&#28857;&#25110;&#35282;&#24230;&#38598;&#25104;&#39046;&#22495;&#30693;&#35782;&#30340;&#32508;&#21512;&#34920;&#31034;&#12290;&#26377;&#21033;&#30340;&#26159;&#65292;&#20854;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#22312;P&#26102;&#38388;&#20869;&#65292;&#20351;&#20854;&#25104;&#20026;&#22823;&#35268;&#27169;&#30693;&#35782;&#38598;&#25104;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#31181;&#24418;&#24335;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21040;&#36798;&#19968;&#20010;&#25193;&#23637;&#36923;&#36753;&#65292;&#31216;&#20026;Standpoint EL+&#65292;&#20801;&#35768;&#20844;&#29702;&#21542;&#23450;&#12289;&#35282;&#33394;&#38142;&#20844;&#29702;&#12289;&#33258;&#29615;&#31561;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21487;&#28385;&#36275;&#24615;&#26816;&#27979;&#28436;&#32462;&#28436;&#31639;&#27861;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#35299;&#20915;&#23454;&#29992;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20854;&#28436;&#32462;&#35268;&#21017;&#30340;&#21407;&#22411;Datalog&#23454;&#29616;&#26469;&#23637;&#31034;&#25105;&#20204;&#28436;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standpoint EL is a multi-modal extension of the popular description logic EL that allows for the integrated representation of domain knowledge relative to diverse standpoints or perspectives. Advantageously, its satisfiability problem has recently been shown to be in PTime, making it a promising framework for large-scale knowledge integration.  In this paper, we show that we can further push the expressivity of this formalism, arriving at an extended logic, called Standpoint EL+, which allows for axiom negation, role chain axioms, self-loops, and other features, while maintaining tractability. This is achieved by designing a satisfiability-checking deduction calculus, which at the same time addresses the need for practical algorithms. We demonstrate the feasibility of our calculus by presenting a prototypical Datalog implementation of its deduction rules.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.13081</link><description>&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#30340;&#32452;&#32455;&#27835;&#29702;&#65306;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Organizational Governance of Emerging Technologies: AI Adoption in Healthcare. (arXiv:2304.13081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#65292;&#20026;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#30340;&#32467;&#26500;&#21644;&#35268;&#33539;&#31934;&#32454;&#21270;&#20102;&#26032;&#20852;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#23613;&#31649;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;AI&#37319;&#29992;&#26041;&#24335;&#65292;&#20294;&#26159;&#20854;&#20351;&#29992;&#21644;&#25972;&#21512;&#21608;&#22260;&#30340;&#32452;&#32455;&#27835;&#29702;&#24448;&#24448;&#34987;&#35748;&#20026;&#19981;&#21487;&#34892;&#12290;&#20581;&#24247;AI&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65288;HAIP&#65289;&#26088;&#22312;&#36890;&#36807;&#27492;&#30740;&#31350;&#26356;&#22909;&#22320;&#23450;&#20041;&#21307;&#30103;&#20445;&#20581;&#20013;AI&#31995;&#32479;&#30340;&#20805;&#20998;&#32452;&#32455;&#27835;&#29702;&#35201;&#27714;&#65292;&#24182;&#25903;&#25345;&#21355;&#29983;&#31995;&#32479;&#39046;&#23548;&#20154;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#35201;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#37319;&#29992;&#30340;&#26631;&#20934;&#22914;&#20309;&#26131;&#20110;&#20351;&#29992;&#21644;&#39640;&#25928;&#36816;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#29305;&#23450;&#30340;&#21355;&#29983;&#31995;&#32479;&#20013;&#65292;&#32472;&#21046;&#20986;&#23454;&#38469;&#26426;&#26500;&#37319;&#29992;AI&#25216;&#26415;&#30340;&#20855;&#20307;&#20915;&#31574;&#28857;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#32654;&#22269;&#20027;&#35201;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#30340;&#39046;&#23548;&#20154;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#20027;&#35201;&#30693;&#24773;&#20154;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#21512;&#20316;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;AI&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32452;&#32455;&#27835;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#38190;&#25511;&#21046;&#28857;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2303.16519</link><description>&lt;p&gt;
&#20174;&#22270;&#24418;&#20844;&#29702;&#22238;&#21040;&#21521;&#37327;&#65306;&#35780;&#20272;&#22522;&#20110;&#22270;&#24418;&#26412;&#20307;&#23884;&#20837;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
From axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings. (arXiv:2303.16519v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#23637;&#20986;&#22810;&#31181;&#26041;&#27861;&#29983;&#25104;&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#23884;&#20837;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#12290;&#23558;&#26412;&#20307;&#23884;&#20837;&#21040;&#22270;&#24418;&#32467;&#26500;&#20013;&#65292;&#21363;&#24341;&#20837;&#19968;&#32452;&#33410;&#28857;&#21644;&#36793;&#32536;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#21644;&#36923;&#36753;&#20844;&#29702;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#24418;&#23884;&#20837;&#23558;&#22270;&#24418;&#23884;&#20837;&#21040;$\mathbb{R}^n$&#20013;&#65292;&#26159;&#19968;&#31181;&#29983;&#25104;&#26412;&#20307;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#23884;&#20837;&#26412;&#20307;&#21040;&#22270;&#24418;&#20013;&#30340;&#26041;&#27861;&#65288;&#22270;&#24418;&#25237;&#24433;&#65289;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24335;&#23646;&#24615;&#65292;&#28041;&#21450;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#30340;&#20844;&#24335;&#31867;&#22411;&#12289;&#25237;&#24433;&#26159;&#21542;&#21487;&#36870;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#26029;&#35328;&#20844;&#24335;&#25110;&#20854;&#28436;&#32462;&#38381;&#21253;&#12290;&#25105;&#20204;&#23450;&#24615;&#22320;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#20102;&#24050;&#29992;&#20110;&#23884;&#20837;&#26412;&#20307;&#30340;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22270;&#24418;&#25237;&#24433;&#23646;&#24615;&#23545;&#20174;&#26412;&#20307;&#23884;&#20837;&#20013;&#39044;&#27979;&#20844;&#24335;&#30340;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;&#25237;&#24433;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several approaches have been developed that generate embeddings for Description Logic ontologies and use these embeddings in machine learning. One approach of generating ontologies embeddings is by first embedding the ontologies into a graph structure, i.e., introducing a set of nodes and edges for named entities and logical axioms, and then applying a graph embedding to embed the graph in $\mathbb{R}^n$. Methods that embed ontologies in graphs (graph projections) have different formal properties related to the type of axioms they can utilize, whether the projections are invertible or not, and whether they can be applied to asserted axioms or their deductive closure. We analyze, qualitatively and quantitatively, several graph projection methods that have been used to embed ontologies, and we demonstrate the effect of the properties of graph projections on the performance of predicting axioms from ontology embeddings. We find that there are substantial differences between different proj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.11835</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#25628;&#32034;&#26041;&#27861;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs. (arXiv:2302.11835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#26080;&#23548;&#25968;&#25628;&#32034;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#20247;&#25152;&#21608;&#30693;&#30340;&#23439;&#35266;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#33509;&#24178;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#26041;&#27861;&#25152;&#20570;&#20986;&#30340;&#8220;&#28151;&#21512;&#31574;&#30053;&#8221;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;&#29305;&#21035;&#39640;&#25928;&#65292;&#24182;&#19988;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#36890;&#24120;&#20250;&#22686;&#21152;&#24615;&#33021;&#65292;&#22240;&#20026;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#30340;&#20559;&#24046;&#37117;&#20250;&#34987;&#32531;&#35299;&#12290;&#36890;&#36807;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#26657;&#20934;&#36816;&#34892;&#36807;&#31243;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#21482;&#26377;&#22312;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#26102;&#25165;&#32487;&#32493;&#21033;&#29992;&#29305;&#23450;&#26041;&#27861;&#65292;&#20294;&#22312;&#35813;&#26041;&#27861;&#36798;&#21040;&#24615;&#33021;&#24179;&#21488;&#26102;&#25506;&#32034;&#26032;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#25628;&#32034;&#26041;&#26696;&#22312;&#20219;&#20309;&#20854;&#20182;&#27979;&#35797;&#30340;&#26041;&#27861;&#25110;&#26041;&#27861;&#32452;&#21512;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19987;&#19994;&#30340;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of "mixed strategies" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;</title><link>http://arxiv.org/abs/2302.10766</link><description>&lt;p&gt;
&#35753;&#20320;&#30340;&#34892;&#20026;&#20117;&#28982;&#26377;&#24207;&#65306;AI&#27861;&#26696;&#21644;&#25216;&#26415;&#36879;&#26126;&#24230;&#30340;&#27604;&#36739;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Get Your Act Together: A Comparative View on Transparency in the AI Act and Technology. (arXiv:2302.10766v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#25552;&#20986;&#20102;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#39118;&#38505;&#30340;&#27604;&#20363;&#26041;&#27861;&#26469;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#65292;&#20854;&#20013;&#35814;&#32454;&#35201;&#27714;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#39046;&#22495;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#20294;&#22312;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20855;&#20307;&#23450;&#20041;&#19978;&#65292;XAI&#19982;&#35813;&#27861;&#26696;&#23384;&#22312;&#22522;&#26412;&#24046;&#24322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;XAI&#21644;&#27431;&#27954;&#27861;&#35268;&#26159;&#22914;&#20309;&#30475;&#24453;&#36879;&#26126;&#24230;&#30340;&#22522;&#26412;&#23450;&#20041;&#30340;&#65292;&#29305;&#21035;&#26159;AI&#27861;&#26696;&#21644;&#30456;&#20851;&#30340;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26088;&#22312;&#30830;&#23450;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#23545;&#40784;&#30340;&#20027;&#35201;&#35201;&#28857;&#65306;&#28548;&#28165;&#36879;&#26126;&#24230;&#30340;&#33539;&#22260;&#65292;XAI&#30340;&#27861;&#24459;&#22320;&#20301;&#65292;&#30417;&#31649;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The European Union has proposed the Artificial Intelligence Act which introduces a proportional risk-based approach to AI regulation including detailed requirements for transparency and explainability. Many of these requirements may be addressed in practice by the field of explainable AI (XAI), however, there are fundamental differences between XAI and the Act regarding what transparency and explainability are. These basic definitions should be aligned to assure that regulation continually translates into appropriate technical practices. To facilitate this alignment, we first give an overview of how XAI and European regulation view basic definitions of transparency with a particular focus on the AI Act and the related General Data Protection Regulation (GDPR). We then present a comparison of XAI and regulatory approaches to identify the main points that would improve alignment between the fields: clarification of the scope of transparency, the legal status of XAI, oversight issues in c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#8220;&#29702;&#24819;&#20107;&#29289;&#8221;&#30340;&#29702;&#35770;&#65292;&#21033;&#29992;&#25277;&#35937;&#30340;&#38381;&#21512;&#31639;&#23376;&#26045;&#21152;&#25512;&#29702;&#35268;&#21017;&#26469;&#25429;&#25417;&#20154;&#23545;&#21738;&#20123;&#20107;&#29289;&#25110;&#27010;&#24565;&#26159;&#29702;&#24819;&#30340;&#20449;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.07412</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#8220;&#29702;&#24819;&#20107;&#29289;&#8221;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A theory of desirable things. (arXiv:2302.07412v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#8220;&#29702;&#24819;&#20107;&#29289;&#8221;&#30340;&#29702;&#35770;&#65292;&#21033;&#29992;&#25277;&#35937;&#30340;&#38381;&#21512;&#31639;&#23376;&#26045;&#21152;&#25512;&#29702;&#35268;&#21017;&#26469;&#25429;&#25417;&#20154;&#23545;&#21738;&#20123;&#20107;&#29289;&#25110;&#27010;&#24565;&#26159;&#29702;&#24819;&#30340;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#27010;&#29575;&#39046;&#22495;&#20013;&#20351;&#29992;&#8220;&#29702;&#24819;&#36172;&#21338;&#8221;&#30340;&#29702;&#35770;&#20316;&#20026;&#21551;&#21457;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#8220;&#29702;&#24819;&#20107;&#29289;&#8221;&#30340;&#29702;&#35770;&#12290;&#23427;&#30340;&#30446;&#30340;&#26159;&#24314;&#27169;&#19968;&#20010;&#20027;&#20307;&#23545;&#21738;&#20123;&#20107;&#29289;&#26159;&#29702;&#24819;&#30340;&#20449;&#24565;&#12290;&#20107;&#29289;&#26412;&#36523;&#30340;&#23646;&#24615;&#24182;&#19981;&#37325;&#35201;&#65292;&#20063;&#19981;&#37325;&#35201;&#23427;&#20204;&#34987;&#23450;&#20041;&#20026;&#29702;&#24819;&#30340;&#24847;&#20041;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#36172;&#21338;&#65292;&#22914;&#26524;&#19968;&#20010;&#20027;&#20307;&#25509;&#21463;&#23427;&#20204;&#65292;&#21017;&#31216;&#23427;&#20204;&#20026;&#29702;&#24819;&#36172;&#21338;&#65292;&#20294;&#21516;&#26679;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#25259;&#33832;&#65292;&#22914;&#26524;&#25105;&#30340;&#26379;&#21451;&#38463;&#29791;&#21916;&#27426;&#21507;&#23427;&#20204;&#65292;&#21017;&#31216;&#23427;&#20204;&#20026;&#29702;&#24819;&#25259;&#33832;&#12290;&#20854;&#20182;&#26377;&#29992;&#30340;&#20363;&#23376;&#21253;&#25324;&#21629;&#39064;&#12289;&#39532;&#24425;&#31080;&#25110;&#20219;&#20309;&#19978;&#36848;&#20107;&#29289;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#25277;&#35937;&#30340;&#38381;&#21512;&#31639;&#23376;&#26469;&#26045;&#21152;&#25512;&#29702;&#35268;&#21017;&#65292;&#24182;&#31216;&#36981;&#24490;&#36825;&#20123;&#35268;&#21017;&#30340;&#27169;&#22411;&#20026;&#36830;&#36143;&#30340;&#12290;&#25105;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#27599;&#31181;&#27169;&#22411;&#37117;&#33021;&#25429;&#25417;&#21040;&#20027;&#20307;&#23545;&#21738;&#20123;&#20107;&#29289;&#26159;&#29702;&#24819;&#30340;&#20449;&#24565;&#65306;&#19968;&#31181;&#26159;&#29702;&#24819;&#20107;&#29289;&#30340;&#38598;&#21512;&#27169;&#22411;&#65292;&#19968;&#31181;&#26159;&#29702;&#24819;&#38598;&#21512;&#30340;&#38598;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the theory of desirable gambles that is used to model uncertainty in the field of imprecise probabilities, I present a theory of desirable things. Its aim is to model a subject's beliefs about which things are desirable. What the things are is not important, nor is what it means for them to be desirable. It can be applied to gambles, calling them desirable if a subject accepts them, but it can just as well be applied to pizzas, calling them desirable if my friend Arthur likes to eat them. Other useful examples of things one might apply this theory to are propositions, horse lotteries, or preferences between any of the above. Regardless of the particular things that are considered, inference rules are imposed by means of an abstract closure operator, and models that adhere to these rules are called coherent. I consider two types of models, each of which can capture a subject's beliefs about which things are desirable: sets of desirable things and sets of desirable sets of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#22312;&#27861;&#23450;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#38169;&#35823;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;GPT-3&#23545;&#23454;&#38469;&#27861;&#35268;&#23384;&#22312;&#32570;&#38519;&#65292;&#19988;&#22312;&#23545;&#20110;&#21512;&#25104;&#27861;&#35268;&#30340;&#38382;&#39064;&#22238;&#31572;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2302.06100</link><description>&lt;p&gt;
GPT-3&#33021;&#36827;&#34892;&#27861;&#23450;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-3 Perform Statutory Reasoning?. (arXiv:2302.06100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#22312;&#27861;&#23450;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#38169;&#35823;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;GPT-3&#23545;&#23454;&#38469;&#27861;&#35268;&#23384;&#22312;&#32570;&#38519;&#65292;&#19988;&#22312;&#23545;&#20110;&#21512;&#25104;&#27861;&#35268;&#30340;&#38382;&#39064;&#22238;&#31572;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#23450;&#25512;&#29702;&#26159;&#19968;&#31181;&#21033;&#29992;&#20107;&#23454;&#21644;&#30001;&#31435;&#27861;&#26426;&#26500;&#29992;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#30340;&#35268;&#21017;&#65288;&#21363;&#27861;&#35268;&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#22522;&#26412;&#27861;&#24459;&#25216;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#24378;&#22823;&#30340;GPT-3&#27169;&#22411;text-davinci-003&#22312;&#19968;&#20010;&#21517;&#20026;SARA&#30340;&#24050;&#24314;&#31435;&#30340;&#27861;&#23450;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;&#34429;&#28982;&#25105;&#20204;&#21462;&#24471;&#20102;&#27604;&#20808;&#21069;&#26368;&#20339;&#21457;&#34920;&#32467;&#26524;&#26356;&#22909;&#30340;GPT-3&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#20063;&#30830;&#35748;&#20102;&#20854;&#20986;&#29616;&#20102;&#20960;&#31181;&#26126;&#26174;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;GPT-3&#23545;SARA&#22522;&#20110;&#23454;&#38469;&#32654;&#22269;&#27861;&#35268;&#30340;&#20808;&#39564;&#30693;&#35782;&#23384;&#22312;&#32570;&#38519;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31616;&#21333;&#30340;&#21512;&#25104;&#27861;&#35268;&#65292;&#30830;&#20445;GPT-3&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-3&#22312;&#22238;&#31572;&#20851;&#20110;&#36825;&#20123;&#31616;&#21333;&#21512;&#25104;&#27861;&#35268;&#30340;&#30452;&#25130;&#20102;&#24403;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.
&lt;/p&gt;</description></item><item><title>NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.12667</link><description>&lt;p&gt;
NeSyFOLD: &#20174;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12667
&lt;/p&gt;
&lt;p&gt;
NeSyFOLD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#33258;&#21160;&#26144;&#23556;&#31639;&#27861;&#26469;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#35821;&#20041;&#27010;&#24565;&#65292;&#24182;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;NeSyFOLD&#65292;&#20174;CNN&#20013;&#25552;&#21462;&#36923;&#36753;&#35268;&#21017;&#24182;&#21019;&#24314;&#19968;&#20010;NeSyFOLD&#27169;&#22411;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;NeSyFOLD&#30340;&#23398;&#20064;&#27969;&#31243;&#22914;&#19979;&#65306;&#65288;i&#65289;&#25105;&#20204;&#39318;&#20808;&#22312;&#36755;&#20837;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;CNN&#65292;&#24182;&#25552;&#21462;&#26368;&#21518;&#19968;&#23618;&#26680;&#30340;&#28608;&#27963;&#20316;&#20026;&#20108;&#36827;&#21046;&#20540;&#65307;&#65288;ii&#65289;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;FOLD-SE-M&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#33021;&#22815;&#20998;&#31867;&#22270;&#20687;&#30340;&#36923;&#36753;&#31243;&#24207;&#8212;&#8212;&#34920;&#31034;&#20026;&#27599;&#20010;&#26680;&#23545;&#24212;&#30340;&#20108;&#36827;&#21046;&#28608;&#27963;&#21521;&#37327;&#65292;&#21516;&#26102;&#20135;&#29983;&#36923;&#36753;&#35299;&#37322;&#12290;&#30001;FOLD-SE-M&#31639;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#20855;&#26377;&#26680;&#32534;&#21495;&#20316;&#20026;&#35859;&#35789;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23558;CNN&#26680;&#26144;&#23556;&#21040;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#27010;&#24565;&#12290;&#36825;&#20010;&#26144;&#23556;&#34987;&#29992;&#26469;&#23558;&#35268;&#21017;&#38598;&#20013;&#30340;&#35859;&#35789;&#21517;&#65288;&#26680;&#32534;&#21495;&#65289;&#26367;&#25442;&#20026;&#23545;&#24212;&#30340;&#35821;&#20041;&#27010;&#24565;&#26631;&#31614;&#12290;&#32467;&#26524;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#21487;&#20197;&#34987;&#20154;&#31867;&#30452;&#35266;&#22320;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;NeSyFOLD&#26694;&#26550;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#21644;&#35299;&#37322;&#24615;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#20869;&#22312;&#35821;&#20041;&#30340;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.02560</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#22810;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation. (arXiv:2212.02560v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#20869;&#22312;&#35821;&#20041;&#30340;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#22312;&#27599;&#20010;&#20851;&#31995;&#20013;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#26032;&#30340;&#20851;&#31995;&#12290;&#20043;&#21069;&#22522;&#20110;&#24230;&#37327;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#31639;&#27861;&#36890;&#36807;&#23558;&#30001;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#23884;&#20837;&#29983;&#25104;&#30340;&#21407;&#22411;&#19982;&#20351;&#29992;&#35757;&#32451;&#30340;&#24230;&#37327;&#20989;&#25968;&#20998;&#26512;&#26597;&#35810;&#35821;&#21477;&#30340;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#20197;&#35782;&#21035;&#36825;&#20123;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#39046;&#22495;&#22987;&#32456;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#39046;&#22495;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23545;&#26410;&#35265;&#20851;&#31995;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#20851;&#31995;&#30340;&#20869;&#22312;&#35821;&#20041;&#20013;&#23398;&#20064;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#21407;&#22411;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#20449;&#24687;&#25506;&#32034;&#20851;&#31995;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#24102;&#26377;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#30340;&#26410;&#35265;&#39046;&#22495;&#20013;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relation extraction aims to recognize novel relations with few labeled sentences in each relation. Previous metric-based few-shot relation extraction algorithms identify relationships by comparing the prototypes generated by the few labeled sentences embedding with the embeddings of the query sentences using a trained metric function. However, as these domains always have considerable differences from those in the training dataset, the generalization ability of these approaches on unseen relations in many domains is limited. Since the prototype is necessary for obtaining relationships between entities in the latent space, we suggest learning more interpretable and efficient prototypes from prior knowledge and the intrinsic semantics of relations to extract new relations in various domains more effectively. By exploring the relationships between relations using prior information, we effectively improve the prototype representation of relations. By using contrastive learning to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLOD&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#21644;&#32416;&#27491;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#20174;&#32780;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13993</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#35299;&#20915;&#26631;&#27880;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Combating noisy labels in object detection datasets. (arXiv:2211.13993v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLOD&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#21644;&#32416;&#27491;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#20174;&#32780;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#30446;&#26631;&#26816;&#27979;&#36825;&#26679;&#30340;&#22256;&#38590;&#20219;&#21153;&#20013;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26102;&#65292;&#36890;&#24120;&#20250;&#25509;&#21463;&#19968;&#23450;&#27604;&#20363;&#30340;&#38169;&#35823;&#26679;&#26412;&#65292;&#20272;&#31639;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36171;&#20104;&#36866;&#24403;&#30340;&#26435;&#37325;&#65292;&#25110;&#32773;&#24573;&#30053;&#19981;&#30830;&#23450;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Confident Learning for Object Detection&#8221;&#65288;CLOD&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#24182;&#24314;&#35758;&#32416;&#27491;&#26041;&#27861;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#25214;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#28040;&#38500;&#23427;&#20204;&#12290;&#21487;&#30097;&#30340;&#36793;&#30028;&#26694;&#21487;&#20197;&#36827;&#34892;&#26816;&#26597;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#22312;&#19981;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of training datasets for deep neural networks is a key factor contributing to the accuracy of resulting models. This effect is amplified in difficult tasks such as object detection. Dealing with errors in datasets is often limited to accepting that some fraction of examples is incorrect, estimating their confidence and assigning appropriate weights or ignoring uncertain ones during training. In this work, we propose a different approach. We introduce the Confident Learning for Object Detection (CLOD) algorithm for assessing the quality of each label in object detection datasets, identifying missing, spurious, mislabeled and mislocated bounding boxes and suggesting corrections. By focusing on finding incorrect examples in the training datasets, we can eliminate them at the root. Suspicious bounding boxes can be reviewed in order to improve the quality of the dataset, leading to better models without further complicating their already complex architectures. The proposed metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12701</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#27169;&#25311;&#20154;&#31867;&#19981;&#26029;&#23398;&#20064;&#21644;&#31215;&#32047;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#36807;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#26032;&#20219;&#21153;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;NLP&#20013;CL&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23427;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;CL&#26377;&#26174;&#30528;&#21306;&#21035;&#12290;&#23427;&#28085;&#30422;&#20102;&#65288;1&#65289;&#25152;&#26377;CL&#35774;&#32622;&#21450;&#29616;&#26377;&#25216;&#26415;&#20998;&#31867;&#65307;&#65288;2&#65289;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65307;&#65288;3&#65289;&#30693;&#35782;&#36801;&#31227;&#65288;KT&#65289;&#65292;&#23545;NLP&#20219;&#21153;&#23588;&#20854;&#37325;&#35201;&#65307;&#20197;&#21450;&#65288;4&#65289;&#19968;&#20123;&#29702;&#35770;&#21644;&#20132;&#20219;&#21153;&#31867;&#20998;&#31163;&#65288;ICS&#65289;&#30340;&#38544;&#21547;&#25361;&#25112;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#19968;&#20123;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#26469;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#20215;&#20540;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#25968;&#25454;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#20351;&#29992; ENN &#37117;&#27604;&#24120;&#35268;&#30340;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2211.01568</link><description>&lt;p&gt;
&#36890;&#36807;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#26469;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#20215;&#20540;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#25968;&#25454;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#20351;&#29992; ENN &#37117;&#27604;&#24120;&#35268;&#30340;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#22823;&#35268;&#27169;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#24494;&#35843;&#26041;&#27861;&#24182;&#19981;&#37325;&#35270;&#25152;&#24494;&#35843;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#20320;&#33021;&#22815;&#23558;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#35757;&#32451;&#25968;&#25454;&#25918;&#22312;&#20248;&#20808;&#32771;&#34385;&#30340;&#20301;&#32622;&#19978;&#65292;&#23601;&#21487;&#20197;&#22312;&#20351;&#29992;&#26356;&#23569;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#22686;&#24191;&#20102;&#19968;&#20010; epinet&#65292;&#36825;&#26159;&#19968;&#20010;&#36741;&#21161;&#20272;&#31639;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24182;&#24418;&#25104;&#19968;&#20010;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#30340;&#23567;&#22411;&#39069;&#22806;&#32593;&#32476;&#12290;ENN&#26159;&#33021;&#22815;&#30693;&#36947;&#33258;&#24049;&#30340;&#19981;&#36275;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010; epinet &#26469;&#20248;&#20808;&#32771;&#34385;&#19981;&#30830;&#23450;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558; BERT &#23545; GLUE &#20219;&#21153;&#30340;&#24494;&#35843;&#24615;&#33021;&#25552;&#39640;&#21040;&#19982;&#19981;&#36827;&#34892;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#25968;&#25454;&#26631;&#31614;&#25968;&#37327;&#20943;&#21322;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21512;&#25104;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992; epinet &#37117;&#20248;&#20110;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often pre-train on large unsupervised text corpora, then fine-tune on additional task-specific data. However, typical fine-tuning schemes do not prioritize the examples that they tune on. We show that, if you can prioritize informative training data, you can achieve better performance while using fewer labels. To do this we augment a language model with an epinet: a small additional network that helps to estimate model uncertainty and forms an \textit{epistemic neural network} (ENN). ENNs are neural networks that can know what they don't know. Using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data than training without prioritization. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16506</link><description>&lt;p&gt;
&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913; (Observable Perfect Equilibrium)
&lt;/p&gt;
&lt;p&gt;
Observable Perfect Equilibrium. (arXiv:2210.16506v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32435;&#20160;&#22343;&#34913;&#25104;&#20026;&#20102;&#21338;&#24328;&#35770;&#30340;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#21338;&#24328;&#21253;&#21547;&#22810;&#20010;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#24517;&#39035;&#30830;&#23450;&#22914;&#20309;&#22312;&#20854;&#20013;&#36873;&#25321;&#65292;&#20197;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#20026;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#25552;&#20986;&#20102;&#20960;&#20010;&#32435;&#20160;&#22343;&#34913;&#32454;&#21270;&#27010;&#24565;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#39076;&#25238;&#25163;&#23436;&#32654;&#22343;&#34913;&#12289;&#25311;&#23436;&#32654;&#22343;&#34913;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#20391;&#25311;&#23436;&#32654;&#22343;&#34913;&#12290;&#36825;&#20123;&#27010;&#24565;&#23545;&#26576;&#20123;&#20219;&#24847;&#23567;&#30340;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#35777;&#22987;&#32456;&#23384;&#22312;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#21457;&#23637;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#24378;&#22823;&#30340;&#20195;&#29702;&#20154;&#65292;&#36825;&#20123;&#27010;&#24565;&#37117;&#19981;&#27491;&#30830;&#12290;&#25105;&#20204;&#20026;&#28216;&#25103;&#26641;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#24182;&#19981;&#19968;&#23450;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#19981;&#21487;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#20855;&#26377;&#40065;&#26834;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Nash equilibrium has emerged as the central game-theoretic solution concept, many important games contain several Nash equilibria and we must determine how to select between them in order to create real strategic agents. Several Nash equilibrium refinement concepts have been proposed and studied for sequential imperfect-information games, the most prominent being trembling-hand perfect equilibrium, quasi-perfect equilibrium, and recently one-sided quasi-perfect equilibrium. These concepts are robust to certain arbitrarily small mistakes, and are guaranteed to always exist; however, we argue that neither of these is the correct concept for developing strong agents in sequential games of imperfect information. We define a new equilibrium refinement concept for extensive-form games called observable perfect equilibrium in which the solution is robust over trembles in publicly-observable action probabilities (not necessarily over all action probabilities that may not be observable by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#22270;&#34920;&#30340;&#31070;&#32463;&#32467;&#26500;&#21270;&#23398;&#20064;&#65288;NSL&#65289;&#26694;&#26550;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#35821;&#38899;&#26679;&#26412;&#21644;&#22270;&#34920;&#35757;&#32451;&#36731;&#37327;&#32423;SER&#27169;&#22411;&#26082;&#21487;&#20197;&#20135;&#29983;&#23567;&#27169;&#22411;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;</title><link>http://arxiv.org/abs/2210.14977</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#24773;&#24863;&#35782;&#21035;&#30340;&#30693;&#35782;&#36801;&#31227;&#19982;&#31070;&#32463;&#32467;&#26500;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer For On-Device Speech Emotion Recognition with Neural Structured Learning. (arXiv:2210.14977v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#22270;&#34920;&#30340;&#31070;&#32463;&#32467;&#26500;&#21270;&#23398;&#20064;&#65288;NSL&#65289;&#26694;&#26550;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#35821;&#38899;&#26679;&#26412;&#21644;&#22270;&#34920;&#35757;&#32451;&#36731;&#37327;&#32423;SER&#27169;&#22411;&#26082;&#21487;&#20197;&#20135;&#29983;&#23567;&#27169;&#22411;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#20013;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#38543;&#30528;&#36793;&#32536;&#35774;&#22791;&#30340;&#24555;&#36895;&#20852;&#36215;&#65292;&#23558;SER&#24212;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#23545;&#20110;&#22823;&#37327;HCI&#24212;&#29992;&#26159;&#26377;&#21069;&#36884;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#26469;&#25552;&#39640;SER&#30340;&#24615;&#33021;&#65292;&#20294;&#36793;&#32536;&#35774;&#22791;&#30340;&#20869;&#23384;&#31354;&#38388;&#21644;&#35745;&#31639;&#33021;&#21147;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#26500;&#24314;&#32508;&#21512;&#22270;&#34920;&#30340;&#31070;&#32463;&#32467;&#26500;&#21270;&#23398;&#20064;&#65288;NSL&#65289;&#26694;&#26550;&#12290;&#22312;&#28304;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;SER&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#22270;&#34920;&#12290;&#28982;&#21518;&#20351;&#29992;&#35821;&#38899;&#26679;&#26412;&#21644;&#22270;&#34920;&#20316;&#20026;&#36755;&#20837;&#65292;&#35757;&#32451;&#30456;&#23545;&#36739;&#36731;&#37327;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#35821;&#38899;&#26679;&#26412;&#21644;&#22270;&#34920;&#35757;&#32451;&#36731;&#37327;&#32423;SER&#27169;&#22411;&#26082;&#21487;&#20197;&#20135;&#29983;&#23567;&#27169;&#22411;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20351;&#29992;&#35821;&#38899;&#26679;&#26412;&#25110;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) has been a popular research topic in human-computer interaction (HCI). As edge devices are rapidly springing up, applying SER to edge devices is promising for a huge number of HCI applications. Although deep learning has been investigated to improve the performance of SER by training complex models, the memory space and computational capability of edge devices represents a constraint for embedding deep learning models. We propose a neural structured learning (NSL) framework through building synthesized graphs. An SER model is trained on a source dataset and used to build graphs on a target dataset. A relatively lightweight model is then trained with the speech samples and graphs together as the input. Our experiments demonstrate that training a lightweight SER model on the target dataset with speech samples and graphs can not only produce small SER models, but also enhance the model performance compared to models with speech samples only and those using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#22312;&#20445;&#35777;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#30340;&#20445;&#38556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#25239;&#20102;&#27169;&#22411;&#30340;&#36864;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.14410</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes. (arXiv:2210.14410v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32852;&#21512;&#20998;&#31867;&#21644;&#22810;&#20010;&#26126;&#30830;&#26816;&#27979;&#31867;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#22312;&#20445;&#35777;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#30340;&#20445;&#38556;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#25239;&#20102;&#27169;&#22411;&#30340;&#36864;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#24320;&#21457;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#26377;&#20445;&#38556;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#32852;&#21512;&#40065;&#26834;&#24615;&#20998;&#31867;&#21644;&#26816;&#27979;&#34987;&#26368;&#36817;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#21487;&#39564;&#35777;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#27491;&#30830;&#20998;&#31867;&#25110;&#20998;&#37197;&#21040;&#8220;&#24323;&#26435;&#8221;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#21487;&#35777;&#26126;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#26126;&#30830;&#24323;&#26435;&#31867;&#21035;&#30340;&#32593;&#32476;&#20013;&#32780;&#33719;&#30410;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#36866;&#24212;&#22320;&#20998;&#37197;&#21040;&#37027;&#20123;&#31867;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#28155;&#21152;&#22810;&#20010;&#24323;&#26435;&#31867;&#21035;&#21487;&#33021;&#20250;&#23548;&#33268;&#8220;&#27169;&#22411;&#36864;&#21270;&#8221;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#31181;&#36864;&#21270;&#65292;&#36890;&#36807;&#20419;&#36827;&#20805;&#20998;&#20351;&#29992;&#22810;&#20010;&#24323;&#26435;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19968;&#33268;&#22320;&#23454;&#29616;&#20102;&#26377;&#21033;&#30340;&#26631;&#20934;&#21644;&#40065;&#26834;&#24615;&#39564;&#35777;&#20934;&#30830;&#24615;&#24179;&#34913;&#28857;&#65292;&#24182;&#22312;&#21508;&#31181;&#36873;&#25321;&#24323;&#26435;&#31867;&#21035;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the "abstain" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to "model degeneracy", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20114;&#34917;&#30340;&#20195;&#29702;&#20219;&#21153;&#32467;&#21512;&#22312;&#19968;&#36215;&#20197;&#26356;&#22909;&#22320;&#32771;&#34385;&#36816;&#21160;&#21644;&#22806;&#35266;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#29289;&#20307;&#20998;&#31867;&#65292;&#24182;&#28155;&#21152;&#20102;&#22810;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2210.07697</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning based Video Anomaly Detection with Attention. (arXiv:2210.07697v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20114;&#34917;&#30340;&#20195;&#29702;&#20219;&#21153;&#32467;&#21512;&#22312;&#19968;&#36215;&#20197;&#26356;&#22909;&#22320;&#32771;&#34385;&#36816;&#21160;&#21644;&#22806;&#35266;&#29305;&#24449;&#65292;&#21516;&#26102;&#32771;&#34385;&#29289;&#20307;&#20998;&#31867;&#65292;&#24182;&#28155;&#21152;&#20102;&#22810;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#36807;&#23558;&#22810;&#20010;&#20195;&#29702;&#20219;&#21153;&#32452;&#21512;&#22312;&#19981;&#21516;&#30340;&#20998;&#25903;&#20013;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#35270;&#39057;&#24322;&#24120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#20114;&#34917;&#30340;&#20195;&#29702;&#20219;&#21153;&#32452;&#21512;&#36215;&#26469;&#20197;&#26356;&#22909;&#22320;&#32771;&#34385;&#36816;&#21160;&#21644;&#22806;&#35266;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#20998;&#21106;&#21644;&#26410;&#26469;&#24103;&#39044;&#27979;&#20219;&#21153;&#32467;&#21512;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#20998;&#25903;&#20013;&#65292;&#20197;&#23398;&#20064;&#29289;&#20307;&#31867;&#21035;&#21644;&#19968;&#33268;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#24182;&#21516;&#26102;&#26816;&#27979;&#21508;&#33258;&#30340;&#24322;&#24120;&#12290;&#22312;&#31532;&#20108;&#20010;&#20998;&#25903;&#20013;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#20960;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#21253;&#25324;&#23545;&#29289;&#20307;&#37096;&#20998;&#30340;&#27880;&#24847;&#21147;&#12289;&#36816;&#21160;&#26041;&#21521;&#30340;&#27880;&#24847;&#21147;&#21644;&#29289;&#20307;&#19982;&#25668;&#20687;&#26426;&#20043;&#38388;&#36317;&#31163;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#32771;&#34385;&#20102;&#29289;&#20307;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning based video anomaly detection methods combine multiple proxy tasks in different branches to detect video anomalies in different situations. Most existing methods either do not combine complementary tasks to effectively cover all motion patterns, or the class of the objects is not explicitly considered. To address the aforementioned shortcomings, we propose a novel multi-task learning based method that combines complementary proxy tasks to better consider the motion and appearance features. We combine the semantic segmentation and future frame prediction tasks in a single branch to learn the object class and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, we added several attention mechanisms to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera. Our qualitative results show that the proposed method considers the object class effectively 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#19982;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#36825;&#19968;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20272;&#31639;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#26102;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.13089</link><description>&lt;p&gt;
&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#65306;&#39044;&#27979;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift. (arXiv:2206.13089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#20998;&#24067;&#30340;&#20934;&#30830;&#24615;&#19982;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#36825;&#19968;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20272;&#31639;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#26102;&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Miller&#31561;&#20154;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#20854;&#22312;&#20960;&#20010;OOD&#22522;&#20934;&#19978;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#24378;&#28872;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20182;&#20204;&#31216;&#20043;&#20026;&#8220;&#20934;&#30830;&#24615;&#22312;&#32447;&#8221;&#12290; &#34429;&#28982;&#36825;&#23545;&#20110;&#27169;&#22411;&#36873;&#25321;&#65288;&#21363;&#65292;ID&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#27169;&#22411;&#26368;&#26377;&#21487;&#33021;&#34920;&#29616;&#26368;&#22909;OOD&#65289;&#26159;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#36825;&#19968;&#20107;&#23454;&#26080;&#27861;&#24110;&#21161;&#20272;&#35745;&#27809;&#26377;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#30340;&#27169;&#22411;&#23454;&#38469;OOD&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#20063;&#36866;&#29992;&#20110;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65306;&#27599;&#24403;&#20934;&#30830;&#24615;&#22312;&#32447;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20219;&#24847;&#20004;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;&#20855;&#26377;&#28508;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#65289;&#30340;&#39044;&#27979;&#22312;OOD&#19978;&#30340;&#21327;&#35758;&#20063;&#19982;&#20854;ID&#21327;&#35758;&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#19982;ID&#21327;&#35758;&#30340;&#26012;&#29575;&#21644;&#20559;&#24046;&#19982;OOD&#19982;ID&#20934;&#30830;&#24230;&#38750;&#24120;&#25509;&#36817;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#25105;&#20204;&#31216;&#20026;&#8220;&#22312;&#32447;&#19968;&#33268;&#24615;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;OOD&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#26631;&#35760;&#30340;OOD&#39564;&#35777;&#38598;&#12290; &#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#65288;ImageNet&#12289;CIFAR-10-C&#21644;-10-P&#65289;&#19978;&#37096;&#32626;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;OOD&#26816;&#27979;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Miller et al. showed that a model's in-distribution (ID) accuracy has a strong linear correlation with its out-of-distribution (OOD) accuracy on several OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''. While a useful tool for model selection (i.e., the model most likely to perform the best OOD is the one with highest ID accuracy), this fact does not help estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper, we show a similar but surprising phenomenon also holds for the agreement between pairs of neural network classifiers: whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement. Furthermore, we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon, which we call
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2204.07182</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29992;&#20110;&#27861;&#38498;&#25991;&#20214;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25104;&#20026;&#27861;&#24459;&#39046;&#22495;&#20013;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#26412;&#25991;&#20197;&#24052;&#35199;&#21496;&#27861;&#31995;&#32479;&#30340;&#26696;&#20363;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36816;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#12290;&#21253;&#25324;BERT&#12289;GPT-2&#12289;RoBERTa&#31561;NLP&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#30340;&#23884;&#20837;&#21521;&#37327;&#34920;&#24449;&#65292;&#36816;&#29992;&#32858;&#31867;&#26041;&#27861;&#23545;&#35785;&#35772;&#26696;&#20214;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Artificial Intelligence (AI) have leveraged promising results in solving complex problems in the area of Natural Language Processing (NLP), being an important tool to help in the expeditious resolution of judicial proceedings in the legal area. In this context, this work targets the problem of detecting the degree of similarity between judicial documents that can be achieved in the inference group, by applying six NLP techniques based on the transformers architecture to a case study of legal proceedings in the Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2 and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian Portuguese language, and then were fine-tuned and specialised for the legal sector using 210,000 legal proceedings. Vector representations of each legal document were calculated based on their embeddings, which were used to cluster the lawsuits, calculating the quality of each model based on the cosine of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#20132;&#36890;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#33258;&#20027;&#20195;&#29702;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#19988;&#23433;&#20840;&#30340;&#30417;&#31649;&#65292;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#12290;</title><link>http://arxiv.org/abs/2112.07569</link><description>&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#20013;&#21487;&#25193;&#23637;&#30417;&#31649;&#33258;&#20027;&#24615;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Cooperation for Scalable Supervision of Autonomy in Mixed Traffic. (arXiv:2112.07569v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#20132;&#36890;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#33258;&#20027;&#20195;&#29702;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#21487;&#25193;&#23637;&#19988;&#23433;&#20840;&#30340;&#30417;&#31649;&#65292;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24615;&#30340;&#36827;&#27493;&#20026;&#35768;&#22810;&#39046;&#22495;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#31215;&#26497;&#25104;&#26524;&#65292;&#20294;&#23454;&#29616;&#20854;&#23433;&#20840;&#37096;&#32626;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20197;&#22914;&#19979;&#21160;&#26426;&#20026;&#20986;&#21457;&#28857;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#21542;&#36991;&#20813;&#38656;&#35201;&#19968;&#20010;&#20154;&#22987;&#32456;&#30417;&#30563;&#19968;&#20010;&#26426;&#22120;&#65311;&#36890;&#36807;&#32771;&#34385;&#36828;&#31243;&#20154;&#31867;&#30417;&#31649;&#21592;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#35268;&#33539;&#21270;&#20102;&#36825;&#20010;&#21487;&#25193;&#23637;&#30417;&#31649;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#33258;&#20027;&#20195;&#29702;&#22914;&#20309;&#21512;&#20316;&#20197;&#23454;&#29616;&#23433;&#20840;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#30001;AVs&#21644;&#20154;&#31867;&#39550;&#39542;&#21592;&#28151;&#21512;&#32452;&#25104;&#30340;&#20132;&#36890;&#20013;&#21512;&#24182;&#30340;&#23433;&#20840;&#20851;&#38190;&#19978;&#19979;&#25991;&#12290;&#20998;&#26512;&#32467;&#26524;&#30830;&#23450;&#20102;&#20154;&#31867;&#30417;&#31649;&#35201;&#27714;&#30340;&#39640;&#21487;&#38752;&#24615;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#36827;&#19968;&#27493;&#34920;&#26126;AV&#21512;&#20316;&#21487;&#20197;&#23558;&#30417;&#31649;&#21487;&#38752;&#24615;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20196;&#20154;&#24847;&#24819;&#19981;&#21040;&#30340;&#38656;&#35201;&#26356;&#23569;&#30340;&#30417;&#31649;&#21592;&#65288;&#27599;&#20010;AV&#65289;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;AV&#37319;&#29992;&#12290;&#36825;&#20123;&#20998;&#26512;&#32467;&#26524;&#21033;&#29992;&#20102;&#25490;&#38431;&#29702;&#35770;&#20998;&#26512;&#12289;&#39034;&#24207;&#32479;&#35745;&#23398;&#20197;&#21450;&#20445;&#23432;&#30340;&#21453;&#24212;&#25511;&#21046;&#31574;&#30053;&#8212;&#8212;&#26102;&#38388;&#21040;&#30896;&#25758;&#36991;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in autonomy offer the potential for dramatic positive outcomes in a number of domains, yet enabling their safe deployment remains an open problem. This work's motivating question is: In safety-critical settings, can we avoid the need to have one human supervise one machine at all times? The work formalizes this scalable supervision problem by considering remotely located human supervisors and investigating how autonomous agents can cooperate to achieve safety. This article focuses on the safety-critical context of autonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs and human drivers. The analysis establishes high reliability upper bounds on human supervision requirements. It further shows that AV cooperation can improve supervision reliability by orders of magnitude and counterintuitively requires fewer supervisors (per AV) as more AVs are adopted. These analytical results leverage queuing-theoretic analysis, order statistics, and a conservative, reac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33192;&#32960;&#21367;&#31215;(DC)&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#25216;&#26415;&#23398;&#20064;&#24863;&#21463;&#37326;&#20013;&#38750;&#38646;&#20803;&#32032;&#20301;&#32622;&#20043;&#38388;&#30340;&#38388;&#38548;&#65292;&#20174;&#32780;&#22686;&#21152;&#24863;&#21463;&#37326;&#22823;&#23567;&#32780;&#19981;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2112.03740</link><description>&lt;p&gt;
&#24102;&#26377;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#33192;&#32960;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Dilated convolution with learnable spacings. (arXiv:2112.03740v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33192;&#32960;&#21367;&#31215;(DC)&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#25216;&#26415;&#23398;&#20064;&#24863;&#21463;&#37326;&#20013;&#38750;&#38646;&#20803;&#32032;&#20301;&#32622;&#20043;&#38388;&#30340;&#38388;&#38548;&#65292;&#20174;&#32780;&#22686;&#21152;&#24863;&#21463;&#37326;&#22823;&#23567;&#32780;&#19981;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#38656;&#35201;&#25317;&#26377;&#36739;&#22823;&#30340;&#24863;&#21463;&#37326;&#25165;&#33021;&#19982;&#35270;&#35273;Transformer&#21644;&#20854;&#27880;&#24847;&#26426;&#21046;&#31454;&#20105;&#12290;&#22312;CNN&#20013;&#65292;&#24863;&#21463;&#37326;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#21367;&#31215;&#26680;&#22823;&#23567;&#26469;&#31616;&#21333;&#22320;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;2D&#24773;&#20917;&#19979;&#65292;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#19982;&#26680;&#22823;&#23567;&#25104;&#20108;&#27425;&#27604;&#20363;&#65292;&#36805;&#36895;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#24182;&#19988;&#35757;&#32451;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#24863;&#21463;&#37326;&#30340;&#22823;&#23567;&#65292;&#32780;&#19981;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#33192;&#32960;&#21367;&#31215;(DC)&#24050;&#32463;&#20026;&#21516;&#26679;&#30340;&#30446;&#30340;&#25552;&#20986;&#12290;DC&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#26680;&#20165;&#21253;&#21547;&#23569;&#25968;&#38750;&#38646;&#20803;&#32032;&#25490;&#21015;&#22312;&#19968;&#20010;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;&#21367;&#31215;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DC&#30340;&#19968;&#31181;&#26032;&#29256;&#26412;&#65292;&#20854;&#20013;&#38750;&#38646;&#20803;&#32032;&#20043;&#38388;&#30340;&#38388;&#38548;&#65292;&#25110;&#31561;&#25928;&#22320;&#35828;&#65292;&#23427;&#20204;&#30340;&#20301;&#32622;&#65292;&#19981;&#20877;&#22266;&#23450;&#65292;&#32780;&#26159;&#21487;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#65292;&#20511;&#21161;&#25554;&#20540;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#24102;&#26377;&#21487;&#23398;&#20064;&#38388;&#38548;&#30340;&#33192;&#32960;&#21367;&#31215;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works indicate that convolutional neural networks (CNN) need large receptive fields (RF) to compete with visual transformers and their attention mechanism. In CNNs, RFs can simply be enlarged by increasing the convolution kernel sizes. Yet the number of trainable parameters, which scales quadratically with the kernel's size in the 2D case, rapidly becomes prohibitive, and the training is notoriously difficult. This paper presents a new method to increase the RF size without increasing the number of parameters. The dilated convolution (DC) has already been proposed for the same purpose. DC can be seen as a convolution with a kernel that contains only a few non-zero elements placed on a regular grid. Here we present a new version of the DC in which the spacings between the non-zero elements, or equivalently their positions, are no longer fixed but learnable via backpropagation thanks to an interpolation technique. We call this method "Dilated Convolution with Learnable Spacings" (
&lt;/p&gt;</description></item></channel></rss>