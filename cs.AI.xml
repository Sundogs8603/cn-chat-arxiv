<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#39640;&#32467;&#26524;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#21161;&#20110;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20294;&#22312;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#20381;&#36182;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04470</link><description>&lt;p&gt;
&#20851;&#20110;&#32467;&#26524;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Actionability of Outcome Prediction. (arXiv:2309.04470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#32467;&#26524;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#21161;&#20110;&#30830;&#23450;&#26368;&#36866;&#21512;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20294;&#22312;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#20381;&#36182;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26080;&#27861;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#24433;&#21709;&#39046;&#22495;&#65292;&#39044;&#27979;&#26410;&#26469;&#32467;&#26524;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#35265;&#24212;&#29992;&#12290;&#20363;&#23376;&#20174;&#39044;&#27979;&#25945;&#32946;&#20013;&#23398;&#29983;&#30340;&#25104;&#21151;&#21040;&#39044;&#27979;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#30142;&#30149;&#39118;&#38505;&#12290;&#20174;&#23454;&#36341;&#32773;&#26469;&#30475;&#65292;&#26368;&#32456;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#32780;&#26159;&#26377;&#25928;&#22320;&#34892;&#21160;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#20165;&#20165;&#20381;&#38752;&#32467;&#26524;&#39044;&#27979;&#36827;&#34892;&#19979;&#28216;&#24178;&#39044;&#21487;&#33021;&#19981;&#20250;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#22312;&#22823;&#22810;&#25968;&#39046;&#22495;&#20013;&#65292;&#27599;&#20010;&#20010;&#20307;&#23384;&#22312;&#22810;&#31181;&#21487;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#36825;&#20351;&#24471;&#37319;&#21462;&#26377;&#25928;&#34892;&#21160;&#30340;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#12290;&#21363;&#20351;&#36830;&#25509;&#20010;&#20307;&#28508;&#22312;&#29366;&#24577;&#19982;&#32467;&#26524;&#30340;&#22240;&#26524;&#26426;&#21046;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#23454;&#20363;&#65288;&#29305;&#23450;&#30340;&#23398;&#29983;&#25110;&#24739;&#32773;&#65289;&#20013;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#38656;&#35201;&#20174;&#39044;&#31639;&#27979;&#37327;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#25512;&#26029;&#20986;&#21738;&#31181;&#21487;&#33021;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#36825;&#20010;&#20010;&#20307;&#26368;&#26377;&#25928;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24819;&#38382;&#65306;&#20934;&#30830;&#30340;&#32467;&#26524;&#39044;&#27979;&#20309;&#26102;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#21512;&#36866;&#30340;&#24178;&#39044;&#25514;&#26045;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting future outcomes is a prevalent application of machine learning in social impact domains. Examples range from predicting student success in education to predicting disease risk in healthcare. Practitioners recognize that the ultimate goal is not just to predict but to act effectively. Increasing evidence suggests that relying on outcome predictions for downstream interventions may not have desired results.  In most domains there exists a multitude of possible interventions for each individual, making the challenge of taking effective action more acute. Even when causal mechanisms connecting the individual's latent states to outcomes is well understood, in any given instance (a specific student or patient), practitioners still need to infer -- from budgeted measurements of latent states -- which of many possible interventions will be most effective for this individual. With this in mind, we ask: when are accurate predictors of outcomes helpful for identifying the most suitable
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04459</link><description>&lt;p&gt;
&#23376;&#35789;&#20316;&#20026;&#25216;&#24039;&#65306;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#35789;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#20855;&#26377;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#38271;&#26399;&#30340;&#12289;&#21327;&#35843;&#30340;&#34892;&#21160;&#24207;&#21015;&#25165;&#33021;&#33719;&#24471;&#20219;&#20309;&#22870;&#21169;&#12290;&#32780;&#19988;&#65292;&#22312;&#36830;&#32493;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#21487;&#33021;&#30340;&#34892;&#21160;&#25968;&#37327;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#36825;&#21482;&#20250;&#22686;&#21152;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#31867;&#26041;&#27861;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#25910;&#38598;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#24418;&#25104;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#34892;&#21160;&#65292;&#36890;&#24120;&#31216;&#20026;&#25216;&#24039;&#65292;&#24182;&#22312;&#36825;&#20010;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#36890;&#24120;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#38656;&#35201;&#19968;&#20010;&#28459;&#38271;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#24320;&#22987;&#20043;&#21069;&#24418;&#25104;&#25216;&#24039;&#12290;&#37492;&#20110;&#20808;&#21069;&#30340;&#35777;&#25454;&#34920;&#26126;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24182;&#19981;&#38656;&#35201;&#23436;&#25972;&#30340;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#24039;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20511;&#37492;&#26469;&#30340;&#20998;&#35789;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
&lt;/p&gt;</description></item><item><title>&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#26377;&#25928;&#34920;&#31034;&#22797;&#26434;&#25299;&#25169;&#30446;&#26631;&#20998;&#24067;&#21644;&#22788;&#29702;&#20808;&#39564;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#30456;&#20851;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#25918;&#23485;&#20102;&#27491;&#35268;&#21270;&#27969;&#30340;&#32422;&#26463;&#24615;&#65292;&#20197;&#24179;&#34913;&#32422;&#26463;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;</title><link>http://arxiv.org/abs/2309.04433</link><description>&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#30340;&#21464;&#21270;&#21644;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Variations and Relaxations of Normalizing Flows. (arXiv:2309.04433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04433
&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#26377;&#25928;&#34920;&#31034;&#22797;&#26434;&#25299;&#25169;&#30446;&#26631;&#20998;&#24067;&#21644;&#22788;&#29702;&#20808;&#39564;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#30456;&#20851;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#25918;&#23485;&#20102;&#27491;&#35268;&#21270;&#27969;&#30340;&#32422;&#26463;&#24615;&#65292;&#20197;&#24179;&#34913;&#32422;&#26463;&#21644;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#65288;NFs&#65289;&#25551;&#36848;&#20102;&#19968;&#31867;&#23558;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#34920;&#31034;&#20026;&#31616;&#21333;&#22522;&#30784;&#20998;&#24067;&#30340;&#19968;&#31995;&#21015;&#21452;&#23556;&#21464;&#25442;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#20505;&#36873;&#21464;&#25442;&#31354;&#38388;&#38480;&#21046;&#20026;&#24494;&#20998;&#21516;&#32986;&#65292;NFs&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#31934;&#30830;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#26082;&#33021;&#28789;&#27963;&#22320;&#20316;&#20026;&#21028;&#21035;&#27169;&#22411;&#65292;&#21448;&#33021;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24494;&#20998;&#21516;&#32986;&#30340;&#38480;&#21046;&#24378;&#21046;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#25152;&#26377;&#20013;&#38388;&#31354;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#32500;&#25968;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#34920;&#31034;&#20855;&#26377;&#22797;&#26434;&#25299;&#25169;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#39564;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#19981;&#21516;&#32986;&#30340;&#24773;&#20917;&#19979;&#65292;&#27491;&#35268;&#21270;&#27969;&#21487;&#33021;&#20250;&#23558;&#36136;&#37327;&#27844;&#28431;&#21040;&#30446;&#26631;&#25903;&#25345;&#20043;&#22806;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#23558;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65288;&#22914;VAEs&#21644;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#65289;&#30340;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;NFs&#30340;&#20005;&#26684;&#21452;&#23556;&#32422;&#26463;&#65292;&#20197;&#23454;&#29616;&#24179;&#34913;&#32422;&#26463;&#19982;&#28789;&#27963;&#24615;&#20043;&#38388;&#30340;&#25240;&#20013;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows (NFs) describe a class of models that express a complex target distribution as the composition of a series of bijective transformations over a simpler base distribution. By limiting the space of candidate transformations to diffeomorphisms, NFs enjoy efficient, exact sampling and density evaluation, enabling NFs to flexibly behave as both discriminative and generative models. Their restriction to diffeomorphisms, however, enforces that input, output and all intermediary spaces share the same dimension, limiting their ability to effectively represent target distributions with complex topologies. Additionally, in cases where the prior and target distributions are not homeomorphic, Normalizing Flows can leak mass outside of the support of the target. This survey covers a selection of recent works that combine aspects of other generative model classes, such as VAEs and score-based diffusion, and in doing so loosen the strict bijectivity constraints of NFs to achieve a bal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32456;&#36523;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;L2DM&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#30693;&#35782;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#21644;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#35821;&#20041;&#8220;&#28798;&#38590;&#24615;&#24573;&#35270;&#8221;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#36755;&#20837;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.04430</link><description>&lt;p&gt;
&#21019;&#36896;&#20320;&#30340;&#19990;&#30028;&#65306;&#32456;&#36523;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Create Your World: Lifelong Text-to-Image Diffusion. (arXiv:2309.04430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32456;&#36523;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;L2DM&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#30693;&#35782;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#21644;&#25991;&#26412;&#25552;&#31034;&#20013;&#30340;&#35821;&#20041;&#8220;&#28798;&#38590;&#24615;&#24573;&#35270;&#8221;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#36755;&#20837;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#27010;&#24565;&#22270;&#20687;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22270;&#20687;&#32763;&#35793;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20248;&#31168;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#20197;&#19968;&#31181;&#27704;&#26080;&#27490;&#22659;&#30340;&#26041;&#24335;&#21512;&#25104;&#29992;&#25143;&#33258;&#24049;&#27010;&#24565;&#30340;&#23454;&#20363;&#21270;&#38382;&#39064;&#65292;&#21363;&#21019;&#24314;&#20320;&#30340;&#19990;&#30028;&#65292;&#26032;&#30340;&#29992;&#25143;&#27010;&#24565;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#24555;&#36895;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;L2DM&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#36807;&#21435;&#36935;&#21040;&#30340;&#27010;&#24565;&#30340;&#30693;&#35782;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#21644;&#25991;&#26412;&#25552;&#31034;&#20013;&#19968;&#20010;&#25110;&#22810;&#20010;&#27010;&#24565;&#30340;&#35821;&#20041;&#8220;&#28798;&#38590;&#24615;&#24573;&#35270;&#8221;&#38382;&#39064;&#12290;&#22312;&#30693;&#35782;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;L2DM&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#20010;&#20219;&#21153;&#24863;&#30693;&#30340;&#35760;&#24518;&#22686;&#24378;&#27169;&#22359;&#21644;&#19968;&#20010;&#24377;&#24615;&#27010;&#24565;&#33976;&#39311;&#27169;&#22359;&#65292;&#20998;&#21035;&#20445;&#25252;&#20102;&#20808;&#21069;&#27010;&#24565;&#30340;&#30693;&#35782;&#21644;&#27599;&#20010;&#36807;&#21435;&#20010;&#24615;&#21270;&#27010;&#24565;&#30340;&#30693;&#35782;&#12290;&#22312;&#20351;&#29992;&#29992;&#25143;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26102;&#65292;&#35299;&#20915;&#26041;&#26696;&#26159;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations of a use's own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L2DM), which intends to overcome knowledge "catastrophic forgetting" for the past encountered concepts, and semantic "catastrophic neglecting" for one or more concepts in the text prompt. In respect of knowledge "catastrophic forgetting", our L2DM framework devises a task-aware memory enhancement module and a elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past personalized concept. When generating images with a user text prompt, the solution to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#36895;&#24230;&#12289;&#23454;&#26102;&#20449;&#24687;&#22788;&#29702;&#21644;&#26102;&#31354;&#20449;&#24687;&#22788;&#29702;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#32593;&#32476;&#25299;&#25169;&#30340;&#29305;&#28857;&#65292;&#24182;&#22238;&#39038;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.04426</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#36827;&#35745;&#31639;&#21644;&#30456;&#20851;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Advanced Computing and Related Applications Leveraging Brain-inspired Spiking Neural Networks. (arXiv:2309.04426v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#36895;&#24230;&#12289;&#23454;&#26102;&#20449;&#24687;&#22788;&#29702;&#21644;&#26102;&#31354;&#20449;&#24687;&#22788;&#29702;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20998;&#26512;&#20102;&#31070;&#32463;&#20803;&#27169;&#22411;&#21644;&#32593;&#32476;&#25299;&#25169;&#30340;&#29305;&#28857;&#65292;&#24182;&#22238;&#39038;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#19968;&#20195;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#30005;&#30913;&#29615;&#22659;&#30340;&#24555;&#36895;&#28436;&#21270;&#20013;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#20223;&#29983;&#29305;&#24449;&#21644;&#25239;&#24178;&#25200;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#36895;&#24230;&#12289;&#23454;&#26102;&#20449;&#24687;&#22788;&#29702;&#21644;&#26102;&#31354;&#20449;&#24687;&#22788;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#31867;&#33041;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#20043;&#19968;&#65292;&#36890;&#36807;&#27169;&#25311;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#20449;&#24687;&#20256;&#36882;&#27169;&#24335;&#23454;&#29616;&#31867;&#33041;&#35745;&#31639;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20116;&#31181;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20248;&#28857;&#12289;&#32570;&#28857;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20116;&#31181;&#32593;&#32476;&#25299;&#25169;&#30340;&#29305;&#28857;&#65307;&#28982;&#21518;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#20174;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#22522;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#35268;&#21017;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#22235;&#31181;&#31867;&#22411;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapid evolution of next-generation brain-inspired artificial intelligence and increasingly sophisticated electromagnetic environment, the most bionic characteristics and anti-interference performance of spiking neural networks show great potential in terms of computational speed, real-time information processing, and spatio-temporal information processing. Data processing. Spiking neural network is one of the cores of brain-like artificial intelligence, which realizes brain-like computing by simulating the structure and information transfer mode of biological neural networks. This paper summarizes the strengths, weaknesses and applicability of five neuronal models and analyzes the characteristics of five network topologies; then reviews the spiking neural network algorithms and summarizes the unsupervised learning algorithms based on synaptic plasticity rules and four types of supervised learning algorithms from the perspectives of unsupervised learning and supervised learning; 
&lt;/p&gt;</description></item><item><title>SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
SynthoGestures&#65306;&#19968;&#31181;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#30340;&#21512;&#25104;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios. (arXiv:2309.04421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#39046;&#22495;&#20013;&#65292;&#20026;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20840;&#38754;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;&#21183;&#65292;&#25552;&#20379;&#23450;&#21046;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#24615;&#33021;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#25311;&#19981;&#21516;&#30340;&#25668;&#20687;&#26426;&#20301;&#32622;&#21644;&#31867;&#22411;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#26426;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#26426;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;SynthoGestures&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#21487;&#20197;&#26367;&#20195;&#25110;&#22686;&#24378;&#30495;&#25163;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#20419;&#36827;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\footnote{\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.04369</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#24577;&#25968;&#25454;&#38598;&#65306;&#28145;&#24230;&#20132;&#20114;&#26041;&#27861;&#29992;&#20110;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Beyond Static Datasets: A Deep Interaction Approach to LLM Evaluation. (arXiv:2309.04369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04369
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#28608;&#21457;&#20102;&#23545;LLM&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;LLM&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#26080;&#27861;&#35780;&#20272;LLM&#22312;&#21160;&#24577;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#22330;&#26223;&#20013;&#28145;&#24230;&#20132;&#20114;&#24191;&#27867;&#23384;&#22312;&#12290;&#20854;&#20182;&#30340;LLM&#35780;&#20272;&#26041;&#27861;&#22522;&#20110;&#20154;&#24037;&#35780;&#20272;&#65292;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;LLM&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#20132;&#20114;&#30340;LLM&#35780;&#20272;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#21487;&#20197;&#35780;&#20272;LLM&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19982;&#20854;&#20182;LLM&#30340;&#28145;&#24230;&#20132;&#20114;&#20013;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#35832;&#22810;&#23454;&#38469;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made progress in various real-world tasks, which stimulates requirements for the evaluation of LLMs. Existing LLM evaluation methods are mainly supervised signal-based which depends on static datasets and cannot evaluate the ability of LLMs in dynamic real-world scenarios where deep interaction widely exists. Other LLM evaluation methods are human-based which are costly and time-consuming and are incapable of large-scale evaluation of LLMs. To address the issues above, we propose a novel Deep Interaction-based LLM-evaluation framework. In our proposed framework, LLMs' performances in real-world domains can be evaluated from their deep interaction with other LLMs in elaborately designed evaluation tasks. Furthermore, our proposed framework is a general evaluation method that can be applied to a host of real-world tasks such as machine translation and code generation. We demonstrate the effectiveness of our proposed method through extensive experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#26631;&#35760;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04367</link><description>&lt;p&gt;
&#20026;&#20998;&#31867;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Classifying 2D Grid-Based Level Completability. (arXiv:2309.04367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;2D&#22522;&#20110;&#32593;&#26684;&#30340;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#26631;&#35760;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#30001;&#31243;&#24207;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20851;&#21345;&#30340;&#21487;&#23436;&#25104;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#28041;&#21450;&#20351;&#29992;&#27714;&#35299;&#22120;&#20195;&#29702;&#26469;&#20998;&#26512;&#21644;&#35299;&#20915;&#20851;&#21345;&#65292;&#32780;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12290;&#23613;&#31649;&#20027;&#21160;&#23398;&#20064;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#21644;&#35821;&#38899;&#35782;&#21035;&#20197;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#22312;&#28216;&#25103;&#35780;&#20272;&#20013;&#65292;&#23427;&#36824;&#27809;&#26377;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#25110;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#23398;&#20064;&#20851;&#21345;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#12290;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;Super Mario Bros.&#12289;Kid Icarus&#21644;&#19968;&#20010;&#31867;&#20284;Zelda&#30340;&#28216;&#25103;&#20013;&#29983;&#25104;&#30340;&#20851;&#21345;&#36827;&#34892;&#21487;&#23436;&#25104;&#24615;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26631;&#35760;&#20851;&#21345;&#19982;&#20351;&#29992;&#38543;&#26426;&#26597;&#35810;&#26631;&#35760;&#20851;&#21345;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#26631;&#35760;&#20851;&#21345;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#32780;&#26597;&#35810;&#26041;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the completability of levels generated by procedural generators such as machine learning models can be challenging, as it can involve the use of solver agents that often require a significant amount of time to analyze and solve levels. Active learning is not yet widely adopted in game evaluations, although it has been used successfully in natural language processing, image and speech recognition, and computer vision, where the availability of labeled data is limited or expensive. In this paper, we propose the use of active learning for learning level completability classification. Through an active learning approach, we train deep-learning models to classify the completability of generated levels for Super Mario Bros., Kid Icarus, and a Zelda-like game. We compare active learning for querying levels to label with completability against random queries. Our results show using an active learning approach to label levels results in better classifier performance with the same am
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04344</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#26029;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#36827;&#34892;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32487;&#25215;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24494;&#35843;&#65292;&#20294;&#36825;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#21363;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboShot&#65292;&#19968;&#31181;&#23436;&#20840;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#23884;&#20837;&#24182;&#29992;&#20110;&#21435;&#38500;&#23884;&#20837;&#20013;&#30340;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;--&#32780;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38646;&#26679;&#26412;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#22312;&#20061;&#20010;&#22270;&#20687;&#21644;NLP&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;RoboShot&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04332</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#38656;&#35201;&#30340;&#26102;&#20505;&#20173;&#28982;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Use Graphs When They Shouldn't. (arXiv:2309.04332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#24418;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;GNNs&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#20351;&#22312;&#24573;&#30053;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24120;&#35268;&#22270;&#23545;&#20110;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#21253;&#25324;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#31561;&#65292;&#23545;&#22270;&#24418;&#36827;&#34892;&#39044;&#27979;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22270;&#24418;&#26631;&#27880;&#38382;&#39064;&#30340;&#23454;&#20363;&#21253;&#25324;&#22270;&#32467;&#26500;(&#21363;&#37051;&#25509;&#30697;&#38453;)&#21644;&#33410;&#28857;&#29305;&#23450;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#22270;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#19981;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#20363;&#22914;&#65292;&#20998;&#23376;&#24615;&#36136;&#22914;&#25705;&#23572;&#36136;&#37327;&#20165;&#20381;&#36182;&#20110;&#32452;&#25104;&#21407;&#23376;(&#33410;&#28857;&#29305;&#24449;)&#65292;&#32780;&#19982;&#20998;&#23376;&#32467;&#26500;&#26080;&#20851;&#12290;&#23613;&#31649;GNNs&#26377;&#33021;&#21147;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24573;&#30053;&#22270;&#32467;&#26500;&#65292;&#20294;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#20250;&#36825;&#26679;&#20570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GNNs&#23454;&#38469;&#19978;&#20542;&#21521;&#20110;&#22312;&#36807;&#25311;&#21512;&#22270;&#32467;&#26500;&#65292;&#21363;&#22312;&#24573;&#30053;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20173;&#22312;&#20351;&#29992;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20998;&#24067;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#21457;&#29616;&#24120;&#35268;&#22270;&#23545;&#36825;&#31181;&#36807;&#25311;&#21512;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictions over graphs play a crucial role in various domains, including social networks, molecular biology, medicine, and more. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Instances of graph labeling problems consist of the graph-structure (i.e., the adjacency matrix), along with node-specific feature vectors. In some cases, this graph-structure is non-informative for the predictive task. For instance, molecular properties such as molar mass depend solely on the constituent atoms (node features), and not on the molecular structure. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.04316</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#20114;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#37327;&#23398;&#20064;&#20154;&#24418;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models. (arXiv:2309.04316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36827;&#34892;&#39640;&#32423;&#21327;&#35843;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#24490;&#29615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23545;&#20110;&#30452;&#35266;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#29992;&#26469;&#34920;&#36798;&#20154;&#31867;&#30340;&#24847;&#22270;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#20256;&#36798;&#25351;&#20196;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#23545;&#21629;&#20196;&#30340;&#29702;&#35299;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#35201;&#36171;&#20104;&#26426;&#22120;&#20154;&#20174;&#36825;&#31181;&#20132;&#20114;&#32463;&#39564;&#20013;&#22686;&#37327;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#20351;&#23427;&#20204;&#33021;&#22815;&#25913;&#36827;&#33258;&#24049;&#30340;&#34892;&#20026;&#25110;&#36991;&#20813;&#26410;&#26469;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#28982;&#20114;&#21160;&#20013;&#23454;&#29616;&#22797;&#26434;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20854;&#23454;&#29616;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#39640;&#23618;&#21327;&#35843;&#30340;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#30340;&#24605;&#24819;&#26159;&#35753;LLM&#22312;&#20132;&#20114;&#24335;&#25511;&#21046;&#21488;&#20013;&#29983;&#25104;Python&#35821;&#21477;&#26469;&#35843;&#29992;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#25351;&#20196;&#12289;&#29615;&#22659;&#35266;&#27979;&#21644;&#25191;&#34892;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#26469;&#20851;&#38381;&#20132;&#20114;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#30340;&#26089;&#26399;&#36864;&#23398;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#20010;&#20307;&#21644;&#32452;&#32455;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.04311</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#26089;&#26399;&#36864;&#23398;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Early Dropout Prediction on Healthy Ageing Applications. (arXiv:2309.04311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#30340;&#26089;&#26399;&#36864;&#23398;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32852;&#21512;&#20010;&#20307;&#21644;&#32452;&#32455;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#38544;&#31169;&#20445;&#25252;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#25252;&#29702;&#24212;&#29992;&#30340;&#25552;&#20379;&#23545;&#20110;&#25913;&#21892;&#32769;&#24180;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#21644;&#20026;&#36816;&#33829;&#21830;&#25552;&#20379;&#26089;&#26399;&#24178;&#39044;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#20581;&#24247;&#32769;&#40836;&#24212;&#29992;&#20013;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#36864;&#23398;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30452;&#25509;&#19982;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#39640;&#24230;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#20010;&#20307;&#27169;&#24335;&#26102;&#38750;&#24120;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#23384;&#22312;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;(PII)&#21644;&#21463;&#21040;&#27861;&#35268;&#30862;&#29255;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;(FML)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20256;&#36755;&#20010;&#20307;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;FML&#19979;&#30340;&#20010;&#20307;&#21644;&#32452;&#32455;&#26469;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#36825;&#26679;&#21487;&#20197;&#23545;&#36328;&#35774;&#22791;&#21644;&#36328;&#24179;&#21488;&#23398;&#20064;&#22330;&#26223;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The provision of social care applications is crucial for elderly people to improve their quality of life and enables operators to provide early interventions. Accurate predictions of user dropouts in healthy ageing applications are essential since they are directly related to individual health statuses. Machine Learning (ML) algorithms have enabled highly accurate predictions, outperforming traditional statistical methods that struggle to cope with individual patterns. However, ML requires a substantial amount of data for training, which is challenging due to the presence of personal identifiable information (PII) and the fragmentation posed by regulations. In this paper, we present a federated machine learning (FML) approach that minimizes privacy concerns and enables distributed training, without transferring individual data. We employ collaborative training by considering individuals and organizations under FML, which models both cross-device and cross-silo learning scenarios. Our a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04296</link><description>&lt;p&gt;
&#22312;COVID-19&#26399;&#38388;&#23548;&#33322;&#19981;&#22312;&#20998;&#24067;&#33539;&#22260;&#20869;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#21644;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;COVID-19&#26399;&#38388;&#38750;&#20998;&#24067;&#26399;&#38388;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#25968;&#25454;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#38750;&#20998;&#24067;&#26399;&#38388;&#26102;&#65292;&#22914;COVID-19&#30340;&#23553;&#38145;&#26399;&#65292;&#25968;&#25454;&#20998;&#24067;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#30340;&#26126;&#26174;&#20559;&#31163;&#12290;&#26412;&#25991;&#37319;&#29992;&#21452;&#37325;&#31574;&#30053;&#65306;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#26356;&#26032;&#27169;&#22411;&#30340;&#26032;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#22312;&#24314;&#31569;&#29289;&#22806;&#37096;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#34892;&#20154;&#35745;&#25968;&#22120;&#25910;&#38598;&#30340;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#27604;&#65292;&#21518;&#32773;&#24120;&#24120;&#20250;&#36973;&#21463;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#26032;&#33719;&#24471;&#30340;&#30693;&#35782;&#24120;&#24120;&#20250;&#25273;&#21435;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#25345;&#32493;&#23398;&#20064;&#21017;&#36890;&#36807;&#20445;&#30041;&#36807;&#21435;&#30340;&#35265;&#35299;&#24182;&#25972;&#21512;&#26032;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23558;FSNet&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#22696;&#23572;&#26412;&#24066;13&#20010;&#24314;&#31569;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
&lt;/p&gt;</description></item><item><title>FIMO&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;IMO&#27700;&#24179;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#21253;&#21547;149&#20010;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04295</link><description>&lt;p&gt;
FIMO: &#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#25361;&#25112;&#24418;&#24335;&#21270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIMO: A Challenge Formal Dataset for Automated Theorem Proving. (arXiv:2309.04295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04295
&lt;/p&gt;
&lt;p&gt;
FIMO&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;IMO&#27700;&#24179;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#21253;&#21547;149&#20010;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIMO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#22269;&#38469;&#25968;&#23398;&#22885;&#26519;&#21305;&#20811;&#31454;&#36187;&#65288;IMO&#65289;&#30340;&#20837;&#22260;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#12290;FIMO&#26088;&#22312;&#20419;&#36827;IMO&#32423;&#21035;&#30340;&#39640;&#32423;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#30446;&#21069;&#19987;&#20026;Lean&#24418;&#24335;&#35821;&#35328;&#35774;&#35745;&#12290;&#23427;&#21253;&#25324;149&#20010;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#65292;&#21516;&#26102;&#38468;&#24102;&#38750;&#27491;&#24335;&#30340;&#38382;&#39064;&#25551;&#36848;&#21644;&#30456;&#24212;&#30340;&#22522;&#20110;LaTeX&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#21021;&#27493;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#22312;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;IMO&#32423;&#21035;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#32467;&#26524;&#20043;&#21069;&#36824;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#31946;&#25351;&#32441;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04292</link><description>&lt;p&gt;
&#27169;&#31946;&#25351;&#32441;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Fingerprinting Transformer Language-Models for Emotion Recognition in Conversations. (arXiv:2309.04292v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#27169;&#31946;&#25351;&#32441;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#25351;&#32441;&#24050;&#25104;&#21151;&#29992;&#20316;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#65292;&#20294;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#25216;&#26415;&#19968;&#26679;&#65292;&#22312;&#24615;&#33021;&#19978;&#24050;&#34987;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#25110;RoBERTa&#65289;&#22823;&#22823;&#36229;&#36234;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21363;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#35828;&#26126;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;ERC&#65292;&#20197;&#33719;&#24471;&#26356;&#31616;&#21333;&#21644;&#26356;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35805;&#35821;&#21450;&#20854;&#20043;&#21069;&#30340;&#23545;&#35805;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;RoBERTa&#65292;&#24182;&#33719;&#21462;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#36866;&#24212;&#30340;&#27169;&#31946;&#25351;&#32441;&#20998;&#31867;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;DailyDialog ERC&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#20351;&#29992;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Fingerprints have been successfully used as an interpretable text classification technique, but, like most other techniques, have been largely surpassed in performance by Large Pre-trained Language Models, such as BERT or RoBERTa. These models deliver state-of-the-art results in several Natural Language Processing tasks, namely Emotion Recognition in Conversations (ERC), but suffer from the lack of interpretability and explainability. In this paper, we propose to combine the two approaches to perform ERC, as a means to obtain simpler and more interpretable Large Language Models-based classifiers. We propose to feed the utterances and their previous conversational turns to a pre-trained RoBERTa, obtaining contextual embedding utterance representations, that are then supplied to an adapted Fuzzy Fingerprint classification module. We validate our approach on the widely used DailyDialog ERC benchmark dataset, in which we obtain state-of-the-art level results using a much lighter mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24207;&#21015;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#24182;&#25353;&#29031;&#20449;&#24687;&#37327;&#26368;&#22823;&#30340;&#20248;&#20808;&#32423;&#20381;&#27425;&#21457;&#36865;&#21333;&#35789;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22270;&#20687;&#20256;&#36755;&#21644;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2309.04287</link><description>&lt;p&gt;
&#36880;&#27493;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#24207;&#21015;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Sequential Semantic Generative Communication for Progressive Text-to-Image Generation. (arXiv:2309.04287v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24207;&#21015;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#24182;&#25353;&#29031;&#20449;&#24687;&#37327;&#26368;&#22823;&#30340;&#20248;&#20808;&#32423;&#20381;&#27425;&#21457;&#36865;&#21333;&#35789;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22270;&#20687;&#20256;&#36755;&#21644;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#31995;&#32479;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#38024;&#23545;&#29616;&#20170;&#26234;&#33021;&#24212;&#29992;&#65292;&#36890;&#36807;&#20256;&#36798;&#24863;&#30693;&#24847;&#20041;&#65288;&#21363;&#25991;&#26412;&#25552;&#31034;&#65289;&#21487;&#20197;&#36827;&#34892;&#25104;&#21151;&#30340;&#36890;&#20449;&#12290;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#25968;&#25454;&#30340;&#21512;&#36866;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#12289;&#29983;&#25104;&#22270;&#20687;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#26041;&#24335;&#12290;&#21033;&#29992;&#25991;&#26412;&#36824;&#21487;&#20197;&#20943;&#23569;&#19982;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#30340;&#36127;&#33655;&#12290;&#21457;&#23556;&#26426;&#36890;&#36807;&#22810;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#23558;&#23458;&#35266;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#25509;&#25910;&#22120;&#20351;&#29992;&#21453;&#21521;&#36807;&#31243;&#37325;&#24314;&#22270;&#20687;&#12290;&#25991;&#26412;&#21477;&#23376;&#20013;&#30340;&#27599;&#20010;&#35789;&#37117;&#26377;&#21508;&#33258;&#30340;&#21477;&#27861;&#35282;&#33394;&#65292;&#36127;&#36131;&#21253;&#21547;&#25991;&#26412;&#30340;&#29305;&#23450;&#20449;&#24687;&#29255;&#27573;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36890;&#20449;&#36127;&#36733;&#25928;&#29575;&#65292;&#21457;&#23556;&#26426;&#25353;&#25215;&#36733;&#26368;&#22810;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#39034;&#24207;&#20381;&#27425;&#21457;&#36865;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes new framework of communication system leveraging promising generation capabilities of multi-modal generative models. Regarding nowadays smart applications, successful communication can be made by conveying the perceptual meaning, which we set as text prompt. Text serves as a suitable semantic representation of image data as it has evolved to instruct an image or generate image through multi-modal techniques, by being interpreted in a manner similar to human cognition. Utilizing text can also reduce the overload compared to transmitting the intact data itself. The transmitter converts objective image to text through multi-model generation process and the receiver reconstructs the image using reverse process. Each word in the text sentence has each syntactic role, responsible for particular piece of information the text contains. For further efficiency in communication load, the transmitter sequentially sends words in priority of carrying the most information until re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04213</link><description>&lt;p&gt;
UQ&#22312;#SMM4H 2023&#19978;&#30340;&#35770;&#25991;&#65306;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#30340;ALEX&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media. (arXiv:2309.04213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEX&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19982;&#20844;&#20849;&#21355;&#29983;&#30456;&#20851;&#30340;&#27963;&#21160;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#30446;&#21069;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#25216;&#26415;&#28041;&#21450;&#21040;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#22914;BERT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20844;&#20849;&#21355;&#29983;&#22495;&#35757;&#32451;LLMs&#30340;&#25104;&#26412;&#23588;&#20854;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#20013;&#36825;&#31181;&#22495;&#20869;&#25968;&#25454;&#38598;&#24448;&#24448;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#24179;&#34913;&#35757;&#32451;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#35774;&#32622;&#27169;&#22411;&#30340;&#24341;&#23548;&#26041;&#24335;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ALEX&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;LLMs&#35299;&#37322;&#26426;&#21046;&#26469;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20844;&#20849;&#21355;&#29983;&#20998;&#26512;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;ALEX&#27169;&#22411;&#22312;Social Media Mining for Health 2023 &#65288;SMM4H&#65289;&#30340;&#20219;&#21153;2&#21644;&#20219;&#21153;4&#20013;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#24471;&#21040;&#20102;&#36739;&#39640;&#30340;&#35780;&#20998;[1]&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312; https:/ /github &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media becomes increasingly popular, more and more activities related to public health emerge. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). However, the costs of training in-domain LLMs for public health are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally imbalanced. To tackle these challenges, the data imbalance issue can be overcome by data augmentation and balanced training. Moreover, the ability of the LLMs can be effectively utilized by prompting the model properly. In this paper, a novel ALEX framework is proposed to improve the performance of public health analysis on social media by adopting an LLMs explanation mechanism. Results show that our ALEX model got the best performance among all submissions in both Task 2 and Task 4 with a high score in Task 1 in Social Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https:// github
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04195</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#32531;&#35299;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#23569;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#65306;&#30001;&#29305;&#23450;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#35757;&#32451;&#32593;&#32476;&#65289;&#21512;&#25104;&#30340;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#20182;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#27979;&#35797;&#32593;&#32476;&#65289;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26550;&#26500;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20849;&#21516;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#33976;&#39311;&#25968;&#25454;&#28041;&#21450;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#23481;&#37327;&#26356;&#22823;&#30340;&#32593;&#32476;&#23545;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04175</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#65292;&#23454;&#29616;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese. (arXiv:2309.04175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#36827;&#34892;&#30693;&#35782;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21487;&#38752;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#33324;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#20855;&#26377;&#20851;&#20110;&#21307;&#23398;&#20107;&#23454;&#30340;&#24187;&#35273;&#30340;&#21709;&#24212;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#20351;&#29992;LLMs&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#35843;&#25972;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#21307;&#23398;&#30693;&#35782;&#24211;&#26469;&#20351;LLMs&#33021;&#22815;&#39640;&#25928;&#25484;&#25569;&#39046;&#22495;&#30693;&#35782;&#24182;&#23454;&#29616;&#21487;&#38752;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;cMedKnowQA&#65292;&#19968;&#20010;&#20174;&#21307;&#23398;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#20013;&#25991;&#21307;&#23398;&#30693;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;cMedKnowQA&#30340;&#30693;&#35782;&#35843;&#25972;&#30340;LLMs&#65292;&#22312;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#21487;&#20197;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#25351;&#23548;&#35843;&#25972;&#26356;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;LLMs&#30340;&#39046;&#22495;&#36866;&#24212;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#26032;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;&#26080;&#35843;&#21442;&#25552;&#31034;&#20998;&#31867;&#30340;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20998;&#31867;&#36890;&#36807;&#21033;&#29992;[MASK]&#26631;&#35760;&#30340;&#36951;&#28431;&#38382;&#39064;&#24418;&#24335;&#26469;&#36866;&#24212;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23558;&#22635;&#20805;&#30340;&#26631;&#35760;&#26144;&#23556;&#21040;&#26631;&#31614;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26469;&#20943;&#23569;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#21171;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#38468;&#21152;&#21487;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#35843;&#21442;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#34920;&#31034;&#31354;&#38388;&#20013;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#39640;&#32500;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#24212;&#35813;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#22522;&#20110;&#27969;&#24418;&#30340;&#31354;&#38388;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#20869;&#31867;&#36817;&#37051;&#32422;&#26463;&#30340;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE-INC&#65289;&#65292;&#29992;&#20110;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#65292;&#23427;&#20445;&#30041;&#20102;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#20316;&#20026;&#20998;&#31867;&#30340;&#24341;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#35843;&#20248;&#65292;&#25105;&#20204;&#30340;LLE-INC&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.04146</link><description>&lt;p&gt;
NESTLE&#65306;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04146
&lt;/p&gt;
&lt;p&gt;
NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27861;&#24459;&#35265;&#35299;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#20998;&#26512;&#65292;&#38656;&#35201;&#20351;&#29992;&#25991;&#26723;&#26816;&#32034;&#24037;&#20855;&#36873;&#25321;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#65292;&#20351;&#29992;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#31995;&#32479;&#23545;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#20197;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#27599;&#20010;&#36807;&#31243;&#37117;&#38656;&#35201;&#19987;&#19994;&#24037;&#20855;&#25110;&#32534;&#31243;&#25216;&#33021;&#65292;&#28982;&#32780;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#21487;&#29992;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;IE&#65292;&#22914;&#26524;IE&#31995;&#32479;&#30340;&#26412;&#20307;&#20013;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#37027;&#20040;&#38656;&#35201;&#33258;&#24049;&#26500;&#24314;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NESTLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#12290;&#36890;&#36807;NESTLE&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;GUI&#36827;&#34892;&#32454;&#33268;&#32423;&#21035;&#30340;&#25511;&#21046;&#26469;&#21487;&#35270;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;NESTLE&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;IE&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#23558;&#21508;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#30340;&#24895;&#26223;&#21644;&#36335;&#32447;&#22270;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#36719;&#20214;&#36136;&#37327;&#12290;&#23454;&#29616;&#36825;&#20010;&#24895;&#26223;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36716;&#21464;&#65292;&#21363;&#36719;&#20214;&#24037;&#31243;2.0.</title><link>http://arxiv.org/abs/2309.04142</link><description>&lt;p&gt;
&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#65306;&#24895;&#26223;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps. (arXiv:2309.04142v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;&#36719;&#20214;&#24037;&#31243;&#20154;&#24037;&#26234;&#33021;&#30340;&#24895;&#26223;&#21644;&#36335;&#32447;&#22270;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#36719;&#20214;&#36136;&#37327;&#12290;&#23454;&#29616;&#36825;&#20010;&#24895;&#26223;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36716;&#21464;&#65292;&#21363;&#36719;&#20214;&#24037;&#31243;2.0.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#26469;&#65292;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#21644;&#25552;&#21319;&#36719;&#20214;&#36136;&#37327;&#12290;&#36807;&#21435;&#30340;&#20004;&#20010;&#21313;&#24180;&#35265;&#35777;&#20102;&#19987;&#38376;&#20026;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#26029;&#28044;&#29616;&#12290;&#36825;&#20010;&#21183;&#22836;&#24314;&#31435;&#20102;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#26368;&#27963;&#36291;&#21644;&#26368;&#21463;&#27426;&#36814;&#30340; Artificial Intelligence for Software Engineering (AI4SE) &#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20960;&#20010;&#37325;&#28857;&#12290;&#23427;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#21644;&#22238;&#39038;&#20102;AI4SE&#30340;&#21382;&#21490;&#12290;&#38543;&#21518;&#65292;&#23427;&#24378;&#35843;&#20102;AI4SE&#22266;&#26377;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24378;&#35843;&#20102;&#23454;&#29616;&#21487;&#20449;&#21644;&#21327;&#21516;&#30340;AI4SE&#30340;&#38656;&#27714;&#12290;&#36827;&#19968;&#27493;&#65292;&#26412;&#25991;&#25551;&#32472;&#20102;&#19968;&#31181;&#24895;&#26223;&#65292;&#21363;&#22914;&#26524;&#20811;&#26381;&#20102;AI4SE&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#23558;&#23454;&#29616;&#21487;&#36798;&#21040;&#30340;&#28508;&#22312;&#39134;&#36291;&#65292;&#24314;&#35758;&#36716;&#21521;&#36719;&#20214;&#24037;&#31243;2.0.
&lt;/p&gt;
&lt;p&gt;
For decades, much software engineering research has been dedicated to devising automated solutions aimed at enhancing developer productivity and elevating software quality. The past two decades have witnessed an unparalleled surge in the development of intelligent solutions tailored for software engineering tasks. This momentum established the Artificial Intelligence for Software Engineering (AI4SE) area, which has swiftly become one of the most active and popular areas within the software engineering field.  This Future of Software Engineering (FoSE) paper navigates through several focal points. It commences with a succinct introduction and history of AI4SE. Thereafter, it underscores the core challenges inherent to AI4SE, particularly highlighting the need to realize trustworthy and synergistic AI4SE. Progressing, the paper paints a vision for the potential leaps achievable if AI4SE's key challenges are surmounted, suggesting a transition towards Software Engineering 2.0. Two strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20256;&#24863;&#22120;&#23398;&#20064;&#28014;&#21160;&#22522;&#24213;&#26426;&#22120;&#20154;&#22806;&#37096;&#20851;&#33410;&#21147;&#30697;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#32593;&#32476;&#22312;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#21644;&#25509;&#35302;&#21147;&#30697;&#26041;&#38754;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#23454;&#29616;&#31283;&#23450;&#27493;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.04138</link><description>&lt;p&gt;
&#28014;&#21160;&#22522;&#24213;&#26426;&#22120;&#20154;&#30340;&#26412;&#20307;&#24863;&#30693;&#22806;&#21147;&#23398;&#20064;&#21450;&#20854;&#22312;&#20154;&#24418;&#26426;&#22120;&#20154;&#27493;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion. (arXiv:2309.04138v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20256;&#24863;&#22120;&#23398;&#20064;&#28014;&#21160;&#22522;&#24213;&#26426;&#22120;&#20154;&#22806;&#37096;&#20851;&#33410;&#21147;&#30697;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#32593;&#32476;&#22312;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#21644;&#25509;&#35302;&#21147;&#30697;&#26041;&#38754;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#29992;&#20110;&#23454;&#29616;&#31283;&#23450;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23454;&#29616;&#20154;&#24418;&#26426;&#22120;&#20154;&#21644;&#20197;&#23433;&#20840;&#20026;&#23548;&#21521;&#30340;&#26426;&#22120;&#20154;&#30340;&#31283;&#23450;&#27493;&#24577;&#65292;&#20272;&#35745;&#22806;&#37096;&#20851;&#33410;&#21147;&#30697;&#21644;&#25509;&#35302;&#21147;&#30697;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#21147;&#30697;&#20256;&#24863;&#22120;&#65288;FTS&#65289;&#27979;&#37327;&#20154;&#24418;&#26426;&#22120;&#20154;&#33050;&#37096;&#30340;&#25509;&#35302;&#21147;&#30697;&#65292;&#20294;FTS&#20250;&#22686;&#21152;&#31995;&#32479;&#30340;&#25104;&#26412;&#12289;&#24815;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#25925;&#38556;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#24863;&#30693;&#20256;&#24863;&#22120;&#65288;&#32534;&#30721;&#22120;&#21644;IMU&#65289;&#23398;&#20064;&#28014;&#21160;&#22522;&#24213;&#26426;&#22120;&#20154;&#22806;&#37096;&#20851;&#33410;&#21147;&#30697;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;GRU&#32593;&#32476;&#24182;&#25910;&#38598;&#20102;&#38543;&#26426;&#34892;&#36208;&#25968;&#25454;&#12290;&#30495;&#23454;&#26426;&#22120;&#20154;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65288;&#20855;&#26377;&#25705;&#25830;&#24314;&#27169;&#30340;&#21160;&#37327;&#35266;&#23519;&#22120;&#65289;&#30456;&#27604;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26356;&#23567;&#22320;&#20272;&#35745;&#22806;&#37096;&#21147;&#30697;&#21644;&#25509;&#35302;&#21147;&#30697;&#12290;&#35813;&#30740;&#31350;&#36824;&#39564;&#35777;&#20102;&#20272;&#35745;&#30340;&#25509;&#35302;&#21147;&#30697;&#21487;&#20197;&#29992;&#20110;&#38646;&#21147;&#30697;&#28857;&#65288;ZMP&#65289;&#21453;&#39304;&#25511;&#21046;&#65292;&#23454;&#29616;&#31283;&#23450;&#27493;&#34892;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26426;&#22120;&#20154;&#30340;&#33050;&#37096;&#21644;&#19978;&#21322;&#36523;&#30340;&#24815;&#24615;&#23384;&#22312;&#21464;&#21270;&#65292;&#35813;&#26041;&#27861;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that the network can estimate the external torque and contact wrench with significantly smaller errors compared to the model-based method, momentum observer (MOB) with friction modeling. The study also validates that the estimated contact wrench can be utilized for zero moment point (ZMP) feedback control, enabling stable walking. Moreover, even when the robot's feet and the inertia of the upper body are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#28857;&#20113;&#36716;&#25442;&#22120;&#26694;&#26550;&#29992;&#20110;3D&#30446;&#26631;&#26816;&#27979;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25237;&#31080;&#25552;&#35758;&#27169;&#22359;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#32593;&#32476;&#30340;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#38656;&#27714;</title><link>http://arxiv.org/abs/2309.04105</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#28857;&#20113;&#36716;&#25442;&#22120;&#29992;&#20110;3D&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Point Clouds Transformer for 3D Object Detection. (arXiv:2309.04105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#28857;&#20113;&#36716;&#25442;&#22120;&#26694;&#26550;&#29992;&#20110;3D&#30446;&#26631;&#26816;&#27979;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25237;&#31080;&#25552;&#35758;&#27169;&#22359;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#32593;&#32476;&#30340;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22330;&#26223;&#29702;&#35299;&#20013;&#65292;&#38656;&#35201;&#23545;3D&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#28857;&#20113;&#36716;&#25442;&#22120;&#26694;&#26550;&#29992;&#20110;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#20854;&#30446;&#30340;&#26159;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#30417;&#30563;&#37327;&#65292;&#22240;&#20026;&#26631;&#27880;3D&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#25237;&#31080;&#25552;&#35758;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23398;&#20064;&#20102;&#38543;&#26426;&#39044;&#35774;&#30340;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#25237;&#31080;&#32593;&#32476;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35774;&#38170;&#28857;&#12290;&#28982;&#21518;&#65292;&#23427;&#23558;&#20449;&#24687;&#33976;&#39311;&#21040;&#23398;&#29983;&#21644;&#25945;&#24072;&#32593;&#32476;&#20013;&#12290;&#22312;&#23398;&#29983;&#32593;&#32476;&#26041;&#38754;&#65292;&#25105;&#20204;&#24212;&#29992;ResNet&#32593;&#32476;&#26469;&#26377;&#25928;&#25552;&#21462;&#23616;&#37096;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#21487;&#33021;&#20250;&#20002;&#22833;&#22823;&#37327;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#25552;&#20379;&#23558;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#32467;&#21512;&#20026;&#23398;&#29983;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#21462;&#20840;&#23616;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;ResNet&#23618;&#25552;&#21462;&#21306;&#22495;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
The annotation of 3D datasets is required for semantic-segmentation and object detection in scene understanding. In this paper we present a framework for the weakly supervision of a point clouds transformer that is used for 3D object detection. The aim is to decrease the required amount of supervision needed for training, as a result of the high cost of annotating a 3D datasets. We propose an Unsupervised Voting Proposal Module, which learns randomly preset anchor points and uses voting network to select prepared anchor points of high quality. Then it distills information into student and teacher network. In terms of student network, we apply ResNet network to efficiently extract local characteristics. However, it also can lose much global information. To provide the input which incorporates the global and local information as the input of student networks, we adopt the self-attention mechanism of transformer to extract global features, and the ResNet layers to extract region proposals
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LoRa&#20449;&#21495;&#30340;&#30636;&#26102;&#39057;&#29575;&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#20302;&#21151;&#29575;&#36890;&#20449;&#20449;&#21495;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#38750;&#39564;&#35777;&#25915;&#20987;&#32773;&#23545;LoRa&#21327;&#35758;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.04088</link><description>&lt;p&gt;
&#20302;&#21151;&#29575;&#36890;&#20449;&#20449;&#21495;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#31867;&#26041;&#27861;&#65306;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#30340;&#38750;&#39564;&#35777;&#29992;&#25143; (arXiv:2309.04088v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Data-driven classification of low-power communication signals by an unauthenticated user using a software-defined radio. (arXiv:2309.04088v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LoRa&#20449;&#21495;&#30340;&#30636;&#26102;&#39057;&#29575;&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#20302;&#21151;&#29575;&#36890;&#20449;&#20449;&#21495;&#30340;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#38750;&#39564;&#35777;&#25915;&#20987;&#32773;&#23545;LoRa&#21327;&#35758;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#35268;&#27169;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#36807;&#20302;&#21151;&#29575;&#36890;&#20449;&#32593;&#32476;&#20132;&#25442;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#32593;&#32476;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#22120;&#20197;&#26377;&#38480;&#30340;&#21151;&#29575;&#22312;&#26410;&#32463;&#35768;&#21487;&#30340;&#39057;&#35889;&#19978;&#38388;&#27463;&#24615;&#22320;&#36890;&#20449;&#29366;&#24577;&#21644;&#25511;&#21046;&#20449;&#21495;&#65292;&#23481;&#26131;&#21463;&#21040;&#31363;&#21548;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#36890;&#20449;&#21327;&#35758;LoRa&#65292;&#22914;&#26524;&#19968;&#20010;&#38750;&#39564;&#35777;&#30340;&#25915;&#20987;&#32773;&#33021;&#22815;&#25104;&#21151;&#35782;&#21035;&#30446;&#26631;&#20449;&#21495;&#30340;&#24102;&#23485;&#21644;&#25193;&#39057;&#22240;&#23376;&#65292;&#23601;&#23481;&#26131;&#21463;&#21040;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#21033;&#29992;LoRa&#20449;&#21495;&#30636;&#26102;&#39057;&#29575;&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#25105;&#20204;&#23558;&#32852;&#21512;&#25512;&#26029;&#36825;&#20004;&#20010;&#26410;&#30693;&#21442;&#25968;&#38382;&#39064;&#19982;&#20998;&#31867;&#38382;&#39064;&#30456;&#20851;&#32852;&#65292;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many large-scale distributed multi-agent systems exchange information over low-power communication networks. In particular, agents intermittently communicate state and control signals in robotic network applications, often with limited power over an unlicensed spectrum, prone to eavesdropping and denial-of-service attacks. In this paper, we argue that a widely popular low-power communication protocol known as LoRa is vulnerable to denial-of-service attacks by an unauthenticated attacker if it can successfully identify a target signal's bandwidth and spreading factor. Leveraging a structural pattern in the LoRa signal's instantaneous frequency representation, we relate the problem of jointly inferring the two unknown parameters to a classification problem, which can be efficiently implemented using neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26354;&#29575;Transformer&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#24212;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23884;&#20837;&#22270;&#20013;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.04082</link><description>&lt;p&gt;
&#23545;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#26354;&#29575;Transformer: &#25296;&#24367;&#20320;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning. (arXiv:2309.04082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26354;&#29575;Transformer&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#24212;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23884;&#20837;&#22270;&#20013;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#24448;&#24448;&#20855;&#26377;&#19981;&#36866;&#21512;&#20856;&#22411;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20998;&#23618;&#25110;&#24490;&#29615;&#32467;&#26500;&#12290;&#34429;&#28982;&#23384;&#22312;&#33021;&#22815;&#21033;&#29992;&#21452;&#26354;&#25110;&#29699;&#38754;&#31354;&#38388;&#23398;&#20064;&#26356;&#20934;&#30830;&#23884;&#20837;&#36825;&#20123;&#32467;&#26500;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23616;&#38480;&#20110;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#20351;&#24471;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#31561;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#22270;Transformer&#65292;&#21487;&#20197;&#36731;&#26494;&#24314;&#27169;&#38271;&#31243;&#20132;&#20114;&#65292;&#20294;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25299;&#23637;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23436;&#20840;&#20056;&#31215;&#31435;&#20307;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24120;&#26354;&#29575;&#31354;&#38388;&#30340;&#21464;&#25442;&#22120;&#30340;&#25512;&#24191;&#12290;&#24403;&#19982;&#26631;&#35760;&#21270;&#30340;&#22270;Transformer&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#36866;&#21512;&#36755;&#20837;&#22270;&#30340;&#26354;&#29575;&#65292;&#26080;&#38656;&#22312;&#19981;&#21516;&#26354;&#29575;&#19978;&#36827;&#34892;&#39069;&#22806;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs naturally exhibit hierarchical or cyclical structures that are unfit for the typical Euclidean space. While there exist graph neural networks that leverage hyperbolic or spherical spaces to learn representations that embed such structures more accurately, these methods are confined under the message-passing paradigm, making the models vulnerable against side-effects such as oversmoothing and oversquashing. More recent work have proposed global attention-based graph Transformers that can easily model long-range interactions, but their extensions towards non-Euclidean geometry are yet unexplored. To bridge this gap, we propose Fully Product-Stereographic Transformer, a generalization of Transformers towards operating entirely on the product of constant curvature spaces. When combined with tokenized graph Transformers, our model can learn the curvature appropriate for the input graph in an end-to-end fashion, without the need of additional tuning on different curvature i
&lt;/p&gt;</description></item><item><title>SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2309.04077</link><description>&lt;p&gt;
SayNav&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26032;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04077
&lt;/p&gt;
&lt;p&gt;
SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25512;&#29702;&#21644;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#23545;&#20110;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#26159;&#20154;&#31867;&#25152;&#20855;&#22791;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SayNav&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23545;&#26410;&#30693;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;SayNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25509;&#22320;&#26426;&#21046;&#65292;&#36880;&#27493;&#26500;&#24314;&#19968;&#20010;&#25506;&#32034;&#29615;&#22659;&#30340;3D&#22330;&#26223;&#22270;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#19988;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#39640;&#23618;&#23548;&#33322;&#35745;&#21010;&#12290;&#28982;&#21518;&#65292;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#25191;&#34892;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#65292;&#23558;&#27599;&#20010;&#35745;&#21010;&#30340;&#27493;&#39588;&#35270;&#20026;&#30701;&#36317;&#31163;&#28857;&#30446;&#26631;&#23548;&#33322;&#23376;&#20219;&#21153;&#12290;SayNav&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#19968;&#27493;&#19968;&#27493;&#30340;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#26032;&#33719;&#21462;&#30340;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#26426;&#39564;&#35777;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;SayNav&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;Koopman&#31639;&#23376;&#29702;&#35770;&#26469;&#21457;&#29616;&#21644;&#32447;&#24615;&#34920;&#31034;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#20110;&#25511;&#21046;&#12290;&#19982;&#33258;&#32534;&#30721;&#22120;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#19978;&#37117;&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2309.04074</link><description>&lt;p&gt;
&#35745;&#31639;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21457;&#29616;&#21644;&#32447;&#24615;&#34920;&#31034;&#30340;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Data-Driven Discovery and Linear Representation of Nonlinear Systems For Control. (arXiv:2309.04074v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20351;&#29992;Koopman&#31639;&#23376;&#29702;&#35770;&#26469;&#21457;&#29616;&#21644;&#32447;&#24615;&#34920;&#31034;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#20110;&#25511;&#21046;&#12290;&#19982;&#33258;&#32534;&#30721;&#22120;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#19978;&#37117;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;Koopman&#31639;&#23376;&#29702;&#35770;&#24320;&#21457;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#21644;&#32447;&#24615;&#21270;&#20197;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#36882;&#24402;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#26469;&#25511;&#21046;&#26368;&#32456;&#30340;&#32447;&#24615;&#31995;&#32479;&#12290;&#20351;&#29992;&#19968;&#20010;&#25670;&#21160;&#31995;&#32479;&#30340;&#23454;&#20363;&#26469;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#27169;&#25311;&#26469;&#39564;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on developing a data-driven framework using Koopman operator theory for system identification and linearization of nonlinear systems for control. Our proposed method presents a deep learning framework with recursive learning. The resulting linear system is controlled using a linear quadratic control. An illustrative example using a pendulum system is presented with simulations on noisy data. We show that our proposed method is trained more efficiently and is more accurate than an autoencoder baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.04069</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22240;&#26524;&#27169;&#22411;&#25512;&#26029;&#29289;&#29702;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Inferring physical laws by artificial intelligence based causal models. (arXiv:2309.04069v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#27493;&#20026;&#31185;&#23398;&#30740;&#31350;&#25171;&#24320;&#20102;&#35768;&#22810;&#26032;&#30340;&#36884;&#24452;&#65292;&#24182;&#20026;&#30693;&#35782;&#21019;&#36896;&#36807;&#31243;&#22686;&#28155;&#20102;&#26032;&#30340;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;ML&#24212;&#29992;&#20027;&#35201;&#22312;&#20851;&#32852;&#20998;&#26512;&#39046;&#22495;&#65292;&#24402;&#32467;&#20026;&#22797;&#26434;&#25968;&#25454;&#25311;&#21512;&#12290;Judea Pearl&#25351;&#20986;&#65292;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24517;&#39035;&#28041;&#21450;&#21040;&#24178;&#39044;&#21644;&#24819;&#35937;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#20219;&#20309;&#26426;&#22120;&#36741;&#21161;&#30340;&#31185;&#23398;&#21457;&#29616;&#37117;&#24517;&#39035;&#21253;&#25324;&#22240;&#26524;&#20998;&#26512;&#21644;&#24178;&#39044;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#23398;&#20064;&#30340;&#29289;&#29702;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#19981;&#20165;&#33021;&#35782;&#21035;&#30456;&#20851;&#24615;&#65292;&#36824;&#33021;&#23637;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#21407;&#21017;&#26469;&#30740;&#31350;&#19968;&#20123;&#33879;&#21517;&#29289;&#29702;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#33021;&#25214;&#20986;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The advances in Artificial Intelligence (AI) and Machine Learning (ML) have opened up many avenues for scientific research, and are adding new dimensions to the process of knowledge creation. However, even the most powerful and versatile of ML applications till date are primarily in the domain of analysis of associations and boil down to complex data fitting. Judea Pearl has pointed out that Artificial General Intelligence must involve interventions involving the acts of doing and imagining. Any machine assisted scientific discovery thus must include casual analysis and interventions. In this context, we propose a causal learning model of physical principles, which not only recognizes correlations but also brings out casual relationships. We use the principles of causal inference and interventions to study the cause-and-effect relationships in the context of some well-known physical phenomena. We show that this technique can not only figure out associations among data, but is also able
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;D&amp;D&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04062</link><description>&lt;p&gt;
3D&#21435;&#22122;&#22120;&#26159;&#22909;&#30340;2D&#25945;&#24072;&#65306;&#36890;&#36807;&#21435;&#22122;&#21644;&#36328;&#27169;&#24577;&#33976;&#39311;&#36827;&#34892;&#20998;&#23376;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation. (arXiv:2309.04062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;D&amp;D&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#20174;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#20998;&#23376;&#34920;&#31034;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#34429;&#28982;&#23384;&#22312;&#21508;&#31181;&#22522;&#20110;2D&#22270;&#24418;&#30340;&#20998;&#23376;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#26174;&#31034;&#20986;&#32479;&#35745;&#23398;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#21435;&#22122;&#20219;&#21153;&#19979;&#22522;&#20110;3D&#26500;&#35937;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#19979;&#28216;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;3D&#26500;&#35937;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#20998;&#23376;&#21407;&#23376;&#22352;&#26631;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22522;&#20110;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D&amp;D&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;3D&#21435;&#22122;&#22120;&#30340;&#34920;&#31034;&#33976;&#39311;&#21040;2D&#22270;&#24418;&#32534;&#30721;&#22120;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#21435;&#22122;&#21644;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21033;&#29992;&#20102;&#20174;&#21435;&#22122;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32780;&#19988;&#24212;&#29992;&#36215;&#26469;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, which led to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In light of this limitation, we propose D&amp;D, a self-supervised molecular representation learning framework that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04019</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#20998;&#26512;&#26159;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#25163;&#21160;&#21019;&#24314;&#30340;&#22522;&#22240;&#21151;&#33021;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#19981;&#23436;&#25972;&#21644;&#19981;&#20855;&#22791;&#29983;&#29289;&#23398;&#19978;&#19979;&#25991;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20854;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20013;&#21457;&#23637;&#20986;&#26377;&#20851;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;GPT-4&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#29992;&#24635;&#32467;&#20854;&#20849;&#35782;&#21151;&#33021;&#30340;&#21517;&#31216;&#26631;&#35760;&#22522;&#22240;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#21644;&#24341;&#25991;&#36827;&#34892;&#35777;&#23454;&#12290;&#22312;&#19982;Gene Ontology&#20013;&#30340;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;GPT-4&#22312;50%&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20102;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21017;&#24674;&#22797;&#20102;&#26356;&#19968;&#33324;&#27010;&#24565;&#30340;&#21517;&#31216;&#12290;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;&#19982;&#22522;&#22240;&#38598;&#21512;&#23500;&#38598;&#30456;&#27604;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#20854;&#25903;&#25345;&#24615;&#38472;&#36848;&#21644;&#24341;&#25991;&#22312;&#20154;&#24037;&#23457;&#26680;&#20013;&#24471;&#21040;&#20102;&#22522;&#26412;&#39564;&#35777;&#12290;&#24555;&#36895;&#32508;&#21512;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#33021;&#21147;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22122;&#22768;&#26597;&#35810;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38169;&#35823;&#27010;&#29575;&#25910;&#25947;&#26102;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#19982;Kullback-Leibler&#24046;&#24322;&#20043;&#38388;&#26377;&#23494;&#20999;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.03986</link><description>&lt;p&gt;
&#22122;&#22768;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Noisy Computing of the $\mathsf{OR}$ and $\mathsf{MAX}$ Functions. (arXiv:2309.03986v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;&#22122;&#22768;&#26597;&#35810;&#35745;&#31639;$\mathsf{OR}$&#21644;$\mathsf{MAX}$&#20989;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38169;&#35823;&#27010;&#29575;&#25910;&#25947;&#26102;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#19982;Kullback-Leibler&#24046;&#24322;&#20043;&#38388;&#26377;&#23494;&#20999;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21547;&#26377;&#22122;&#22768;&#30340;&#26597;&#35810;&#26469;&#35745;&#31639;&#19968;&#20010;&#21253;&#21547;$n$&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26597;&#35810;&#22312;&#26576;&#20010;&#22266;&#23450;&#30340;&#24050;&#30693;&#27010;&#29575;$p \in (0,1/2)$&#19979;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;$n$&#20010;&#20301;&#30340;$\mathsf{OR}$&#20989;&#25968;&#65288;&#20854;&#20013;&#26597;&#35810;&#23545;&#24212;&#20110;&#20301;&#30340;&#22122;&#22768;&#35835;&#25968;&#65289;&#21644;$n$&#20010;&#23454;&#25968;&#30340;$\mathsf{MAX}$&#20989;&#25968;&#65288;&#20854;&#20013;&#26597;&#35810;&#23545;&#24212;&#20110;&#26377;&#22122;&#22768;&#30340;&#20004;&#20004;&#27604;&#36739;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#35823;&#24046;&#27010;&#29575;$\delta = o(1)$&#36805;&#36895;&#25910;&#25947;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#36825;&#20004;&#20010;&#20989;&#25968;&#25152;&#38656;&#30340;&#26399;&#26395;&#26597;&#35810;&#25968;&#37327;&#20026;\[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] &#65292;&#20854;&#20013; $D_{\mathsf{KL}}(p \| 1-p)$&#34920;&#31034;$\mathsf{Bern}(p)$&#21644;$\mathsf{Bern}(1-p)$&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#24046;&#24322;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20004;&#20010;&#20989;&#25968;&#30340;&#19978;&#19979;&#30028;&#20013;&#37117;&#21152;&#24378;&#20102;&#23545;$p$&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing a function of $n$ variables using noisy queries, where each query is incorrect with some fixed and known probability $p \in (0,1/2)$. Specifically, we consider the computation of the $\mathsf{OR}$ function of $n$ bits (where queries correspond to noisy readings of the bits) and the $\mathsf{MAX}$ function of $n$ real numbers (where queries correspond to noisy pairwise comparisons). We show that an expected number of queries of \[ (1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p \| 1-p)} \] is both sufficient and necessary to compute both functions with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}}(p \| 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. Compared to previous work, our results tighten the dependence on $p$ in both the upper and lower bounds for the two functions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#20013;&#21019;&#24314;&#25968;&#21315;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21305;&#37197;&#25152;&#38656;&#22768;&#38899;&#30340;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#26377;&#22768;&#35835;&#29289;&#21644;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03926</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#33258;&#21160;&#21019;&#24314;&#26377;&#22768;&#35835;&#29289;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Automatic Audiobook Creation. (arXiv:2309.03926v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#20013;&#21019;&#24314;&#25968;&#21315;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#21305;&#37197;&#25152;&#38656;&#22768;&#38899;&#30340;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#26377;&#22768;&#35835;&#29289;&#21644;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#22768;&#35835;&#29289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#23398;&#20316;&#21697;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#35835;&#32773;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;&#21046;&#20316;&#12289;&#32534;&#36753;&#21644;&#21457;&#24067;&#26377;&#22768;&#35835;&#29289;&#21487;&#33021;&#38656;&#35201;&#25968;&#30334;&#23567;&#26102;&#30340;&#20154;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;&#22312;&#32447;&#30005;&#23376;&#20070;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26377;&#22768;&#35835;&#29289;&#30340;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20174;&#39033;&#30446;&#21476;&#33150;&#22561;&#30005;&#23376;&#20070;&#25910;&#34255;&#20013;&#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#25968;&#21315;&#26412;&#39640;&#36136;&#37327;&#12289;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#20070;&#65292;&#35782;&#21035;&#20986;&#36866;&#21512;&#26391;&#35835;&#30340;&#30005;&#23376;&#20070;&#20869;&#23481;&#30340;&#21512;&#36866;&#23376;&#38598;&#65292;&#24182;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#30334;&#26412;&#20070;&#31821;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#26377;&#22768;&#35835;&#29289;&#30340;&#26391;&#35835;&#36895;&#24230;&#21644;&#39118;&#26684;&#12289;&#24773;&#24863;&#35821;&#35843;&#65292;&#29978;&#33267;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#38899;&#39057;&#26469;&#21305;&#37197;&#25152;&#38656;&#30340;&#22768;&#38899;&#12290;&#36825;&#39033;&#24037;&#20316;&#36129;&#29486;&#20102;5000&#22810;&#26412;&#24320;&#25918;&#35768;&#21487;&#30340;&#26377;&#22768;&#35835;&#29289;&#20197;&#21450;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#21019;&#24314;&#33258;&#24049;&#23450;&#21046;&#30340;&#26377;&#22768;&#35835;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#20102;&#35745;&#31639;&#26102;&#38388;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#39044;&#27979;&#26368;&#20339;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.03924</link><description>&lt;p&gt;
&#20026;&#32473;&#23450;&#35745;&#31639;&#26102;&#38388;&#38480;&#21046;&#30340;&#20551;&#24067;&#23572;&#20248;&#21270;&#38382;&#39064;&#33258;&#21160;&#36873;&#25321;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Algorithm Selection for Pseudo-Boolean Optimization with Given Computational Time Limits. (arXiv:2309.03924v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#20102;&#35745;&#31639;&#26102;&#38388;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#33021;&#22815;&#39044;&#27979;&#26368;&#20339;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#35299;&#31639;&#22120;&#32452;&#21512;&#20013;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#24067;&#23572;&#28385;&#36275;&#12289;&#26053;&#34892;&#21830;&#12289;&#22270;&#30528;&#33394;&#31561;&#21508;&#31181;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#31216;&#20026;&#20803;&#27714;&#35299;&#22120;&#65292;&#23427;&#20204;&#20197;&#38382;&#39064;&#23454;&#20363;&#21644;&#35299;&#31639;&#22120;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#26368;&#20339;&#27714;&#35299;&#22120;&#24182;&#25191;&#34892;&#20197;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#24120;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20250;&#38543;&#30528;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20102;&#20219;&#20309;&#26102;&#38388;&#36873;&#25321;&#22120;&#65292;&#23427;&#32771;&#34385;&#20102;&#38382;&#39064;&#23454;&#20363;&#21644;&#29992;&#25143;&#35774;&#23450;&#30340;&#35745;&#31639;&#26102;&#38388;&#38480;&#21046;&#65292;&#24182;&#22312;&#25351;&#23450;&#30340;&#26102;&#38388;&#38480;&#21046;&#20869;&#39044;&#27979;&#26368;&#20339;&#27714;&#35299;&#22120;&#12290;&#35774;&#35745;&#24102;&#26377;&#8220;&#20219;&#20309;&#26102;&#38388;&#8221;&#29305;&#24615;&#30340;&#20803;&#27714;&#35299;&#22120;&#27604;&#19981;&#24102;&#27492;&#29305;&#24615;&#30340;&#20803;&#27714;&#35299;&#22120;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35774;&#35745;&#38024;&#23545;NP&#22256;&#38590;&#38382;&#39064;&#30340;&#20219;&#24847;&#26102;&#38388;&#20803;&#27714;&#35299;&#22120;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) techniques have been proposed to automatically select the best solver from a portfolio of solvers, based on predicted performance. These techniques have been applied to various problems, such as Boolean Satisfiability, Traveling Salesperson, Graph Coloring, and others.  These methods, known as meta-solvers, take an instance of a problem and a portfolio of solvers as input. They then predict the best-performing solver and execute it to deliver a solution. Typically, the quality of the solution improves with a longer computational time. This has led to the development of anytime selectors, which consider both the instance and a user-prescribed computational time limit. Anytime meta-solvers predict the best-performing solver within the specified time limit.  Constructing an anytime meta-solver is considerably more challenging than building a meta-solver without the "anytime" feature. In this study, we focus on the task of designing anytime meta-solvers for the NP-har
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#12290;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;</title><link>http://arxiv.org/abs/2309.03918</link><description>&lt;p&gt;
&#31649;&#29702;&#33034;&#39635;&#21050;&#28608;&#26415;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A recommender for the management of chronic pain in patients undergoing spinal cord stimulation. (arXiv:2309.03918v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#12290;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26041;&#27861;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#39635;&#21050;&#28608;&#26415;&#65288;Spinal cord stimulation&#65292;SCS&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#27835;&#30103;&#24930;&#24615;&#30140;&#30171;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#20854;&#36890;&#36807;&#26893;&#20837;&#35013;&#32622;&#21521;&#33034;&#39635;&#20256;&#36882;&#30005;&#33033;&#20914;&#65292;&#22312;&#32473;&#20104;&#36866;&#24403;&#30340;&#21050;&#28608;&#21442;&#25968;&#26102;&#65292;&#21487;&#20197;&#25513;&#30422;&#25110;&#38459;&#26029;&#30140;&#30171;&#20449;&#21495;&#12290;&#20248;&#21270;&#21050;&#28608;&#21442;&#25968;&#30340;&#36873;&#25321;&#36890;&#24120;&#22312;&#20020;&#24202;&#30001;&#21307;&#29983;&#36127;&#36131;&#65292;&#32780;&#22312;&#23478;&#24237;&#20013;&#30340;SCS&#20248;&#21270;&#21017;&#30001;&#24739;&#32773;&#33258;&#24049;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31649;&#29702;&#36827;&#34892;SCS&#30340;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#65288;CMAB&#65289;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#20026;&#24739;&#32773;&#25512;&#33616;SCS&#35774;&#32622;&#65292;&#26088;&#22312;&#25913;&#21892;&#20854;&#29366;&#20917;&#12290;&#36825;&#20123;&#25512;&#33616;&#36890;&#36807;&#25968;&#23383;&#20581;&#24247;&#29983;&#24577;&#31995;&#32479;&#30452;&#25509;&#21457;&#36865;&#32473;&#24739;&#32773;&#65292;&#24182;&#19982;&#24739;&#32773;&#30417;&#27979;&#31995;&#32479;&#32467;&#21512;&#65292;&#20026;&#24930;&#24615;&#30140;&#30171;&#24739;&#32773;&#30340;&#25972;&#20010;&#27835;&#30103;&#36807;&#31243;&#25552;&#20379;&#38381;&#29615;&#20851;&#24576;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#25509;&#21463;SCS&#26893;&#20837;&#30340;ENVISION&#24739;&#32773;&#20013;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spinal cord stimulation (SCS) is a therapeutic approach used for the management of chronic pain. It involves the delivery of electrical impulses to the spinal cord via an implanted device, which when given suitable stimulus parameters can mask or block pain signals. Selection of optimal stimulation parameters usually happens in the clinic under the care of a provider whereas at-home SCS optimization is managed by the patient. In this paper, we propose a recommender system for the management of pain in chronic pain patients undergoing SCS. In particular, we use a contextual multi-armed bandit (CMAB) approach to develop a system that recommends SCS settings to patients with the aim of improving their condition. These recommendations, sent directly to patients though a digital health ecosystem, combined with a patient monitoring system closes the therapeutic loop around a chronic pain patient over their entire patient journey. We evaluated the system in a cohort of SCS-implanted ENVISION 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.03404</link><description>&lt;p&gt;
&#36890;&#35759;&#21644;&#21442;&#32771;&#26354;&#30446;&#22312;&#28151;&#38899;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65306;&#26469;&#33258;&#19987;&#19994;&#28151;&#38899;&#24072;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#38899;&#20048;&#28151;&#38899;&#38656;&#35201;&#25216;&#26415;&#21644;&#21019;&#36896;&#21147;&#30340;&#31934;&#28251;&#65292;&#20294;&#19982;&#23458;&#25143;&#30340;&#28165;&#26224;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28151;&#38899;&#24072;&#24517;&#39035;&#29702;&#35299;&#23458;&#25143;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#65292;&#24182;&#20849;&#21516;&#21162;&#21147;&#23454;&#29616;&#25152;&#38656;&#30340;&#38899;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#33402;&#26415;&#23478;&#19982;&#24037;&#31243;&#24072;&#20043;&#38388;&#20132;&#25442;&#30340;&#28436;&#31034;&#28151;&#38899;&#31561;&#25351;&#21335;&#65292;&#36890;&#24120;&#21487;&#20197;&#36798;&#25104;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#65292;&#26377;&#26102;&#36824;&#21487;&#20197;&#20351;&#29992;&#35821;&#20041;&#26415;&#35821;&#36827;&#34892;&#35328;&#35828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30001;&#20004;&#20010;&#38454;&#27573;&#26500;&#25104;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#26088;&#22312;&#20102;&#35299;&#19987;&#19994;&#28151;&#38899;&#24072;&#22914;&#20309;&#19982;&#23458;&#25143;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20116;&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#65292;&#26088;&#22312;&#20102;&#35299;&#20182;&#20204;&#30340;&#27807;&#36890;&#31574;&#30053;&#12289;&#21019;&#36896;&#36807;&#31243;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;&#22522;&#20110;&#36825;&#20123;&#35775;&#35848;&#30340;&#25512;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#38382;&#21367;&#65292;&#24182;&#23545;22&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.02185</link><description>&lt;p&gt;
BEVTrack&#65306;&#19968;&#31181;&#38024;&#23545;&#40479;&#30640;&#22270;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#31616;&#21333;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View. (arXiv:2309.02185v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22806;&#35266;&#21464;&#21270;&#12289;&#24178;&#25200;&#29289;&#21644;&#28857;&#20113;&#30340;&#39640;&#31232;&#30095;&#24615;&#65292;&#28857;&#20113;&#20013;&#30340;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#29289;&#20307;&#36890;&#24120;&#22312;&#36830;&#32493;&#24103;&#20013;&#20445;&#25345;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#20027;&#35201;&#27700;&#24179;&#31227;&#21160;&#12290;&#36825;&#31181;&#31354;&#38388;&#36830;&#32493;&#24615;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36319;&#36394;&#22120;&#36890;&#24120;&#37319;&#29992;&#28857;&#32423;&#34920;&#31034;&#65292;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#31181;&#34920;&#31034;&#30340;&#19981;&#35268;&#21017;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#24182;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#26469;&#24314;&#31435;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;BEVTrack&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#29992;&#20110;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#22522;&#32447;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#30340;&#28857;&#20113;&#36716;&#25442;&#20026;&#24120;&#35265;&#30340;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;BEVTrack&#36890;&#36807;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#24182;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#36319;&#36394;&#30340;&#36816;&#21160;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye-View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and con
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26159;&#36731;&#24494;&#30340;&#26631;&#31614;&#22122;&#22768;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.01706</link><description>&lt;p&gt;
&#20851;&#20110;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Post-hoc GNN Explainers to Label Noise. (arXiv:2309.01706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26159;&#36731;&#24494;&#30340;&#26631;&#31614;&#22122;&#22768;&#20063;&#20250;&#20005;&#37325;&#24433;&#21709;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22266;&#26377;&#40657;&#30418;&#38480;&#21046;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#35757;&#32451;&#21518;&#30340;GNN&#34920;&#29616;&#34892;&#20026;&#30340;&#31934;&#30830;&#21644;&#28145;&#21051;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013;&#26368;&#36817;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#22312;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#26102;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#31243;&#24230;&#26631;&#31614;&#22122;&#22768;&#19979;&#21508;&#31181;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#35265;&#35299;&#65306;&#39318;&#20808;&#65292;&#21518;&#32493;GNN&#35299;&#37322;&#22120;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#26159;&#23545;GNN&#34920;&#29616;&#26080;&#20851;&#32039;&#35201;&#30340;&#36731;&#24494;&#26631;&#31614;&#22122;&#22768;&#65292;&#20063;&#20250;&#20005;&#37325;&#25439;&#23475;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#21319;&#39640;&#36880;&#28176;&#24674;&#22797;&#35299;&#37322;&#25928;&#26524;&#23637;&#24320;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05978</link><description>&lt;p&gt;
CyberForce: &#19968;&#20010;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#32531;&#35299;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05978
&lt;/p&gt;
&lt;p&gt;
CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;(IoT)&#33539;&#20363;&#30340;&#25193;&#23637;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;IoT&#35774;&#22791;&#23545;&#24694;&#24847;&#36719;&#20214;&#20107;&#20214;&#30340;&#33030;&#24369;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;(MTD)&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22686;&#24378;IoT&#35774;&#22791;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#26032;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#21644;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#36873;&#25321;&#26377;&#25928;&#30340;MTD&#25216;&#26415;&#25152;&#38656;&#30340;&#26102;&#38388;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;IoT&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CyberForce&#65292;&#19968;&#20010;&#37319;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;(FRL)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#20307;&#19988;&#20445;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;CyberForce&#32467;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#30001;&#21313;&#21488;&#30495;&#23454;IoT&#24179;&#21488;&#35774;&#22791;&#32452;&#25104;&#30340;&#32852;&#37030;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20845;&#20010;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#31232;&#30095;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02060
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#39640;&#31232;&#30095;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;&#20256;&#32479;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#25928;&#26524;&#19981;&#20339;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#65292;&#33719;&#24471;&#26082;&#39640;&#31934;&#30830;&#21448;&#39640;&#31232;&#30095;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#31038;&#21306;&#24050;&#32463;&#23545;&#20960;&#31181;&#39640;&#24615;&#33021;&#30340;&#21098;&#26525;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#31232;&#30095;&#24615;&#21644;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26631;&#20934;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#30340;&#20132;&#20114;&#20102;&#35299;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#26469;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31232;&#30095;&#22522;&#20934;&#26469;&#30740;&#31350;&#39640;&#31232;&#30095;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#30340;&#23494;&#38598;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#26159;&#27425;&#20248;&#30340;&#65292;&#23548;&#33268;&#27424;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26082;&#21487;&#20197;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#65288;&#22914;ResNet50/ImageNet&#65289;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT/GLUE&#65289;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art resul
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.10616</link><description>&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#25361;&#25112;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10616
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#21040;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#30001;&#20110;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#27169;&#22411;&#21516;&#36136;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#32852;&#37030;&#23398;&#20064;&#36890;&#24120;&#38754;&#20020;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#32593;&#32476;&#29615;&#22659;&#21644;&#30828;&#20214;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#12290;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064; (HFL) &#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#19988;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20010;&#20027;&#39064;&#36827;&#34892;&#20851;&#20110;&#30740;&#31350;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#30340;&#31995;&#32479;&#35843;&#26597;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102; HFL &#20013;&#26469;&#33258;&#20116;&#20010;&#26041;&#38754;&#30340;&#21508;&#31181;&#30740;&#31350;&#25361;&#25112;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#27169;&#22411;&#24322;&#36136;&#24615;&#12289;&#36890;&#20449;&#24322;&#36136;&#24615;&#12289;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102; HFL &#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377; HFL &#26041;&#27861;&#30340;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#20854;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10502</link><description>&lt;p&gt;
EENED&#65306;&#22522;&#20110;&#21367;&#31215;&#21464;&#21387;&#22120;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;EEG&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#65288;Transformer&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#32780;CNN&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#22914;&#38191;&#40831;&#27874;&#20043;&#31867;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#21367;&#31215;&#27169;&#22359;&#65292;EENED&#21487;&#20197;&#23398;&#20064;&#24739;&#32773;EEG&#20449;&#21495;&#29305;&#24449;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#27880;&#24847;&#21040;&#19982;&#30315;&#30187;&#23494;&#20999;&#30456;&#20851;&#30340;&#23616;&#37096;EEG&#24322;&#24120;&#31361;&#21464;&#65292;&#22914;&#23574;&#38160;&#27874;&#30340;&#20986;&#29616;&#21644;&#32531;&#24930;&#27874;&#30340;&#25955;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;Transformer&#21644;CNN&#25429;&#25417;EEG&#20449;&#21495;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#24456;&#24555;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#24773;&#20917;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#37319;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#21592;&#38656;&#35201;&#26356;&#20026;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#20197;&#21450;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.03407</link><description>&lt;p&gt;
&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24212;&#29992;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adoption of AI Technology in the Music Mixing Workflow: An Investigation. (arXiv:2304.03407v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#38899;&#20048;&#28151;&#38899;&#24037;&#20316;&#27969;&#20013;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#24773;&#20917;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#37319;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#21592;&#38656;&#35201;&#26356;&#20026;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#20197;&#21450;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#38899;&#20048;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#25512;&#21160;&#38899;&#20048;&#21019;&#20316;&#12289;&#21046;&#20316;&#21644;&#28151;&#38899;&#30340;&#37325;&#22823;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#28151;&#38899;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#24403;&#21069;&#29366;&#24577;&#20197;&#21450;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#23545;&#20854;&#37319;&#32435;&#24773;&#20917;&#12290;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12289;&#22522;&#20110;&#38382;&#21367;&#30340;&#30740;&#31350;&#21644;&#20998;&#26512;&#32593;&#32476;&#35770;&#22363;&#65292;&#26412;&#30740;&#31350;&#30830;&#35748;&#20102;&#21253;&#25324;&#19994;&#20313;&#29233;&#22909;&#32773;&#12289;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#22312;&#20869;&#30340;&#19977;&#20010;&#29992;&#25143;&#32676;&#20307;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;AI&#28151;&#38899;&#24037;&#20855;&#33021;&#22815;&#31616;&#21270;&#36807;&#31243;&#24182;&#20026;&#19994;&#20313;&#29233;&#22909;&#32773;&#25552;&#20379;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#32844;&#19994;&#19994;&#20313;&#29233;&#22909;&#32773;&#38656;&#35201;&#31934;&#30830;&#30340;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#65292;&#32780;&#19987;&#19994;&#20154;&#21592;&#38500;&#20102;&#38656;&#35201;&#25511;&#21046;&#21644;&#33258;&#23450;&#20041;&#36873;&#39033;&#22806;&#65292;&#36824;&#38656;&#35201;&#36741;&#21161;&#21644;&#21327;&#20316;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#20026;&#35774;&#35745;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#25928;&#30340;AI&#28151;&#38899;&#24037;&#20855;&#25552;&#20379;&#20102;&#31574;&#30053;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of artificial intelligence (AI) technology in the music industry is driving a significant change in the way music is being composed, produced and mixed. This study investigates the current state of AI in the mixing workflows and its adoption by different user groups. Through semi-structured interviews, a questionnaire-based study, and analyzing web forums, the study confirms three user groups comprising amateurs, pro-ams, and professionals. Our findings show that while AI mixing tools can simplify the process and provide decent results for amateurs, pro-ams seek precise control and customization options, while professionals desire control and customization options in addition to assistive and collaborative technologies. The study provides strategies for designing effective AI mixing tools for different user groups and outlines future directions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11219</link><description>&lt;p&gt;
NeTO: &#36879;&#26126;&#29289;&#20307;&#30340;&#31070;&#32463;&#37325;&#24314;&#19982;&#33258;&#36974;&#25377;&#24863;&#30693;&#25240;&#23556;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeTO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36890;&#36807;&#37319;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;NeTO&#65292;&#36890;&#36807;&#20307;&#28210;&#26579;&#20174;2D&#22270;&#20687;&#20013;&#25429;&#25417;&#22266;&#20307;&#36879;&#26126;&#29289;&#20307;&#30340;3D&#20960;&#20309;&#20307;&#12290;&#36879;&#26126;&#29289;&#20307;&#30340;&#37325;&#24314;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#19981;&#36866;&#21512;&#36890;&#29992;&#30340;&#37325;&#24314;&#25216;&#26415;&#65292;&#22240;&#20026;&#20854;&#22256;&#25200;&#20110;&#38236;&#38754;&#20809;&#20256;&#36755;&#29616;&#35937;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25240;&#23556;&#36861;&#36394;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#20248;&#21270;&#19981;&#31283;&#23450;&#21644;&#32454;&#33410;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#37319;&#29992;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#38590;&#20197;&#20248;&#21270;&#65292;&#19988;&#24573;&#30053;&#20102;&#33258;&#36974;&#25377;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#38544;&#24335;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#20316;&#20026;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36974;&#25377;&#24863;&#30693;&#30340;&#25240;&#23556;&#36861;&#36394;&#36890;&#36807;&#20307;&#28210;&#26579;&#26469;&#20248;&#21270;SDF&#22330;&#12290;&#38544;&#24335;&#34920;&#31034;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;&#38598;&#21512;&#19979;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11694</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#36319;&#36394;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#36319;&#36394;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#26469;&#33258;&#27668;&#21160;&#21147;&#30340;&#38459;&#21147;&#21644;&#21147;&#30697;&#21464;&#21270;&#26159;&#28151;&#27788;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#31934;&#30830;&#35782;&#21035;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22235;&#26059;&#32764;&#36319;&#36394;&#31995;&#32479;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#31616;&#21333;&#8220;&#24178;&#25200;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#36712;&#36857;&#36319;&#36394;&#22120;&#65292;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#19982;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;SMPC&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#27668;&#21160;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#8220;&#21463;&#38480;&#20998;&#24067;&#24335;&#24378;&#21270;&#24178;&#25200;&#20272;&#35745;&#22120;&#8221;&#65288;ConsDRED&#65289;&#20934;&#30830;&#22320;&#35782;&#21035;&#30495;&#23454;&#27668;&#21160;&#25928;&#24212;&#19982;&#20272;&#35745;&#20540;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#31616;&#21270;&#20223;&#23556;&#24178;&#25200;&#21453;&#39304;&#36827;&#34892;&#25511;&#21046;&#21442;&#25968;&#21270;&#65292;&#20197;&#20445;&#35777;&#20984;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;SMPC&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;ConsDRED&#33267;&#23569;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11137</link><description>&lt;p&gt;
Fairguard: &#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#21033;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#20844;&#27491;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#36816;&#34892;&#22312;&#35745;&#31639;&#39044;&#27979;&#26694;&#26550;&#19978;&#65292;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#30740;&#31350;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#30340;&#30495;&#23454;&#22478;&#24066;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20559;&#35265;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fairguard&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24494;&#35266;&#23618;&#38754;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20844;&#27491;&#30340;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#12290;Fairguard&#26694;&#26550;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38745;&#24577;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#36873;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#26465;&#20214;&#26469;&#20943;&#23569;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#39044;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#32452;&#20214;&#26469;&#35843;&#33410;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#29983;&#25104;&#26410;&#26469;&#30340;&#20844;&#27491;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
&lt;/p&gt;</description></item><item><title>TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.05880</link><description>&lt;p&gt;
TikTalk: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#30495;&#23454;&#19990;&#30028;&#38386;&#32842;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05880
&lt;/p&gt;
&lt;p&gt;
TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20013;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;TikTalk&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#20102;38K&#20010;&#35270;&#39057;&#65292;&#20197;&#21450;&#29992;&#25143;&#22312;&#20854;&#19979;&#21457;&#24067;&#30340;367K&#20010;&#23545;&#35805;&#12290;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#35266;&#30475;&#35270;&#39057;&#26102;&#30340;&#22810;&#27169;&#24577;&#32463;&#39564;&#36827;&#34892;&#33258;&#21457;&#24615;&#23545;&#35805;&#65292;&#36825;&#26377;&#21161;&#20110;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#38386;&#32842;&#29615;&#22659;&#12290;&#19982;&#20043;&#21069;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#20013;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#23548;&#33268;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25429;&#25417;&#20154;&#31867;&#20852;&#36259;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#12290;&#36825;&#20123;&#20107;&#23454;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;TikTalk&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#38386;&#32842;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#23545;&#35805;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#22312;&#22256;&#38590;&#25506;&#32034;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#25506;&#32034;&#22312;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13742</link><description>&lt;p&gt;
&#22312;&#22256;&#38590;&#30340;&#25506;&#32034;&#38382;&#39064;&#20013;&#35780;&#20272;&#36136;&#37327;&#22810;&#26679;&#24615;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Assessing Quality-Diversity Neuro-Evolution Algorithms Performance in Hard Exploration Problems. (arXiv:2211.13742v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#22312;&#22256;&#38590;&#25506;&#32034;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#25506;&#32034;&#22312;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#30340;&#19968;&#20010;&#36855;&#20154;&#20043;&#22788;&#22312;&#20110;&#23427;&#33021;&#20135;&#29983;&#19968;&#31995;&#21015;&#22312;&#21508;&#33258;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#30340;&#29983;&#29289;&#20307;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#26041;&#27861;&#26159;&#28789;&#24863;&#26469;&#33258;&#20110;&#36825;&#19968;&#35266;&#23519;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#26524;&#65292;&#20174;&#32709;&#33152;&#35774;&#35745;&#21040;&#26426;&#22120;&#20154;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#26469;&#35299;&#20915;&#22823;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#65292;&#22810;&#26679;&#24615;&#26412;&#36523;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#30446;&#26631;&#12290;&#22810;&#26679;&#24615;&#20063;&#21487;&#20197;&#25104;&#20026;&#22686;&#24378;&#20855;&#26377;&#27450;&#39575;&#24615;&#22870;&#21169;&#20449;&#21495;&#30340;&#20219;&#21153;&#25506;&#32034;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#34429;&#28982;QD&#31038;&#21306;&#24050;&#32463;&#28145;&#20837;&#30740;&#31350;&#20102;&#21069;&#32773;&#65292;&#20294;&#21518;&#32773;&#22312;&#25991;&#29486;&#20013;&#30456;&#23545;&#36739;&#23569;&#12290;&#25506;&#32034;&#26159;&#35768;&#22810;&#39046;&#22495;&#22914;&#24378;&#21270;&#23398;&#20064;&#20013;&#35797;&#22270;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#26680;&#24515;&#65292;QD&#26041;&#27861;&#26159;&#20811;&#26381;&#30456;&#20851;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#20449;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#22312;&#36825;&#20123;&#25511;&#21046;&#38382;&#39064;&#19978;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fascinating aspect of nature lies in its ability to produce a collection of organisms that are all high-performing in their niche. Quality-Diversity (QD) methods are evolutionary algorithms inspired by this observation, that obtained great results in many applications, from wing design to robot adaptation. Recently, several works demonstrated that these methods could be applied to perform neuro-evolution to solve control problems in large search spaces. In such problems, diversity can be a target in itself. Diversity can also be a way to enhance exploration in tasks exhibiting deceptive reward signals. While the first aspect has been studied in depth in the QD community, the latter remains scarcer in the literature. Exploration is at the heart of several domains trying to solve control problems such as Reinforcement Learning and QD methods are promising candidates to overcome the challenges associated. Therefore, we believe that standardized benchmarks exhibiting control problems in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.12693</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#25968;&#25454;&#22312;&#30005;&#21147;&#32593;&#36890;&#35759;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#24050;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#23613;&#31649;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24448;&#24448;&#19981;&#20250;&#27880;&#24847;&#21040;&#12290;&#21482;&#26377;&#24403;&#30005;&#21147;&#32593;&#19981;&#20877;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#25165;&#20250;&#29305;&#21035;&#24847;&#35782;&#21040;&#36825;&#31181;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#37325;&#22823;&#21464;&#21270;&#65292;&#22914;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;&#20809;&#20239;&#12289;&#39118;&#21147;&#28065;&#36718;&#26426;&#31561;&#65289;&#30340;&#36807;&#28193;&#20197;&#21450;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#65288;&#30005;&#21160;&#27773;&#36710;&#12289;&#23478;&#24237;&#30005;&#27744;&#31995;&#32479;&#31561;&#65289;&#30340;&#33021;&#28304;&#28040;&#36153;&#32773;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32473;&#30005;&#21147;&#32593;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#65288;PLC&#65289;&#22522;&#30784;&#35774;&#26045;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598; FiN-1 &#21644; FiN-2 &#22312;&#24503;&#22269;&#20302;&#21387;&#30005;&#32593;&#30340;&#19968;&#37096;&#20998;&#23454;&#38469;&#20351;&#29992;&#20013;&#25910;&#38598;&#65292;&#21521;&#22823;&#32422;440&#19975;&#20154;&#25552;&#20379;&#26381;&#21153;&#65292;&#24182;&#26174;&#31034;5100&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#36229;&#36807;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#29992;&#20363;&#65292;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.12880</link><description>&lt;p&gt;
&#20855;&#26377;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#20854;&#21508;&#20010;&#29289;&#20307;&#30340;&#36523;&#20221;&#21644;&#23039;&#21183;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#30340;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#30690;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;(VSA)&#30340;&#35745;&#31639;&#26694;&#26550;&#65307;&#65288;2&#65289;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#20013;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#38750;&#21487;&#20132;&#25442;&#24615;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;&#65288;HRN&#65289;&#30340;&#35774;&#35745;&#65292;&#24403;&#20004;&#32773;&#32467;&#21512;&#20351;&#29992;&#26102;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#12290;VSA&#26694;&#26550;&#20351;&#29992;&#30690;&#37327;&#32465;&#23450;&#25805;&#20316;&#26469;&#20135;&#29983;&#29983;&#25104;&#24335;&#22270;&#20687;&#27169;&#22411;&#65292;&#20854;&#20013;&#32465;&#23450;&#20316;&#20026;&#20960;&#20309;&#21464;&#25442;&#30340;&#31561;&#21464;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22330;&#26223;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#21521;&#37327;&#20056;&#31215;&#30340;&#21644;&#65292;&#32780;&#36825;&#20123;&#21521;&#37327;&#20056;&#31215;&#21487;&#20197;&#36890;&#36807;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22240;&#24335;&#20998;&#35299;&#26469;&#39640;&#25928;&#22320;&#25512;&#26029;&#29289;&#20307;&#21644;&#23427;&#20204;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding a visual scene by inferring identities and poses of its individual objects is still and open problem. Here we propose a neuromorphic solution that utilizes an efficient factorization network based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued resonator networks on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN ena
&lt;/p&gt;</description></item><item><title>TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.02760</link><description>&lt;p&gt;
TREE-G:&#20915;&#31574;&#26641;&#23545;&#25239;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREE-G: Decision Trees Contesting Graph Neural Networks. (arXiv:2207.02760v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02760
&lt;/p&gt;
&lt;p&gt;
TREE-G&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#20462;&#25913;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#35010;&#20989;&#25968;&#21644;&#25351;&#38024;&#26426;&#21046;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#20110;&#24102;&#26377;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;&#26131;&#20110;&#24212;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20915;&#31574;&#26641;&#24182;&#23558;&#25299;&#25169;&#20449;&#24687;&#19982;&#22270;&#30340;&#39030;&#28857;&#19978;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#32467;&#21512;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TREE-G&#12290;TREE-G&#20462;&#25913;&#20102;&#26631;&#20934;&#20915;&#31574;&#26641;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#26032;&#22411;&#20998;&#35010;&#20989;&#25968;&#12290;&#36825;&#20010;&#20998;&#35010;&#20989;&#25968;&#19981;&#20165;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#38024;&#26426;&#21046;&#65292;&#20801;&#35768;&#20998;&#35010;&#33410;&#28857;&#20351;&#29992;&#22312;&#20808;&#21069;&#20998;&#35010;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20998;&#35010;&#20989;&#25968;&#33021;&#22815;&#36866;&#24212;&#39044;&#27979;&#20219;&#21153;&#21644;&#24403;&#21069;&#30340;&#22270;&#12290;&#25105;&#20204;&#23545;TREE-G&#30340;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#21644;&#39030;&#28857;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that incorporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks
&lt;/p&gt;</description></item><item><title>NMA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#26426;&#21046;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32771;&#34385;&#24191;&#21578;&#23637;&#31034;&#39034;&#24207;&#21644;&#20301;&#32622;&#23545;&#29992;&#25143;&#28857;&#20987;&#30340;&#24433;&#21709;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#20840;&#23616;&#30340;&#22806;&#37096;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10018</link><description>&lt;p&gt;
NMA: &#31070;&#32463;&#32593;&#32476;&#22810;&#27133;&#24191;&#21578;&#25293;&#21334;&#24102;&#26377;&#22806;&#37096;&#24615;
&lt;/p&gt;
&lt;p&gt;
NMA: Neural Multi-slot Auctions with Externalities for Online Advertising. (arXiv:2205.10018v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10018
&lt;/p&gt;
&lt;p&gt;
NMA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#26426;&#21046;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32771;&#34385;&#24191;&#21578;&#23637;&#31034;&#39034;&#24207;&#21644;&#20301;&#32622;&#23545;&#29992;&#25143;&#28857;&#20987;&#30340;&#24433;&#21709;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#20840;&#23616;&#30340;&#22806;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#25293;&#21334;&#20026;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#21644;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#24102;&#26469;&#20102;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#25910;&#20837;&#12290;GSP&#25293;&#21334;&#26426;&#21046;&#24050;&#25104;&#20026;&#34892;&#19994;&#20013;&#24191;&#21578;&#25293;&#21334;&#26426;&#21046;&#30340;&#22522;&#20934;&#65292;&#20294;&#22823;&#22810;&#25968;&#22522;&#20110;GSP&#30340;&#24037;&#19994;&#23454;&#36341;&#20551;&#35774;&#29992;&#25143;&#28857;&#20987;&#20165;&#19982;&#24191;&#21578;&#26412;&#36523;&#26377;&#20851;&#65292;&#24573;&#35270;&#20102;&#22806;&#37096;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25293;&#21334;&#26426;&#21046;NMA&#65292;&#23427;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#32771;&#34385;&#24191;&#21578;&#23637;&#31034;&#39034;&#24207;&#21644;&#20301;&#32622;&#23545;&#29992;&#25143;&#28857;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20840;&#23616;&#30340;&#22806;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online advertising driven by auctions brings billions of dollars in revenue for social networking services and e-commerce platforms. GSP auctions, which are simple and easy to understand for advertisers, have almost become the benchmark for ad auction mechanisms in the industry. However, most GSP-based industrial practices assume that the user click only relies on the ad itself, which overlook the effect of external items, referred to as externalities. Recently, DNA has attempted to upgrade GSP with deep neural networks and models local externalities to some extent. However, it only considers set-level contexts from auctions and ignores the order and displayed position of ads, which is still suboptimal. Although VCG-based multi-slot auctions (e.g., VCG, WVCG) make it theoretically possible to model global externalities (e.g., the order and positions of ads and so on), they lack an efficient balance of both revenue and social welfare. In this paper, we propose novel auction mechanisms n
&lt;/p&gt;</description></item></channel></rss>