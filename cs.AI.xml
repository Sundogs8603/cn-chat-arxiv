<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#30005;&#21830;&#24191;&#21578;&#31995;&#32479;&#20013;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09107</link><description>&lt;p&gt;
&#30005;&#21830;&#20013;&#20248;&#21270;&#36190;&#21161;&#20135;&#21697;&#30340;&#23454;&#36341;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Practical Lessons on Optimizing Sponsored Products in eCommerce. (arXiv:2304.09107v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#30005;&#21830;&#24191;&#21578;&#31995;&#32479;&#20013;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#22522;&#20110;&#20301;&#32622;&#30340;&#21435;&#20559;&#24046;&#12289;&#28857;&#20987;-&#36716;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65288;&#21253;&#25324;&#27973;&#23618;&#27169;&#22411;&#65292;&#22914;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#25454;&#21644;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#65292;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#19978;&#36848;&#38382;&#39064;; &#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#23454;&#29992;&#26694;&#26550;&#22312;&#26469;&#33258;&#22312;&#32447;&#36141;&#29289;&#32593;&#31449;&#27969;&#37327;&#26085;&#24535;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#30410;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#26694;&#26550;&#19982;&#25968;&#25454;&#21644;&#29305;&#24449;&#24037;&#31243;&#20063;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study multiple problems from sponsored product optimization in ad system, including position-based de-biasing, click-conversion multi-task learning, and calibration on predicted click-through-rate (pCTR). We propose a practical machine learning framework that provides the solutions to such problems without structural change to existing machine learning models, thus can be combined with most machine learning models including shallow models (e.g. gradient boosting decision trees, support vector machines). In this paper, we first propose data and feature engineering techniques to handle the aforementioned problems in ad system; after that, we evaluate the benefit of our practical framework on real-world data sets from our traffic logs from online shopping site. We show that our proposed practical framework with data and feature engineering can also handle the perennial problems in ad systems and bring increments to multiple evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#31361;&#20986;&#20102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.09102</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#27714;&#35299;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#31361;&#20986;&#20102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36880;&#27493;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#25945;&#32946;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#20351;&#29992;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#24050;&#25104;&#20026;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#65288;&#22914;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65288;PAL&#65289;&#65289;&#23545;&#20110;&#38656;&#35201;&#38472;&#36848;&#24615;&#25512;&#29702;&#30340;&#38382;&#39064;&#20855;&#26377;&#20559;&#35265;&#65292;&#32780;&#23545;&#20110;&#31616;&#21333;&#30340;&#36807;&#31243;&#38382;&#39064;&#21017;&#19981;&#22826;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#35789;&#38382;&#39064;&#36880;&#27493;&#27491;&#24335;&#21270;&#20026;&#19968;&#32452;&#21464;&#37327;&#21644;&#26041;&#31243;&#24335;&#30340;LLM&#19982;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GSM8K&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;PAL&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;ALGEBRA&#19978;&#21017;&#26126;&#26174;&#20248;&#20110;PAL&#65292;&#35813;&#25968;&#25454;&#38598;&#20174;&#20195;&#25968;&#25945;&#31185;&#20070;&#20013;&#25552;&#21462;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#21333;&#35789;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#26102;&#20351;&#29992;&#22768;&#26126;&#24615;&#21644;&#36880;&#27493;&#34920;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09099</link><description>&lt;p&gt;
MATURE-HEALTH: MAndatory FeaTURE&#36873;&#25321;&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30005;&#35299;&#36136;&#23545;&#20110;&#20154;&#20307;&#22120;&#23448;&#30340;&#36866;&#24403;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#19981;&#21487;&#23569;&#65292;&#22240;&#20026;&#30005;&#35299;&#36136;&#22833;&#34913;&#21487;&#33021;&#26159;&#28508;&#22312;&#30149;&#29702;&#29983;&#29702;&#23398;&#21457;&#23637;&#30340;&#25351;&#31034;&#12290;&#39640;&#25928;&#30417;&#27979;&#30005;&#35299;&#36136;&#22833;&#34913;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#26426;&#20250;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#20005;&#26684;&#36981;&#24490;&#33829;&#20859;&#25511;&#21046;&#39278;&#39135;&#20197;&#24179;&#34913;&#30005;&#35299;&#36136;&#20174;&#32780;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;MATURE Health&#65292;&#35813;&#31995;&#32479;&#39044;&#27979;&#34880;&#28082;&#20013;&#24517;&#38656;&#30005;&#35299;&#36136;&#21644;&#20854;&#20182;&#29289;&#36136;&#30340;&#19981;&#24179;&#34913;&#65292;&#28982;&#21518;&#25512;&#33616;&#21547;&#26377;&#24179;&#34913;&#33829;&#20859;&#30340;&#39135;&#29289;&#65292;&#20197;&#36991;&#20813;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#30340;&#21457;&#29983;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#21040;&#29992;&#25143;&#26368;&#36817;&#30340;&#23454;&#39564;&#23460;&#32467;&#26524;&#21644;&#27599;&#26085;&#39135;&#29289;&#25668;&#20837;&#37327;&#26469;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#12290;MATURE Health&#20381;&#36182;&#20110;MATURE Food&#31639;&#27861;&#25512;&#33616;&#39135;&#29289;&#65292;&#21518;&#32773;&#20165;&#25512;&#33616;&#37027;&#20123;
&lt;/p&gt;
&lt;p&gt;
Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#30340;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09087</link><description>&lt;p&gt;
MDDL: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;Feed&#20301;&#32622;&#20998;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#30340;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20301;&#32622;&#20998;&#37197;&#31995;&#32479;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20026;&#21508;&#36890;&#36947;&#30340;&#29289;&#21697;&#20998;&#37197;&#21512;&#36866;&#30340;&#20301;&#32622;&#65292;&#28982;&#21518;&#28151;&#21512;&#21040;Feed&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20351;&#29992;&#20004;&#31181;&#25968;&#25454;&#65306;&#31574;&#30053;&#25968;&#25454;&#21644;&#38543;&#26426;&#25968;&#25454;&#12290;&#31574;&#30053;&#25968;&#25454;&#26469;&#33258;&#24403;&#21069;&#22312;&#32447;&#27169;&#22411;&#65292;&#23427;&#21463;&#21040;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20998;&#24067;&#19981;&#22343;&#34913;&#30340;&#22256;&#25200;&#65292;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#39640;&#20272;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38543;&#26426;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#22343;&#21248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20998;&#24067;&#65292;&#20294;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#24456;&#38590;&#33719;&#21462;&#65292;&#22240;&#20026;&#38543;&#26426;&#25506;&#32034;&#21487;&#33021;&#20250;&#23545;&#24179;&#21488;&#25910;&#20837;&#21644;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#21033;&#29992;&#20004;&#31181;&#25968;&#25454;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#24050;&#25104;&#20026;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#65288;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;RL&#27169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#65292;MDDL&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;IT&#37096;&#38376;&#21592;&#24037;&#30340;&#38382;&#21367;&#35843;&#26597;&#30740;&#31350;&#20102;&#20182;&#20204;&#21363;&#23558;&#20351;&#29992;&#30340;&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#20869;&#23481;&#29983;&#25104;&#24037;&#20855;&#30340;&#24847;&#24895;&#65292;&#32467;&#26524;&#26174;&#31034;&#23454;&#29992;&#24615;&#39640;&#30340;&#24037;&#20855;&#26356;&#23481;&#26131;&#34987;&#25509;&#21463;&#21644;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09064</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#20132;&#20114;&#24335;&#20869;&#23481;&#29983;&#25104;&#65306;IT&#37096;&#38376;&#21592;&#24037;&#30693;&#35273;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department. (arXiv:2304.09064v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;IT&#37096;&#38376;&#21592;&#24037;&#30340;&#38382;&#21367;&#35843;&#26597;&#30740;&#31350;&#20102;&#20182;&#20204;&#21363;&#23558;&#20351;&#29992;&#30340;&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#20869;&#23481;&#29983;&#25104;&#24037;&#20855;&#30340;&#24847;&#24895;&#65292;&#32467;&#26524;&#26174;&#31034;&#23454;&#29992;&#24615;&#39640;&#30340;&#24037;&#20855;&#26356;&#23481;&#26131;&#34987;&#25509;&#21463;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;LLM&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#21487;&#20197;&#26356;&#22909;&#22320;&#35775;&#38382;&#25110;&#29983;&#25104;&#20869;&#23481;&#12290;&#30446;&#21069;&#20851;&#20110;LLM&#25216;&#26415;&#29983;&#25104;&#24037;&#20855;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#36825;&#20123;&#24037;&#20855;&#22312;&#29983;&#25104;&#30456;&#20851;&#20869;&#23481;&#65288;&#20195;&#30721;&#12289;&#25991;&#26412;&#25110;&#22270;&#20687;&#65289;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#29983;&#25104;&#24037;&#20855;&#35774;&#35745;&#21644;&#20351;&#29992;&#30456;&#20851;&#30340;&#20262;&#29702;&#38382;&#39064;&#20284;&#20046;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#24433;&#21709;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#20844;&#20247;&#21487;&#25509;&#21463;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#38382;&#21367;&#35843;&#26597;&#65292;&#22522;&#20110;&#27979;&#37327;&#20351;&#29992;&#24847;&#24895;&#30340;&#32463;&#39564;&#27169;&#22411;&#65288;Davis, 1989&#30340;TAM&#21644;Venkatesh&#31561;&#20154;&#30340;UTAUT2&#65289;&#65292;&#26088;&#22312;&#30830;&#23450;IT&#20844;&#21496;&#21592;&#24037;&#22312;&#24037;&#20316;&#29615;&#22659;&#20013;&#20351;&#29992;&#29983;&#25104;&#24037;&#20855;&#30340;&#24847;&#24895;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#23545;&#29983;&#25104;&#24037;&#20855;&#30340;&#21487;&#25509;&#21463;&#24615;&#35780;&#20215;&#36739;&#20026;&#19968;&#33324;&#65292;&#20294;&#24037;&#20855;&#30340;&#23454;&#29992;&#24615;&#34987;&#35748;&#20026;&#36234;&#39640;&#65292;&#20351;&#29992;&#24847;&#24895;&#23601;&#36234;&#24378;&#28872;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#8230;&#8230;&#65288;&#26410;&#23436;&#25972;&#20844;&#24320;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, AI has seen many advances in the field of NLP. This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content. Current studies on LLM-based generative tools are mainly interested in the performance of such tools in generating relevant content (code, text or image). However, ethical concerns related to the design and use of generative tools seem to be growing, impacting the public acceptability for specific tasks. This paper presents a questionnaire survey to identify the intention to use generative tools by employees of an IT company in the context of their work. This survey is based on empirical models measuring intention to use (TAM by Davis, 1989, and UTAUT2 by Venkatesh and al., 2008). Our results indicate a rather average acceptability of generative tools, although the more useful the tool is perceived to be, the higher the intention to use seems to be. Furthermore, our analyses suggest th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2304.09015</link><description>&lt;p&gt;
PaTeCon&#65306;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#29992;&#20110;&#20914;&#31361;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs. (arXiv:2304.09015v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09015
&lt;/p&gt;
&lt;p&gt;
PaTeCon&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#26469;&#32500;&#25252;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#26816;&#27979;&#28508;&#22312;&#30340;&#26102;&#38388;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30740;&#31350;&#31038;&#21306;&#20013;&#65292;&#26102;&#38388;&#20107;&#23454;&#25351;&#29305;&#23450;&#26102;&#38388;&#27573;&#20869;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#26102;&#38388;&#38480;&#21046;&#32473;KG&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#32500;&#25252;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#21015;&#20030;&#26102;&#38388;&#32422;&#26463;&#26469;&#26816;&#27979;&#20914;&#31361;&#65292;&#36825;&#24456;&#36153;&#21147;&#19988;&#21487;&#33021;&#23384;&#22312;&#31890;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#26102;&#38388;&#32422;&#26463;&#25366;&#25496;&#26041;&#27861;PaTeCon&#65292;&#23427;&#20351;&#29992;&#33258;&#21160;&#30830;&#23450;&#30340;&#22270;&#24418;&#27169;&#24335;&#21450;&#20854;&#30456;&#20851;&#32479;&#35745;&#20449;&#24687;&#20195;&#26367;&#20154;&#24037;&#19987;&#23478;&#26469;&#29983;&#25104;&#26102;&#38388;&#32422;&#26463;&#12290;&#20855;&#20307;&#22320;&#65292;PaTeCon&#26681;&#25454;&#20854;&#27979;&#37327;&#24471;&#20998;&#21160;&#24577;&#22320;&#23558;&#31867;&#38480;&#21046;&#38468;&#21152;&#21040;&#20505;&#36873;&#32422;&#26463;&#19978;&#12290;&#25105;&#20204;&#22522;&#20110;&#32500;&#22522;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;PaTeCon&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#21307;&#25252;&#20154;&#21592;&#26356;&#39640;&#25928;&#22320;&#33719;&#21462;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#32959;&#30244;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08999</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#25552;&#21462;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#25552;&#21462;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#21307;&#25252;&#20154;&#21592;&#26356;&#39640;&#25928;&#22320;&#33719;&#21462;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#32959;&#30244;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#24739;&#32773;&#30340;&#25991;&#26412;&#20581;&#24247;&#35760;&#24405;&#36890;&#24120;&#24456;&#20887;&#38271;&#19988;&#39640;&#24230;&#19981;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21307;&#25252;&#20154;&#21592;&#33719;&#21462;&#23436;&#25972;&#24739;&#32773;&#27835;&#30103;&#29366;&#20917;&#30340;&#23436;&#25972;&#27010;&#36848;&#38750;&#24120;&#32791;&#26102;&#12290;&#30001;&#20110;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#21644;/&#25110;&#20302;&#25928;&#30340;&#27835;&#30103;&#31243;&#24207;&#65292;&#22240;&#27492;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#23558;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#26377;&#25928;&#22320;&#27010;&#25324;&#36825;&#20123;&#35760;&#24405;&#30340;&#31995;&#32479;&#12290;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#33521;&#35821;&#20020;&#24202;&#25991;&#26412;&#65292;&#36825;&#20010;&#30446;&#26631;&#24050;&#32463;&#37096;&#20998;&#23454;&#29616;&#65292;&#28982;&#32780;&#65292;&#30740;&#31350;&#31038;&#21306;&#20173;&#32570;&#20047;&#38024;&#23545;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#20174;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#32959;&#30244;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#36807;&#31243;&#12289;&#33647;&#29289;&#21644;&#30142;&#30149;&#12290;&#36825;&#20010;&#39033;&#30446;&#19982;&#33889;&#33796;&#29273;&#32959;&#30244;&#30740;&#31350;&#25152;&#21512;&#20316;&#23436;&#25104;&#65292;&#35813;&#25152;&#38500;&#20102;&#25317;&#26377;&#21313;&#22810;&#24180;&#30340;&#21463;&#20445;&#25252;&#30340;&#21307;&#30103;&#35760;&#24405;&#22806;&#65292;&#22312;&#25972;&#20010;&#24320;&#21457;&#36807;&#31243;&#20013;&#36824;&#25552;&#20379;&#20102;&#32959;&#30244;&#23398;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual health records of cancer patients are usually protracted and highly unstructured, making it very time-consuming for health professionals to get a complete overview of the patient's therapeutic course. As such limitations can lead to suboptimal and/or inefficient treatment procedures, healthcare providers would greatly benefit from a system that effectively summarizes the information of those records. With the advent of deep neural models, this objective has been partially attained for English clinical texts, however, the research community still lacks an effective solution for languages with limited resources. In this paper, we present the approach we developed to extract procedures, drugs, and diseases from oncology health records written in European Portuguese. This project was conducted in collaboration with the Portuguese Institute for Oncology which, besides holding over $10$ years of duly protected medical records, also provided oncologist expertise throughout the develop
&lt;/p&gt;</description></item><item><title>D2CSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26032;&#27169;&#22411;&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#26469;&#35745;&#31639;&#20855;&#26377;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#33021;&#21147;&#30340;&#21477;&#23376;&#21521;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#21482;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#24494;&#35843;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.08991</link><description>&lt;p&gt;
D2CSE: &#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08991
&lt;/p&gt;
&lt;p&gt;
D2CSE&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26032;&#27169;&#22411;&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#26469;&#35745;&#31639;&#20855;&#26377;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#33021;&#21147;&#30340;&#21477;&#23376;&#21521;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#21482;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#24494;&#35843;&#65292;&#24182;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;D2CSE&#30340;&#22522;&#20110;&#24046;&#24322;&#24863;&#30693;&#30340;&#28145;&#24230;&#36830;&#32493;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;D2CSE&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#31070;&#32463;&#26550;&#26500;&#26469;&#35745;&#31639;&#21477;&#23376;&#21521;&#37327;&#65292;&#20351;&#24471;&#20854;&#23545;&#20110;&#22312;&#31867;&#20284;&#21477;&#23376;&#20013;&#21306;&#20998;&#24494;&#22937;&#24046;&#24322;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#19982;&#38656;&#35201;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22788;&#29702;&#21407;&#22987;&#21644;&#25439;&#22351;&#65288;&#24494;&#22937;&#20462;&#25913;&#65289;&#21477;&#23376;&#23545;&#30340;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;D2CSE&#20165;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65288;&#21363;&#65292;&#23545;&#27604;&#23398;&#20064;&#21644;&#26465;&#20214;&#26367;&#25442;&#26631;&#35760;&#26816;&#27979;&#65289;&#33258;&#20027;&#24341;&#23548;&#22320;&#20248;&#21270;&#20102;&#36830;&#32493;&#25552;&#31034;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#22810;&#20010;PLMs&#30340;&#24494;&#35843;&#12290;D2CSE&#23558;&#21333;&#20010;PLM&#37325;&#36733;&#21040;&#36830;&#32493;&#25552;&#31034;&#19978;&#65292;&#22823;&#22823;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#12290; D2CSE&#30340;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#32422;&#20026;&#29616;&#26377;&#26041;&#27861;&#30340;1&#65285;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\% of existing approaches while substantially improving the quality of sentence embeddings. W
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2304.08968</link><description>&lt;p&gt;
&#38543;&#26426;&#40550;&#40521;&#23547;&#25214;&#38543;&#26426;&#40550;&#40521;&#65306;LLMs&#26131;&#20110;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;
&lt;/p&gt;
&lt;p&gt;
Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08968
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#27880;&#24847;&#21147;&#38761;&#21629;&#20351;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24471;&#20197;&#25193;&#23637;&#24182;&#23454;&#29616;&#36234;&#26469;&#36234;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26368;&#36817;&#30001;&#20110;&#23545;&#35805;&#24494;&#35843;&#32780;&#22312;&#20844;&#20247;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#20351;&#20854;&#34892;&#20026;&#31526;&#21512;&#20844;&#20247;&#23545;&#20110;AI&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31361;&#20986;&#20063;&#21152;&#22823;&#20102;&#20851;&#27880;LLMs&#35823;&#29992;&#30340;&#20808;&#21069;&#25285;&#24551;&#65292;&#24182;&#23548;&#33268;&#20986;&#29616;&#35768;&#22810;&#22312;&#37326;&#22806;&#26816;&#27979;LLMs&#30340;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#24037;&#20855;&#37117;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26816;&#27979;LLMs&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#35828;&#26126;&#25105;&#20204;&#30340;&#24037;&#20316;&#28041;&#21450;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GKLS&#21457;&#29983;&#22120;&#36827;&#34892;&#20102;&#35745;&#31639;&#21644;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;GKLS&#21457;&#29983;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#26356;&#39640;&#32500;&#24230;&#20248;&#21270;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2304.08913</link><description>&lt;p&gt;
GKLS&#21457;&#29983;&#22120;&#30340;&#35745;&#31639;&#21644;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Computational and Exploratory Landscape Analysis of the GKLS Generator. (arXiv:2304.08913v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GKLS&#21457;&#29983;&#22120;&#36827;&#34892;&#20102;&#35745;&#31639;&#21644;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#21457;&#29616;GKLS&#21457;&#29983;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#26356;&#39640;&#32500;&#24230;&#20248;&#21270;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GKLS&#21457;&#29983;&#22120;&#26159;&#29992;&#20110;&#27979;&#35797;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#30340;&#26368;&#24120;&#29992;&#27979;&#35797;&#24179;&#21488;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GKLS&#21457;&#29983;&#22120;&#36827;&#34892;&#20102;&#35745;&#31639;&#20998;&#26512;&#21644;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;(ELA)&#12290;&#25105;&#20204;&#20351;&#29992;GKLS&#29983;&#25104;&#30340;&#32463;&#20856;&#31867;&#21644;&#26032;&#29983;&#25104;&#30340;&#31867;&#38382;&#39064;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#26469;&#33258;&#36827;&#21270;&#21644;&#30830;&#23450;&#24615;&#31038;&#21306;&#65289;&#22312;5&#32500;&#21644;10&#32500;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;GKLS&#29983;&#25104;&#22120;&#20135;&#29983;&#20102;&#8220;&#22823;&#28023;&#25438;&#38024;&#8221;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#20013;&#20248;&#21270;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;GKLS&#21457;&#29983;&#22120;&#36827;&#34892;&#20102;ELA&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21478;&#22806;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;(BBOB&#21644;CEC 2014)&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#32467;&#26524;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The GKLS generator is one of the most used testbeds for benchmarking global optimization algorithms. In this paper, we conduct both a computational analysis and the Exploratory Landscape Analysis (ELA) of the GKLS generator. We utilize both canonically used and newly generated classes of GKLS-generated problems and show their use in benchmarking three state-of-the-art methods (from evolutionary and deterministic communities) in dimensions 5 and 10. We show that the GKLS generator produces ``needle in a haystack'' type problems that become extremely difficult to optimize in higher dimensions. Furthermore, we conduct the ELA on the GKLS generator and then compare it to the ELA of two other widely used benchmark sets (BBOB and CEC 2014), and discuss the meaningfulness of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ROS slam&#24037;&#20855;&#31665;&#21644;Nav2&#23548;&#33322;&#31995;&#32479;&#26694;&#26550;&#26500;&#24314;&#30340;&#27169;&#25311;&#26080;&#20154;&#26426;&#65292;&#33021;&#22815;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#33258;&#20027;&#31227;&#21160;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.08893</link><description>&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#65306;&#23460;&#20869;&#26080;&#20154;&#26426;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Autonomous Systems: Autonomous Systems: Indoor Drone Navigation. (arXiv:2304.08893v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ROS slam&#24037;&#20855;&#31665;&#21644;Nav2&#23548;&#33322;&#31995;&#32479;&#26694;&#26550;&#26500;&#24314;&#30340;&#27169;&#25311;&#26080;&#20154;&#26426;&#65292;&#33021;&#22815;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#33258;&#20027;&#31227;&#21160;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33258;&#20027;&#25968;&#25454;&#25910;&#38598;&#21644;&#23460;&#20869;&#24863;&#30693;&#25216;&#26415;&#12290;&#24403;&#20154;&#25511;&#21046;&#30340;&#26080;&#20154;&#26426;&#21487;&#33021;&#19981;&#23454;&#38469;&#25110;&#19981;&#21487;&#38752;&#26102;&#65292;&#20363;&#22914;&#22312;&#26410;&#30693;&#25110;&#21361;&#38505;&#30340;&#22320;&#28857;&#65292;&#20351;&#29992;&#33258;&#20027;&#26080;&#20154;&#26426;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#33410;&#32422;&#25104;&#26412;&#21644;&#38477;&#20302;&#39118;&#38505;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;gazebo&#27169;&#25311;&#24037;&#20855;&#21644;&#31216;&#20026;Nav2&#30340;ros&#23548;&#33322;&#31995;&#32479;&#26694;&#26550;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#22235;&#36724;&#39134;&#34892;&#22120;&#65292;&#33021;&#22815;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#33258;&#20027;&#31227;&#21160;&#12290;&#34429;&#28982;Nav2&#22312;&#38470;&#22320;&#26426;&#22120;&#20154;&#21644;&#36710;&#36742;&#20013;&#25104;&#21151;&#23637;&#31034;&#20102;&#33258;&#20027;&#23548;&#33322;&#30340;&#21151;&#33021;&#65292;&#20294;&#22312;&#26080;&#20154;&#26426;&#20013;&#36824;&#27809;&#26377;&#36798;&#25104;&#27492;&#30446;&#26631;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;ROS&#30340;slam&#24037;&#20855;&#31665;&#21644;Nav2&#23548;&#33322;&#31995;&#32479;&#26694;&#26550;&#26500;&#24314;&#19968;&#20010;&#27169;&#25311;&#30340;&#26080;&#20154;&#26426;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;&#65288;&#26080;gps&#65289;&#29615;&#22659;&#20013;&#33258;&#20027;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drones are a promising technology for autonomous data collection and indoor sensing. In situations when human-controlled UAVs may not be practical or dependable, such as in uncharted or dangerous locations, the usage of autonomous UAVs offers flexibility, cost savings, and reduced risk. The system creates a simulated quadcopter capable of autonomously travelling in an indoor environment using the gazebo simulation tool and the ros navigation system framework known as Navigaation2. While Nav2 has successfully shown the functioning of autonomous navigation in terrestrial robots and vehicles, the same hasn't been accomplished with unmanned aerial vehicles and still has to be done. The goal is to use the slam toolbox for ROS and the Nav2 navigation system framework to construct a simulated drone that can move autonomously in an indoor (gps-less) environment.
&lt;/p&gt;</description></item><item><title>NPS&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#24182;&#24555;&#36895;&#29983;&#25104;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.08880</link><description>&lt;p&gt;
NPS: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#31934;&#20934;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NPS: A Framework for Accurate Program Sampling Using Graph Neural Network. (arXiv:2304.08880v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08880
&lt;/p&gt;
&lt;p&gt;
NPS&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#24182;&#24555;&#36895;&#29983;&#25104;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#32467;&#26463;&#65292;&#29616;&#20195;&#22788;&#29702;&#22120;&#65288;&#22914;RISC-V&#33258;&#23450;&#20041;&#25193;&#23637;&#65289;&#38656;&#35201;&#24555;&#36895;&#30340;&#26550;&#26500;&#21019;&#26032;&#26469;&#32500;&#25345;&#24615;&#33021;&#22686;&#38271;&#65292;&#31243;&#24207;&#37319;&#26679;&#26159;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#20013;&#20851;&#38190;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#23427;&#36873;&#25321;&#24037;&#20316;&#37327;&#27169;&#25311;&#30340;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31243;&#24207;&#37319;&#26679;&#65288;NPS&#65289;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#24555;&#29031;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#12290;NPS&#37319;&#29992;AssemblyNet&#36827;&#34892;&#23884;&#20837;&#29983;&#25104;&#65292;&#21033;&#29992;&#24212;&#29992;&#31243;&#24207;&#30340;&#20195;&#30721;&#32467;&#26500;&#21644;&#36816;&#34892;&#26102;&#29366;&#24577;&#65292;&#23558;AssemblyNet&#20316;&#20026;NPS&#30340;&#22270;&#27169;&#22411;&#21644;&#31070;&#32463;&#26550;&#26500;&#65292;&#25429;&#33719;&#31243;&#24207;&#30340;&#34892;&#20026;&#65292;&#22312;&#25968;&#25454;&#35745;&#31639;&#12289;&#20195;&#30721;&#36335;&#24452;&#21644;&#25968;&#25454;&#27969;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NPS&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;SimPoint&#65292;&#22312;&#25552;&#39640;&#24037;&#20316;&#36127;&#36733;&#20195;&#34920;&#24615;&#29575;&#30340;&#21516;&#26102;&#65292;&#23558;&#27169;&#25311;&#26102;&#38388;&#32553;&#30701;&#39640;&#36798;60%&#12290;
&lt;/p&gt;
&lt;p&gt;
With the end of Moore's Law, there is a growing demand for rapid architectural innovations in modern processors, such as RISC-V custom extensions, to continue performance scaling. Program sampling is a crucial step in microprocessor design, as it selects representative simulation points for workload simulation. While SimPoint has been the de-facto approach for decades, its limited expressiveness with Basic Block Vector (BBV) requires time-consuming human tuning, often taking months, which impedes fast innovation and agile hardware development. This paper introduces Neural Program Sampling (NPS), a novel framework that learns execution embeddings using dynamic snapshots of a Graph Neural Network. NPS deploys AssemblyNet for embedding generation, leveraging an application's code structures and runtime states. AssemblyNet serves as NPS's graph model and neural architecture, capturing a program's behavior in aspects such as data computation, code path, and data flow. AssemblyNet is trained
&lt;/p&gt;</description></item><item><title>&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#26368;&#32456;&#29992;&#25143;&#30340;&#35748;&#30693;&#36127;&#33655;&#12289;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#26102;&#38388;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25512;&#33616;&#20351;&#29992;&#23454;&#29616;&#26080;&#20851;&#30340;&#22320;&#26041; XAI &#35828;&#26126;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08861</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#35748;&#30693;&#36127;&#33655;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Impact Of Explainable AI On Cognitive Load: Insights From An Empirical Study. (arXiv:2304.08861v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08861
&lt;/p&gt;
&lt;p&gt;
&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#26368;&#32456;&#29992;&#25143;&#30340;&#35748;&#30693;&#36127;&#33655;&#12289;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#26102;&#38388;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25512;&#33616;&#20351;&#29992;&#23454;&#29616;&#26080;&#20851;&#30340;&#22320;&#26041; XAI &#35828;&#26126;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36825;&#19968;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#26088;&#22312;&#35299;&#20915;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#32570;&#38519;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;XAI &#26356;&#22810;&#22320;&#38754;&#21521;&#24320;&#21457;&#20154;&#21592;&#32780;&#38750;&#26368;&#32456;&#29992;&#25143;&#12290;&#22240;&#27492;&#65292;&#26368;&#32456;&#29992;&#25143;&#24120;&#24120;&#19981;&#24895;&#24847;&#20351;&#29992;&#22522;&#20110; XAI &#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#20851;&#20110;&#26368;&#32456;&#29992;&#25143;&#22312;&#20351;&#29992; XAI &#35828;&#26126;&#26102;&#30340;&#34892;&#20026;&#32570;&#20047;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033; COVID-19 &#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#23545;271&#21517;&#28508;&#22312;&#21307;&#29983;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#27979;&#37327;&#20102;&#20182;&#20204;&#22312;&#20351;&#29992;&#19981;&#21516;&#30340;&#23454;&#29616;&#26080;&#20851; XAI &#35828;&#26126;&#31867;&#22411;&#26102;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#35828;&#26126;&#31867;&#22411;&#24378;&#28872;&#24433;&#21709;&#20102;&#26368;&#32456;&#29992;&#25143;&#30340;&#35748;&#30693;&#36127;&#33655;&#12289;&#20219;&#21153;&#34920;&#29616;&#21644;&#20219;&#21153;&#26102;&#38388;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#31934;&#31070;&#25928;&#29575;&#24230;&#37327;&#26631;&#20934;&#65292;&#20854;&#20013;&#22320;&#26041; XAI &#35828;&#26126;&#31867;&#22411;&#25490;&#21517;&#26368;&#20339;&#65292;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the emerging research field of explainable artificial intelligence (XAI) claims to address the lack of explainability in high-performance machine learning models, in practice, XAI targets developers rather than actual end-users. Unsurprisingly, end-users are often unwilling to use XAI-based decision support systems. Similarly, there is limited interdisciplinary research on end-users' behavior during XAI explanations usage, rendering it unknown how explanations may impact cognitive load and further affect end-user performance. Therefore, we conducted an empirical study with 271 prospective physicians, measuring their cognitive load, task performance, and task time for distinct implementation-independent XAI explanation types using a COVID-19 use case. We found that these explanation types strongly influence end-users' cognitive load, task performance, and task time. Further, we contextualized a mental efficiency metric, ranking local XAI explanation types best, to provide recommen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#32676;&#20307;MCDM&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38169;&#35823;&#26377;&#19977;&#31181;&#65292;&#21253;&#25324;&#29992;&#38169;&#35823;&#30340;&#24179;&#22343;&#26041;&#27861;&#12289;&#35823;&#29992;&#26631;&#20934;&#20559;&#24046;&#21644;&#36317;&#31163;&#20989;&#25968;&#31561;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#25104;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#21644;&#20998;&#26512;&#20248;&#20808;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2304.08859</link><description>&lt;p&gt;
&#25581;&#31034;&#32676;&#20307;MCDM&#20013;&#32858;&#21512;&#21644;&#20998;&#25955;&#35884;&#35823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unveiling and unraveling aggregation and dispersion fallacies in group MCDM. (arXiv:2304.08859v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#32676;&#20307;MCDM&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38169;&#35823;&#26377;&#19977;&#31181;&#65292;&#21253;&#25324;&#29992;&#38169;&#35823;&#30340;&#24179;&#22343;&#26041;&#27861;&#12289;&#35823;&#29992;&#26631;&#20934;&#20559;&#24046;&#21644;&#36317;&#31163;&#20989;&#25968;&#31561;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#25104;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#21644;&#20998;&#26512;&#20248;&#20808;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#20915;&#31574;&#20013;&#30340;&#20248;&#20808;&#32423;&#20256;&#36798;&#20102;&#19968;&#20010;&#20934;&#21017;&#20248;&#20808;&#20110;&#21478;&#19968;&#20010;&#30340;&#30456;&#20851;&#20559;&#22909;&#65292;&#36890;&#24120;&#36890;&#36807;&#26045;&#21152;&#38750;&#36127;&#21644;&#21333;&#20301;&#21644;&#32422;&#26463;&#26469;&#21453;&#26144;&#12290;&#36825;&#20123;&#20248;&#20808;&#32423;&#30340;&#22788;&#29702;&#19982;&#20854;&#20182;&#38750;&#32422;&#26463;&#25968;&#25454;&#19981;&#21516;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#23548;&#33268;&#20102;&#35884;&#35823;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32676;&#20307;MCDM&#20013;&#30340;&#19977;&#31181;&#26222;&#36941;&#38169;&#35823;&#65292;&#20197;&#21450;&#22522;&#20110;&#32452;&#25104;&#25968;&#25454;&#20998;&#26512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36991;&#20813;&#35823;&#29992;&#32479;&#35745;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#32452;&#25104;&#26041;&#27861;&#26469;&#32858;&#21512;DM&#32452;&#30340;&#20248;&#20808;&#32423;&#65292;&#24182;&#26174;&#31034;&#32452;&#25104;&#20998;&#26512;&#30340;&#32467;&#26524;&#19982;&#26631;&#20934;&#21270;&#20960;&#20309;&#24179;&#22343;&#20540;&#30456;&#21516;&#65292;&#36825;&#24847;&#21619;&#30528;&#24212;&#36991;&#20813;&#31639;&#26415;&#24179;&#22343;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#23427;&#26159;&#20960;&#20309;&#24179;&#22343;&#25968;&#30340;&#24378;&#22823;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;&#31163;&#25955;&#24230;&#37327;&#65292;&#21253;&#25324;&#26631;&#20934;&#20559;&#24046;&#21644;&#36317;&#31163;&#20989;&#25968;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Priorities in multi-criteria decision-making (MCDM) convey the relevance preference of one criterion over another, which is usually reflected by imposing the non-negativity and unit-sum constraints. The processing of such priorities is different than other unconstrained data, but this point is often neglected by researchers, which results in fallacious statistical analysis. This article studies three prevalent fallacies in group MCDM along with solutions based on compositional data analysis to avoid misusing statistical operations. First, we use a compositional approach to aggregate the priorities of a group of DMs and show that the outcome of the compositional analysis is identical to the normalized geometric mean, meaning that the arithmetic mean should be avoided. Furthermore, a new aggregation method is developed, which is a robust surrogate for the geometric mean. We also discuss the errors in computing measures of dispersion, including standard deviation and distance functions. D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08855</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#21306;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#40065;&#26834;&#24615;&#23545;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift. (arXiv:2304.08855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#31283;&#23450;&#24615;&#36890;&#24120;&#19981;&#33021;&#28385;&#36275;&#65292;&#23548;&#33268;&#20102;&#25152;&#23398;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#20986;&#29616;&#20102;&#24847;&#22806;&#30340;&#34920;&#29616;&#12290;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36755;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#19981;&#21516;&#65292;&#20294;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20445;&#25345;&#19981;&#21464;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#22495;&#36827;&#34892;&#20998;&#35299;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20998;&#31867;&#22120;&#22312;&#27599;&#20010;&#22495;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#20108;&#32500;&#20998;&#31867;&#38382;&#39064;&#20013;&#27169;&#25311;&#20102;&#20998;&#24067;&#21464;&#21270;&#65292;&#38543;&#21518;&#36827;&#34892;&#20102;&#26356;&#39640;&#32500;&#24230;&#30340;&#22235;&#32500;&#23454;&#39564;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning methods assume that the input data distribution is the same in the training and testing phases. However, in practice, this stationarity is usually not met and the distribution of inputs differs, leading to unexpected performance of the learned model in deployment. The issue in which the training and test data inputs follow different probability distributions while the input-output relationship remains unchanged is referred to as covariate shift. In this paper, the performance of conventional machine learning models was experimentally evaluated in the presence of covariate shift. Furthermore, a region-based evaluation was performed by decomposing the domain of probability density function of the input data to assess the classifier's performance per domain region. Distributional changes were simulated in a two-dimensional classification problem. Subsequently, a higher four-dimensional experiments were conducted. Based on the experimental analysis, the Random Forests
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08804</link><description>&lt;p&gt;
&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#19982;&#20934;&#30830;&#24615;&#30340;&#30456;&#20114;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making. (arXiv:2304.08804v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#35813;&#26694;&#26550;&#25581;&#31034;&#20102;&#24403;&#20154;&#31867;&#22312;&#20915;&#31574;&#20013;&#36807;&#24230;&#20381;&#36182;AI&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#21487;&#33021;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#23558;&#20154;&#31867;&#32622;&#20110;&#20915;&#31574;&#29615;&#36335;&#20013;&#22830;&#30340;&#20027;&#35201;&#25215;&#35834;&#26159;&#65292;&#20182;&#20204;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#31526;&#21512;&#20854;&#27491;&#30830;&#30340;&#21644;&#35206;&#30422;&#20854;&#38169;&#35823;&#30340;&#24314;&#35758;&#26469;&#34917;&#20805;AI&#31995;&#32479;&#12290;&#28982;&#32780;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#20154;&#31867;&#20542;&#21521;&#20110;&#36807;&#24230;&#25110;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#65292;&#36825;&#24847;&#21619;&#30528;&#20182;&#20204;&#35201;&#20040;&#20381;&#20174;&#38169;&#35823;&#30340;&#24314;&#35758;&#65292;&#35201;&#20040;&#35206;&#30422;&#27491;&#30830;&#30340;&#24314;&#35758;&#12290;&#36825;&#31181;&#20381;&#36182;&#34892;&#20026;&#23545;&#20915;&#31574;&#20934;&#30830;&#24615;&#26377;&#23475;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20381;&#36182;&#34892;&#20026;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#26694;&#26550;&#65292;&#20351;&#36825;&#31181;&#30456;&#20114;&#20851;&#31995;&#26356;&#21152;&#20855;&#20307;&#21270;&#12290;&#35813;&#26694;&#26550;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#21644;&#27604;&#36739;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#33719;&#24471;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#24178;&#39044;&#65288;&#20363;&#22914;&#35299;&#37322;&#65289;&#24433;&#21709;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#26694;&#26550;&#20013;&#25512;&#20986;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#65288;i&#65289;&#24403;&#20154;&#31867;&#19981;&#36275;&#22320;&#20381;&#36182;AI&#24314;&#35758;&#26102;&#65292;&#25913;&#21892;&#20449;&#20219;&#23558;&#26174;&#30528;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20182;&#20204;&#36807;&#24230;&#20381;&#36182;&#26102;&#65292;&#20449;&#20219;&#30340;&#25913;&#21892;&#21364;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.08801</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#34892;&#20026;&#65292;&#20351;&#24471;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#20026;&#23545;&#35805;&#20195;&#29702;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#36807;&#21435;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#21457;&#35328;&#20154;&#20010;&#20154;&#20449;&#24687;&#21019;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21069;&#25552;&#65292;&#21363;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#24050;&#32463;&#34987;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512; (SPC)&#20219;&#21153;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;SPC&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#20154;&#20135;&#29983;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#22312;&#32473;&#23450;&#23545;&#35805;&#30340;&#24773;&#20917;&#19979;&#65292;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25152;&#26377;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08799</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65306;&#22522;&#20110;&#39592;&#26550;&#20113;&#30528;&#33394;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization. (arXiv:2304.08799v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#19977;&#32500;&#39592;&#26550;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#36880;&#28176;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#26631;&#27880;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#23545;&#26410;&#26631;&#27880;&#30340;&#39592;&#26550;&#25968;&#25454;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;23&#39033;&#23454;&#35777;&#30740;&#31350;&#20851;&#20110;&#29992;&#25143;&#20449;&#20219;&#23450;&#20041;&#12289;&#24433;&#21709;&#22240;&#32032;&#21644;&#27979;&#37327;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29305;&#23450;&#24773;&#22659;&#20013;&#25551;&#36848;&#29992;&#25143;&#20449;&#20219;&#23450;&#20041;&#24212;&#35813;&#26159;&#37325;&#28857;&#65292;&#32780;&#19981;&#26159;&#27604;&#36739;&#23450;&#20041;&#12290;&#29992;&#25143;&#23545;AI&#33021;&#21147;&#31995;&#32479;&#30340;&#20449;&#20219;&#21463;&#21040;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#30340;&#24433;&#21709;&#65292;&#21363;&#31038;&#20250;&#20262;&#29702;&#32771;&#34385;&#12289;&#25216;&#26415;&#21644;&#35774;&#35745;&#29305;&#24449;&#12289;&#20197;&#21450;&#29992;&#25143;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.08795</link><description>&lt;p&gt;
AI&#33021;&#21147;&#31995;&#32479;&#20013;&#29992;&#25143;&#20449;&#20219;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65306;&#22522;&#20110;&#20154;&#26426;&#20132;&#20114;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review of User Trust in AI-Enabled Systems: An HCI Perspective. (arXiv:2304.08795v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#32508;&#36848;&#20102;23&#39033;&#23454;&#35777;&#30740;&#31350;&#20851;&#20110;&#29992;&#25143;&#20449;&#20219;&#23450;&#20041;&#12289;&#24433;&#21709;&#22240;&#32032;&#21644;&#27979;&#37327;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#29305;&#23450;&#24773;&#22659;&#20013;&#25551;&#36848;&#29992;&#25143;&#20449;&#20219;&#23450;&#20041;&#24212;&#35813;&#26159;&#37325;&#28857;&#65292;&#32780;&#19981;&#26159;&#27604;&#36739;&#23450;&#20041;&#12290;&#29992;&#25143;&#23545;AI&#33021;&#21147;&#31995;&#32479;&#30340;&#20449;&#20219;&#21463;&#21040;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#30340;&#24433;&#21709;&#65292;&#21363;&#31038;&#20250;&#20262;&#29702;&#32771;&#34385;&#12289;&#25216;&#26415;&#21644;&#35774;&#35745;&#29305;&#24449;&#12289;&#20197;&#21450;&#29992;&#25143;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#33021;&#21147;&#31995;&#32479;&#30340;&#20449;&#20219;&#26085;&#30410;&#34987;&#35748;&#20026;&#26159;&#20419;&#36827;&#37319;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#24050;&#32463;&#26377;&#20154;&#24314;&#35758;&#65292;AI&#33021;&#21147;&#31995;&#32479;&#24517;&#39035;&#36229;&#36234;&#25216;&#26415;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36716;&#21521;&#25509;&#32435;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#39046;&#22495;&#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;23&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#20851;&#20110;&#29992;&#25143;&#20449;&#20219;&#23450;&#20041;&#12289;&#24433;&#21709;&#22240;&#32032;&#21644;&#27979;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#20026;&#26410;&#26469;&#25216;&#26415;&#21644;&#35774;&#35745;&#25112;&#30053;&#12289;&#30740;&#31350;&#21644;&#35745;&#21010;&#25552;&#20379;&#27934;&#23519;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#23384;&#22312;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#20449;&#20219;&#12290;&#36873;&#25321;&#26368;&#36866;&#21512;&#25551;&#36848;&#29305;&#23450;&#24773;&#22659;&#20013;&#29992;&#25143;&#23545;&#20449;&#20219;&#30340;&#23450;&#20041;&#24212;&#35813;&#26159;&#37325;&#28857;&#65292;&#32780;&#19981;&#26159;&#27604;&#36739;&#23450;&#20041;&#12290;&#21457;&#29616;&#29992;&#25143;&#23545;AI&#33021;&#21147;&#31995;&#32479;&#30340;&#20449;&#20219;&#21463;&#21040;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#30340;&#24433;&#21709;&#65292;&#21363;&#31038;&#20250;&#20262;&#29702;&#32771;&#34385;&#12289;&#25216;&#26415;&#21644;&#35774;&#35745;&#29305;&#24449;&#12289;&#20197;&#21450;&#29992;&#25143;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
User trust in Artificial Intelligence (AI) enabled systems has been increasingly recognized and proven as a key element to fostering adoption. It has been suggested that AI-enabled systems must go beyond technical-centric approaches and towards embracing a more human centric approach, a core principle of the human-computer interaction (HCI) field. This review aims to provide an overview of the user trust definitions, influencing factors, and measurement methods from 23 empirical studies to gather insight for future technical and design strategies, research, and initiatives to calibrate the user AI relationship. The findings confirm that there is more than one way to define trust. Selecting the most appropriate trust definition to depict user trust in a specific context should be the focus instead of comparing definitions. User trust in AI-enabled systems is found to be influenced by three main themes, namely socio-ethical considerations, technical and design features, and user characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26426;&#20250;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#19977;&#30446;&#26631;&#24085;&#32047;&#25176;&#20248;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;1&#20301;&#32763;&#36716;&#26469;&#35745;&#31639;&#25152;&#26377;&#38656;&#35201;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;&#26426;&#20250;&#32422;&#26463;&#30340;&#25903;&#37197;&#38598;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2304.08774</link><description>&lt;p&gt;
&#20855;&#26377;&#26426;&#20250;&#32422;&#26463;&#38382;&#39064;&#30340;&#19977;&#30446;&#26631;&#24085;&#32047;&#25176;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
3-Objective Pareto Optimization for Problems with Chance Constraints. (arXiv:2304.08774v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26426;&#20250;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#19977;&#30446;&#26631;&#24085;&#32047;&#25176;&#20248;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;1&#20301;&#32763;&#36716;&#26469;&#35745;&#31639;&#25152;&#26377;&#38656;&#35201;&#30340;&#26435;&#34913;&#65292;&#24182;&#22312;&#26426;&#20250;&#32422;&#26463;&#30340;&#25903;&#37197;&#38598;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#22810;&#30446;&#26631;&#31639;&#27861;&#22312;&#24085;&#32047;&#25176;&#20248;&#21270;&#20013;&#24050;&#32463;&#25104;&#21151;&#22320;&#25226;&#19968;&#20010;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#25918;&#23485;&#25104;&#19968;&#20010;&#38468;&#21152;&#30446;&#26631;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20855;&#26377;&#26426;&#20250;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#20351;&#29992;&#19977;&#30446;&#26631;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#26435;&#34913;&#20102;&#38543;&#26426;&#25104;&#20998;&#30340;&#26399;&#26395;&#25104;&#26412;&#12289;&#26041;&#24046;&#20197;&#21450;&#32473;&#23450;&#30340;&#30830;&#23450;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#36825;&#20010;&#19977;&#30446;&#26631;&#20844;&#24335;&#19982;&#26368;&#36817;&#30740;&#31350;&#30340;&#20855;&#26377;&#27491;&#24577;&#20998;&#24067;&#30340;&#38543;&#26426;&#25104;&#20998;&#30340;&#26426;&#20250;&#32422;&#26463;&#21452;&#30446;&#26631;&#20844;&#24335;&#30456;&#27604;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#30830;&#23450;&#24615;&#22522;&#25968;&#32422;&#26463;&#26102;&#65292;&#19977;&#30446;&#26631;&#20844;&#24335;&#21482;&#38656;&#35201;&#20351;&#29992;1&#20301;&#32763;&#36716;&#23601;&#21487;&#20197;&#35745;&#31639;&#20986;&#25152;&#26377;&#38656;&#35201;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26426;&#20250;&#32422;&#26463;&#30340;&#25903;&#37197;&#38598;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#36825;&#20010;&#32463;&#20856;&#30340;NP-hard&#38382;&#39064;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary multi-objective algorithms have successfully been used in the context of Pareto optimization where a given constraint is relaxed into an additional objective. In this paper, we explore the use of 3-objective formulations for problems with chance constraints. Our formulation trades off the expected cost and variance of the stochastic component as well as the given deterministic constraint. We point out benefits that this 3-objective formulation has compared to a bi-objective one recently investigated for chance constraints with Normally distributed stochastic components. Our analysis shows that the 3-objective formulation allows to compute all required trade-offs using 1-bit flips only, when dealing with a deterministic cardinality constraint. Furthermore, we carry out experimental investigations for the chance constrained dominating set problem and show the benefit for this classical NP-hard problem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08767
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#23433;&#20840;&#24212;&#29992;&#20013;&#21487;&#38752;&#37096;&#32626;&#30340;&#20005;&#37325;&#23041;&#32961;&#65292;&#31245;&#24494;&#20462;&#25913;&#36755;&#20837;&#21363;&#21487;&#35823;&#23548;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#20559;&#31163;&#27491;&#24120;&#26679;&#26412;&#30340;&#22522;&#30784;&#25968;&#25454;&#27969;&#24418;&#65292;&#32780;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27491;&#24120;&#30340;NLP&#25968;&#25454;&#27969;&#24418;&#12290;&#20026;&#20102;&#25506;&#32034;&#22914;&#20309;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#65288;MLMD&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#22312;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#20135;&#29983;&#26126;&#26174;&#21487;&#21306;&#20998;&#30340;&#20449;&#21495;&#12290;MLMD&#20855;&#26377;&#21363;&#25554;&#21363;&#29992;&#30340;&#20351;&#29992;&#26041;&#27861;&#65288;&#21363;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21463;&#23475;&#27169;&#22411;&#65289;&#29992;&#20110;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#32780;&#19988;&#19981;&#21463;&#20998;&#31867;&#20219;&#21153;&#12289;&#21463;&#23475;&#27169;&#22411;&#32467;&#26500;&#21644;&#24453;&#38450;&#24481;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20803;&#35748;&#30693;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20449;&#24687;&#35823;&#23548;&#20195;&#29702;&#65292;&#22312;Twitter&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35797;&#28857;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21333;&#19968;&#31574;&#30053;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#21442;&#19982;&#30340;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2304.08759</link><description>&lt;p&gt;
&#26234;&#33021;&#22806;&#39592;&#39612;&#65306;&#22522;&#20110;&#20803;&#35748;&#30693;&#20195;&#29702;&#30340;&#25171;&#20987;&#20449;&#24687;&#35823;&#23548;&#31574;&#30053;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exoskeleton for the Mind: Exploring Strategies Against Misinformation with a Metacognitive Agent. (arXiv:2304.08759v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20803;&#35748;&#30693;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20449;&#24687;&#35823;&#23548;&#20195;&#29702;&#65292;&#22312;Twitter&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35797;&#28857;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#21333;&#19968;&#31574;&#30053;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#21442;&#19982;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#65292;&#20449;&#24687;&#35823;&#23548;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#24456;&#23569;&#26377;&#24050;&#30693;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#20102;&#25552;&#39640;&#20449;&#24687;&#24847;&#35782;&#30340;&#24037;&#20855;&#65292;&#20294;&#36825;&#20123;&#37117;&#26159;&#23553;&#38381;&#30340;&#31995;&#32479;&#65292;&#23578;&#26410;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#12290;&#20854;&#20182;&#20154;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#24037;&#20855;&#21644;&#31574;&#30053;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#22312;&#38745;&#24577;&#21050;&#28608;&#12289;&#30740;&#31350;&#21592;&#25552;&#31034;&#25110;&#20302;&#20445;&#30495;&#21407;&#22411;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#35748;&#30693;&#29702;&#35770;&#35780;&#20272;&#20110;Twitter&#20869;&#37096;&#30340;&#21453;&#20449;&#24687;&#35823;&#23548;&#20195;&#29702;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65288;n=17&#65289;&#21644;&#19968;&#39033;&#22810;&#37096;&#20998;&#23454;&#39564;&#30740;&#31350;&#65288;n=57&#65292;n=49&#65289;&#65292;&#20854;&#20013;&#21442;&#19982;&#32773;&#20307;&#39564;&#20102;&#19977;&#20010;&#29256;&#26412;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#29256;&#26412;&#37117;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#31574;&#30053;&#20248;&#20110;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;&#36879;&#26126;&#24230;&#21644;&#28165;&#26224;&#24230;&#22312;&#20195;&#29702;&#30340;&#24213;&#23618;&#36923;&#36753;&#19978;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#21450;&#23545;&#20110;&#21453;&#22797;&#25509;&#35302;&#35823;&#23548;&#20449;&#24687;&#21644;&#32570;&#20047;&#29992;&#25143;&#21442;&#19982;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is a global problem in modern social media platforms with few solutions known to be effective. Social media platforms have offered tools to raise awareness of information, but these are closed systems that have not been empirically evaluated. Others have developed novel tools and strategies, but most have been studied out of context using static stimuli, researcher prompts, or low fidelity prototypes. We offer a new anti-misinformation agent grounded in theories of metacognition that was evaluated within Twitter. We report on a pilot study (n=17) and multi-part experimental study (n=57, n=49) where participants experienced three versions of the agent, each deploying a different strategy. We found that no single strategy was superior over the control. We also confirmed the necessity of transparency and clarity about the agent's underlying logic, as well as concerns about repeated exposure to misinformation and lack of user engagement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.08754</link><description>&lt;p&gt;
W-MAE&#65306;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#20855;&#26377;&#30452;&#25509;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#30340;&#38271;&#26399;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#30340;&#36830;&#32493;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#25216;&#26415;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#30340;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#22825;&#27668;&#27169;&#22411;W-MAE&#12290;W-MAE&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#22312;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;W-MAE&#20197;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#27599;&#20845;&#23567;&#26102;&#36873;&#25321;&#19968;&#27425;&#26679;&#26412;&#65292;&#20165;&#20351;&#29992;&#20004;&#24180;&#30340;ERA5&#25968;&#25454;&#65292;&#23545;W-MAE&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23558;W-MAE&#19982;FourCastNet&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AsymSAT&#30340;GNN-based SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#21464;&#37327;&#20381;&#36182;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35813;&#26041;&#27861;&#30340;&#27714;&#35299;&#33021;&#21147;&#65292;&#22312;&#22823;&#22411;&#27979;&#35797;&#38598;&#19978;&#35299;&#20915;&#30340;SAT&#23454;&#20363;&#25968;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.08738</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#20013;&#30340;&#21464;&#37327;&#20381;&#36182;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Addressing Variable Dependency in GNN-based SAT Solving. (arXiv:2304.08738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AsymSAT&#30340;GNN-based SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#21464;&#37327;&#20381;&#36182;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35813;&#26041;&#27861;&#30340;&#27714;&#35299;&#33021;&#21147;&#65292;&#22312;&#22823;&#22411;&#27979;&#35797;&#38598;&#19978;&#35299;&#20915;&#30340;SAT&#23454;&#20363;&#25968;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#65288;&#36817;&#20284;&#65289;SAT&#27714;&#35299;&#12290;&#20856;&#22411;&#30340;GNN-based SAT&#27714;&#35299;&#22120;&#24182;&#34892;&#39044;&#27979;SAT&#35299;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#23545;&#31216;SAT&#38382;&#39064;&#32452;&#20013;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;SAT&#38382;&#39064;&#20013;&#24067;&#23572;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#24182;&#34892;&#39044;&#27979;&#20445;&#35777;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;AsymSAT&#65292;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#38598;&#25104;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#29983;&#25104;&#21464;&#37327;&#20998;&#37197;&#30340;&#20381;&#36182;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20381;&#36182;&#21464;&#37327;&#39044;&#27979;&#25193;&#23637;&#20102;GNN-based&#26041;&#27861;&#30340;&#27714;&#35299;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#25552;&#39640;&#20102;&#22823;&#22411;&#27979;&#35797;&#38598;&#19978;&#35299;&#20915;&#30340;SAT&#23454;&#20363;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean satisfiability problem (SAT) is fundamental to many applications. Existing works have used graph neural networks (GNNs) for (approximate) SAT solving. Typical GNN-based end-to-end SAT solvers predict SAT solutions concurrently. We show that for a group of symmetric SAT problems, the concurrent prediction is guaranteed to produce a wrong answer because it neglects the dependency among Boolean variables in SAT problems. % We propose AsymSAT, a GNN-based architecture which integrates recurrent neural networks to generate dependent predictions for variable assignments. The experiment results show that dependent variable prediction extends the solving capability of the GNN-based method as it improves the number of solved SAT instances on large test sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08733</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#26377;&#30456;&#21516;&#30340;&#30524;&#30555;&#21527;&#65311;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33391;&#22909;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#27169;&#20223;&#20174;&#35757;&#32451;&#26631;&#31614;&#20013;&#23398;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#26399;&#35270;&#35273;&#30740;&#31350;&#30340;&#22823;&#37096;&#20998;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#20934;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#27169;&#22411;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#26041;&#38754;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#37327;&#21270;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#26469;&#28304;&#38169;&#35823;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#38590;&#24230;&#32423;&#21035;&#23545;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#65292;&#25506;&#35752;&#20154;&#31867;&#19982;&#26426;&#22120;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#24322;&#12290;&#21363;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#30456;&#20284;&#65292;&#31572;&#26696;&#30340;&#20998;&#24067;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#65292;&#20854;&#34920;&#29616;&#27604;&#21333;&#29420;&#30340;&#20154;&#25110;&#26426;&#22120;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
&lt;/p&gt;</description></item><item><title>LTC-SE&#26159;&#19968;&#31181;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#23558;&#22810;&#31181;&#31070;&#32463;&#20803;&#27169;&#22411;&#32479;&#19968;&#65292;&#20854;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#28385;&#36275;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#24615;&#33021;&#35201;&#27714;&#65292;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08691</link><description>&lt;p&gt;
LTC-SE: &#25193;&#23637;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems. (arXiv:2304.08691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08691
&lt;/p&gt;
&lt;p&gt;
LTC-SE&#26159;&#19968;&#31181;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#23558;&#22810;&#31181;&#31070;&#32463;&#20803;&#27169;&#22411;&#32479;&#19968;&#65292;&#20854;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#28385;&#36275;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#24615;&#33021;&#35201;&#27714;&#65292;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LTC-SE&#65292;&#36825;&#26159;Hasani&#31561;&#20154;&#20110;2021&#24180;&#26368;&#21021;&#25552;&#20986;&#30340;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#35813;&#31639;&#27861;&#23558;&#28431;&#30005;&#31215;&#20998;-&#28779;&#31070;&#32463;&#20803;&#27169;&#22411;&#19982;&#36830;&#32493;&#26102;&#38388;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;CTRNN&#65289;&#12289;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;LTC-SE&#30340;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#22686;&#24378;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#20197;&#28385;&#36275;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#21644;&#20005;&#26684;&#24615;&#33021;&#35201;&#27714;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#29420;&#29305;&#32422;&#26463;&#12290;&#26356;&#26032;&#21518;&#30340;&#20195;&#30721;&#26159;&#19968;&#20010;&#19982;TensorFlow 2.x&#20860;&#23481;&#30340;&#32508;&#21512;&#31867;&#24211;&#65292;&#20026;LTCCell&#12289;CTRNN&#12289;NODE&#21644;CTGRU&#31867;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#37197;&#32622;&#36873;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20197;&#24448;&#30340;&#29256;&#26412;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#20248;&#21270;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;Keras&#20989;&#25968;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#28165;&#26224;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#25913;&#36827;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LTC-SE, an improved version of the Liquid Time-Constant (LTC) neural network algorithm originally proposed by Hasani et al. in 2021. This algorithm unifies the Leaky-Integrate-and-Fire (LIF) spiking neural network model with Continuous-Time Recurrent Neural Networks (CTRNNs), Neural Ordinary Differential Equations (NODEs), and bespoke Gated Recurrent Units (GRUs). The enhancements in LTC-SE focus on augmenting flexibility, compatibility, and code organization, targeting the unique constraints of embedded systems with limited computational resources and strict performance requirements. The updated code serves as a consolidated class library compatible with TensorFlow 2.x, offering comprehensive configuration options for LTCCell, CTRNN, NODE, and CTGRU classes. We evaluate LTC-SE against its predecessors, showcasing the advantages of our optimizations in user experience, Keras function compatibility, and code clarity. These refinements expand the applicability of liquid neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20132;&#20114;&#24335;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08670</link><description>&lt;p&gt;
&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#12289;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text. (arXiv:2304.08670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20132;&#20114;&#24335;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#35745;&#31639;&#35774;&#22791;&#21644;&#25968;&#23383;&#23186;&#20171;&#36827;&#34892;&#20219;&#21153;&#65292;&#23558;&#20197;&#21069;&#25163;&#21160;&#23436;&#25104;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#25968;&#23383;&#21270;&#29256;&#26412;&#30340;&#20219;&#20309;&#26041;&#27861;&#37117;&#20250;&#21463;&#21040;&#27426;&#36814;&#12290;&#23613;&#31649;&#20170;&#22825;&#21487;&#20197;&#22312;&#32447;&#23436;&#25104;&#35768;&#22810;&#25991;&#26723;&#20219;&#21153;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#24212;&#29992;&#21644;&#39046;&#22495;&#26080;&#27861;&#36991;&#20813;&#25163;&#20889;&#25991;&#26412;&#65292;&#36825;&#20351;&#25163;&#20889;&#25991;&#26723;&#30340;&#25968;&#23383;&#21270;&#25104;&#20026;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#22823;&#37096;&#20998;&#30340;&#23581;&#35797;&#24050;&#32463;&#36716;&#21521;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#26356;&#22797;&#26434;&#21644;&#26356;&#28145;&#20837;&#30340;&#32593;&#32476;&#65292;&#24182;&#20445;&#35777;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26377;&#26356;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20170;&#22825;&#29992;&#20110;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#24211;&#37117;&#26159;&#25163;&#21160;&#25110;&#21322;&#33258;&#21160;&#27880;&#37322;&#30340;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#36890;&#24120;&#35268;&#27169;&#36739;&#23567;&#65292;&#26080;&#27861;&#28385;&#36275;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#12289;&#20132;&#20114;&#24335;&#30340;&#27880;&#37322;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#30340;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#25968;&#25454;&#31649;&#29702;&#27169;&#22359;&#12289;&#27880;&#37322;&#27169;&#22359;&#21644;&#27169;&#22411;&#35757;&#32451;&#27169;&#22359;&#12290;&#27880;&#37322;&#27169;&#22359;&#29992;&#25143;&#21451;&#22909;&#65292;&#20801;&#35768;&#20132;&#20114;&#24335;&#27880;&#37322;&#33609;&#20889;&#21644;&#21360;&#21047;&#33521;&#35821;&#25163;&#20889;&#25991;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#27880;&#37322;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the surging inclination towards carrying out tasks on computational devices and digital mediums, any method that converts a task that was previously carried out manually, to a digitized version, is always welcome. Irrespective of the various documentation tasks that can be done online today, there are still many applications and domains where handwritten text is inevitable, which makes the digitization of handwritten documents a very essential task. Over the past decades, there has been extensive research on offline handwritten text recognition. In the recent past, most of these attempts have shifted to Machine learning and Deep learning based approaches. In order to design more complex and deeper networks, and ensure stellar performances, it is essential to have larger quantities of annotated data. Most of the databases present for offline handwritten text recognition today, have either been manually annotated or semi automatically annotated with a lot of manual involvement. Thes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GAN&#29983;&#25104;&#22823;&#37327;&#34394;&#20551;&#23456;&#29289;&#22270;&#20687;&#65292;&#19978;&#20256;&#21040;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#23456;&#29289;&#22270;&#20687;&#36134;&#25143;&#30456;&#21516;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#65292;&#20026;&#23456;&#29289;&#30103;&#27861;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.08665</link><description>&lt;p&gt;
Insta&#65288;nt&#65289;&#23456;&#29289;&#30103;&#27861;&#65306;GAN&#29983;&#25104;&#30340;&#22270;&#20687;&#29992;&#20110;&#27835;&#30103;&#24615;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Insta(nt) Pet Therapy: GAN-generated Images for Therapeutic Social Media Content. (arXiv:2304.08665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GAN&#29983;&#25104;&#22823;&#37327;&#34394;&#20551;&#23456;&#29289;&#22270;&#20687;&#65292;&#19978;&#20256;&#21040;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#36798;&#21040;&#20102;&#19982;&#20256;&#32479;&#23456;&#29289;&#22270;&#20687;&#36134;&#25143;&#30456;&#21516;&#30340;&#29992;&#25143;&#21442;&#19982;&#24230;&#65292;&#20026;&#23456;&#29289;&#30103;&#27861;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26597;&#30475;&#23456;&#29289;&#22270;&#20687;&#23545;&#20154;&#30340;&#27835;&#30103;&#25928;&#26524;&#24050;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#23456;&#29289;&#20027;&#20154;&#25293;&#25668;&#29031;&#29255;&#24182;&#19978;&#20256;&#65292;&#22240;&#27492;&#24456;&#38590;&#33719;&#24471;&#22823;&#35268;&#27169;&#30340;&#36825;&#31181;&#20869;&#23481;&#29983;&#20135;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26694;&#26550;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#34394;&#20551;&#23456;&#29289;&#22270;&#20687;&#12290;&#36825;&#20123;&#22270;&#20687;&#19978;&#20256;&#21040;Instagram&#36134;&#25143;&#65292;&#20351;&#29992;&#25143;&#21442;&#19982;&#24230;&#19982;&#20256;&#32479;&#23456;&#29289;&#29031;&#29255;&#36134;&#25143;&#20013;&#30340;&#22270;&#29255;&#19968;&#26679;&#39640;&#65292;&#24378;&#35843;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#23456;&#29289;&#30103;&#27861;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The positive therapeutic effect of viewing pet images online has been well-studied. However, it is difficult to obtain large-scale production of such content since it relies on pet owners to capture photographs and upload them. I use a Generative Adversarial Network-based framework for the creation of fake pet images at scale. These images are uploaded on an Instagram account where they drive user engagement at levels comparable to those seen with images from accounts with traditional pet photographs, underlining the applicability of the framework to be used for pet-therapy social media content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#23558;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#12289;&#36830;&#32493;&#30340;&#36339;&#36291;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.08663</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#30340;&#36830;&#32493;&#22810;&#21151;&#33021;&#36339;&#36291;
&lt;/p&gt;
&lt;p&gt;
Continuous Versatile Jumping Using Learned Action Residuals. (arXiv:2304.08663v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#23558;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#12289;&#36830;&#32493;&#30340;&#36339;&#36291;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36339;&#36291;&#23545;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36890;&#36807;&#22256;&#38590;&#22320;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23039;&#24577;&#25511;&#21046;&#22120;&#65292;&#23427;&#23558;&#25163;&#21160;&#35774;&#35745;&#30340;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#19982;&#23398;&#20064;&#21040;&#30340;&#21097;&#20313;&#31574;&#30053;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#30001;&#20110;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#20026;&#39640;&#25928;&#35757;&#32451;warm start&#31574;&#30053;&#65292;&#25152;&#20197;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20811;&#26381;&#20102;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24182;&#25552;&#39640;&#20102;&#36339;&#36291;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#20302;&#23618;&#20840;&#36523;&#25511;&#21046;&#22120;&#23558;&#23039;&#21183;&#25511;&#21046;&#22120;&#30340;&#36523;&#20307;&#23039;&#21183;&#21629;&#20196;&#36716;&#25442;&#25104;&#30005;&#26426;&#21629;&#20196;&#12290;&#32463;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#30340;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#65292;&#25191;&#34892;&#22810;&#21151;&#33021;&#30340;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#65292;&#21253;&#25324;&#39640;&#36798;50cm&#12289;&#21521;&#21069;60cm&#30340;&#20840;&#21521;&#36339;&#36291;&#21644;&#39640;&#36798;90&#24230;&#30340;&#36339;&#36291;&#36716;&#21521;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20197;&#33719;&#21462;&#26356;&#22810;&#32467;&#26524;:https://sites.google.com/view/jumping-rl/home
&lt;/p&gt;
&lt;p&gt;
Jumping is essential for legged robots to traverse through difficult terrains. In this work, we propose a hierarchical framework that combines optimal control and reinforcement learning to learn continuous jumping motions for quadrupedal robots. The core of our framework is a stance controller, which combines a manually designed acceleration controller with a learned residual policy. As the acceleration controller warm starts policy for efficient training, the trained policy overcomes the limitation of the acceleration controller and improves the jumping stability. In addition, a low-level whole-body controller converts the body pose command from the stance controller to motor commands. After training in simulation, our framework can be deployed directly to the real robot, and perform versatile, continuous jumping motions, including omni-directional jumps at up to 50cm high, 60cm forward, and jump-turning at up to 90 degrees. Please visit our website for more results: https://sites.goo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;(LC)$^2$&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;2.5D&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#27809;&#26377;&#20808;&#39564;&#28857;&#20113;&#22320;&#22270;&#30340;LiDAR&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.08660</link><description>&lt;p&gt;
(LC)$^2$:&#22522;&#20110;LiDAR-Camera&#24490;&#29615;&#32422;&#26463;&#30340;&#36328;&#27169;&#24577;&#22320;&#28857;&#35782;&#21035;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
(LC)$^2$: LiDAR-Camera Loop Constraints For Cross-Modal Place Recognition. (arXiv:2304.08660v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08660
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;(LC)$^2$&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;2.5D&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#27809;&#26377;&#20808;&#39564;&#28857;&#20113;&#22320;&#22270;&#30340;LiDAR&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#20301;&#19968;&#30452;&#26159;&#33258;&#20027;&#23548;&#33322;&#30340;&#19968;&#20010;&#38590;&#39064;&#65292;&#22320;&#28857;&#35782;&#21035;&#21644;&#37325;&#26032;&#23450;&#20301;&#23545;&#20110;&#26426;&#22120;&#20154;&#30340;&#23454;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#27979;&#37327;&#25968;&#25454;&#21040;&#23450;&#20301;&#25551;&#36848;&#31526;&#30340;&#19968;&#33268;&#36716;&#25442;&#12290;&#34903;&#26223;&#22270;&#20687;&#23481;&#26131;&#33719;&#21462;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#22806;&#35266;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;LiDAR&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#26500;&#24314;&#28857;&#20113;&#25968;&#25454;&#24211;&#25104;&#26412;&#39640;&#65292;&#28857;&#20113;&#21482;&#22312;&#26377;&#38480;&#30340;&#22320;&#26041;&#23384;&#22312;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;2.5D&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#25968;&#25454;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;(LC)$^2$&#65292;&#29992;&#20110;&#23454;&#29616;&#27809;&#26377;&#20808;&#39564;&#28857;&#20113;&#22320;&#22270;&#30340;LiDAR&#23450;&#20301;&#12290;&#20026;&#27492;&#65292;&#22312;&#21305;&#37197;&#20043;&#21069;&#65292;&#23558;LiDAR&#27979;&#37327;&#34920;&#31034;&#20026;&#36317;&#31163;&#22270;&#20687;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localization has been a challenging task for autonomous navigation. A loop detection algorithm must overcome environmental changes for the place recognition and re-localization of robots. Therefore, deep learning has been extensively studied for the consistent transformation of measurements into localization descriptors. Street view images are easily accessible; however, images are vulnerable to appearance changes. LiDAR can robustly provide precise structural information. However, constructing a point cloud database is expensive, and point clouds exist only in limited places. Different from previous works that train networks to produce shared embedding directly between the 2D image and 3D point cloud, we transform both data into 2.5D depth images for matching. In this work, we propose a novel cross-matching method, called (LC)$^2$, for achieving LiDAR localization without a prior point cloud map. To this end, LiDAR measurements are expressed in the form of range images before matching
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#35780;&#20272;&#65306;&#35805;&#35821;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08637
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#24191;&#27867;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#21508;&#31181;&#36755;&#20986;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#19982;&#36755;&#20986;&#30149;&#24577;&#65288;&#20363;&#22914;&#65292;&#21453;&#20107;&#23454;&#21644;&#36923;&#36753;&#19978;&#30340;&#38169;&#35823;&#38472;&#36848;&#65289;&#20197;&#21450;&#19981;&#20445;&#25345;&#20027;&#39064;&#31561;&#26041;&#38754;&#30340;&#20851;&#31995;&#20013;&#65292;&#35760;&#24518;&#25991;&#26412;&#30334;&#20998;&#27604;&#12289;&#29420;&#29305;&#25991;&#26412;&#30334;&#20998;&#27604;&#21644;&#25972;&#20307;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;80.0&#65285;&#30340;&#36755;&#20986;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#20063;&#26356;&#26377;&#21487;&#33021;&#34987;&#35748;&#20026;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#21644;&#35780;&#20272;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#65292;&#22312;&#35780;&#20272;&#30340;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#30340;&#35760;&#24518;&#25991;&#26412;&#29575;&#26377;&#25152;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#23398;&#20064;&#12289;&#35760;&#24518;&#21644;&#35780;&#20272;&#20248;&#36136;&#25991;&#26412;&#30340;&#28508;&#22312;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25552;&#39640;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#20998;&#31867;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08602</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#26234;&#33021;&#30005;&#32593;&#30340;&#20132;&#21449;&#36335;&#24452;&#65306;&#27010;&#36848;&#65292;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives. (arXiv:2304.08602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25552;&#39640;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#20998;&#31867;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#20351;&#24471;&#28040;&#36153;&#32773;&#30340;&#38544;&#31169;&#25104;&#20026;&#26234;&#33021;&#30005;&#32593;&#65288;SGs&#65289;&#20013;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#29992;&#20110;&#19981;&#21516;&#26381;&#21153;&#26102;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23558;&#35757;&#32451;&#25512;&#21521;&#36793;&#32536;&#65292;&#20026;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#20102;&#24456;&#22909;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;SGs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#32570;&#28857;&#65292;&#20027;&#35201;&#21253;&#25324;&#36127;&#36733;&#39044;&#27979;&#65292;&#30005;&#21160;&#27773;&#36710;&#65292;&#25925;&#38556;&#35786;&#26029;&#65292;&#36127;&#36733;&#20998;&#35299;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#20027;&#35201;&#35774;&#35745;&#36235;&#21183;&#21644;&#21487;&#33021;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consumer's privacy is a main concern in Smart Grids (SGs) due to the sensitivity of energy data, particularly when used to train machine learning models for different services. These data-driven models often require huge amounts of data to achieve acceptable performance leading in most cases to risks of privacy leakage. By pushing the training to the edge, Federated Learning (FL) offers a good compromise between privacy preservation and the predictive performance of these models. The current paper presents an overview of FL applications in SGs while discussing their advantages and drawbacks, mainly in load forecasting, electric vehicles, fault diagnoses, load disaggregation and renewable energies. In addition, an analysis of main design trends and possible taxonomies is provided considering data partitioning, the communication topology, and security mechanisms. Towards the end, an overview of main challenges facing this technology and potential future directions is presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#27880;&#37322;&#21644;&#30456;&#26426;-LiDAR&#21518;&#26399;&#34701;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20415;&#20110;&#20934;&#30830;&#36827;&#34892;&#28857;&#20113;&#25968;&#25454;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.08591</link><description>&lt;p&gt;
PALF: &#39044;&#27880;&#37322;&#21644;&#30456;&#26426;-LiDAR&#21518;&#26399;&#34701;&#21512;&#65292;&#20026;&#28857;&#20113;&#30340;&#26131;&#27880;&#37322;&#25552;&#20379;&#20415;&#21033;
&lt;/p&gt;
&lt;p&gt;
PALF: Pre-Annotation and Camera-LiDAR Late Fusion for the Easy Annotation of Point Clouds. (arXiv:2304.08591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#27880;&#37322;&#21644;&#30456;&#26426;-LiDAR&#21518;&#26399;&#34701;&#21512;&#30340;&#31639;&#27861;&#65292;&#20197;&#20415;&#20110;&#20934;&#30830;&#36827;&#34892;&#28857;&#20113;&#25968;&#25454;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#65292;3D&#29289;&#20307;&#26816;&#27979;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31361;&#30772;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#30340;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#19982;2D&#22270;&#20687;&#26631;&#31614;&#19981;&#21516;&#65292;&#28857;&#20113;&#25968;&#25454;&#30340;&#27880;&#37322;&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#19981;&#35268;&#21017;&#24615;&#21644;&#20302;&#20998;&#36776;&#29575;&#31561;&#22256;&#38590;&#65292;&#27880;&#37322;&#25928;&#29575;&#27604;2D&#22270;&#20687;&#20302;&#24471;&#22810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28857;&#20113;&#25968;&#25454;&#27880;&#37322;&#31639;&#27861;&#65292;&#20351;&#29992;&#39044;&#27880;&#37322;&#21644;&#30456;&#26426;-LiDAR&#21518;&#26399;&#34701;&#21512;&#31639;&#27861;&#65292;&#20197;&#20415;&#20110;&#20934;&#30830;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#31181;&#39044;&#27880;&#37322;&#31639;&#27861;&#65292;&#21033;&#29992;3D&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#36866;&#24212;&#25311;&#21512;&#65292;&#20197;&#20415;&#20110;&#27880;&#37322;&#28857;&#20113;&#65307;&#65288;2&#65289;&#19968;&#31181;&#30456;&#26426;-LiDAR&#21518;&#26399;&#34701;&#21512;&#31639;&#27861;&#65292;&#21033;&#29992;2D&#21644;3D&#32467;&#26524;&#36827;&#34892;&#36731;&#26494;&#30340;&#38169;&#35823;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection has become indispensable in the field of autonomous driving. To date, gratifying breakthroughs have been recorded in 3D object detection research, attributed to deep learning. However, deep learning algorithms are data-driven and require large amounts of annotated point cloud data for training and evaluation. Unlike 2D image labels, annotating point cloud data is difficult due to the limitations of sparsity, irregularity, and low resolution, which requires more manual work, and the annotation efficiency is much lower than 2D image.Therefore, we propose an annotation algorithm for point cloud data, which is pre-annotation and camera-LiDAR late fusion algorithm to easily and accurately annotate. The contributions of this study are as follows. We propose (1) a pre-annotation algorithm that employs 3D object detection and auto fitting for the easy annotation of point clouds, (2) a camera-LiDAR late fusion algorithm using 2D and 3D results for easily error checking, whic
&lt;/p&gt;</description></item><item><title>Generative Disco&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#65307;&#29992;&#25143;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#21487;&#35270;&#21270;&#65292;&#21487;&#29983;&#25104;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#65292;&#24341;&#20837;&#20102;&#8220;&#36807;&#28193;&#8221;&#21644;&#8220;&#20445;&#25345;&#8221;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#24182;&#36866;&#29992;&#20110;&#19987;&#19994;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2304.08551</link><description>&lt;p&gt;
&#21019;&#20316;&#36842;&#26031;&#31185;&#65306;&#38899;&#20048;&#21487;&#35270;&#21270;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generative Disco: Text-to-Video Generation for Music Visualization. (arXiv:2304.08551v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08551
&lt;/p&gt;
&lt;p&gt;
Generative Disco&#26159;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#65307;&#29992;&#25143;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#21487;&#35270;&#21270;&#65292;&#21487;&#29983;&#25104;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#65292;&#24341;&#20837;&#20102;&#8220;&#36807;&#28193;&#8221;&#21644;&#8220;&#20445;&#25345;&#8221;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#24182;&#36866;&#29992;&#20110;&#19987;&#19994;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26159;&#38899;&#20048;&#20307;&#39564;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25918;&#22823;&#38899;&#20048;&#20256;&#36798;&#30340;&#24773;&#24863;&#21644;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#38899;&#20048;&#21487;&#35270;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#32791;&#26102;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Generative Disco&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24110;&#21161;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#12290;&#29992;&#25143;&#36873;&#25321;&#35201;&#21487;&#35270;&#21270;&#30340;&#38899;&#20048;&#38388;&#38548;&#65292;&#28982;&#21518;&#36890;&#36807;&#23450;&#20041;&#24320;&#22987;&#21644;&#32467;&#26463;&#25552;&#31034;&#26469;&#21442;&#25968;&#21270;&#35813;&#21487;&#35270;&#21270;&#12290;&#36825;&#20123;&#25552;&#31034;&#26681;&#25454;&#38899;&#20048;&#30340;&#33410;&#22863;&#36827;&#34892;&#21464;&#24418;&#21644;&#29983;&#25104;&#65292;&#20197;&#20135;&#29983;&#21453;&#24212;&#38899;&#39057;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25913;&#36827;&#29983;&#25104;&#35270;&#39057;&#30340;&#35774;&#35745;&#27169;&#24335;&#65306;&#8220;&#36807;&#28193;&#8221;&#65292;&#34920;&#36798;&#39068;&#33394;&#12289;&#26102;&#38388;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#30340;&#21464;&#21270;&#65292;&#8220;&#20445;&#25345;&#8221;&#65292;&#40723;&#21169;&#35270;&#35273;&#37325;&#28857;&#21644;&#19968;&#33268;&#24615;&#12290;&#19987;&#19994;&#20154;&#21592;&#21442;&#19982;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#24841;&#24742;&#24615;&#12289;&#26131;&#20110;&#25506;&#32034;&#21644;&#39640;&#24230;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;Generative Disco&#22312;&#19987;&#19994;&#20154;&#22763;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#20197;&#21450;AI&#29983;&#25104;&#20869;&#23481;&#19982;&#29256;&#26435;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visuals are a core part of our experience of music, owing to the way they can amplify the emotions and messages conveyed through the music. However, creating music visualization is a complex, time-consuming, and resource-intensive process. We introduce Generative Disco, a generative AI system that helps generate music visualizations with large language models and text-to-image models. Users select intervals of music to visualize and then parameterize that visualization by defining start and end prompts. These prompts are warped between and generated according to the beat of the music for audioreactive video. We introduce design patterns for improving generated videos: "transitions", which express shifts in color, time, subject, or style, and "holds", which encourage visual emphasis and consistency. A study with professionals showed that the system was enjoyable, easy to explore, and highly expressive. We conclude on use cases of Generative Disco for professionals and how AI-generated c
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#21644;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#35823;&#24046;&#21644;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08498</link><description>&lt;p&gt;
&#25490;&#21517;&#25439;&#22833;&#21644;&#20132;&#38169;&#23398;&#20064;&#20943;&#23569;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology. (arXiv:2304.08498v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#21644;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#35823;&#24046;&#21644;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#26377;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22270;&#20687;&#25628;&#32034;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#30149;&#29702;&#23398;&#26723;&#26696;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#38382;&#39064;&#12290;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#26159;AI&#20559;&#35265;&#21644;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#19968;&#20010;&#26356;&#29305;&#21035;&#30340;&#28145;&#24230;&#27169;&#22411;&#32570;&#28857;&#26159;&#23545;&#25628;&#32034;&#21151;&#33021;&#30340;&#26080;&#30693;&#12290;&#21069;&#32773;&#24433;&#21709;&#27599;&#20010;&#27169;&#22411;&#65292;&#21518;&#32773;&#21482;&#24433;&#21709;&#25628;&#32034;&#21644;&#21305;&#37197;&#12290;&#30001;&#20110;&#32570;&#20047;&#22522;&#20110;&#25490;&#21517;&#30340;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24517;&#39035;&#22522;&#20110;&#20998;&#31867;&#35823;&#24046;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#25152;&#24471;&#30340;&#23884;&#20837;&#36827;&#34892;&#22270;&#20687;&#25628;&#32034;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#21508;&#31181;&#21307;&#38498;&#30340;&#22823;&#22411;&#22270;&#20687;&#24211;&#65292;&#28145;&#24230;&#27169;&#22411;&#20284;&#20046;&#20063;&#23481;&#26131;&#20135;&#29983;&#20869;&#37096;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#25351;&#23548;&#29305;&#24449;&#25552;&#21462;&#26397;&#21521;&#25628;&#32034;&#30340;&#21305;&#37197;&#23548;&#21521;&#24615;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#20998;&#31867;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#20869;&#37096;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision, significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality. The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08495</link><description>&lt;p&gt;
&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#32676;&#20307;&#25928;&#29992;&#20248;&#21270;&#65306;&#19968;&#31181;&#31574;&#30053;&#24615;&#21644;&#20247;&#21253;&#24847;&#35782;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#31243;&#25512;&#33616;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#22797;&#26434;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#21040;&#20248;&#21270;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#35832;&#22810;&#21442;&#25968;&#65292;&#22914;&#26223;&#28857;&#21463;&#27426;&#36814;&#31243;&#24230;&#12289;&#25490;&#38431;&#26102;&#38388;&#12289;&#27493;&#34892;&#26102;&#38388;&#21644;&#33829;&#19994;&#26102;&#38388;&#31561;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#20154;&#35270;&#35282;&#19978;&#65292;&#26410;&#33021;&#35299;&#20915;&#33258;&#28982;&#20154;&#32676;&#34892;&#20026;&#24341;&#36215;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#22914;&#36138;&#23146;&#36335;&#30001;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25112;&#30053;&#21644;&#20247;&#21253;&#24847;&#35782;&#34892;&#31243;&#25512;&#33616;&#65288;SCAIR&#65289;&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#36335;&#32447;&#25512;&#33616;&#31574;&#30053;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#32534;&#30721;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#21644;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#20844;&#22253;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#21508;&#31181;&#31454;&#20105;&#24615;&#21644;&#29616;&#23454;&#30340;&#22522;&#32447;&#27979;&#35797;&#65292;&#35777;&#26126;SCAIR&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26234;&#33021;&#23478;&#23621;&#19978;&#19979;&#25991;&#24863;&#30693;&#29615;&#22659;&#30340;&#21407;&#22411;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#27169;&#25311;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#39044;&#23450;&#20041;&#35268;&#21017;&#38477;&#20302;&#35774;&#22791;&#36816;&#34892;&#30340;&#25805;&#20316;&#25104;&#26412;&#65292;&#24182;&#30417;&#25511;&#23621;&#27665;&#30340;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2304.08494</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#27169;&#22411;&#30340;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Smart Home Environment Modelled with a Multi-Agent System. (arXiv:2304.08494v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#26234;&#33021;&#23478;&#23621;&#19978;&#19979;&#25991;&#24863;&#30693;&#29615;&#22659;&#30340;&#21407;&#22411;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#27169;&#25311;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#39044;&#23450;&#20041;&#35268;&#21017;&#38477;&#20302;&#35774;&#22791;&#36816;&#34892;&#30340;&#25805;&#20316;&#25104;&#26412;&#65292;&#24182;&#30417;&#25511;&#23621;&#27665;&#30340;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#26159;&#25351;&#36890;&#36807;&#33258;&#21160;&#21270;&#25216;&#26415;&#31649;&#29702;&#23478;&#29992;&#30005;&#22120;&#21644;&#31995;&#32479;&#65292;&#20197;&#24110;&#21161;&#26085;&#24120;&#29983;&#27963;&#30340;&#23621;&#20303;&#22320;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#35813;&#21407;&#22411;&#27169;&#25311;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#29615;&#22659;&#65292;&#22312;&#35774;&#35745;&#30340;&#26234;&#33021;&#23478;&#23621;&#20013;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20351;&#29992;&#19977;&#20010;&#26234;&#33021;&#20307;&#21644;&#20116;&#20010;&#20301;&#32622;&#22312;&#25151;&#23376;&#37324;&#36827;&#34892;&#20102;&#27169;&#25311;&#12290;&#19978;&#19979;&#25991;&#24863;&#30693;&#26234;&#33021;&#20307;&#22522;&#20110;&#20026;&#26085;&#24120;&#27963;&#21160;&#35774;&#35745;&#30340;&#39044;&#23450;&#20041;&#35268;&#21017;&#36827;&#34892;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#24314;&#35758;&#26088;&#22312;&#38477;&#20302;&#35774;&#22791;&#36816;&#34892;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#26410;&#26469;&#65292;&#23621;&#20303;&#22312;&#26234;&#33021;&#23478;&#23621;&#20869;&#30340;&#23621;&#27665;&#20581;&#24247;&#26041;&#38754;&#30340;&#30417;&#25511;&#23558;&#32500;&#25345;&#20182;&#20204;&#30340;&#26085;&#24120;&#20581;&#24247;&#29983;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
A smart home can be considered a place of residence that enables the management of appliances and systems to help with day-to-day life by automated technology. In the current paper is described a prototype that simulates a context-aware environment, developed in a designed smart home. The smart home environment has been simulated using three agents and five locations in a house. The context-aware agents behave based on predefined rules designed for daily activities. Our proposal aims to reduce operational cost of running devices. In the future, monitors of health aspects belonging to home residents will sustain their healthy life daily.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#35843;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#65292;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.08493</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#30340;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications. (arXiv:2304.08493v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#35843;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#65292;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892; (CTDE) &#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (MADRL) &#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#12290;&#20026;&#27492;&#65292;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#38598;&#20013;&#24335;&#35757;&#32451;&#20013;&#29992;&#20110;&#21327;&#20316;&#22810;&#20010;&#26234;&#33021;&#20307;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#30340;&#24635;&#26381;&#21153;&#36136;&#37327; (QoS)&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel centralized training and distributed execution (CTDE)-based multi-agent deep reinforcement learning (MADRL) method for multiple unmanned aerial vehicles (UAVs) control in autonomous mobile access applications. For the purpose, a single neural network is utilized in centralized training for cooperation among multiple agents while maximizing the total quality of service (QoS) in mobile access applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.07874</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#12289;&#22522;&#20110;Vision Transformer&#30340;&#38750;&#22343;&#36136;&#21435;&#38654;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#21435;&#38654;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#22343;&#36136;&#38654;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#22312;&#24212;&#29992;&#20110;&#23384;&#22312;&#38750;&#22343;&#36136;&#38654;&#30340;&#22270;&#20687;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;NTIRE&#25361;&#25112;&#20171;&#32461;&#30340;NH-HAZE23&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#19968;&#20010;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#38750;&#22343;&#36136;&#38654;&#19981;&#31526;&#21512;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#23545;&#22823;&#37327;&#30340;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#19982;&#20854;&#28165;&#26224;&#23545;&#24212;&#39033;&#36827;&#34892;&#37197;&#23545;&#65292;&#32780;NH-HAZE23&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#38750;&#22343;&#36136;&#21435;&#38654;&#25968;&#25454;&#38598;&#25193;&#20805;NH-HAZE23&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07869</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#27969;&#21160;&#24615;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#20960;&#31181;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#39046;&#22495;&#30475;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#21364;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#20197;&#30830;&#23450;&#21738;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39046;&#22495;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#22312; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#25506;&#32034;&#21033;&#29992;&#21508;&#31181; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#26469;&#22686;&#24378;&#23427;&#30340;&#31574;&#30053;&#12290;&#35813;&#23454;&#29616;&#35797;&#22270;&#35299;&#24320; NMT &#24212;&#29992;&#31243;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#30830;&#23450;&#19981;&#21516;&#30340;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20462;&#25913;&#25152;&#36848;&#24212;&#29992;&#31243;&#24207;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.06970</link><description>&lt;p&gt;
H2TNE&#65306;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces. (arXiv:2304.06970v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; H2TNE &#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#26102;&#24577;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#19982;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;temporal HIN&#65289;&#23884;&#20837;&#65292;&#26088;&#22312;&#23558;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#34920;&#31034;&#20026;&#20302;&#32500;&#31354;&#38388;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#35768;&#22810;&#20851;&#20110;&#26102;&#38388;HIN&#23884;&#20837;&#30340;&#21162;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#20123;&#21487;&#35266;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#35768;&#22810;&#32593;&#32476;&#37117;&#26174;&#31034;&#20986;&#20998;&#23618;&#23646;&#24615;&#21644;&#24130;&#24459;&#20998;&#24067;&#65292;&#24182;&#19981;&#26159;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#31561;&#36317;&#30340;&#12290;&#26368;&#36817;&#65292;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23545;&#20855;&#26377;&#20998;&#23618;&#21644;&#24130;&#24459;&#32467;&#26500;&#30340;&#25968;&#25454;&#26159;&#26377;&#25928;&#30340;&#12290;&#21463;&#36825;&#20010;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#26354;&#24322;&#26500;&#26102;&#38388;&#32593;&#32476;&#23884;&#20837;&#65288;H2TNE&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#24577;HIN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26102;&#38388;&#21644;&#24322;&#36136;&#24615;&#21452;&#37325;&#32422;&#26463;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#26469;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#28982;&#21518;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Temporal heterogeneous information network (temporal HIN) embedding, aiming to represent various types of nodes of different timestamps into low dimensional spaces while preserving structural and semantic information, is of vital importance in diverse real-life tasks. Researchers have made great efforts on temporal HIN embedding in Euclidean spaces and got some considerable achievements. However, there is always a fundamental conflict that many real-world networks show hierarchical property and power-law distribution, and are not isometric of Euclidean spaces. Recently, representation learning in hyperbolic spaces has been proved to be valid for data with hierarchical and power-law structure. Inspired by this character, we propose a hyperbolic heterogeneous temporal network embedding (H2TNE) model for temporal HINs. Specifically, we leverage a temporally and heterogeneously double-constrained random walk strategy to capture the structural and semantic information, and then calculate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.06336</link><description>&lt;p&gt;
&#22810;&#23646;&#24615;&#22810;&#38454;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#20174;&#22810;&#20851;&#31995;&#32593;&#32476;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#20803;&#36335;&#24452;&#65292;&#23427;&#26174;&#30528;&#22320;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#23646;&#24615;&#30340;&#22810;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMOGCN&#65289;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#33410;&#28857;&#36830;&#25509;&#20013;&#26500;&#24314;&#19981;&#21516;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#20043;&#21518;&#65292;&#20174;&#21508;&#31181;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#21160;&#34701;&#21512;&#20013;&#38468;&#21152;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#12290;&#36825;&#20010;&#36807;&#31243;&#30001;&#20174;&#33410;&#28857;&#21516;&#36136;&#24615;&#36890;&#36807;&#23646;&#24615;&#35780;&#20215;&#25552;&#21462;&#30340;&#33410;&#28857;&#35821;&#20041;&#20449;&#24687;&#30417;&#30563;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#19968;&#23618;&#31616;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#65288;CoT&#65289;&#23545;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#31639;&#26415;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#25552;&#31034;&#19981;&#20877;&#26377;&#25928;&#65292;&#20294;&#22312;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#26377;&#25928;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#35757;&#32451;&#25512;&#29702;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;/&#20559;&#24046;&#30340;&#39118;&#38505;&#65292;&#38656;&#35201;&#22312;&#26356;&#22810;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#35780;&#20272;&#21644;&#25913;&#36827;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03262</link><description>&lt;p&gt;
ChatGPT&#20309;&#26102;&#38656;&#35201;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
When do you need Chain-of-Thought Prompting for ChatGPT?. (arXiv:2304.03262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#65288;CoT&#65289;&#23545;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#31639;&#26415;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#25552;&#31034;&#19981;&#20877;&#26377;&#25928;&#65292;&#20294;&#22312;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#26377;&#25928;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#35757;&#32451;&#25512;&#29702;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;/&#20559;&#24046;&#30340;&#39118;&#38505;&#65292;&#38656;&#35201;&#22312;&#26356;&#22810;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#35780;&#20272;&#21644;&#25913;&#36827;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#65292;&#20363;&#22914;&#65292;&#22312;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#20013;&#28155;&#21152;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#65292;&#21487;&#20197;&#23558;GPT-3&#22312;MultiArith&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20174;17.7&#65285;&#25552;&#39640;&#21040;78.7&#65285;&#12290;&#28982;&#32780;&#65292;&#19981;&#28165;&#26970;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#26159;&#21542;&#23545;&#26356;&#36817;&#26399;&#30340;&#25351;&#20196;&#24494;&#35843;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#26377;&#25928;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;ChatGPT&#19978;&#65292;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#25512;&#29702;&#65289;&#19981;&#20877;&#26377;&#25928;&#65292;&#20294;&#23545;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#20173;&#28982;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#22312;&#21069;&#32773;&#30340;&#20219;&#21153;&#19978;&#65292;ChatGPT&#36890;&#24120;&#34920;&#29616;&#26368;&#20339;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#27809;&#26377;&#34987;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;ChatGPT&#21487;&#33021;&#24050;&#32463;&#36890;&#36807;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#19988;&#21363;&#20351;&#27809;&#26377;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#65292;&#20063;&#20250;&#22312;&#24212;&#29992;&#20110;&#30456;&#21516;&#30340;&#26597;&#35810;&#26102;&#38544;&#21547;&#22320;&#36981;&#24490;&#27492;&#31867;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21453;&#26144;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21463;&#35757;&#32451;&#25512;&#29702;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;/&#20559;&#24046;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#35780;&#20272;&#21644;&#25913;&#36827;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#40065;&#26834;&#24615;&#22312;&#26356;&#22810;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step reasoning from Large Language Models~(LLMs). For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7\% to 78.7\%. However, it is not clear whether CoT is still effective on more recent instruction finetuned (IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer effective for certain tasks such as arithmetic reasoning while still keeping effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT usually achieves the best performance and can generate CoT even without being instructed to do so. Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instruct
&lt;/p&gt;</description></item><item><title>BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.03144</link><description>&lt;p&gt;
BotTriNet: &#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#32479;&#19968;&#39640;&#25928;&#30340;&#23884;&#20837;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning. (arXiv:2304.03144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03144
&lt;/p&gt;
&lt;p&gt;
BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#21457;&#29616;&#26426;&#22120;&#20154;&#36134;&#25143;&#20197;&#38450;&#27490;&#23427;&#20204;&#20405;&#29359;&#21644;&#39578;&#25200;&#30495;&#23454;&#29992;&#25143;&#26159;&#19968;&#20010;&#25345;&#20037;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20316;BOTTRINET&#30340;&#32479;&#19968;&#23884;&#20837;&#24335;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#36134;&#25143;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;&#30340;&#20551;&#35774;&#26159;&#19978;&#19979;&#25991;&#33258;&#28982;&#22320;&#25581;&#31034;&#36134;&#25143;&#20010;&#24615;&#21644;&#20064;&#24815;&#12290;&#22914;&#26524;&#31995;&#32479;&#33021;&#22815;&#20351;&#29992;&#23884;&#20837;&#25216;&#26415;&#26377;&#25928;&#22320;&#25552;&#21462;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37027;&#20040;&#20869;&#23481;&#23601;&#26159;&#20016;&#23500;&#21644;&#26377;&#20215;&#20540;&#30340;&#12290;&#38500;&#20102;&#29983;&#25104;&#35789;&#12289;&#21477;&#21644;&#36134;&#25143;&#23884;&#20837;&#30340;&#19968;&#33324;&#23884;&#20837;&#24335;&#26694;&#26550;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#20803;&#32452;&#32593;&#32476;&#26469;&#35843;&#25972;&#21407;&#22987;&#23884;&#20837;&#65288;&#30001;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#29983;&#25104;&#65289;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#35780;&#20272;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#26426;&#22120;&#20154;&#36134;&#25143;&#31867;&#21035;&#21644;&#20116;&#20010;&#26426;&#22120;&#20154;&#26679;&#26412;&#38598;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20004;&#20010;&#20869;&#23481;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;98.34%&#21644;F1&#24471;&#20998;97.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
A persistently popular topic in online social networks is the rapid and accurate discovery of bot accounts to prevent their invasion and harassment of genuine users. We propose a unified embedding framework called BOTTRINET, which utilizes textual content posted by accounts for bot detection based on the assumption that contexts naturally reveal account personalities and habits. Content is abundant and valuable if the system efficiently extracts bot-related information using embedding techniques. Beyond the general embedding framework that generates word, sentence, and account embeddings, we design a triplet network to tune the raw embeddings (produced by traditional natural language processing techniques) for better classification performance. We evaluate detection accuracy and f1score on a real-world dataset CRESCI2017, comprising three bot account categories and five bot sample sets. Our system achieves the highest average accuracy of 98.34% and f1score of 97.99% on two content-inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01016</link><description>&lt;p&gt;
&#24555;&#36895;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#21033;&#29992;KALE&#36827;&#34892;&#21518;&#32622;KL&#23545;&#40784;&#30340;&#24322;&#24418;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#35757;&#32451; (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#23545;MSMARCO&#12289;&#33258;&#28982;&#38382;&#31572;&#12289;&#38382;&#31572;&#28216;&#25103;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#21069;&#21518;&#35757;&#32451;&#21387;&#32553;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#31995;&#32479;&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#24322;&#24418;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback Leibler Alignment of Embeddings (KALE)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35009;&#21098;&#21644;&#23545;&#40784;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;KALE&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21452;&#32534;&#30721;&#22120;&#35757;&#32451;&#21518;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#26597;&#35810;&#32534;&#30721;&#22120;&#36827;&#34892;&#21387;&#32553;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#12290;&#20351;&#29992;KALE&#21644;&#19981;&#23545;&#31216;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.00746</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS
&lt;/p&gt;
&lt;p&gt;
OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#25163;&#31295;&#22788;&#29702;&#38754;&#20020;&#26377;&#38480;&#30340;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#31867;&#21035;&#20986;&#29616;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292;&#36890;&#36807;&#20165;&#19968;&#20010;&#27880;&#37322;&#26679;&#26412;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26032;&#39062;&#23383;&#31526;&#12290;&#28789;&#24863;&#28304;&#33258;&#35748;&#30693;&#30740;&#31350;&#65292;&#24341;&#20837;&#31354;&#38388;&#23545;&#40784;&#27169;&#22359;&#65292;&#22522;&#20110;&#19968;&#20010;&#25903;&#25345;&#22270;&#20687;&#21457;&#29616;&#12289;&#20851;&#27880;&#21644;&#23398;&#20064;&#26597;&#35810;&#22270;&#20687;&#20013;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#23588;&#20854;&#26159;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#36317;&#31163;&#24230;&#37327;&#30340;&#23884;&#20837;&#31354;&#38388;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20855;&#26377;&#22788;&#29702;&#26032;&#39062;&#23383;&#31526;&#21644;&#31526;&#21495;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#65288;DBH&#65289;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical manuscript processing poses challenges like limited annotated training data and novel class emergence. To address this, we propose a novel One-shot learning-based Text Spotting (OTS) approach that accurately and reliably spots novel characters with just one annotated support sample. Drawing inspiration from cognitive research, we introduce a spatial alignment module that finds, focuses on, and learns the most discriminative spatial regions in the query image based on one support image. Especially, since the low-resource spotting task often faces the problem of example imbalance, we propose a novel loss function called torus loss which can make the embedding space of distance metric more discriminative. Our approach is highly efficient and requires only a few training samples while exhibiting the remarkable ability to handle novel characters, and symbols. To enhance dataset diversity, a new manuscript dataset that contains the ancient Dongba hieroglyphics (DBH) is created. We
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11020</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification. (arXiv:2303.11020v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#21452;&#27969;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(TDNN)&#26159;&#25991;&#26412;&#26080;&#20851;&#35828;&#35805;&#20154;&#39564;&#35777;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20256;&#32479;&#30340;TDNN&#26469;&#35828;&#65292;&#25429;&#25417;&#34987;&#35777;&#26126;&#23545;&#20110;&#40065;&#26834;&#35828;&#35805;&#20154;&#34920;&#31034;&#21644;&#38271;&#26102;&#38388;&#35828;&#35805;&#20154;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;(&#20363;&#22914;&#33258;&#25105;&#20851;&#27880;)&#23545;&#20110;&#36755;&#20837;&#20196;&#29260;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#24403;&#24212;&#29992;&#20110;TDNN&#20013;&#20855;&#26377;&#22823;&#23610;&#23544;&#29305;&#24449;&#26144;&#23556;&#26102;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#26080;&#27861;&#25215;&#21463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TDNN&#30340;&#20840;&#23616;&#28388;&#27874;&#22120;(GFTDNN)&#65292;&#23427;&#24212;&#29992;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;FFT/IFFT&#21644;&#19968;&#32452;&#21487;&#24494;&#39057;&#22495;&#28388;&#27874;&#22120;&#26469;&#39640;&#25928;&#22320;&#24314;&#27169;&#35821;&#38899;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#20026;&#22686;&#24378;&#20840;&#23616;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65292;&#29305;&#21035;&#35774;&#35745;&#20102;&#21160;&#24577;&#28388;&#27874;&#31574;&#30053;&#21644;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;TDNN(DS-TDNN)&#65292;&#23558;&#22522;&#26412;&#36890;&#36947;&#20998;&#25104;&#20004;&#20010;&#24182;&#34892;&#36890;&#36947;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#39057;&#22495;&#28388;&#27874;&#22120;&#35757;&#32451;&#27599;&#20010;&#36890;&#36947;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22823;&#35268;&#27169;&#35828;&#35805;&#20154;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26631;&#20934;TDNN&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The time-delay neural network (TDNN) is one of the state-of-the-art models for text-independent speaker verification. However, it is difficult for conventional TDNN to capture global context that has been proven critical for robust speaker representations and long-duration speaker verification in many recent works. Besides, the common solutions, e.g., self-attention, have quadratic complexity for input tokens, which makes them computationally unaffordable when applied to the feature maps with large sizes in TDNN. To address these issues, we propose the Global Filter for TDNN, which applies log-linear complexity FFT/IFFT and a set of differentiable frequency-domain filters to efficiently model the long-term dependencies in speech. Besides, a dynamic filtering strategy, and a sparse regularization method are specially designed to enhance the performance of the global filter and prevent it from overfitting. Furthermore, we construct a dual-stream TDNN (DS-TDNN), which splits the basic cha
&lt;/p&gt;</description></item><item><title>BotShape&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10214</link><description>&lt;p&gt;
BotShape&#65306;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns. (arXiv:2303.10214v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10214
&lt;/p&gt;
&lt;p&gt;
BotShape&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#34892;&#20026;&#27169;&#24335;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#20934;&#30830;&#26816;&#27979;&#26426;&#22120;&#20154;&#36134;&#25143;&#24182;&#20943;&#36731;&#20854;&#23545;&#30495;&#23454;&#29992;&#25143;&#30340;&#26377;&#23475;&#24433;&#21709;&#65288;&#22914;&#35823;&#23548;&#12289;&#35875;&#35328;&#21644;&#22403;&#22334;&#20449;&#24687;&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#21407;&#22987;&#20107;&#20214;&#26085;&#24535;&#26500;&#24314;&#20102;&#34892;&#20026;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#20102;&#20851;&#38190;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#36134;&#25143;&#19982;&#30495;&#23454;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#26426;&#22120;&#20154;&#36134;&#25143;&#20043;&#38388;&#30456;&#20284;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#31995;&#32479;BotShape&#65292;&#33258;&#21160;&#25429;&#25417;&#34892;&#20026;&#24207;&#21015;&#21644;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#20998;&#31867;&#22120;&#26816;&#27979;&#26426;&#22120;&#20154;&#36134;&#25143;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#20998;&#31867;&#22120;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;98.52&#65285;&#65292;&#24179;&#22343;f1&#20998;&#25968;&#20026;96.65&#65285;&#12290;&#19982;&#20854;&#20182;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;BotShape&#26159;&#19968;&#31181;&#26032;&#30340;&#36134;&#21495;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#37325;&#35201;&#30340;&#34892;&#20026;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
An essential topic in online social network security is how to accurately detect bot accounts and relieve their harmful impacts (e.g., misinformation, rumor, and spam) on genuine users. Based on a real-world data set, we construct behavioral sequences from raw event logs. After extracting critical characteristics from behavioral time series, we observe differences between bots and genuine users and similar patterns among bot accounts. We present a novel social bot detection system BotShape, to automatically catch behavioral sequences and characteristics as features for classifiers to detect bots. We evaluate the detection performance of our system in ground-truth instances, showing an average accuracy of 98.52% and an average f1-score of 96.65% on various types of classifiers. After comparing it with other research, we conclude that BotShape is a novel approach to profiling an account, which could improve performance for most methods by providing significant behavioral features.
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GenMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#30340;&#36890;&#29992;&#19977;&#32500;&#22810;&#29289;&#20307;&#25628;&#32034;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#35268;&#21010;&#36755;&#20986;&#19968;&#20010;&#20845;&#33258;&#30001;&#24230;&#35270;&#28857;&#65292;&#25104;&#21151;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03178</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#19977;&#32500;&#22810;&#29289;&#20307;&#25628;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A System for Generalized 3D Multi-Object Search. (arXiv:2303.03178v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GenMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#30340;&#36890;&#29992;&#19977;&#32500;&#22810;&#29289;&#20307;&#25628;&#32034;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#35268;&#21010;&#36755;&#20986;&#19968;&#20010;&#20845;&#33258;&#30001;&#24230;&#35270;&#28857;&#65292;&#25104;&#21151;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29289;&#20307;&#36827;&#34892;&#25628;&#32034;&#26159;&#26426;&#22120;&#20154;&#30340;&#19968;&#39033;&#22522;&#26412;&#25216;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#29289;&#20307;&#25628;&#32034;&#26368;&#32456;&#33021;&#22815;&#25104;&#20026;&#31867;&#20284;&#20110;&#29289;&#20307;&#26816;&#27979;&#21644;SLAM&#31561;&#26426;&#22120;&#20154;&#30340;&#21363;&#25554;&#21363;&#29992;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#27809;&#26377;&#19968;&#20010;3D&#29289;&#20307;&#25628;&#32034;&#31995;&#32479;&#33021;&#22815;&#36866;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#12290;&#22522;&#20110;&#26368;&#36817;&#21033;&#29992;&#20843;&#21449;&#26641;&#32467;&#26500;&#34920;&#31034;3D&#20449;&#24565;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GenMOS&#65288;&#36890;&#29992;&#22810;&#29289;&#20307;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#30340;&#19977;&#32500;&#22810;&#29289;&#20307;&#25628;&#32034;&#65288;MOS&#65289;&#36890;&#29992;&#31995;&#32479;&#12290;GenMOS&#20197;&#26412;&#22320;&#21306;&#22495;&#30340;&#28857;&#20113;&#35266;&#23519;&#12289;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#21644;&#26426;&#22120;&#20154;&#35270;&#35282;&#23450;&#20301;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#35268;&#21010;&#36755;&#20986;&#19968;&#20010;&#20845;&#33258;&#30001;&#24230;&#35270;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;GenMOS&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;&#19977;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27169;&#25311;&#36974;&#25377;&#65307;&#65288;2&#65289;&#20449;&#24687;&#21344;&#29992;&#24182;&#21021;&#22987;&#21270;&#20843;&#21449;&#26641;&#20449;&#24565;&#65307;&#65288;3&#65289;&#20026;&#27599;&#20010;&#29289;&#20307;&#37319;&#26679;&#20449;&#24565;&#30456;&#20851;&#30340;3D&#20301;&#23039;&#20808;&#39564;&#12290;&#35813;&#31995;&#32479;&#22312;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;GenMOS&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for objects is a fundamental skill for robots. As such, we expect object search to eventually become an off-the-shelf capability for robots, similar to e.g., object detection and SLAM. In contrast, however, no system for 3D object search exists that generalizes across real robots and environments. In this paper, building upon a recent theoretical framework that exploited the octree structure for representing belief in 3D, we present GenMOS (Generalized Multi-Object Search), the first general-purpose system for multi-object search (MOS) in a 3D region that is robot-independent and environment-agnostic. GenMOS takes as input point cloud observations of the local region, object detection results, and localization of the robot's view pose, and outputs a 6D viewpoint to move to through online planning. In particular, GenMOS uses point cloud observations in three ways: (1) to simulate occlusion; (2) to inform occupancy and initialize octree belief; and (3) to sample a belief-depend
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00735</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#19982;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#24377;&#24615;&#30340;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#38656;&#35201;&#23545;&#21608;&#22260;&#36947;&#36335;&#29992;&#25143;&#26410;&#26469;&#34892;&#20026;&#20570;&#20986;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20026;&#21709;&#24212;&#27492;&#38656;&#27714;&#21450;&#30456;&#20851;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;MTP-GO&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22330;&#26223;&#36827;&#34892;&#32534;&#30721;&#65292;&#29983;&#25104;&#24213;&#23618;&#36816;&#21160;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36816;&#21160;&#27169;&#22411;&#37319;&#29992;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#20854;&#20013;&#30340;&#29366;&#24577;&#36716;&#31227;&#20989;&#25968;&#23558;&#21644;&#20854;&#20182;&#37096;&#20998;&#19968;&#36215;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#33719;&#24471;&#22810;&#27169;&#24577;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25216;&#26415;&#21152;&#36895;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#20351;&#29992;&#29305;&#27530;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#24182;&#21487;&#20197;&#20351;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;22%&#12290;</title><link>http://arxiv.org/abs/2301.09262</link><description>&lt;p&gt;
AttMEMO: &#22312;&#22823;&#20869;&#23384;&#31995;&#32479;&#19978;&#21033;&#29992;&#35760;&#24518;&#21270;&#21152;&#36895;Transformers
&lt;/p&gt;
&lt;p&gt;
AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25216;&#26415;&#21152;&#36895;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#20351;&#29992;&#29305;&#27530;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#24182;&#21487;&#20197;&#20351;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22240;&#20854;&#20248;&#36234;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#21534;&#21520;&#37327;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Transformer&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#36739;&#38271;&#12290;&#29616;&#26377;&#30340;Transformer&#25512;&#29702;&#21152;&#36895;&#24037;&#20316;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#35201;&#20040;&#26159;&#30001;&#20110;&#20462;&#25913;Transformer&#26550;&#26500;&#65292;&#35201;&#20040;&#26159;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35760;&#24518;&#21270;&#21152;&#36895;Transformer&#27169;&#22411;&#30340;&#26426;&#20250;&#65292;&#32780;&#19981;&#28041;&#21450;&#20197;&#19978;&#38480;&#21046;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#29420;&#29305;&#35266;&#23519;&#65292;&#21363;&#22312;&#25512;&#29702;&#24207;&#21015;&#20869;Attention&#35745;&#31639;&#20013;&#23384;&#22312;&#20016;&#23500;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21033;&#29992;&#26032;&#20852;&#30340;&#22823;&#20869;&#23384;&#31995;&#32479;&#30340;&#35760;&#24518;&#21270;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#25216;&#26415;&#26469;&#26597;&#25214;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#35745;&#31639;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#22914;&#20869;&#23384;&#26144;&#23556;&#21644;&#36873;&#25321;&#24615;&#35760;&#24518;&#21270;&#65292;&#20197;&#36991;&#20813;&#20869;&#23384;&#22797;&#21046;&#21644;&#19981;&#24517;&#35201;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#20351;&#24471;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;&#20102;22%&#65292;&#32780;&#19988;&#20869;&#23384;&#38656;&#27714;&#36866;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#29992;&#20110;&#21508;&#31181;Transformer&#27169;&#22411;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#37325;&#35201;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.07016</link><description>&lt;p&gt;
&#24847;&#35782;&#26159;&#23398;&#20064;&#30340;&#36807;&#31243;&#65306;&#36890;&#36807;&#32465;&#23450;&#23398;&#20064;&#30340;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#23558;&#33258;&#24049;&#24863;&#30693;&#20026;&#26377;&#24847;&#35782;&#30340;
&lt;/p&gt;
&lt;p&gt;
Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29305;&#23450;&#22797;&#26434;&#39046;&#22495;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#39640;&#25928;&#22320;&#27867;&#21270;&#20173;&#28982;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22312;&#20154;&#31867;&#36523;&#19978;&#65292;&#36825;&#31181;&#23398;&#20064;&#36890;&#36807;&#22768;&#26126;&#24615;&#23384;&#20648;&#36807;&#31243;&#36827;&#34892;&#65292;&#24182;&#19988;&#19982;&#24847;&#35782;&#23494;&#20999;&#30456;&#20851;&#12290;&#39044;&#27979;&#22788;&#29702;&#34987;&#25512;&#24191;&#20026;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#26694;&#26550;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#30382;&#36136;&#22914;&#20309;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#24863;&#30693;&#27169;&#22411;&#65292;&#29992;&#20110;&#24863;&#23448;&#25968;&#25454;&#21644;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22788;&#29702;&#23545;&#20110;&#24555;&#36895;&#32452;&#25104;&#24335;&#23398;&#20064;&#25110;&#24847;&#35782;&#20043;&#35868;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#30452;&#25509;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#65292;&#36890;&#36807;&#36890;&#36807;&#32465;&#23450;&#39044;&#27979;&#20013;&#30340;&#23618;&#27425;&#27169;&#22411;&#26469;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20174;&#21333;&#20010;&#31034;&#20363;&#20013;&#20026;&#24863;&#30693;&#21644;&#34892;&#21160;&#24418;&#25104;&#24037;&#20316;&#35760;&#24518;&#65292;&#22312;&#26032;&#24773;&#20917;&#19979;&#28789;&#27963;&#27867;&#21270;&#65292;&#36825;&#21487;&#36890;&#36807;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#22768;&#26126;&#24615;&#35760;&#24518;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22312;&#32447;&#23618;&#32423;&#39044;&#27979;&#32465;&#23450;&#8221;&#65292;&#20063;&#21487;&#33021;&#26159;&#31995;&#32479;&#24863;&#30693;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#20851;&#20110;&#24863;&#30693;&#30340;&#12289;&#36816;&#21160;&#30340;&#12289;&#35748;&#30693;&#30340;&#21644;&#24773;&#24863;&#30340;&#24847;&#35782;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20855;&#26377;&#36827;&#21270;&#21644;&#21457;&#32946;&#29983;&#29289;&#23398;&#30340;&#28145;&#21051;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12050</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Semantic Framework for Neural-Symbolic Computing. (arXiv:2212.12050v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#31995;&#32479;&#65292;&#23545;&#20110;&#19968;&#31995;&#21015;AI&#38382;&#39064;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#22343;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#25152;&#38656;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#20204;&#35748;&#20026;&#36825;&#26159;&#27599;&#31181;&#26041;&#27861;&#20869;&#22312;&#24369;&#28857;&#25152;&#33268;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#20123;&#24369;&#28857;&#20284;&#20046;&#26159;&#20114;&#34917;&#30340;&#65292;&#31526;&#21495;&#31995;&#32479;&#25797;&#38271;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#30340;&#20107;&#29289;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#35797;&#22270;&#21033;&#29992;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#12290;&#36890;&#24120;&#36825;&#26159;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#27809;&#26377;&#20844;&#20849;&#30340;&#32534;&#30721;&#23450;&#20041;&#21487;&#20379;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#31526;&#21495;AI&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#35777;&#26126;&#23427;&#36275;&#20197;&#35299;&#37322;&#22823;&#37327;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#31526;&#21495;&#31995;&#32479;&#26893;&#26681;&#20110;&#20854;&#39046;&#22495;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#21508;&#31181;&#31526;&#21495;&#31995;&#32479;&#22312;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#30340;&#31070;&#32463;&#34920;&#24449;&#21644;&#20351;&#29992;&#22266;&#23450;&#31070;&#32463;&#34920;&#24449;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two approaches to AI, neural networks and symbolic systems, have been proven very successful for an array of AI problems. However, neither has been able to achieve the general reasoning ability required for human-like intelligence. It has been argued that this is due to inherent weaknesses in each approach. Luckily, these weaknesses appear to be complementary, with symbolic systems being adept at the kinds of things neural networks have trouble with and vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry by combining neural networks and symbolic AI into integrated systems. Often this has been done by encoding symbolic knowledge into neural networks. Unfortunately, although many different methods for this have been proposed, there is no common definition of an encoding to compare them. We seek to rectify this problem by introducing a semantic framework for neural-symbolic AI, which is then shown to be general enough to account for a large family of neural-symb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#20801;&#35768;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.06134</link><description>&lt;p&gt;
&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#22810;&#26679;&#21644;&#21487;&#34892;&#20219;&#21153;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks. (arXiv:2211.06134v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#20801;&#35768;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#26426;&#22120;&#20154;&#20855;&#22791;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#30340;&#25216;&#33021;&#24211;&#12290;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33719;&#24471;&#36825;&#31181;&#25216;&#33021;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#33719;&#21462;&#35206;&#30422;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#22797;&#26434;&#30340;&#25163;&#21160;&#21171;&#21160;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#25216;&#33021;&#12290; ATR&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#21021;&#22987;&#29615;&#22659;&#29366;&#24577;&#21644;&#25805;&#20316;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#32039;&#20945;&#20219;&#21153;&#34920;&#31034;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#22312;&#27169;&#25311;&#20013;&#31243;&#24207;&#29983;&#25104;&#25152;&#36873;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#35757;&#32451;&#20219;&#21153;&#30340;&#20027;&#21160;&#36873;&#25321;&#20351;&#24471;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#25216;&#33021;&#31574;&#30053;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#25110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving real-world manipulation tasks requires robots to have a repertoire of skills applicable to a wide range of circumstances. When using learning-based methods to acquire such skills, the key challenge is to obtain training data that covers diverse and feasible variations of the task, which often requires non-trivial manual labor and domain knowledge. In this work, we introduce Active Task Randomization (ATR), an approach that learns robust skills through the unsupervised generation of training tasks. ATR selects suitable tasks, which consist of an initial environment state and manipulation goal, for learning robust skills by balancing the diversity and feasibility of the tasks. We propose to predict task diversity and feasibility by jointly learning a compact task representation. The selected tasks are then procedurally generated in simulation using graph-based parameterization. The active selection of these training tasks enables skill policies trained with our framework to robus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#20998;&#26512;&#20102;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#20915;&#31574;&#30340;&#21407;&#29702;&#65292;&#21457;&#29616;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21033;&#29992;&#38750;&#22240;&#26524;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#65292;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#20250;&#21066;&#24369;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#24182;&#23548;&#33268;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.03046</link><description>&lt;p&gt;
&#30693;&#35782;&#23601;&#26159;&#21147;&#37327;&#65306;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#20351;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#26356;&#20855;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#20998;&#26512;&#20102;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#20915;&#31574;&#30340;&#21407;&#29702;&#65292;&#21457;&#29616;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21033;&#29992;&#38750;&#22240;&#26524;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#65292;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#20250;&#21066;&#24369;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#24182;&#23548;&#33268;&#27495;&#35270;&#38382;&#39064;&#65292;&#25552;&#20986;&#22522;&#20110;&#22240;&#26524;&#24178;&#39044;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#26159;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#35268;&#21017;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#21028;&#20915;&#30340;&#27861;&#24459;&#36741;&#21161;&#24037;&#20855;&#65292;&#26088;&#22312;&#32531;&#35299;&#26377;&#38480;&#27861;&#24459;&#20174;&#19994;&#20154;&#21592;&#30340;&#24040;&#22823;&#24037;&#20316;&#36127;&#25285;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#37319;&#29992;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;LJP&#20219;&#21153;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26681;&#25454;&#26080;&#20851;&#65288;&#25110;&#38750;&#22240;&#26524;&#65289;&#20449;&#24687;&#36827;&#34892;&#21028;&#20915;&#39044;&#27979;&#12290;&#36829;&#21453;&#27861;&#24459;&#35268;&#21017;&#19981;&#20165;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#36824;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#27495;&#35270;&#12290;&#26412;&#25991;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#27169;&#22411;&#65288;SCMs&#65289;&#29702;&#35770;&#22320;&#20998;&#26512;&#20102;LJP&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#20570;&#20986;&#20915;&#31574;&#20197;&#21450;&#20026;&#20160;&#20040;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#36890;&#36807;&#20256;&#32479;&#30340;&#27979;&#35797;&#33539;&#24335;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20004;&#31181;&#20998;&#21035;&#22522;&#20110;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#22240;&#26524;&#24178;&#39044;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact descriptions according to rule of law, serves as legal assistance to mitigate the great work burden of limited legal practitioners. Most existing methods apply various large-scale pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent improvements. However, we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination. In this paper, we use causal structural models (SCMs) to theoretically analyze how LJP models learn to make decisions and why they can succeed in passing the traditional testing paradigm without learning causality. According to our analysis, we provide two solutions intervening on data and model by causality, respectively. In detail, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23545;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#25552;&#20986;&#20102;FedTP&#65292;&#22312;&#23398;&#20064;&#23458;&#25143;&#31471;&#20010;&#24615;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.01572</link><description>&lt;p&gt;
FedTP: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;Transformer&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23545;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#25552;&#20986;&#20102;FedTP&#65292;&#22312;&#23398;&#20064;&#23458;&#25143;&#31471;&#20010;&#24615;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#20316;&#22320;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20811;&#26381;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#23558;Transformer&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#23384;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#65292;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23454;&#38469;&#19978;&#20250;&#23545;&#33258;&#27880;&#24847;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#24433;&#21709;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedTP&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#32780;&#19981;&#26159;&#20351;&#29992;&#32431;&#20010;&#24615;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26377;&#25439;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#30340;&#26377;&#25439;&#34920;&#31034;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20998;&#35299;&#21407;&#22987;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#27867;&#21270;&#36807;&#31243;&#20013;&#20887;&#20313;&#20869;&#23481;&#30340;&#24178;&#25200;&#65292;&#20197;&#27492;&#24212;&#23545;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.06601</link><description>&lt;p&gt;
&#25439;&#22833;&#29109;&#26426;&#20250;&#19979;&#30340;&#27867;&#21270;&#65306;&#21033;&#29992;&#24191;&#27867;&#30340;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#35270;&#35273;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks. (arXiv:2210.06601v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26377;&#25439;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#30340;&#26377;&#25439;&#34920;&#31034;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20998;&#35299;&#21407;&#22987;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#27867;&#21270;&#36807;&#31243;&#20013;&#20887;&#20313;&#20869;&#23481;&#30340;&#24178;&#25200;&#65292;&#20197;&#27492;&#24212;&#23545;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#25968;&#25454;&#38598;&#30340;&#21033;&#29992;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#27867;&#21270;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#19978;&#33719;&#21462;&#26410;&#35265;&#36807;&#30340;&#26102;&#38388;&#24310;&#36831;&#20219;&#21153;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#21516;&#26102;&#32467;&#21512;&#23398;&#20064;&#21040;&#30340;&#26377;&#25439;&#22833;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#12290;&#24403;&#38754;&#23545;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#26426;&#20250;&#27169;&#22411;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#26377;&#25439;&#34920;&#31034;&#20316;&#20026;&#23376;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26377;&#25439;&#34920;&#31034;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#65292;&#23427;&#20204;&#24378;&#35843;&#26377;&#20851;&#29366;&#24577;&#21644;&#30446;&#26631;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#25277;&#35937;&#20986;&#22952;&#30861;&#27867;&#21270;&#30340;&#20887;&#20313;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#20026;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25552;&#20379;&#23376;&#30446;&#26631;&#35268;&#21010;&#65292;&#20026;&#31574;&#30053;&#25552;&#20379;&#32039;&#20945;&#30340;&#36755;&#20837;&#65292;&#20063;&#20026;&#22870;&#21169;&#25552;&#20379;&#20102;&#36739;&#22909;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of broad datasets has proven to be crucial for generalization for a wide range of fields. However, how to effectively make use of diverse multi-task data for novel downstream tasks still remains a grand challenge in robotics. To tackle this challenge, we introduce a framework that acquires goal-conditioned policies for unseen temporally extended tasks via offline reinforcement learning on broad data, in combination with online fine-tuning guided by subgoals in learned lossy representation space. When faced with a novel task goal, the framework uses an affordance model to plan a sequence of lossy representations as subgoals that decomposes the original task into easier problems. Learned from the broad data, the lossy representation emphasizes task-relevant information about states and goals while abstracting away redundant contexts that hinder generalization. It thus enables subgoal planning for unseen tasks, provides a compact input to the policy, and facilitates reward
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2207.12647</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#22312;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#25429;&#25417;&#36328;&#27169;&#24577;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#32780;&#26410;&#33021;&#21457;&#29616;&#30495;&#27491;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#30495;&#23454;&#22320;&#22522;&#20110;&#20027;&#23548;&#35270;&#35273;&#35777;&#25454;&#21644;&#38382;&#39064;&#24847;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#36328;&#27169;&#24577;&#20107;&#20214;&#32423;&#29702;&#35299;&#65292;&#38656;&#35201;&#32852;&#21512;&#24314;&#27169;&#20107;&#20214;&#30340;&#26102;&#38388;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#65292;&#21363;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65292;&#32858;&#28966;&#20110;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#65292;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#26469;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#30340;&#26032;&#22411;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;&#20026;&#20102;&#21457;&#29616;&#36328;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#65288;CVLR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20849;&#21516;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.12934</link><description>&lt;p&gt;
&#22312;&#31867;&#22411;&#21270;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#20013;&#26816;&#39564;&#27010;&#29575;&#35745;&#31639;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Checking Trustworthiness of Probabilistic Computations in a Typed Natural Deduction System. (arXiv:2206.12934v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TPTND&#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#39564;&#24182;&#25512;&#23548;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#65292;&#20855;&#26377;&#21487;&#26816;&#26597;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; TPTND &#30340;&#27010;&#29575;&#31867;&#22411;&#33258;&#28982;&#28436;&#31639;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26088;&#22312;&#25512;&#23548;&#26377;&#20851;&#27010;&#29575;&#35745;&#31639;&#36807;&#31243;&#30340;&#21487;&#20449;&#24615;&#23646;&#24615;&#65292;&#20363;&#22914;&#24403;&#20170;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37027;&#20123;&#23646;&#24615;&#12290;TPTND &#20013;&#30340;&#25512;&#23548;&#34987;&#35299;&#37322;&#20026;&#20174;&#32473;&#23450;&#30340;&#20998;&#31867;&#20998;&#24067;&#20013;&#25552;&#21462; n &#20010;&#21487;&#33021;&#22797;&#26434;&#36755;&#20986;&#26679;&#26412;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36755;&#20986;&#26679;&#26412;&#30340;&#21487;&#20449;&#24615;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#20551;&#35774;&#27979;&#35797;&#65292;&#21363;&#35745;&#31639;&#20986;&#29616;&#26377;&#30340;&#39057;&#29575;&#19982;&#39044;&#26399;&#30340;&#27010;&#29575;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36825;&#20010;&#28436;&#31639;&#31995;&#32479;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#26816;&#26597;&#36825;&#31181;&#21487;&#20449;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#39033;&#25552;&#20379;&#20102;&#35745;&#31639;&#35821;&#20041;&#65292;&#24182;&#23450;&#20041;&#20102;&#36923;&#36753;&#36816;&#31639;&#31526;&#20197;&#21450;&#20449;&#20219;&#36816;&#31639;&#31526;&#30340;&#24341;&#20837;&#21644;&#28040;&#35299;&#35268;&#21017;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#20803;&#29702;&#35770;&#23646;&#24615;&#65292;&#23588;&#20854;&#26159;&#33021;&#22815;&#30830;&#23450;&#21738;&#20123;&#39033;&#28436;&#21270;&#21644;&#36923;&#36753;&#35268;&#21017;&#24212;&#29992;&#26102;&#65292;&#35745;&#31639;&#20173;&#28982;&#26159;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the probabilistic typed natural deduction calculus TPTND, designed to reason about and derive trustworthiness properties of probabilistic computational processes, like those underlying current AI applications. Derivability in TPTND is interpreted as the process of extracting $n$ samples of possibly complex outputs with a certain frequency from a given categorical distribution. We formalize trust for such outputs as a form of hypothesis testing on the distance between such frequency and the intended probability. The main advantage of the calculus is to render such notion of trustworthiness checkable. We present a computational semantics for the terms over which we reason and then the semantics of TPTND, where logical operators as well as a Trust operator are defined through introduction and elimination rules. We illustrate structural and metatheoretical properties, with particular focus on the ability to establish under which term evolutions and logical rules ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#23618;&#35268;&#21010;&#22120;&#21644;&#28508;&#31354;&#38388;&#20013;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#26469;&#20998;&#35299;&#30446;&#26631;&#25104;&#23376;&#30446;&#26631;&#65292;&#24182;&#22312;&#20197;&#21069;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#37319;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.08129</link><description>&lt;p&gt;
&#35745;&#21010;&#21040;&#23454;&#36341;&#65306;&#22312;&#28508;&#31354;&#38388;&#20013;&#32452;&#21512;&#30446;&#26631;&#30340;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#23618;&#35268;&#21010;&#22120;&#21644;&#28508;&#31354;&#38388;&#20013;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#26469;&#20998;&#35299;&#30446;&#26631;&#25104;&#23376;&#30446;&#26631;&#65292;&#24182;&#22312;&#20197;&#21069;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#37319;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#26469;&#23436;&#25104;&#29616;&#23454;&#19990;&#30028;&#20013;&#26080;&#32467;&#26500;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#33719;&#24471;&#21487;&#20197;&#22312;&#21629;&#20196;&#19979;&#36798;&#26102;&#21040;&#36798;&#21487;&#37197;&#32622;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#38750;&#24120;&#38590;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Planning to Practice&#65288;PTP&#65289;&#65292;&#19968;&#31181;&#23454;&#29616;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#22810;&#20010;&#19981;&#21516;&#31867;&#22411;&#20132;&#20114;&#25165;&#33021;&#35299;&#20915;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#23618;&#22320;&#20998;&#35299;&#20102;&#21040;&#36798;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#22312;&#20302;&#32423;&#26080;&#27169;&#22411;&#31574;&#30053;&#20013;&#30340;&#28508;&#31354;&#38388;&#20013;&#35774;&#32622;&#20013;&#38388;&#23376;&#30446;&#26631;&#30340;&#39640;&#32423;&#35745;&#21010;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PTP&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#24456;&#39640;&#25928;&#22320;&#37319;&#38598;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#65292;&#20174;&#32780;&#20026;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2204.13382</link><description>&lt;p&gt;
&#20943;&#23569;&#36164;&#28304;&#21463;&#38480;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval. (arXiv:2204.13382v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#65292;&#20174;&#32780;&#20026;&#23545;&#27604;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35757;&#32451;&#22270;&#20687;-&#23383;&#24149;&#26816;&#32034;&#65288;ICR&#65289;&#26041;&#27861;&#65292;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26159;&#20248;&#21270;&#20989;&#25968;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;ICR&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#30340;&#24433;&#21709;&#12290;&#39044;&#27979;&#29305;&#24449;&#26159;&#27491;&#30830;&#25351;&#31034;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#22810;&#20010;&#39044;&#27979;&#29305;&#24449;&#26102;&#65292;&#32534;&#30721;&#22120;&#27169;&#22411;&#24448;&#24448;&#20250;&#25233;&#21046;&#20887;&#20313;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#19981;&#38656;&#35201;&#23398;&#20064;&#21306;&#20998;&#27491;&#38754;&#21644;&#36127;&#38754;&#23545;&#12290;&#34429;&#28982;&#26377;&#20123;&#39044;&#27979;&#29305;&#24449;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#20887;&#20313;&#30340;&#65292;&#20294;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#21487;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#36164;&#28304;&#21463;&#38480;ICR&#26041;&#27861;&#20013;&#39044;&#27979;&#29305;&#24449;&#25233;&#21046;&#30340;&#26041;&#27861;&#65306;&#28508;&#22312;&#30446;&#26631;&#35299;&#30721;&#65288;LTD&#65289;&#12290;&#25105;&#20204;&#22312;&#23545;&#27604;ICR&#26694;&#26550;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#35299;&#30721;&#22120;&#65292;&#20197;&#22312;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#37325;&#24314;&#36755;&#20837;&#23383;&#24149;&#65292;&#20174;&#32780;&#38450;&#27490;&#22270;&#20687;&#21644;&#23383;&#24149;&#32534;&#30721;&#22120;&#22312;&#19981;&#21305;&#37197;&#30340;&#36127;&#38754;&#23545;&#20013;&#25233;&#21046;&#39044;&#27979;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;Flikr30k&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;LTD&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#20013;&#30340;&#22522;&#32447;&#26041;&#27861;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;MDP&#21644;&#23616;&#37096;&#36716;&#31227;&#26465;&#20214;&#19979;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#30340;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2204.04780</link><description>&lt;p&gt;
&#32422;&#26463;MDP&#21644;&#23616;&#37096;&#36716;&#31227;&#26465;&#20214;&#19979;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#30340;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Fully Polynomial Time Approximation Scheme for Constrained MDPs and Stochastic Shortest Path under Local Transitions. (arXiv:2204.04780v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;MDP&#21644;&#23616;&#37096;&#36716;&#31227;&#26465;&#20214;&#19979;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#30340;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#26102;&#38388;&#27573;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;C-MDP&#65289;&#26159;&#22312;&#25805;&#20316;&#38480;&#21046;&#19979;&#35268;&#21010;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#20117;&#30693;&#27169;&#22411;&#12290;&#27010;&#29575;&#32422;&#26463;MDP&#65288;CC-MDP&#65289;&#26159;&#19968;&#31181;&#21464;&#20307;&#65292;&#20801;&#35768;&#38480;&#21046;&#36829;&#35268;&#27010;&#29575;&#65292;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#26159;&#24517;&#38656;&#30340;&#12290;CC-MDP&#20063;&#21487;&#20197;&#23558;&#19968;&#31867;MDP&#24314;&#27169;&#20026;&#24102;&#32456;&#28857;&#30340;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#65288;SSP&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#30528;&#19968;&#31181;&#22312;&#30446;&#26631;&#27010;&#29575;&#21644;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#65288;C&#65289;C-MDP&#30340;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#23616;&#37096;&#36716;&#31227;&#30340;&#37325;&#35201;&#21464;&#20307;&#12290;&#22312;&#36825;&#31181;&#21464;&#20307;&#20013;&#65292;&#29366;&#24577;&#21487;&#36798;&#24615;&#34920;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#23616;&#37096;&#24615;&#21644;&#29420;&#31435;&#24615;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#32473;&#23450;&#26102;&#38388;&#28857;&#19978;&#65292;&#20849;&#20139;&#26576;&#20123;&#21487;&#36798;&#26410;&#26469;&#29366;&#24577;&#30340;&#29366;&#24577;&#25968;&#37327;&#22987;&#32456;&#20445;&#25345;&#19981;&#21464;&#12290;&#21363;&#20351;&#23545;&#20110;&#35268;&#21010;&#26102;&#38388;&#20026;2&#30340;&#24773;&#20917;&#65292;&#23616;&#37096;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#65288;C&#65289;C-MDP&#20173;&#26159;NP&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;C&#65289;C-MDP&#30340;&#19968;&#20010;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fixed-horizon constrained Markov Decision Process (C-MDP) is a well-known model for planning in stochastic environments under operating constraints. Chance-Constrained MDP (CC-MDP) is a variant that allows bounding the probability of constraint violation, which is desired in many safety-critical applications. CC-MDP can also model a class of MDPs, called Stochastic Shortest Path (SSP), under dead-ends, where there is a trade-off between the probability-to-goal and cost-to-goal. This work studies the structure of (C)C-MDP, particularly an important variant that involves local transition. In this variant, the state reachability exhibits a certain degree of locality and independence from the remaining states. More precisely, the number of states, at a given time, that share some reachable future states is always constant. (C)C-MDP under local transition is NP-Hard even for a planning horizon of two. In this work, we propose a fully polynomial-time approximation scheme for (C)C-MDP tha
&lt;/p&gt;</description></item><item><title>&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.08063</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#30693;&#35782;&#25277;&#21462;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08063
&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#25277;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#36890;&#24120;&#36973;&#21463;&#25968;&#25454;&#21294;&#20047;&#21644;&#20986;&#29616;&#26410;&#35265;&#31867;&#22411;&#65288;&#20302;&#36164;&#28304;&#24773;&#22659;&#65289;&#30340;&#22256;&#25200;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#24191;&#27867;&#30740;&#31350;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;KE&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#24037;&#20316;&#31995;&#32479;&#24615;&#22320;&#20998;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39640;&#36164;&#28304;&#25968;&#25454;&#65292;&#65288;2&#65289;&#21033;&#29992;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#30740;&#21487;&#20197;&#24110;&#21161;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#24847;&#65292;&#25552;&#21319;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#24565;&#38388;&#30340;&#20851;&#31995;&#26500;&#24314;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#37322;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2201.07798</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#27010;&#24565;&#30340;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35748;&#30693;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts. (arXiv:2201.07798v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#24565;&#38388;&#30340;&#20851;&#31995;&#26500;&#24314;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#37322;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20108;&#32500;&#23381;&#26399;&#26816;&#26597;&#20013;&#65292;&#32974;&#20799;&#26631;&#20934;&#25195;&#25551;&#24179;&#38754;&#30340;&#26816;&#27979;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#22810;&#24180;&#30340;&#22521;&#35757;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21487;&#20197;&#21327;&#21161;&#32463;&#39564;&#19981;&#36275;&#30340;&#21307;&#29983;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#20020;&#24202;&#21307;&#29983;&#30340;&#35748;&#30693;&#35282;&#24230;&#25552;&#20379;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27010;&#24565;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(GCN)&#26500;&#24314;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#19968;&#20010;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal standard scan plane detection during 2-D mid-pregnancy examinations is a highly complex task, which requires extensive medical knowledge and years of training. Although deep neural networks (DNN) can assist inexperienced operators in these tasks, their lack of transparency and interpretability limit their application. Despite some researchers have been committed to visualizing the decision process of DNN, most of them only focus on the pixel-level features and do not take into account the medical prior knowledge. In this work, we propose an interpretable framework based on key medical concepts, which provides explanations from the perspective of clinicians' cognition. Moreover, we utilize a concept-based graph convolutional neural(GCN) network to construct the relationships between key medical concepts. Extensive experimental analysis on a private dataset has shown that the proposed method provides easy-to-understand insights about reasoning results for clinicians.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;DQN&#21644;Rainbow&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;Atari&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#22312;&#32447;&#32593;&#32476;&#21644;&#30446;&#26631;&#32593;&#32476;&#20043;&#38388;&#24341;&#20837;&#19968;&#23450;&#30340;&#25509;&#36817;&#24230;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.05848</link><description>&lt;p&gt;
&#20351;&#29992;&#36739;&#24930;&#30340;&#22312;&#32447;&#32593;&#32476;&#23454;&#29616;&#26356;&#24555;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;DQN&#21644;Rainbow&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;Atari&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#22312;&#32447;&#32593;&#32476;&#21644;&#30446;&#26631;&#32593;&#32476;&#20043;&#38388;&#24341;&#20837;&#19968;&#23450;&#30340;&#25509;&#36817;&#24230;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#20004;&#20010;&#32593;&#32476;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#20248;&#21270;&#65306;&#19968;&#20010;&#22312;&#32447;&#32593;&#32476;&#21644;&#19968;&#20010;&#30446;&#26631;&#32593;&#32476;&#65292;&#21518;&#32773;&#24102;&#26377;&#19968;&#23450;&#30340;&#24310;&#36831;&#12290;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#23545;&#25239;&#21551;&#21457;&#24335;&#24341;&#23548;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#21363;DQN&#21644;Rainbow&#65289;&#20013;&#24341;&#20837;&#20102;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#28608;&#21169;&#22312;&#32447;&#32593;&#32476;&#20445;&#25345;&#19982;&#30446;&#26631;&#32593;&#32476;&#30340;&#25509;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#23384;&#22312;&#22122;&#22768;&#26356;&#26032;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20195;&#29702;&#31216;&#20026;DQN Pro&#21644;Rainbow Pro&#65292;&#23427;&#20204;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#23545;&#20110;&#21407;&#22987;&#31639;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31616;&#21333;&#24605;&#24819;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20195;&#30721;&#21487;&#22312;Github.com/amazon-research/fast-rl-with-slow-updates &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26059;&#36716;&#20108;&#20540;&#21270;&#22823;&#22411;ResNet&#65288;RBLResNet&#65289;&#65292;&#21487;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#12290;&#36890;&#36807;&#22810;&#32423;&#20998;&#31867;&#21644;&#20445;&#30041;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#21151;&#29575;&#30340;RBLResNet&#25171;&#21253;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#36798;&#21040;&#33267;&#39640;94.49%&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#20108;&#20803;&#21644;&#23454;&#20540;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2110.14357</link><description>&lt;p&gt;
&#20108;&#20540;&#21270; ResNet&#65306;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#23454;&#29616;&#40065;&#26834;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge. (arXiv:2110.14357v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26059;&#36716;&#20108;&#20540;&#21270;&#22823;&#22411;ResNet&#65288;RBLResNet&#65289;&#65292;&#21487;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#12290;&#36890;&#36807;&#22810;&#32423;&#20998;&#31867;&#21644;&#20445;&#30041;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#21151;&#29575;&#30340;RBLResNet&#25171;&#21253;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#36798;&#21040;&#33267;&#39640;94.49%&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#20108;&#20803;&#21644;&#23454;&#20540;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#65288;AMC&#65289;&#65292;&#24182;&#19988;&#32467;&#26524;&#38750;&#24120;&#26377;&#21069;&#36884;&#12290;&#28982;&#32780;&#65292;DNN&#20855;&#26377;&#39640;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#32593;&#32476;&#19981;&#23454;&#29992;&#12290;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25163;&#30340;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26059;&#36716;&#20108;&#20540;&#21270;&#22823;&#22411;ResNet&#65288;RBLResNet&#65289;&#29992;&#20110;AMC&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#32593;&#32476;&#19978;&#37096;&#32626;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#20004;&#31181;&#25552;&#20986;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#65288;i&#65289;&#22810;&#32423;&#20998;&#31867;&#65288;MC&#65289;&#65292;&#21644;&#65288;ii&#65289;&#20445;&#30041;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#21151;&#29575;&#30340;&#22810;&#20010;RBLResNet&#25171;&#21253;&#65292;&#21487;&#20197;&#32553;&#23567;RBLResNet&#21644;&#29616;&#26377;&#20351;&#29992;&#28014;&#28857;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32467;&#26500;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;MC&#26041;&#27861;&#22312;Deepsig&#25968;&#25454;&#38598;&#30340;24&#31181;&#35843;&#21046;&#31867;&#21035;&#30340;&#20840;&#37096;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;93.39&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#20108;&#20803;&#26435;&#37325;&#21644;&#28608;&#27963;&#12290;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#26041;&#27861;&#36827;&#19968;&#27493;&#23558;&#24615;&#33021;&#25552;&#39640;&#21040;10 dB&#26102;94.49%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#20108;&#20803;&#21644;&#23454;&#20540;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep neural networks (DNNs) have been used extensively for automatic modulation classification (AMC), and the results have been quite promising. However, DNNs have high memory and computation requirements making them impractical for edge networks where the devices are resource-constrained. They are also vulnerable to adversarial attacks, which is a significant security concern. This work proposes a rotated binary large ResNet (RBLResNet) for AMC that can be deployed at the edge network because of low memory and computational complexity. The performance gap between the RBLResNet and existing architectures with floating-point weights and activations can be closed by two proposed ensemble methods: (i) multilevel classification (MC), and (ii) bagging multiple RBLResNets while retaining low memory and computational power. The MC method achieves an accuracy of $93.39\%$ at $10$dB over all the $24$ modulation classes of the Deepsig dataset. This performance is comparable to state-of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2107.08574</link><description>&lt;p&gt;
&#19968;&#31181;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#30340;&#35843;&#21046;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32570;&#22833;&#21644;&#36136;&#37327;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#24320;&#21457;&#32773;&#36890;&#24120;&#21482;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#31934;&#24515;&#31579;&#36873;&#20986;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20250;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#12290;&#36825;&#21463;&#21551;&#21457;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#35843;&#21046;&#65292;&#30382;&#36136;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#30340;&#21487;&#38752;&#24615;&#21644;&#20854;&#20182;&#25968;&#25454;&#30340;&#23384;&#22312;&#31243;&#24230;&#19978;&#19979;&#35843;&#33410;&#36755;&#20837;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21487;&#38752;&#24615;&#24471;&#20998;&#20316;&#20026;&#35843;&#21046;&#20449;&#21495;&#65292;&#21457;&#29616;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#65288;&#21253;&#25324;&#39069;&#22806;&#30340;&#32570;&#22833;&#25968;&#25454;&#65289;&#26356;&#21152;&#40065;&#26834;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#25554;&#34917;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23436;&#20840;&#36339;&#36807;&#25554;&#34917;&#36807;&#31243;&#33410;&#30465;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#37327;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#26032;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#27425;&#25968;&#22823;&#22823;&#20943;&#23569;&#65292;&#20294;&#20173;&#20445;&#25345;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2106.07203</link><description>&lt;p&gt;
&#22522;&#20110;&#26222;&#36866;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#23376;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#37327;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#26032;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#27425;&#25968;&#22823;&#22823;&#20943;&#23569;&#65292;&#20294;&#20173;&#20445;&#25345;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26222;&#36866;&#20989;&#25968;&#36924;&#36817;&#65288;FA&#65289;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#29702;&#35299;&#32479;&#35745;&#22797;&#26434;&#24615;&#25110;&#36951;&#25022;&#36793;&#30028;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36828;&#26410;&#24471;&#21040;&#29702;&#35299;&#8212;&#8212;&#20107;&#23454;&#19978;&#65292;&#20989;&#25968;&#31867;&#19978;&#30340;&#31616;&#21333;&#20248;&#21270;&#38382;&#39064;&#21487;&#33021;&#21516;&#26679;&#38590;&#20197;&#22788;&#29702;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#27979;&#37327;RL&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#20351;&#29992;&#35813;&#27979;&#37327;&#25351;&#23548;&#25506;&#32034;&#12290;&#23545;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#21644;&#22797;&#26434;&#24230;&#26377;&#30028;&#30340;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#21482;&#38656;&#35201;&#26356;&#26032;$\propto\operatorname{poly}\log(K)$ &#27425;&#65292;&#23601;&#21487;&#20197;&#36816;&#34892; $K$ &#27425;RL&#31639;&#27861;&#32780;&#20173;&#28982;&#23454;&#29616;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#26356;&#26032;&#31574;&#30053;&#33267;&#23569;&#35201; $\Omega(K)$ &#27425;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20248;&#21270;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\propto\operatorname{poly}\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HINT&#65292;&#26088;&#22312;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#20026;&#20102;&#26816;&#39564;&#27169;&#22411;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.01403</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31995;&#32479;&#21270;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;&#26497;&#31616;&#20027;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics. (arXiv:2103.01403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HINT&#65292;&#26088;&#22312;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#20026;&#20102;&#26816;&#39564;&#27169;&#22411;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#25484;&#25569;&#31639;&#26415;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#30340;&#20363;&#22806;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Handwritten arithmetic with INTegers&#65288;HINT&#65289;&#26469;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#22312;HINT&#20013;&#65292;&#26426;&#22120;&#34987;&#36171;&#20104;&#23398;&#20064;&#22914;&#20309;&#20174;&#21407;&#22987;&#20449;&#21495;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#24863;&#30693;&#27010;&#24565;&#65288;&#21363;&#24863;&#30693;&#65289;&#65292;&#22914;&#20309;&#23558;&#22810;&#20010;&#27010;&#24565;&#32467;&#26500;&#21270;&#32452;&#21512;&#20197;&#24418;&#25104;&#26377;&#25928;&#34920;&#36798;&#24335;&#65288;&#21363;&#35821;&#27861;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#27010;&#24565;&#20197;&#25903;&#25345;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#35821;&#20041;&#65289;&#65292;&#20840;&#37096;&#22312;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31995;&#32479;&#21270;&#36890;&#29992;&#33021;&#21147;&#65292;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#20851;&#20110;&#19977;&#23618;&#32423;&#21035;&#30340;&#23398;&#20064;&#27010;&#24565;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#20998;&#21106;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#27010;&#24565;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20102;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36824;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;HINT&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we
&lt;/p&gt;</description></item></channel></rss>